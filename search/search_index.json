{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to Ganesh Documents # Overview # This the documets where all the Cloud, DevOps, DevSecOps, Security, Compliance, Micro service related information is prepared by Ganesh kinkar giri. Below are the document present. # Cloud DevOps DevSecOps Security Compliance Micro service","title":"Welcome to Ganesh Documents"},{"location":"index.html#welcome-to-ganesh-documents","text":"","title":"Welcome to Ganesh Documents"},{"location":"index.html#overview","text":"This the documets where all the Cloud, DevOps, DevSecOps, Security, Compliance, Micro service related information is prepared by Ganesh kinkar giri.","title":"Overview"},{"location":"index.html#below-are-the-document-present","text":"Cloud DevOps DevSecOps Security Compliance Micro service","title":"Below are the document present."},{"location":"Documentsdetails.html","text":"","title":"Documents details"},{"location":"AIML/aiml-overview.html","text":"What is Machine Learning # In the real world, we are surrounded by humans who can learn everything from their experiences with their learning capability, and we have computers or machines which work on our instructions. But can a machine also learn from experiences or past data like a human does? So here comes the role of Machine Learning. Introduction to Machine Learning # A subset of artificial intelligence known as machine learning focuses primarily on the creation of algorithms that enable a computer to independently learn from data and previous experiences. Machine learning algorithms create a mathematical model that, without being explicitly programmed, aids in making predictions or decisions with the assistance of sample historical data, or training data. For the purpose of developing predictive models, machine learning brings together statistics and computer science. Classification of Machine Learning # At a broad level, machine learning can be classified into three types: # Supervised learning # Unsupervised learning # Reinforcement learning #","title":"Overview"},{"location":"AIML/aiml-overview.html#what-is-machine-learning","text":"In the real world, we are surrounded by humans who can learn everything from their experiences with their learning capability, and we have computers or machines which work on our instructions. But can a machine also learn from experiences or past data like a human does? So here comes the role of Machine Learning.","title":"What is Machine Learning"},{"location":"AIML/aiml-overview.html#introduction-to-machine-learning","text":"A subset of artificial intelligence known as machine learning focuses primarily on the creation of algorithms that enable a computer to independently learn from data and previous experiences. Machine learning algorithms create a mathematical model that, without being explicitly programmed, aids in making predictions or decisions with the assistance of sample historical data, or training data. For the purpose of developing predictive models, machine learning brings together statistics and computer science.","title":"Introduction to Machine Learning"},{"location":"AIML/aiml-overview.html#classification-of-machine-learning","text":"","title":"Classification of Machine Learning"},{"location":"AIML/aiml-overview.html#at-a-broad-level-machine-learning-can-be-classified-into-three-types","text":"","title":"At a broad level, machine learning can be classified into three types:"},{"location":"AIML/aiml-overview.html#supervised-learning","text":"","title":"Supervised learning"},{"location":"AIML/aiml-overview.html#unsupervised-learning","text":"","title":"Unsupervised learning"},{"location":"AIML/aiml-overview.html#reinforcement-learning","text":"","title":"Reinforcement learning"},{"location":"AIML/DeepLearning/Autoencoder.html","text":"","title":"Autoencoder"},{"location":"AIML/DeepLearning/BERT.html","text":"","title":"BERT"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html","text":"Deep Learning Algorithms # What is Deep Learning Algorithm? Deep learning can be defined as the method of machine learning and artificial intelligence that is intended to intimidate humans and their actions based on certain human brain functions to make effective decisions. It is a very important data science element that channels its modeling based on data-driven techniques under predictive modeling and statistics . To drive such a human-like ability to adapt and learn and to function accordingly, there have to be some strong forces which we popularly called algorithms . Deep learning algorithms are dynamically made to run through several layers of neural networks, which are nothing but a set of decision-making networks that are pre-trained to serve a task. Later, each of these is passed through simple layered representations and move on to the next layer. However, most machine learning is trained to work fairly well on datasets that have to deal with hundreds of features or columns. For a data set to be structured or unstructured, machine learning tends to fail mostly because they fail to recognize a simple image having a dimension of 800x1000 in RGB. It becomes quite unfeasible for a traditional machine learning algorithm to handle such depths. This is where deep learning . Importance of Deep Learning # Deep learning algorithms play a crucial role in determining the features and can handle the large number of processes for the data that might be structured or unstructured. Although, deep learning algorithms can overkill some tasks that might involve complex problems because they need access to huge amounts of data so that they can function effectively. For example, there's a popular deep learning tool that recognizes images namely Imagenet that has access to 14 million images in its dataset-driven algorithms. It is a highly comprehensive tool that has defined a next-level benchmark for deep learning tools that aim images as their dataset. Deep learning algorithms are highly progressive algorithms that learn about the image by passing it through each neural network layer. The layers are highly sensitive to detect low-level features of the image like edges and pixels and henceforth the combined layers take this information and form holistic representations by comparing it with previous data. For example, the middle layer might be programmed to detect some special parts of the object in the photograph which other deep trained layers are programmed to detect special objects like dogs, trees, utensils, etc. However, if we talk out the simple task that involves less complexity and a data-driven resource, deep learning algorithms fail to generalize simple data. This is one of the main reasons deep learning is not considered effective as linear or boosted tree models. Simple models aim to churn out custom data, track fraudulent transactions and deal with less complex datasets with fewer features. Also, there are various cases like multiclass classification where deep learning can be effective because it involves smaller but more structured datasets but is not preferred usually. Deep Learning Algorithms # 1. Convolutional Neural Networks (CNNs) # CNN's popularly known as ConvNets majorly consists of several layers and are specifically used for image processing and detection of objects. CNNs have wide usage in identifying the image of the satellites, medical image processing, series forecasting, and anomaly detection. CNNs process the data by passing it through multiple layers and extracting features to exhibit convolutional operations. The Convolutional Layer consists of Rectified Linear Unit (ReLU) that outlasts to rectify the feature map. The Pooling layer is used to rectify these feature maps into the next feed. Pooling is generally a sampling algorithm that is down-sampled and it reduces the dimensions of the feature map. Later, the result generated consists of 2-D arrays consisting of single, long, continuous , and linear vector flattened in the map. The next layer i.e., called Fully Connected Layer which forms the flattened matrix or 2-D array fetched from the Pooling Layer as input and identifies the image by classifying it. 2. Long Short Term Memory Networks (LSTMs) # LSTMs can be defined as Recurrent Neural Networks (RNN) that are programmed to learn and adapt for dependencies for the long term. It can memorize and recall past data for a greater period and by default, it is its sole behavior. LSTMs are designed to retain over time and henceforth they are majorly used in time series predictions because they can restrain memory or previous inputs. This analogy comes from their chain-like structure consisting of four interacting layers that communicate with each other differently. Besides applications of time series prediction, they can be used to construct speech recognizers, development in pharmaceuticals, and composition of music loops as well. 3. Recurrent Neural Networks (RNNs) # Recurrent Neural Networks or RNNs consist of some directed connections that form a cycle that allow the input provided from the LSTMs to be used as input in the current phase of RNNs. These inputs are deeply embedded as inputs and enforce the memorization ability of LSTMs lets these inputs get absorbed for a period in the internal memory. RNNs are therefore dependent on the inputs that are preserved by LSTMs and work under the synchronization phenomenon of LSTMs. RNNs are mostly used in captioning the image, time series analysis, recognizing handwritten data, and translating data to machines. RNNs follow the work approach by putting output feeds (t-1) time if the time is defined as t. Next, the output determined by t is feed at input time t+1. Similarly, these processes are repeated for all the input consisting of any length. There's also a fact about RNNs is that they store historical information and there's no increase in the input size even if the model size is increased. RNNs look something like this when unfolded. 4. Generative Adversarial Networks (GANs) # GANs are defined as deep learning algorithms that are used to generate new instances of data that match the training data. GAN usually consists of two components namely a generator that learns to generate false data and a discriminator that adapts itself by learning from this false data. Over some time, GANs have gained immense usage since they are frequently being used to clarify astronomical images and simulate lensing the gravitational dark matter. It is also used in video games to increase graphics for 2D textures by recreating them in higher resolution like 4K . They are also used in creating realistic cartoons character and also rendering human faces and 3D object rendering . GANs work in simulation by generating and understanding the fake data and the real data. During the training to understand these data, the generator produces different kinds of fake data where the discriminator quickly learns to adapt and respond to it as false data. GANs then send these recognized results for updating. Consider the below image to visualize the functioning. 5. Radial Basis Function Networks (RBFNs) # RBFNs are specific types of neural networks that follow a feed-forward approach and make use of radial functions as activation functions. They consist of three layers namely the input layer, hidden layer, and output layer which are mostly used for time-series prediction, regression testing, and classification . RBFNs do these tasks by measuring the similarities present in the training data set. They usually have an input vector that feeds these data into the input layer thereby confirming the identification and rolling out results by comparing previous data sets. Precisely, the input layer has neurons that are sensitive to these data and the nodes in the layer are efficient in classifying the class of data. Neurons are originally present in the hidden layer though they work in close integration with the input layer. The hidden layer contains Gaussian transfer functions that are inversely proportional to the distance of the output from the neuron's center. The output layer has linear combinations of the radial-based data where the Gaussian functions are passed in the neuron as parameter and output is generated. Consiider the given image below to understand the process thoroughly. 6. Multilayer Perceptrons (MLPs) # MLPs are the base of deep learning technology. It belongs to a class of feed-forward neural networks having various layers of perceptrons . These perceptrons have various activation functions in them. MLPs also have connected input and output layers and their number is the same. Also, there's a layer that remains hidden amidst these two layers. MLPs are mostly used to build image and speech recognition systems or some other types of the translation software . The working of MLPs starts by feeding the data in the input layer. The neurons present in the layer form a graph to establish a connection that passes in one direction. The weight of this input data is found to exist between the hidden layer and the input layer. MLPs use activation functions to determine which nodes are ready to fire. These activation functions include tanh function, sigmoid and ReLUs . MLPs are mainly used to train the models to understand what kind of co-relation the layers are serving to achieve the desired output from the given data set. 7. Self Organizing Maps (SOMs) # SOMs were invented by Teuvo Kohenen for achieving data visualization to understand the dimensions of data through artificial and self-organizing neural networks. The attempts to achieve data visualization to solve problems are mainly done by what humans cannot visualize. These data are generally high-dimensional so there are lesser chances of human involvement and of course less error. SOMs help in visualizing the data by initializing weights of different nodes and then choose random vectors from the given training data. They examine each node to find the relative weights so that dependencies can be understood. The winning node is decided and that is called Best Matching Unit (BMU). Later, SOMs discover these winning nodes but the nodes reduce over time from the sample vector. So, the closer the node to BMU more is the more chance to recognize the weight and carry out further activities. There are also multiple iterations done to ensure that no node closer to BMU is missed. One example of such is the RGB color combinations that we use in our daily tasks. Consider the below image to understand how they function. 8. Deep Belief Networks (DBNs) # DBNs are called generative models because they have various layers of latent as well as stochastic variables. The latent variable is called a hidden unit because they have binary values. DBNs are also called Boltzmann Machines because the RGM layers are stacked over each other to establish communication with previous and consecutive layers. DBNs are used in applications like video and image recognition as well as capturing motional objects. DBNs are powered by Greedy algorithms . The layer to layer approach by leaning through a top-down approach to generate weights is the most common way DBNs function. DBNs use step by step approach of Gibbs sampling on the hidden two-layer at the top. Then, these stages draw a sample from the visible units using a model that follows the ancestral sampling method. DBNs learn from the values present in the latent value from every layer following the bottom-up pass approach. 9. Restricted Boltzmann Machines (RBMs) # RBMs were developed by Geoffrey Hinton and resemble stochastic neural networks that learn from the probability distribution in the given input set. This algorithm is mainly used in the field of dimension reduction, regression and classification, topic modeling and are considered the building blocks of DBNs. RBIs consist of two layers namely the visible layer and the hidden layer . Both of these layers are connected through hidden units and have bias units connected to nodes that generate the output. Usually, RBMs have two phases namely forward pass and backward pass . The functioning of RBMs is carried out by accepting inputs and translating them to numbers so that inputs are encoded in the forward pass. RBMs take into account the weight of every input, and the backward pass takes these input weights and translates them further into reconstructed inputs. Later, both of these translated inputs, along with individual weights, are combined. These inputs are then pushed to the visible layer where the activation is carried out, and output is generated that can be easily reconstructed. To understand this process, consider the below image. Autoencoders # Autoencoders are a special type of neural network where inputs are outputs are found usually identical. It was designed to primarily solve the problems related to unsupervised learning. Autoencoders are highly trained neural networks that replicate the data. It is the reason why the input and output are generally the same. They are used to achieve tasks like pharma discovery, image processing, and population prediction . Autoencoders constitute three components namely the encoder, the code, and the decoder . Autoencoders are built in such a structure that they can receive inputs and transform them into various representations. The attempts to copy the original input by reconstructing them is more accurate. They do this by encoding the image or input, reduce the size. If the image is not visible properly they are passed to the neural network for clarification. Then, the clarified image is termed a reconstructed image and this resembles as accurate as of the previous image. To understand this complex process, see the below-provided image.","title":"Deep Learning Algorithms"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#deep-learning-algorithms","text":"What is Deep Learning Algorithm? Deep learning can be defined as the method of machine learning and artificial intelligence that is intended to intimidate humans and their actions based on certain human brain functions to make effective decisions. It is a very important data science element that channels its modeling based on data-driven techniques under predictive modeling and statistics . To drive such a human-like ability to adapt and learn and to function accordingly, there have to be some strong forces which we popularly called algorithms . Deep learning algorithms are dynamically made to run through several layers of neural networks, which are nothing but a set of decision-making networks that are pre-trained to serve a task. Later, each of these is passed through simple layered representations and move on to the next layer. However, most machine learning is trained to work fairly well on datasets that have to deal with hundreds of features or columns. For a data set to be structured or unstructured, machine learning tends to fail mostly because they fail to recognize a simple image having a dimension of 800x1000 in RGB. It becomes quite unfeasible for a traditional machine learning algorithm to handle such depths. This is where deep learning .","title":"Deep Learning Algorithms"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#importance-of-deep-learning","text":"Deep learning algorithms play a crucial role in determining the features and can handle the large number of processes for the data that might be structured or unstructured. Although, deep learning algorithms can overkill some tasks that might involve complex problems because they need access to huge amounts of data so that they can function effectively. For example, there's a popular deep learning tool that recognizes images namely Imagenet that has access to 14 million images in its dataset-driven algorithms. It is a highly comprehensive tool that has defined a next-level benchmark for deep learning tools that aim images as their dataset. Deep learning algorithms are highly progressive algorithms that learn about the image by passing it through each neural network layer. The layers are highly sensitive to detect low-level features of the image like edges and pixels and henceforth the combined layers take this information and form holistic representations by comparing it with previous data. For example, the middle layer might be programmed to detect some special parts of the object in the photograph which other deep trained layers are programmed to detect special objects like dogs, trees, utensils, etc. However, if we talk out the simple task that involves less complexity and a data-driven resource, deep learning algorithms fail to generalize simple data. This is one of the main reasons deep learning is not considered effective as linear or boosted tree models. Simple models aim to churn out custom data, track fraudulent transactions and deal with less complex datasets with fewer features. Also, there are various cases like multiclass classification where deep learning can be effective because it involves smaller but more structured datasets but is not preferred usually.","title":"Importance of Deep Learning"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#deep-learning-algorithms_1","text":"","title":"Deep Learning Algorithms"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#1-convolutional-neural-networks-cnns","text":"CNN's popularly known as ConvNets majorly consists of several layers and are specifically used for image processing and detection of objects. CNNs have wide usage in identifying the image of the satellites, medical image processing, series forecasting, and anomaly detection. CNNs process the data by passing it through multiple layers and extracting features to exhibit convolutional operations. The Convolutional Layer consists of Rectified Linear Unit (ReLU) that outlasts to rectify the feature map. The Pooling layer is used to rectify these feature maps into the next feed. Pooling is generally a sampling algorithm that is down-sampled and it reduces the dimensions of the feature map. Later, the result generated consists of 2-D arrays consisting of single, long, continuous , and linear vector flattened in the map. The next layer i.e., called Fully Connected Layer which forms the flattened matrix or 2-D array fetched from the Pooling Layer as input and identifies the image by classifying it.","title":"1. Convolutional Neural Networks (CNNs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#2-long-short-term-memory-networks-lstms","text":"LSTMs can be defined as Recurrent Neural Networks (RNN) that are programmed to learn and adapt for dependencies for the long term. It can memorize and recall past data for a greater period and by default, it is its sole behavior. LSTMs are designed to retain over time and henceforth they are majorly used in time series predictions because they can restrain memory or previous inputs. This analogy comes from their chain-like structure consisting of four interacting layers that communicate with each other differently. Besides applications of time series prediction, they can be used to construct speech recognizers, development in pharmaceuticals, and composition of music loops as well.","title":"2. Long Short Term Memory Networks (LSTMs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#3-recurrent-neural-networks-rnns","text":"Recurrent Neural Networks or RNNs consist of some directed connections that form a cycle that allow the input provided from the LSTMs to be used as input in the current phase of RNNs. These inputs are deeply embedded as inputs and enforce the memorization ability of LSTMs lets these inputs get absorbed for a period in the internal memory. RNNs are therefore dependent on the inputs that are preserved by LSTMs and work under the synchronization phenomenon of LSTMs. RNNs are mostly used in captioning the image, time series analysis, recognizing handwritten data, and translating data to machines. RNNs follow the work approach by putting output feeds (t-1) time if the time is defined as t. Next, the output determined by t is feed at input time t+1. Similarly, these processes are repeated for all the input consisting of any length. There's also a fact about RNNs is that they store historical information and there's no increase in the input size even if the model size is increased. RNNs look something like this when unfolded.","title":"3. Recurrent Neural Networks (RNNs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#4-generative-adversarial-networks-gans","text":"GANs are defined as deep learning algorithms that are used to generate new instances of data that match the training data. GAN usually consists of two components namely a generator that learns to generate false data and a discriminator that adapts itself by learning from this false data. Over some time, GANs have gained immense usage since they are frequently being used to clarify astronomical images and simulate lensing the gravitational dark matter. It is also used in video games to increase graphics for 2D textures by recreating them in higher resolution like 4K . They are also used in creating realistic cartoons character and also rendering human faces and 3D object rendering . GANs work in simulation by generating and understanding the fake data and the real data. During the training to understand these data, the generator produces different kinds of fake data where the discriminator quickly learns to adapt and respond to it as false data. GANs then send these recognized results for updating. Consider the below image to visualize the functioning.","title":"4. Generative Adversarial Networks (GANs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#5-radial-basis-function-networks-rbfns","text":"RBFNs are specific types of neural networks that follow a feed-forward approach and make use of radial functions as activation functions. They consist of three layers namely the input layer, hidden layer, and output layer which are mostly used for time-series prediction, regression testing, and classification . RBFNs do these tasks by measuring the similarities present in the training data set. They usually have an input vector that feeds these data into the input layer thereby confirming the identification and rolling out results by comparing previous data sets. Precisely, the input layer has neurons that are sensitive to these data and the nodes in the layer are efficient in classifying the class of data. Neurons are originally present in the hidden layer though they work in close integration with the input layer. The hidden layer contains Gaussian transfer functions that are inversely proportional to the distance of the output from the neuron's center. The output layer has linear combinations of the radial-based data where the Gaussian functions are passed in the neuron as parameter and output is generated. Consiider the given image below to understand the process thoroughly.","title":"5. Radial Basis Function Networks (RBFNs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#6-multilayer-perceptrons-mlps","text":"MLPs are the base of deep learning technology. It belongs to a class of feed-forward neural networks having various layers of perceptrons . These perceptrons have various activation functions in them. MLPs also have connected input and output layers and their number is the same. Also, there's a layer that remains hidden amidst these two layers. MLPs are mostly used to build image and speech recognition systems or some other types of the translation software . The working of MLPs starts by feeding the data in the input layer. The neurons present in the layer form a graph to establish a connection that passes in one direction. The weight of this input data is found to exist between the hidden layer and the input layer. MLPs use activation functions to determine which nodes are ready to fire. These activation functions include tanh function, sigmoid and ReLUs . MLPs are mainly used to train the models to understand what kind of co-relation the layers are serving to achieve the desired output from the given data set.","title":"6. Multilayer Perceptrons (MLPs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#7-self-organizing-maps-soms","text":"SOMs were invented by Teuvo Kohenen for achieving data visualization to understand the dimensions of data through artificial and self-organizing neural networks. The attempts to achieve data visualization to solve problems are mainly done by what humans cannot visualize. These data are generally high-dimensional so there are lesser chances of human involvement and of course less error. SOMs help in visualizing the data by initializing weights of different nodes and then choose random vectors from the given training data. They examine each node to find the relative weights so that dependencies can be understood. The winning node is decided and that is called Best Matching Unit (BMU). Later, SOMs discover these winning nodes but the nodes reduce over time from the sample vector. So, the closer the node to BMU more is the more chance to recognize the weight and carry out further activities. There are also multiple iterations done to ensure that no node closer to BMU is missed. One example of such is the RGB color combinations that we use in our daily tasks. Consider the below image to understand how they function.","title":"7. Self Organizing Maps (SOMs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#8-deep-belief-networks-dbns","text":"DBNs are called generative models because they have various layers of latent as well as stochastic variables. The latent variable is called a hidden unit because they have binary values. DBNs are also called Boltzmann Machines because the RGM layers are stacked over each other to establish communication with previous and consecutive layers. DBNs are used in applications like video and image recognition as well as capturing motional objects. DBNs are powered by Greedy algorithms . The layer to layer approach by leaning through a top-down approach to generate weights is the most common way DBNs function. DBNs use step by step approach of Gibbs sampling on the hidden two-layer at the top. Then, these stages draw a sample from the visible units using a model that follows the ancestral sampling method. DBNs learn from the values present in the latent value from every layer following the bottom-up pass approach.","title":"8. Deep Belief Networks (DBNs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#9-restricted-boltzmann-machines-rbms","text":"RBMs were developed by Geoffrey Hinton and resemble stochastic neural networks that learn from the probability distribution in the given input set. This algorithm is mainly used in the field of dimension reduction, regression and classification, topic modeling and are considered the building blocks of DBNs. RBIs consist of two layers namely the visible layer and the hidden layer . Both of these layers are connected through hidden units and have bias units connected to nodes that generate the output. Usually, RBMs have two phases namely forward pass and backward pass . The functioning of RBMs is carried out by accepting inputs and translating them to numbers so that inputs are encoded in the forward pass. RBMs take into account the weight of every input, and the backward pass takes these input weights and translates them further into reconstructed inputs. Later, both of these translated inputs, along with individual weights, are combined. These inputs are then pushed to the visible layer where the activation is carried out, and output is generated that can be easily reconstructed. To understand this process, consider the below image.","title":"9. Restricted Boltzmann Machines (RBMs)"},{"location":"AIML/DeepLearning/DeepLearning-algorithms.html#autoencoders","text":"Autoencoders are a special type of neural network where inputs are outputs are found usually identical. It was designed to primarily solve the problems related to unsupervised learning. Autoencoders are highly trained neural networks that replicate the data. It is the reason why the input and output are generally the same. They are used to achieve tasks like pharma discovery, image processing, and population prediction . Autoencoders constitute three components namely the encoder, the code, and the decoder . Autoencoders are built in such a structure that they can receive inputs and transform them into various representations. The attempts to copy the original input by reconstructing them is more accurate. They do this by encoding the image or input, reduce the size. If the image is not visible properly they are passed to the neural network for clarification. Then, the clarified image is termed a reconstructed image and this resembles as accurate as of the previous image. To understand this complex process, see the below-provided image.","title":"Autoencoders"},{"location":"AIML/DeepLearning/DeepLearning-overview.html","text":"Deep Learning # To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence. At the outer most ring you have artificial intelligence (using computers to reason). One layer inside of that is machine learning. With artificial neural networks and deep learning at the center. Broadly speaking, deep learning is a more approachable name for an artificial neural network. The \u201cdeep\u201d in deep learning refers to the depth of the network. An artificial neural network can be very shallow. Neural networks are inspired by the structure of the cerebral cortex. At the basic level is the perceptron, the mathematical representation of a biological neuron. Like in the cerebral cortex, there can be several layers of interconnected perceptrons. The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input to each node in the next layer. There are generally no connections between nodes in the same layer and the last layer produces the outputs. We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and are only activated by nodes in the previous layer. Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data. Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type of machine learning. Machine learning involves computer intelligence that doesn\u2019t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. Machine learning typical requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra. There are two broad classes of machine learning methods: Supervised learning Unsupervised learning In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems. For example, let\u2019s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data \u2013 i.e. employee bonus amount and tenure \u2013 we could use supervised machine learning. With unsupervised learning, there aren\u2019t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It\u2019s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon\u2019s \u201ccustomers who also bought\u2026\u201d recommendations are a type of associative task. While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique. Why is Deep Learning Important? # Computers have long had techniques for recognizing features inside of images. The results weren\u2019t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks. Facebook has had great success with identifying faces in photographs by using deep learning. It\u2019s not just a marginal improvement, but a game changer: \u201cAsked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.\u201d Speech recognition is a another area that\u2019s felt deep learning\u2019s impact. Spoken languages are so vast and ambiguous. Baidu \u2013 one of the leading search engines of China \u2013 has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin. What is particularly fascinating, is that generalizing the two languages didn\u2019t require much additional design effort: \u201cHistorically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,\u201d Andrew Ng says, chief scientist at Baidu. \u201cThe learning algorithms are now so general that you can just learn.\u201d Google is now using deep learning to manage the energy at the company\u2019s data centers. They\u2019ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings. Deep Learning Microservices # Here\u2019s a quick overview of some deep learning use cases and microservices. Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what\u2019s in the image. DeepFilter is a style transfer service for applying artistic filters to images. The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google\u2019s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image. Open Source Deep Learning Frameworks # Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here\u2019s an overview of each: DL4J: JVM-based Distrubted Integrates with Hadoop and Spark Theano: Very popular in Academia Fairly low level Interfaced with via Python and Numpy Torch: Lua based In house versions used by Facebook and Twitter Contains pretrained models TensorFlow: Google written successor to Theano Interfaced with via Python and Numpy Highly parallel Can be somewhat slow for certain problem sets Caffe: Not general purpose. Focuses on machine-vision problems Implemented in C++ and is very fast Not easily extensible Has a Python interface McCulloch and Pitts Neuron # In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components: A set of weights corresponding to synapses (inputs) An adder for summing input signals; analogous to cell membrane that collects charge An activation function for determining when the neuron fires, based on accumulated input A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a network can they perform useful work. Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons. Perceptron # A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network. Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently. The number of inputs and outputs are determined by the data. Weights are stored as a N x K matrix, with N observations and K neurons, with specifying the weight on the ith observation on the jth neuron. Learning with Perceptrons # Example: Logical functions # Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables x1 and x2, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables. %matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() from scipy import optimize from ipywidgets import * from IPython.display import SVG from sklearn import datasets AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)}) AND x1 x2 y 0 0 0 0 1 0 1 0 2 1 0 0 3 1 1 1 First, we need to initialize weights to small, random values (can be positive and negative). w = np.random.randn(3)*1e-4 Then, a simple activation function for calculating g(h): g = lambda inputs, weights: np.where(np.dot(inputs, weights)>0, 1, 0) Finally, a training function that iterates the learning algorithm, returning the adapted weights. def train(inputs, targets, weights, eta, n_iterations): # Add the inputs that match the bias node inputs = np.c_[inputs, -np.ones((len(inputs), 1))] for n in range(n_iterations): activations = g(inputs, weights); weights -= eta*np.dot(np.transpose(inputs), activations - targets) return(weights) Let's test it first on the AND function. inputs = AND[['x1','x2']] target = AND['y'] w = train(inputs, target, w, 0.25, 10) Checking the performance: g(np.c_[inputs, -np.ones((len(inputs), 1))], w) array([0, 0, 0, 1]) Thus, it has learned the function perfectly. Now for OR: OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)}) OR x1 x2 y 0 0 0 0 1 0 1 1 2 1 0 1 3 1 1 1 w = np.random.randn(3)*1e-4 inputs = OR[['x1','x2']] target = OR['y'] w = train(inputs, target, w, 0.25, 20) g(np.c_[inputs, -np.ones((len(inputs), 1))], w) array([0, 1, 1, 1]) Also 100% correct. Exercise: XOR Now try running the model on the XOR function, where a one is returned for either x1 or x2 being true, but not both. What happens here? Let's explore the problem graphically: AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter') plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--'); XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)}) XOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter'); Multi-layer Perceptron # The solution to fitting more complex (i.e. non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply add more weights. There are two ways to add complexity: Add backward connections, so that output neurons feed back to input nodes, resulting in a recurrent network Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a multi-layer perceptron The latter approach is more common in applications of neural networks. How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction. Updating a multi-layer perceptron (MLP) is a matter of: 1. moving forward through the network, calculating outputs given inputs and current weight estimates 2. moving backward updating weights according to the resulting error from forward propagation. In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer. Backpropagation # Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters. These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent. This may be the most common method for training neural networks. Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors. Review: The chain rule # Notation # Backpropagation in general # Backpropagation in practice # Toy Python example # Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations. The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function. # Ensure python 3 forward compatibility from __future__ import print_function import numpy as np def sigmoid(x): return 1/(1 + np.exp(-x)) class SigmoidLayer: def __init__(self, n_input, n_output): self.W = np.random.randn(n_output, n_input) self.b = np.random.randn(n_output, 1) def output(self, X): if X.ndim == 1: X = X.reshape(-1, 1) return sigmoid(self.W.dot(X) + self.b) class SigmoidNetwork: def __init__(self, layer_sizes): ''' :parameters: - layer_sizes : list of int List of layer sizes of length L+1 (including the input dimensionality) ''' self.layers = [] for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]): self.layers.append(SigmoidLayer(n_input, n_output)) def train(self, X, y, learning_rate=0.2): X = np.array(X) y = np.array(y) if X.ndim == 1: X = X.reshape(-1, 1) if y.ndim == 1: y = y.reshape(1, -1) # Forward pass - compute a^n for n in {0, ... L} layer_outputs = [X] for layer in self.layers: layer_outputs.append(layer.output(layer_outputs[-1])) # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1} cost_partials = [layer_outputs[-1] - y] for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])): cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output)) cost_partials.reverse() # Compute weight gradient step W_updates = [] for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]): W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1]) # and biases b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]] for W_update, b_update, layer in zip(W_updates, b_updates, self.layers): layer.W -= W_update*learning_rate layer.b -= b_update*learning_rate def output(self, X): a = np.array(X) if a.ndim == 1: a = a.reshape(-1, 1) for layer in self.layers: a = layer.output(a) return a nn = SigmoidNetwork([2, 2, 1]) X = np.array([[0, 1, 0, 1], [0, 0, 1, 1]]) y = np.array([0, 1, 1, 0]) for n in range(int(1e3)): nn.train(X, y, learning_rate=1.) print(\"Input\\tOutput\\tQuantized\") for i in [[0, 0], [1, 0], [0, 1], [1, 1]]: print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] > .5))) logistic = lambda h, beta: 1./(1 + np.exp(-beta * h)) @interact(beta=(-1, 25)) def logistic_plot(beta=5): hvals = np.linspace(-2, 2) plt.plot(hvals, logistic(hvals, beta)) hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h)) @interact(theta=(-1, 25)) def tanh_plot(theta=5): hvals = np.linspace(-2, 2) h = hvals*theta plt.plot(hvals, hyperbolic_tangent(h)) Gradient Descent # import numpy as np # Define the sigmoid activation function and its derivative def sigmoid(x): return 1 / (1 + np.exp(-x)) def sigmoid_derivative(x): return x * (1 - x) # Input dataset (X) and output dataset (y) X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([[0], [1], [1], [0]]) # Initialize weights and biases randomly input_layer_neurons = X.shape[1] hidden_layer_neurons = 2 output_layer_neurons = 1 # Weight matrices W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons)) W2 = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons)) # Bias vectors b1 = np.random.uniform(size=(1, hidden_layer_neurons)) b2 = np.random.uniform(size=(1, output_layer_neurons)) # Learning rate learning_rate = 0.5 # Training the neural network for epoch in range(10000): # Forward propagation hidden_layer_input = np.dot(X, W1) + b1 hidden_layer_output = sigmoid(hidden_layer_input) output_layer_input = np.dot(hidden_layer_output, W2) + b2 predicted_output = sigmoid(output_layer_input) # Compute the error error = y - predicted_output # Backpropagation # Calculate the gradient for the output layer d_predicted_output = error * sigmoid_derivative(predicted_output) # Calculate the error for the hidden layer hidden_layer_error = d_predicted_output.dot(W2.T) d_hidden_layer_output = hidden_layer_error * sigmoid_derivative(hidden_layer_output) # Update the weights and biases W2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate W1 += X.T.dot(d_hidden_layer_output) * learning_rate b1 += np.sum(d_hidden_layer_output, axis=0, keepdims=True) * learning_rate # Display the final predicted output print(\"Final predicted output:\\n\", predicted_output) # Display the final weights and biases print(\"\\nFinal weights for W1:\\n\", W1) print(\"\\nFinal weights for W2:\\n\", W2) print(\"\\nFinal biases for b1:\\n\", b1) print(\"\\nFinal biases for b2:\\n\", b2) Explanation: # Initialization : Input dataset X and output dataset y . Weight matrices W1 and W2 and bias vectors b1 and b2 are initialized randomly. The learning rate is set to 0.5. Forward Propagation : Compute the input and output for the hidden layer. Compute the input and output for the output layer (predicted output). Error Calculation : Compute the error by subtracting the predicted output from the actual output. Backpropagation : Compute the gradient of the error with respect to the predicted output. Compute the error propagated back to the hidden layer. Compute the gradient of the hidden layer output. Weight and Bias Update : Update the weights and biases for both layers using the computed gradients and learning rate. Training Loop : The above steps are repeated for a specified number of epochs (10,000 in this case). After training, the final predicted output, weights, and biases are printed. This simple example uses a neural network with one hidden layer to demonstrate the key concepts of backpropagation. this activation function may take any of several forms, such as a logistic function. Example of Deep Learning # In the example given above, we provide the raw data of images to the first layer of the input layer. After then, these input layer will determine the patterns of local contrast that means it will differentiate on the basis of colors, luminosity, etc. Then the 1st hidden layer will determine the face feature, i.e., it will fixate on eyes, nose, and lips, etc. And then, it will fixate those face features on the correct face template. So, in the 2nd hidden layer, it will actually determine the correct face here as it can be seen in the above image, after which it will be sent to the output layer. Likewise, more hidden layers can be added to solve more complex problems, for example, if you want to find out a particular kind of face having large or light complexions. So, as and when the hidden layers increase, we are able to solve complex problems. Types of Deep Learning Networks # 1. Feed Forward Neural Network # A feed-forward neural network is none other than an Artificial Neural Network, which ensures that the nodes do not form a cycle. In this kind of neural network, all the perceptrons are organized within layers, such that the input layer takes the input, and the output layer generates the output. Since the hidden layers do not link with the outside world, it is named as hidden layers. Each of the perceptrons contained in one single layer is associated with each node in the subsequent layer. It can be concluded that all of the nodes are fully connected. It does not contain any visible or invisible connection between the nodes in the same layer. There are no back-loops in the feed-forward network. To minimize the prediction error, the backpropagation algorithm can be used to update the weight values. Applications: # Data Compression # Pattern Recognition # Computer Vision # Sonar Target Recognition # Speech Recognition # Handwritten Characters Recognition # 2. Recurrent Neural Network # Recurrent neural networks are yet another variation of feed-forward networks. Here each of the neurons present in the hidden layers receives an input with a specific delay in time. The Recurrent neural network mainly accesses the preceding info of existing iterations. For example, to guess the succeeding word in any sentence, one must have knowledge about the words that were previously used. It not only processes the inputs but also shares the length as well as weights crossways time. It does not let the size of the model to increase with the increase in the input size. However, the only problem with this recurrent neural network is that it has slow computational speed as well as it does not contemplate any future input for the current state. It has a problem with reminiscing prior information. Applications: # Machine Translation # Robot Control # Time Series Prediction # Speech Recognition # Speech Synthesis # Time Series Anomaly Detection # Rhythm Learning # Music Composition # 3. Convolutional Neural Network # Convolutional Neural Networks are a special kind of neural network mainly used for image classification, clustering of images and object recognition. DNNs enable unsupervised construction of hierarchical image representations. To achieve the best accuracy, deep convolutional neural networks are preferred more than any other neural network. Applications: # Identify Faces, Street Signs, Tumors. # Image Recognition. # Video Analysis. # NLP. # Anomaly Detection. # Drug Discovery. # Checkers Game. # Time Series Forecasting. # 4. Restricted Boltzmann Machine # RBMs are yet another variant of Boltzmann Machines. Here the neurons present in the input layer and the hidden layer encompasses symmetric connections amid them. However, there is no internal association within the respective layer. But in contrast to RBM, Boltzmann machines do encompass internal connections inside the hidden layer. These restrictions in BMs helps the model to train efficiently. Applications: # Filtering. # Feature Learning. # Classification. # Risk Detection. # Business and Economic analysis. # 5. Autoencoders # An autoencoder neural network is another kind of unsupervised machine learning algorithm. Here the number of hidden cells is merely small than that of the input cells. But the number of input cells is equivalent to the number of output cells. An autoencoder network is trained to display the output similar to the fed input to force AEs to find common patterns and generalize the data. The autoencoders are mainly used for the smaller representation of the input. It helps in the reconstruction of the original data from compressed data. This algorithm is comparatively simple as it only necessitates the output identical to the input. Encoder: Convert input data in lower dimensions. Decoder: Reconstruct the compressed data. Applications: # Classification. # Clustering. # Feature Compression. # Deep learning applications # Self-Driving Cars In self-driven cars, it is able to capture the images around it by processing a huge amount of data, and then it will decide which actions should be incorporated to take a left or right or should it stop. So, accordingly, it will decide what actions it should take, which will further reduce the accidents that happen every year. Voice Controlled Assistance When we talk about voice control assistance, then Siri is the one thing that comes into our mind. So, you can tell Siri whatever you want it to do it for you, and it will search it for you and display it for you. Automatic Image Caption Generation Whatever image that you upload, the algorithm will work in such a way that it will generate caption accordingly. If you say blue colored eye, it will display a blue-colored eye with a caption at the bottom of the image. Automatic Machine Translation With the help of automatic machine translation, we are able to convert one language into another with the help of deep learning. Limitations It only learns through the observations. It comprises of biases issues. Advantages It lessens the need for feature engineering. It eradicates all those costs that are needless. It easily identifies difficult defects. It results in the best-in-class performance on problems. Disadvantages It requires an ample amount of data. It is quite expensive to train. It does not have strong theoretical groundwork. Introduction # Brains biological network provides basis for connecting elements in a real-life scenario for information processing and insight generation. A hierarchy of neurons connected through layers, where the output of one layer becomes the input for another layers, the information passes from one layer to another layer as weights. The weights associated with each neuron contain insights so that recognition and reasoning becomes easier for the next level. Artificial neural network is a very popular and effective method that consists of layers associated with weights. The association between different layers is governed by mathematical equation that passes information from one layer to the other. A bunch of mathematical equations are at work inside one artificial neural network model. Neural Networks # Task # What is Deep Learning (DL)? # A machine learning subfield of learning representations of data. Exceptional effective at learning patterns. Deep learning algorithms attempt to learn (multiple levels of) representation by using a hierarchy of multiple layers. If you provide the system tons of information, it begins to understand it and respond in useful ways. Why is DL useful? # Manually designed features are often over-specified, incomplete and take a long time to design and validate Learned Features are easy to adapt, fast to learn Deep learning provides a very flexible, (almost?) universal, learnable framework for representing world, visual and linguistic information. Can learn both unsupervised and supervised Effective end-to-end joint system learning Utilize large amounts of training data In ~2010 DL started outperforming other ML techniques first in speech and vision, then NLP Types of Neural Networks # Single hidden layer neural network: this is the simplest form of neural network as in this there is only one hidden layer. Multiple hidden layer neural networks: in this form more than one hidden layer will connect the input data with the output data. The complexity of calculation increases in this form as it requires more computational power to the system to process information Feed forward neural networks: in this form of neural network architecture, the information is passed one directionally from one layer to another layer; there is no iteration from the first level of learning. Back propagation neural networks: in this form of neural network there are two important steps, feed forward works in passing information from input to the hidden and from hidden to output layer and secondly it calculates error and propagate it back to the previous layers.","title":"Deep Learning Overview"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#deep-learning","text":"To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence. At the outer most ring you have artificial intelligence (using computers to reason). One layer inside of that is machine learning. With artificial neural networks and deep learning at the center. Broadly speaking, deep learning is a more approachable name for an artificial neural network. The \u201cdeep\u201d in deep learning refers to the depth of the network. An artificial neural network can be very shallow. Neural networks are inspired by the structure of the cerebral cortex. At the basic level is the perceptron, the mathematical representation of a biological neuron. Like in the cerebral cortex, there can be several layers of interconnected perceptrons. The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input to each node in the next layer. There are generally no connections between nodes in the same layer and the last layer produces the outputs. We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and are only activated by nodes in the previous layer. Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data. Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type of machine learning. Machine learning involves computer intelligence that doesn\u2019t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. Machine learning typical requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra. There are two broad classes of machine learning methods: Supervised learning Unsupervised learning In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems. For example, let\u2019s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data \u2013 i.e. employee bonus amount and tenure \u2013 we could use supervised machine learning. With unsupervised learning, there aren\u2019t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It\u2019s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon\u2019s \u201ccustomers who also bought\u2026\u201d recommendations are a type of associative task. While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique.","title":"Deep Learning"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#why-is-deep-learning-important","text":"Computers have long had techniques for recognizing features inside of images. The results weren\u2019t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks. Facebook has had great success with identifying faces in photographs by using deep learning. It\u2019s not just a marginal improvement, but a game changer: \u201cAsked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.\u201d Speech recognition is a another area that\u2019s felt deep learning\u2019s impact. Spoken languages are so vast and ambiguous. Baidu \u2013 one of the leading search engines of China \u2013 has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin. What is particularly fascinating, is that generalizing the two languages didn\u2019t require much additional design effort: \u201cHistorically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,\u201d Andrew Ng says, chief scientist at Baidu. \u201cThe learning algorithms are now so general that you can just learn.\u201d Google is now using deep learning to manage the energy at the company\u2019s data centers. They\u2019ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.","title":"Why is Deep Learning Important?"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#deep-learning-microservices","text":"Here\u2019s a quick overview of some deep learning use cases and microservices. Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what\u2019s in the image. DeepFilter is a style transfer service for applying artistic filters to images. The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google\u2019s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image.","title":"Deep Learning Microservices"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#open-source-deep-learning-frameworks","text":"Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here\u2019s an overview of each: DL4J: JVM-based Distrubted Integrates with Hadoop and Spark Theano: Very popular in Academia Fairly low level Interfaced with via Python and Numpy Torch: Lua based In house versions used by Facebook and Twitter Contains pretrained models TensorFlow: Google written successor to Theano Interfaced with via Python and Numpy Highly parallel Can be somewhat slow for certain problem sets Caffe: Not general purpose. Focuses on machine-vision problems Implemented in C++ and is very fast Not easily extensible Has a Python interface","title":"Open Source Deep Learning Frameworks"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#mcculloch-and-pitts-neuron","text":"In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components: A set of weights corresponding to synapses (inputs) An adder for summing input signals; analogous to cell membrane that collects charge An activation function for determining when the neuron fires, based on accumulated input A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a network can they perform useful work. Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.","title":"McCulloch and Pitts Neuron"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#perceptron","text":"A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network. Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently. The number of inputs and outputs are determined by the data. Weights are stored as a N x K matrix, with N observations and K neurons, with specifying the weight on the ith observation on the jth neuron.","title":"Perceptron"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#learning-with-perceptrons","text":"","title":"Learning with Perceptrons"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#example-logical-functions","text":"Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables x1 and x2, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables. %matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() from scipy import optimize from ipywidgets import * from IPython.display import SVG from sklearn import datasets AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)}) AND x1 x2 y 0 0 0 0 1 0 1 0 2 1 0 0 3 1 1 1 First, we need to initialize weights to small, random values (can be positive and negative). w = np.random.randn(3)*1e-4 Then, a simple activation function for calculating g(h): g = lambda inputs, weights: np.where(np.dot(inputs, weights)>0, 1, 0) Finally, a training function that iterates the learning algorithm, returning the adapted weights. def train(inputs, targets, weights, eta, n_iterations): # Add the inputs that match the bias node inputs = np.c_[inputs, -np.ones((len(inputs), 1))] for n in range(n_iterations): activations = g(inputs, weights); weights -= eta*np.dot(np.transpose(inputs), activations - targets) return(weights) Let's test it first on the AND function. inputs = AND[['x1','x2']] target = AND['y'] w = train(inputs, target, w, 0.25, 10) Checking the performance: g(np.c_[inputs, -np.ones((len(inputs), 1))], w) array([0, 0, 0, 1]) Thus, it has learned the function perfectly. Now for OR: OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)}) OR x1 x2 y 0 0 0 0 1 0 1 1 2 1 0 1 3 1 1 1 w = np.random.randn(3)*1e-4 inputs = OR[['x1','x2']] target = OR['y'] w = train(inputs, target, w, 0.25, 20) g(np.c_[inputs, -np.ones((len(inputs), 1))], w) array([0, 1, 1, 1]) Also 100% correct. Exercise: XOR Now try running the model on the XOR function, where a one is returned for either x1 or x2 being true, but not both. What happens here? Let's explore the problem graphically: AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter') plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--'); XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)}) XOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter');","title":"Example: Logical functions"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#multi-layer-perceptron","text":"The solution to fitting more complex (i.e. non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply add more weights. There are two ways to add complexity: Add backward connections, so that output neurons feed back to input nodes, resulting in a recurrent network Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a multi-layer perceptron The latter approach is more common in applications of neural networks. How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction. Updating a multi-layer perceptron (MLP) is a matter of: 1. moving forward through the network, calculating outputs given inputs and current weight estimates 2. moving backward updating weights according to the resulting error from forward propagation. In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.","title":"Multi-layer Perceptron"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#backpropagation","text":"Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters. These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent. This may be the most common method for training neural networks. Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.","title":"Backpropagation"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#review-the-chain-rule","text":"","title":"Review: The chain rule"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#notation","text":"","title":"Notation"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#backpropagation-in-general","text":"","title":"Backpropagation in general"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#backpropagation-in-practice","text":"","title":"Backpropagation in practice"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#toy-python-example","text":"Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations. The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function. # Ensure python 3 forward compatibility from __future__ import print_function import numpy as np def sigmoid(x): return 1/(1 + np.exp(-x)) class SigmoidLayer: def __init__(self, n_input, n_output): self.W = np.random.randn(n_output, n_input) self.b = np.random.randn(n_output, 1) def output(self, X): if X.ndim == 1: X = X.reshape(-1, 1) return sigmoid(self.W.dot(X) + self.b) class SigmoidNetwork: def __init__(self, layer_sizes): ''' :parameters: - layer_sizes : list of int List of layer sizes of length L+1 (including the input dimensionality) ''' self.layers = [] for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]): self.layers.append(SigmoidLayer(n_input, n_output)) def train(self, X, y, learning_rate=0.2): X = np.array(X) y = np.array(y) if X.ndim == 1: X = X.reshape(-1, 1) if y.ndim == 1: y = y.reshape(1, -1) # Forward pass - compute a^n for n in {0, ... L} layer_outputs = [X] for layer in self.layers: layer_outputs.append(layer.output(layer_outputs[-1])) # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1} cost_partials = [layer_outputs[-1] - y] for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])): cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output)) cost_partials.reverse() # Compute weight gradient step W_updates = [] for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]): W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1]) # and biases b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]] for W_update, b_update, layer in zip(W_updates, b_updates, self.layers): layer.W -= W_update*learning_rate layer.b -= b_update*learning_rate def output(self, X): a = np.array(X) if a.ndim == 1: a = a.reshape(-1, 1) for layer in self.layers: a = layer.output(a) return a nn = SigmoidNetwork([2, 2, 1]) X = np.array([[0, 1, 0, 1], [0, 0, 1, 1]]) y = np.array([0, 1, 1, 0]) for n in range(int(1e3)): nn.train(X, y, learning_rate=1.) print(\"Input\\tOutput\\tQuantized\") for i in [[0, 0], [1, 0], [0, 1], [1, 1]]: print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] > .5))) logistic = lambda h, beta: 1./(1 + np.exp(-beta * h)) @interact(beta=(-1, 25)) def logistic_plot(beta=5): hvals = np.linspace(-2, 2) plt.plot(hvals, logistic(hvals, beta)) hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h)) @interact(theta=(-1, 25)) def tanh_plot(theta=5): hvals = np.linspace(-2, 2) h = hvals*theta plt.plot(hvals, hyperbolic_tangent(h))","title":"Toy Python example"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#gradient-descent","text":"import numpy as np # Define the sigmoid activation function and its derivative def sigmoid(x): return 1 / (1 + np.exp(-x)) def sigmoid_derivative(x): return x * (1 - x) # Input dataset (X) and output dataset (y) X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([[0], [1], [1], [0]]) # Initialize weights and biases randomly input_layer_neurons = X.shape[1] hidden_layer_neurons = 2 output_layer_neurons = 1 # Weight matrices W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons)) W2 = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons)) # Bias vectors b1 = np.random.uniform(size=(1, hidden_layer_neurons)) b2 = np.random.uniform(size=(1, output_layer_neurons)) # Learning rate learning_rate = 0.5 # Training the neural network for epoch in range(10000): # Forward propagation hidden_layer_input = np.dot(X, W1) + b1 hidden_layer_output = sigmoid(hidden_layer_input) output_layer_input = np.dot(hidden_layer_output, W2) + b2 predicted_output = sigmoid(output_layer_input) # Compute the error error = y - predicted_output # Backpropagation # Calculate the gradient for the output layer d_predicted_output = error * sigmoid_derivative(predicted_output) # Calculate the error for the hidden layer hidden_layer_error = d_predicted_output.dot(W2.T) d_hidden_layer_output = hidden_layer_error * sigmoid_derivative(hidden_layer_output) # Update the weights and biases W2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate W1 += X.T.dot(d_hidden_layer_output) * learning_rate b1 += np.sum(d_hidden_layer_output, axis=0, keepdims=True) * learning_rate # Display the final predicted output print(\"Final predicted output:\\n\", predicted_output) # Display the final weights and biases print(\"\\nFinal weights for W1:\\n\", W1) print(\"\\nFinal weights for W2:\\n\", W2) print(\"\\nFinal biases for b1:\\n\", b1) print(\"\\nFinal biases for b2:\\n\", b2)","title":"Gradient Descent"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#explanation","text":"Initialization : Input dataset X and output dataset y . Weight matrices W1 and W2 and bias vectors b1 and b2 are initialized randomly. The learning rate is set to 0.5. Forward Propagation : Compute the input and output for the hidden layer. Compute the input and output for the output layer (predicted output). Error Calculation : Compute the error by subtracting the predicted output from the actual output. Backpropagation : Compute the gradient of the error with respect to the predicted output. Compute the error propagated back to the hidden layer. Compute the gradient of the hidden layer output. Weight and Bias Update : Update the weights and biases for both layers using the computed gradients and learning rate. Training Loop : The above steps are repeated for a specified number of epochs (10,000 in this case). After training, the final predicted output, weights, and biases are printed. This simple example uses a neural network with one hidden layer to demonstrate the key concepts of backpropagation. this activation function may take any of several forms, such as a logistic function.","title":"Explanation:"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#example-of-deep-learning","text":"In the example given above, we provide the raw data of images to the first layer of the input layer. After then, these input layer will determine the patterns of local contrast that means it will differentiate on the basis of colors, luminosity, etc. Then the 1st hidden layer will determine the face feature, i.e., it will fixate on eyes, nose, and lips, etc. And then, it will fixate those face features on the correct face template. So, in the 2nd hidden layer, it will actually determine the correct face here as it can be seen in the above image, after which it will be sent to the output layer. Likewise, more hidden layers can be added to solve more complex problems, for example, if you want to find out a particular kind of face having large or light complexions. So, as and when the hidden layers increase, we are able to solve complex problems.","title":"Example of Deep Learning"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#types-of-deep-learning-networks","text":"","title":"Types of Deep Learning Networks"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#1-feed-forward-neural-network","text":"A feed-forward neural network is none other than an Artificial Neural Network, which ensures that the nodes do not form a cycle. In this kind of neural network, all the perceptrons are organized within layers, such that the input layer takes the input, and the output layer generates the output. Since the hidden layers do not link with the outside world, it is named as hidden layers. Each of the perceptrons contained in one single layer is associated with each node in the subsequent layer. It can be concluded that all of the nodes are fully connected. It does not contain any visible or invisible connection between the nodes in the same layer. There are no back-loops in the feed-forward network. To minimize the prediction error, the backpropagation algorithm can be used to update the weight values.","title":"1. Feed Forward Neural Network"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#applications","text":"","title":"Applications:"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#data-compression","text":"","title":"Data Compression"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#pattern-recognition","text":"","title":"Pattern Recognition"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#computer-vision","text":"","title":"Computer Vision"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#sonar-target-recognition","text":"","title":"Sonar Target Recognition"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#speech-recognition","text":"","title":"Speech Recognition"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#handwritten-characters-recognition","text":"","title":"Handwritten Characters Recognition"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#2-recurrent-neural-network","text":"Recurrent neural networks are yet another variation of feed-forward networks. Here each of the neurons present in the hidden layers receives an input with a specific delay in time. The Recurrent neural network mainly accesses the preceding info of existing iterations. For example, to guess the succeeding word in any sentence, one must have knowledge about the words that were previously used. It not only processes the inputs but also shares the length as well as weights crossways time. It does not let the size of the model to increase with the increase in the input size. However, the only problem with this recurrent neural network is that it has slow computational speed as well as it does not contemplate any future input for the current state. It has a problem with reminiscing prior information.","title":"2. Recurrent Neural Network"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#applications_1","text":"","title":"Applications:"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#machine-translation","text":"","title":"Machine Translation"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#robot-control","text":"","title":"Robot Control"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#time-series-prediction","text":"","title":"Time Series Prediction"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#speech-recognition_1","text":"","title":"Speech Recognition"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#speech-synthesis","text":"","title":"Speech Synthesis"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#time-series-anomaly-detection","text":"","title":"Time Series Anomaly Detection"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#rhythm-learning","text":"","title":"Rhythm Learning"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#music-composition","text":"","title":"Music Composition"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#3-convolutional-neural-network","text":"Convolutional Neural Networks are a special kind of neural network mainly used for image classification, clustering of images and object recognition. DNNs enable unsupervised construction of hierarchical image representations. To achieve the best accuracy, deep convolutional neural networks are preferred more than any other neural network.","title":"3. Convolutional Neural Network"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#applications_2","text":"","title":"Applications:"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#identify-faces-street-signs-tumors","text":"","title":"Identify Faces, Street Signs, Tumors."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#image-recognition","text":"","title":"Image Recognition."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#video-analysis","text":"","title":"Video Analysis."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#nlp","text":"","title":"NLP."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#anomaly-detection","text":"","title":"Anomaly Detection."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#drug-discovery","text":"","title":"Drug Discovery."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#checkers-game","text":"","title":"Checkers Game."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#time-series-forecasting","text":"","title":"Time Series Forecasting."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#4-restricted-boltzmann-machine","text":"RBMs are yet another variant of Boltzmann Machines. Here the neurons present in the input layer and the hidden layer encompasses symmetric connections amid them. However, there is no internal association within the respective layer. But in contrast to RBM, Boltzmann machines do encompass internal connections inside the hidden layer. These restrictions in BMs helps the model to train efficiently.","title":"4. Restricted Boltzmann Machine"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#applications_3","text":"","title":"Applications:"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#filtering","text":"","title":"Filtering."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#feature-learning","text":"","title":"Feature Learning."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#classification","text":"","title":"Classification."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#risk-detection","text":"","title":"Risk Detection."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#business-and-economic-analysis","text":"","title":"Business and Economic analysis."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#5-autoencoders","text":"An autoencoder neural network is another kind of unsupervised machine learning algorithm. Here the number of hidden cells is merely small than that of the input cells. But the number of input cells is equivalent to the number of output cells. An autoencoder network is trained to display the output similar to the fed input to force AEs to find common patterns and generalize the data. The autoencoders are mainly used for the smaller representation of the input. It helps in the reconstruction of the original data from compressed data. This algorithm is comparatively simple as it only necessitates the output identical to the input. Encoder: Convert input data in lower dimensions. Decoder: Reconstruct the compressed data.","title":"5. Autoencoders"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#applications_4","text":"","title":"Applications:"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#classification_1","text":"","title":"Classification."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#clustering","text":"","title":"Clustering."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#feature-compression","text":"","title":"Feature Compression."},{"location":"AIML/DeepLearning/DeepLearning-overview.html#deep-learning-applications","text":"Self-Driving Cars In self-driven cars, it is able to capture the images around it by processing a huge amount of data, and then it will decide which actions should be incorporated to take a left or right or should it stop. So, accordingly, it will decide what actions it should take, which will further reduce the accidents that happen every year. Voice Controlled Assistance When we talk about voice control assistance, then Siri is the one thing that comes into our mind. So, you can tell Siri whatever you want it to do it for you, and it will search it for you and display it for you. Automatic Image Caption Generation Whatever image that you upload, the algorithm will work in such a way that it will generate caption accordingly. If you say blue colored eye, it will display a blue-colored eye with a caption at the bottom of the image. Automatic Machine Translation With the help of automatic machine translation, we are able to convert one language into another with the help of deep learning. Limitations It only learns through the observations. It comprises of biases issues. Advantages It lessens the need for feature engineering. It eradicates all those costs that are needless. It easily identifies difficult defects. It results in the best-in-class performance on problems. Disadvantages It requires an ample amount of data. It is quite expensive to train. It does not have strong theoretical groundwork.","title":"Deep learning applications"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#introduction","text":"Brains biological network provides basis for connecting elements in a real-life scenario for information processing and insight generation. A hierarchy of neurons connected through layers, where the output of one layer becomes the input for another layers, the information passes from one layer to another layer as weights. The weights associated with each neuron contain insights so that recognition and reasoning becomes easier for the next level. Artificial neural network is a very popular and effective method that consists of layers associated with weights. The association between different layers is governed by mathematical equation that passes information from one layer to the other. A bunch of mathematical equations are at work inside one artificial neural network model.","title":"Introduction"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#neural-networks","text":"","title":"Neural Networks"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#task","text":"","title":"Task"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#what-is-deep-learning-dl","text":"A machine learning subfield of learning representations of data. Exceptional effective at learning patterns. Deep learning algorithms attempt to learn (multiple levels of) representation by using a hierarchy of multiple layers. If you provide the system tons of information, it begins to understand it and respond in useful ways.","title":"What is Deep Learning (DL)?"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#why-is-dl-useful","text":"Manually designed features are often over-specified, incomplete and take a long time to design and validate Learned Features are easy to adapt, fast to learn Deep learning provides a very flexible, (almost?) universal, learnable framework for representing world, visual and linguistic information. Can learn both unsupervised and supervised Effective end-to-end joint system learning Utilize large amounts of training data In ~2010 DL started outperforming other ML techniques first in speech and vision, then NLP","title":"Why is DL useful?"},{"location":"AIML/DeepLearning/DeepLearning-overview.html#types-of-neural-networks","text":"Single hidden layer neural network: this is the simplest form of neural network as in this there is only one hidden layer. Multiple hidden layer neural networks: in this form more than one hidden layer will connect the input data with the output data. The complexity of calculation increases in this form as it requires more computational power to the system to process information Feed forward neural networks: in this form of neural network architecture, the information is passed one directionally from one layer to another layer; there is no iteration from the first level of learning. Back propagation neural networks: in this form of neural network there are two important steps, feed forward works in passing information from input to the hidden and from hidden to output layer and secondly it calculates error and propagate it back to the previous layers.","title":"Types of Neural Networks"},{"location":"AIML/DeepLearning/Keras.html","text":"Keras # Keras is an open-source high-level Neural Network library, which is written in Python is capable enough to run on Theano, TensorFlow, or CNTK. It was developed by one of the Google engineers, Francois Chollet. It is made user-friendly, extensible, and modular for facilitating faster experimentation with deep neural networks. It not only supports Convolutional Networks and Recurrent Networks individually but also their combination. It cannot handle low-level computations, so it makes use of the Backend library to resolve it. The backend library act as a high-level API wrapper for the low-level API, which lets it run on TensorFlow, CNTK, or Theano. What makes Keras special? # Focus on user experience has always been a major part of Keras. Large adoption in the industry. It is a multi backend and supports multi-platform, which helps all the encoders come together for coding. Research community present for Keras works amazingly with the production community. Easy to grasp all concepts. It supports fast prototyping. It seamlessly runs on CPU as well as GPU. It provides the freedom to design any architecture, which then later is utilized as an API for the project. It is really very simple to get started with. Easy production of models actually makes Keras special. How Keras support the claim of being multi-backend and multi-platform? # Keras can be developed in R as well as Python, such that the code can be run with TensorFlow, Theano, CNTK, or MXNet as per the requirement. Keras can be run on CPU, NVIDIA GPU, AMD GPU, TPU, etc. It ensures that producing models with Keras is really simple as it totally supports to run with TensorFlow serving, GPU acceleration (WebKeras, Keras.js), Android (TF, TF Lite), iOS (Native CoreML) and Raspberry Pi. Keras Backend # Keras being a model-level library helps in developing deep learning models by offering high-level building blocks. All the low-level computations such as products of Tensor, convolutions, etc. are not handled by Keras itself, rather they depend on a specialized tensor manipulation library that is well optimized to serve as a backend engine. Keras has managed it so perfectly that instead of incorporating one single library of tensor and performing operations related to that particular library, it offers plugging of different backend engines into Keras. Keras consist of three backend engines, which are as follows: TensorFlow TensorFlow is a Google product, which is one of the most famous deep learning tools widely used in the research area of machine learning and deep neural network. It came into the market on 9th November 2015 under the Apache License 2.0. It is built in such a way that it can easily run on multiple CPUs and GPUs as well as on mobile operating systems. It consists of various wrappers in distinct languages such as Java, C++, or Python. Theano Theano was developed at the University of Montreal, Quebec, Canada, by the MILA group. It is an open-source python library that is widely used for performing mathematical operations on multi-dimensional arrays by incorporating scipy and numpy. It utilizes GPUs for faster computation and efficiently computes the gradients by building symbolic graphs automatically. It has come out to be very suitable for unstable expressions, as it first observes them numerically and then computes them with more stable algorithms. CNTK Microsoft Cognitive Toolkit is deep learning's open-source framework. It consists of all the basic building blocks, which are required to form a neural network. The models are trained using C++ or Python, but it incorporates C# or Java to load the model for making predictions. Advantages of Keras # Keras encompasses the following advantages, which are as follows: It is very easy to understand and incorporate the faster deployment of network models. It has huge community support in the market as most of the AI companies are keen on using it. It supports multi backend, which means you can use any one of them among TensorFlow, CNTK, and Theano with Keras as a backend according to your requirement. Since it has an easy deployment, it also holds support for cross-platform. Following are the devices on which Keras can be deployed: iOS with CoreML Android with TensorFlow Android Web browser with .js support Cloud engine Raspberry pi It supports Data parallelism, which means Keras can be trained on multiple GPU's at an instance for speeding up the training time and processing a huge amount of data. Disadvantages of Keras # The only disadvantage is that Keras has its own pre-configured layers, and if you want to create an abstract layer, it won't let you because it cannot handle low-level APIs. It only supports high-level API running on the top of the backend engine (TensorFlow, Theano, and CNTK). #Based on the extracted code from the notebook, I'll transform the script to use Keras for defining and training a neural network that learns the AND and OR logic gates. I'll ensure that the neural network is correctly set up and that the previous errors and syntax issues are fixed. #Here is the modified script using Keras: import numpy as np import pandas as pd from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Define the AND dataset AND = pd.DataFrame({'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1], 'y': [0, 0, 0, 1]}) inputs_and = AND[['x1', 'x2']] targets_and = AND['y'] # Define the OR dataset OR = pd.DataFrame({'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1], 'y': [0, 1, 1, 1]}) inputs_or = OR[['x1', 'x2']] targets_or = OR['y'] # Build the model def build_model(): model = Sequential() model.add(Dense(2, input_dim=2, activation='relu')) # 2 neurons for input layer model.add(Dense(1, activation='sigmoid')) # 1 neuron for output layer model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model # Train and evaluate the model for AND logic gate model_and = build_model() model_and.fit(inputs_and, targets_and, epochs=100, verbose=0) loss_and, accuracy_and = model_and.evaluate(inputs_and, targets_and) print(f'AND Gate - Loss: {loss_and}, Accuracy: {accuracy_and}') # Train and evaluate the model for OR logic gate model_or = build_model() model_or.fit(inputs_or, targets_or, epochs=100, verbose=0) loss_or, accuracy_or = model_or.evaluate(inputs_or, targets_or) print(f'OR Gate - Loss: {loss_or}, Accuracy: {accuracy_or}') # Predict using the trained models predictions_and = model_and.predict(inputs_and) predictions_or = model_or.predict(inputs_or) print(\"AND Gate Predictions:\") print(np.round(predictions_and)) print(\"OR Gate Predictions:\") print(np.round(predictions_or)) ### Explanation: #- **Data Preparation**: The AND and OR datasets are defined as pandas DataFrames. #- **Model Building**: A function `build_model` is created to define the neural network with Keras. The network consists of an input layer with 2 neurons (corresponding to the 2 input features) and an output layer with 1 neuron for binary classification. #- **Training and Evaluation**: The model is trained separately for the AND and OR datasets. The training process is silent (verbose=0), and after training, the model's loss and accuracy are printed. #- **Predictions**: The trained models are used to predict the outputs for the AND and OR datasets, and the predictions are printed. #This script ensures proper use of Keras for defining, training, and evaluating neural networks for the given logic gate tasks. Example-2 # import numpy as np # Sample data for linear regression X = np.array([[1], [2], [3], [4], [5]]) y = np.array([1, 3, 3, 2, 5]) # Linear regression with numpy A = np.vstack([X.T, np.ones(len(X))]).T m, c = np.linalg.lstsq(A, y, rcond=None)[0] print(f\"Slope: {m}, Intercept: {c}\") import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Sample data for linear regression X = np.array([[1], [2], [3], [4], [5]]) y = np.array([1, 3, 3, 2, 5]) # Define the model model = Sequential() model.add(Dense(1, input_dim=1, kernel_initializer='normal', activation='linear')) # Compile the model model.compile(optimizer='adam', loss='mean_squared_error') # Train the model model.fit(X, y, epochs=100, verbose=0) # Get the model parameters (weights and biases) weights = model.layers[0].get_weights() slope = weights[0][0][0] intercept = weights[1][0] print(f\"Slope: {slope}, Intercept: {intercept}\") Example-3 # import matplotlib.pyplot as plt import numpy as np import sklearn.datasets import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam from sklearn.model_selection import train_test_split # Generate a dataset and plot it np.random.seed(0) X, y = sklearn.datasets.make_moons(200, noise=0.20) # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Build and compile a Keras model def build_keras_model(hidden_dim): model = Sequential() model.add(Dense(hidden_dim, input_dim=2, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01))) model.add(Dense(2, activation='softmax')) model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model # Train the model def train_keras_model(hidden_dim): model = build_keras_model(hidden_dim) model.fit(X_train, y_train, epochs=2000, batch_size=32, verbose=0) return model # Plot decision boundary def plot_decision_boundary(model, X, y): x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = np.argmax(Z, axis=1) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.seismic) plt.show() # Build a model with a 3-dimensional hidden layer and plot the decision boundary model = train_keras_model(3) plot_decision_boundary(model, X, y) # Visualize models with different hidden layer sizes plt.figure(figsize=(16, 32)) hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50] for i, nn_hdim in enumerate(hidden_layer_dimensions): plt.subplot(5, 2, i+1) plt.title(f'Hidden Layer size {nn_hdim}') model = train_keras_model(nn_hdim) plot_decision_boundary(model, X, y) plt.show() Keras version # import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import MeanSquaredError # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1) y = y.reshape(-1, 1) # Reshape to match output dimensions # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Define the model model = Sequential() model.add(Dense(units=1, input_dim=X_train.shape[1], activation='linear')) # Compile the model model.compile(optimizer=SGD(learning_rate=0.01), loss=MeanSquaredError()) # Train the model history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() #Keras version # import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import MeanSquaredError # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) from tensorflow.keras.layers import Dense, Dropout # Define the model model = Sequential() model.add(Dense(units=128, input_dim=X_train.shape[1], activation='sigmoid')) # First hidden layer H1 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=64, activation='relu')) # Second hidden layer H2 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=32, activation='sigmoid')) # Third hidden layer H3 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=16, activation='relu')) # First hidden layer H4 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=8, activation='sigmoid')) # Second hidden layer H5 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=5, activation='relu')) # Third hidden layer H6 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=1, activation='linear')) # Output layer # Compile the model model.compile(optimizer=SGD(learning_rate=0.05), loss=MeanSquaredError()) model.summary() # Train the model history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) # Evaluate the model on test set y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Call the function mse, r2 = calculate_accuracy(y_test_inverse, y_pred) # Evaluate the model on train set y_pred_train = model.predict(X_train) y_pred_train = scaler_y.inverse_transform(y_pred_train) y_train_inverse = scaler_y.inverse_transform(y_train) # Call the function mse, r2 = calculate_accuracy(y_train_inverse, y_pred_train) from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.wrappers.scikit_learn import KerasRegressor from sklearn.model_selection import GridSearchCV from tensorflow.keras.wrappers.scikit_learn import KerasRegressor from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout from tensorflow.keras.optimizers import SGD # Define the model creation function def create_model(learning_rate=0.01, dropout_rate=0.0): model = Sequential() model.add(Dense(units=128, input_dim=X_train.shape[1], activation='sigmoid')) # First hidden layer H1 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=64, activation='relu')) # Second hidden layer H2 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=32, activation='sigmoid')) # Third hidden layer H3 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=16, activation='relu')) # First hidden layer H4 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=8, activation='sigmoid')) # Second hidden layer H5 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=5, activation='relu')) # Third hidden layer H6 #model.add(Dropout(0.2)) # Dropout layer with 20% rate optimizer = SGD(learning_rate=learning_rate) model.compile(optimizer=optimizer, loss='mean_squared_error') return model # Create the model using KerasRegressor model = KerasRegressor(build_fn=create_model, verbose=0) # Define the hyperparameters to tune param_grid = { 'learning_rate': [0.01, 0.05, 0.1], 'dropout_rate': [0.2, 0.5], 'batch_size': [16, 32], 'epochs': [100] } # Add early stopping and model checkpoint callbacks early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True) model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1) # Use grid search with callbacks grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3) grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint]) # Summarize the results print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\") # Summarize the results print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\") import os os.getcwd() from tensorflow.keras.models import load_model checkpoint_filepath = '/Users/pradmishra/Documents/Deep Learning Contents/DL5/best_model.h5' # Load the saved model checkpoint best_model = load_model(checkpoint_filepath) # Evaluate the loaded model on test data loss = best_model.evaluate(X_test, y_test) print(f'Loaded model loss on test data: {loss}') # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() Using Keras # # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) model = Sequential([ Dense(64, input_dim=X_train.shape[1], activation='relu'), Dense(32, activation='relu'), Dense(16, activation='relu'), Dense(1, activation='linear') ]) model.compile(optimizer='adam', loss=MeanSquaredError()) history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1) y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"Keras"},{"location":"AIML/DeepLearning/Keras.html#keras","text":"Keras is an open-source high-level Neural Network library, which is written in Python is capable enough to run on Theano, TensorFlow, or CNTK. It was developed by one of the Google engineers, Francois Chollet. It is made user-friendly, extensible, and modular for facilitating faster experimentation with deep neural networks. It not only supports Convolutional Networks and Recurrent Networks individually but also their combination. It cannot handle low-level computations, so it makes use of the Backend library to resolve it. The backend library act as a high-level API wrapper for the low-level API, which lets it run on TensorFlow, CNTK, or Theano.","title":"Keras"},{"location":"AIML/DeepLearning/Keras.html#what-makes-keras-special","text":"Focus on user experience has always been a major part of Keras. Large adoption in the industry. It is a multi backend and supports multi-platform, which helps all the encoders come together for coding. Research community present for Keras works amazingly with the production community. Easy to grasp all concepts. It supports fast prototyping. It seamlessly runs on CPU as well as GPU. It provides the freedom to design any architecture, which then later is utilized as an API for the project. It is really very simple to get started with. Easy production of models actually makes Keras special.","title":"What makes Keras special?"},{"location":"AIML/DeepLearning/Keras.html#how-keras-support-the-claim-of-being-multi-backend-and-multi-platform","text":"Keras can be developed in R as well as Python, such that the code can be run with TensorFlow, Theano, CNTK, or MXNet as per the requirement. Keras can be run on CPU, NVIDIA GPU, AMD GPU, TPU, etc. It ensures that producing models with Keras is really simple as it totally supports to run with TensorFlow serving, GPU acceleration (WebKeras, Keras.js), Android (TF, TF Lite), iOS (Native CoreML) and Raspberry Pi.","title":"How Keras support the claim of being multi-backend and multi-platform?"},{"location":"AIML/DeepLearning/Keras.html#keras-backend","text":"Keras being a model-level library helps in developing deep learning models by offering high-level building blocks. All the low-level computations such as products of Tensor, convolutions, etc. are not handled by Keras itself, rather they depend on a specialized tensor manipulation library that is well optimized to serve as a backend engine. Keras has managed it so perfectly that instead of incorporating one single library of tensor and performing operations related to that particular library, it offers plugging of different backend engines into Keras. Keras consist of three backend engines, which are as follows: TensorFlow TensorFlow is a Google product, which is one of the most famous deep learning tools widely used in the research area of machine learning and deep neural network. It came into the market on 9th November 2015 under the Apache License 2.0. It is built in such a way that it can easily run on multiple CPUs and GPUs as well as on mobile operating systems. It consists of various wrappers in distinct languages such as Java, C++, or Python. Theano Theano was developed at the University of Montreal, Quebec, Canada, by the MILA group. It is an open-source python library that is widely used for performing mathematical operations on multi-dimensional arrays by incorporating scipy and numpy. It utilizes GPUs for faster computation and efficiently computes the gradients by building symbolic graphs automatically. It has come out to be very suitable for unstable expressions, as it first observes them numerically and then computes them with more stable algorithms. CNTK Microsoft Cognitive Toolkit is deep learning's open-source framework. It consists of all the basic building blocks, which are required to form a neural network. The models are trained using C++ or Python, but it incorporates C# or Java to load the model for making predictions.","title":"Keras Backend"},{"location":"AIML/DeepLearning/Keras.html#advantages-of-keras","text":"Keras encompasses the following advantages, which are as follows: It is very easy to understand and incorporate the faster deployment of network models. It has huge community support in the market as most of the AI companies are keen on using it. It supports multi backend, which means you can use any one of them among TensorFlow, CNTK, and Theano with Keras as a backend according to your requirement. Since it has an easy deployment, it also holds support for cross-platform. Following are the devices on which Keras can be deployed: iOS with CoreML Android with TensorFlow Android Web browser with .js support Cloud engine Raspberry pi It supports Data parallelism, which means Keras can be trained on multiple GPU's at an instance for speeding up the training time and processing a huge amount of data.","title":"Advantages of Keras"},{"location":"AIML/DeepLearning/Keras.html#disadvantages-of-keras","text":"The only disadvantage is that Keras has its own pre-configured layers, and if you want to create an abstract layer, it won't let you because it cannot handle low-level APIs. It only supports high-level API running on the top of the backend engine (TensorFlow, Theano, and CNTK). #Based on the extracted code from the notebook, I'll transform the script to use Keras for defining and training a neural network that learns the AND and OR logic gates. I'll ensure that the neural network is correctly set up and that the previous errors and syntax issues are fixed. #Here is the modified script using Keras: import numpy as np import pandas as pd from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Define the AND dataset AND = pd.DataFrame({'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1], 'y': [0, 0, 0, 1]}) inputs_and = AND[['x1', 'x2']] targets_and = AND['y'] # Define the OR dataset OR = pd.DataFrame({'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1], 'y': [0, 1, 1, 1]}) inputs_or = OR[['x1', 'x2']] targets_or = OR['y'] # Build the model def build_model(): model = Sequential() model.add(Dense(2, input_dim=2, activation='relu')) # 2 neurons for input layer model.add(Dense(1, activation='sigmoid')) # 1 neuron for output layer model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model # Train and evaluate the model for AND logic gate model_and = build_model() model_and.fit(inputs_and, targets_and, epochs=100, verbose=0) loss_and, accuracy_and = model_and.evaluate(inputs_and, targets_and) print(f'AND Gate - Loss: {loss_and}, Accuracy: {accuracy_and}') # Train and evaluate the model for OR logic gate model_or = build_model() model_or.fit(inputs_or, targets_or, epochs=100, verbose=0) loss_or, accuracy_or = model_or.evaluate(inputs_or, targets_or) print(f'OR Gate - Loss: {loss_or}, Accuracy: {accuracy_or}') # Predict using the trained models predictions_and = model_and.predict(inputs_and) predictions_or = model_or.predict(inputs_or) print(\"AND Gate Predictions:\") print(np.round(predictions_and)) print(\"OR Gate Predictions:\") print(np.round(predictions_or)) ### Explanation: #- **Data Preparation**: The AND and OR datasets are defined as pandas DataFrames. #- **Model Building**: A function `build_model` is created to define the neural network with Keras. The network consists of an input layer with 2 neurons (corresponding to the 2 input features) and an output layer with 1 neuron for binary classification. #- **Training and Evaluation**: The model is trained separately for the AND and OR datasets. The training process is silent (verbose=0), and after training, the model's loss and accuracy are printed. #- **Predictions**: The trained models are used to predict the outputs for the AND and OR datasets, and the predictions are printed. #This script ensures proper use of Keras for defining, training, and evaluating neural networks for the given logic gate tasks.","title":"Disadvantages of Keras"},{"location":"AIML/DeepLearning/Keras.html#example-2","text":"import numpy as np # Sample data for linear regression X = np.array([[1], [2], [3], [4], [5]]) y = np.array([1, 3, 3, 2, 5]) # Linear regression with numpy A = np.vstack([X.T, np.ones(len(X))]).T m, c = np.linalg.lstsq(A, y, rcond=None)[0] print(f\"Slope: {m}, Intercept: {c}\") import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Sample data for linear regression X = np.array([[1], [2], [3], [4], [5]]) y = np.array([1, 3, 3, 2, 5]) # Define the model model = Sequential() model.add(Dense(1, input_dim=1, kernel_initializer='normal', activation='linear')) # Compile the model model.compile(optimizer='adam', loss='mean_squared_error') # Train the model model.fit(X, y, epochs=100, verbose=0) # Get the model parameters (weights and biases) weights = model.layers[0].get_weights() slope = weights[0][0][0] intercept = weights[1][0] print(f\"Slope: {slope}, Intercept: {intercept}\")","title":"Example-2"},{"location":"AIML/DeepLearning/Keras.html#example-3","text":"import matplotlib.pyplot as plt import numpy as np import sklearn.datasets import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam from sklearn.model_selection import train_test_split # Generate a dataset and plot it np.random.seed(0) X, y = sklearn.datasets.make_moons(200, noise=0.20) # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Build and compile a Keras model def build_keras_model(hidden_dim): model = Sequential() model.add(Dense(hidden_dim, input_dim=2, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01))) model.add(Dense(2, activation='softmax')) model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model # Train the model def train_keras_model(hidden_dim): model = build_keras_model(hidden_dim) model.fit(X_train, y_train, epochs=2000, batch_size=32, verbose=0) return model # Plot decision boundary def plot_decision_boundary(model, X, y): x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = np.argmax(Z, axis=1) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.seismic) plt.show() # Build a model with a 3-dimensional hidden layer and plot the decision boundary model = train_keras_model(3) plot_decision_boundary(model, X, y) # Visualize models with different hidden layer sizes plt.figure(figsize=(16, 32)) hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50] for i, nn_hdim in enumerate(hidden_layer_dimensions): plt.subplot(5, 2, i+1) plt.title(f'Hidden Layer size {nn_hdim}') model = train_keras_model(nn_hdim) plot_decision_boundary(model, X, y) plt.show()","title":"Example-3"},{"location":"AIML/DeepLearning/Keras.html#keras-version","text":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import MeanSquaredError # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1) y = y.reshape(-1, 1) # Reshape to match output dimensions # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Define the model model = Sequential() model.add(Dense(units=1, input_dim=X_train.shape[1], activation='linear')) # Compile the model model.compile(optimizer=SGD(learning_rate=0.01), loss=MeanSquaredError()) # Train the model history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"Keras version"},{"location":"AIML/DeepLearning/Keras.html#keras-version_1","text":"import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import MeanSquaredError # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) from tensorflow.keras.layers import Dense, Dropout # Define the model model = Sequential() model.add(Dense(units=128, input_dim=X_train.shape[1], activation='sigmoid')) # First hidden layer H1 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=64, activation='relu')) # Second hidden layer H2 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=32, activation='sigmoid')) # Third hidden layer H3 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=16, activation='relu')) # First hidden layer H4 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=8, activation='sigmoid')) # Second hidden layer H5 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=5, activation='relu')) # Third hidden layer H6 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=1, activation='linear')) # Output layer # Compile the model model.compile(optimizer=SGD(learning_rate=0.05), loss=MeanSquaredError()) model.summary() # Train the model history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) # Evaluate the model on test set y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Call the function mse, r2 = calculate_accuracy(y_test_inverse, y_pred) # Evaluate the model on train set y_pred_train = model.predict(X_train) y_pred_train = scaler_y.inverse_transform(y_pred_train) y_train_inverse = scaler_y.inverse_transform(y_train) # Call the function mse, r2 = calculate_accuracy(y_train_inverse, y_pred_train) from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.wrappers.scikit_learn import KerasRegressor from sklearn.model_selection import GridSearchCV from tensorflow.keras.wrappers.scikit_learn import KerasRegressor from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout from tensorflow.keras.optimizers import SGD # Define the model creation function def create_model(learning_rate=0.01, dropout_rate=0.0): model = Sequential() model.add(Dense(units=128, input_dim=X_train.shape[1], activation='sigmoid')) # First hidden layer H1 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=64, activation='relu')) # Second hidden layer H2 model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=32, activation='sigmoid')) # Third hidden layer H3 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=16, activation='relu')) # First hidden layer H4 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=8, activation='sigmoid')) # Second hidden layer H5 #model.add(Dropout(0.2)) # Dropout layer with 20% rate model.add(Dense(units=5, activation='relu')) # Third hidden layer H6 #model.add(Dropout(0.2)) # Dropout layer with 20% rate optimizer = SGD(learning_rate=learning_rate) model.compile(optimizer=optimizer, loss='mean_squared_error') return model # Create the model using KerasRegressor model = KerasRegressor(build_fn=create_model, verbose=0) # Define the hyperparameters to tune param_grid = { 'learning_rate': [0.01, 0.05, 0.1], 'dropout_rate': [0.2, 0.5], 'batch_size': [16, 32], 'epochs': [100] } # Add early stopping and model checkpoint callbacks early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True) model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1) # Use grid search with callbacks grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3) grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint]) # Summarize the results print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\") # Summarize the results print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\") import os os.getcwd() from tensorflow.keras.models import load_model checkpoint_filepath = '/Users/pradmishra/Documents/Deep Learning Contents/DL5/best_model.h5' # Load the saved model checkpoint best_model = load_model(checkpoint_filepath) # Evaluate the loaded model on test data loss = best_model.evaluate(X_test, y_test) print(f'Loaded model loss on test data: {loss}') # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"#Keras version"},{"location":"AIML/DeepLearning/Keras.html#using-keras","text":"# Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) model = Sequential([ Dense(64, input_dim=X_train.shape[1], activation='relu'), Dense(32, activation='relu'), Dense(16, activation='relu'), Dense(1, activation='linear') ]) model.compile(optimizer='adam', loss=MeanSquaredError()) history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1) y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"Using Keras"},{"location":"AIML/DeepLearning/LSTM.html","text":"","title":"LSTM"},{"location":"AIML/DeepLearning/Pytorch.html","text":"Linear Regression DL using PT KS and TF # import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression # Generate synthetic data X, y = make_regression(n_samples=10000, n_features=10, noise=0.1) y = y.reshape(-1, 1) # Reshape to match output dimensions # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape X_train[0:5] # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Convert to PyTorch tensors X_train = torch.tensor(X_train, dtype=torch.float32) X_test = torch.tensor(X_test, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.float32) class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x) # Instantiate the model input_dim = X_train.shape[1] output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.05) num_epochs = 1000 for epoch in range(num_epochs): model.train() # Forward pass outputs = model(X_train) loss = criterion(outputs, y_train) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 100 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') model.eval() model.eval() with torch.no_grad(): predicted = model(X_test) predicted = scaler_y.inverse_transform(predicted.numpy()) actual = scaler_y.inverse_transform(y_test.numpy()) # Plot predictions vs actual plt.scatter(actual, predicted, color='blue') plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() Boston Housing Price Prediction # import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import pandas as pd boston = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\") boston.info() boston.head() y = boston.pop(\"medv\") X = boston y = np.array(y) y y.reshape(-1, 1) Load the dataset # y = y.reshape(-1, 1) # Reshape to match output dimensions Split data into training and test sets # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) Standardize the data # scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) Convert to PyTorch tensors # X_train = torch.tensor(X_train, dtype=torch.float32) X_test = torch.tensor(X_test, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.float32) X_train.shape, X_test.shape, y_train.shape, y_test.shape class LinearRegressionModel(nn.Module): def init (self, input_dim, output_dim): super(LinearRegressionModel, self). init () self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x) Instantiate the model # input_dim = X_train.shape[1] output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.05) num_epochs = 5000 for epoch in range(num_epochs): model.train() # Forward pass outputs = model(X_train) loss = criterion(outputs, y_train) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 100 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') model.eval() with torch.no_grad(): predicted = model(X_test) predicted = scaler_y.inverse_transform(predicted.numpy()) actual = scaler_y.inverse_transform(y_test.numpy()) Plot predictions vs actual # plt.scatter(actual, predicted, color='blue') plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() from sklearn.metrics import mean_squared_error, r2_score def calculate_accuracy(actual, predicted): # Calculate Mean Squared Error mse = mean_squared_error(actual, predicted) # Calculate R\u00b2 score r2 = r2_score(actual, predicted) print(f'Mean Squared Error (MSE): {mse:.4f}') print(f'R\u00b2 Score: {r2:.4f}') return mse, r2 Call the function # mse, r2 = calculate_accuracy(actual, predicted) ```","title":"Pytorch"},{"location":"AIML/DeepLearning/Pytorch.html#linear-regression-dl-using-pt-ks-and-tf","text":"import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression # Generate synthetic data X, y = make_regression(n_samples=10000, n_features=10, noise=0.1) y = y.reshape(-1, 1) # Reshape to match output dimensions # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape X_train[0:5] # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Convert to PyTorch tensors X_train = torch.tensor(X_train, dtype=torch.float32) X_test = torch.tensor(X_test, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.float32) class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x) # Instantiate the model input_dim = X_train.shape[1] output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.05) num_epochs = 1000 for epoch in range(num_epochs): model.train() # Forward pass outputs = model(X_train) loss = criterion(outputs, y_train) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 100 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') model.eval() model.eval() with torch.no_grad(): predicted = model(X_test) predicted = scaler_y.inverse_transform(predicted.numpy()) actual = scaler_y.inverse_transform(y_test.numpy()) # Plot predictions vs actual plt.scatter(actual, predicted, color='blue') plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"Linear Regression DL using PT KS and TF"},{"location":"AIML/DeepLearning/Pytorch.html#boston-housing-price-prediction","text":"import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import pandas as pd boston = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\") boston.info() boston.head() y = boston.pop(\"medv\") X = boston y = np.array(y) y y.reshape(-1, 1)","title":"Boston Housing Price Prediction"},{"location":"AIML/DeepLearning/Pytorch.html#load-the-dataset","text":"y = y.reshape(-1, 1) # Reshape to match output dimensions","title":"Load the dataset"},{"location":"AIML/DeepLearning/Pytorch.html#split-data-into-training-and-test-sets","text":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","title":"Split data into training and test sets"},{"location":"AIML/DeepLearning/Pytorch.html#standardize-the-data","text":"scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test)","title":"Standardize the data"},{"location":"AIML/DeepLearning/Pytorch.html#convert-to-pytorch-tensors","text":"X_train = torch.tensor(X_train, dtype=torch.float32) X_test = torch.tensor(X_test, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.float32) X_train.shape, X_test.shape, y_train.shape, y_test.shape class LinearRegressionModel(nn.Module): def init (self, input_dim, output_dim): super(LinearRegressionModel, self). init () self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x)","title":"Convert to PyTorch tensors"},{"location":"AIML/DeepLearning/Pytorch.html#instantiate-the-model","text":"input_dim = X_train.shape[1] output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.05) num_epochs = 5000 for epoch in range(num_epochs): model.train() # Forward pass outputs = model(X_train) loss = criterion(outputs, y_train) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 100 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') model.eval() with torch.no_grad(): predicted = model(X_test) predicted = scaler_y.inverse_transform(predicted.numpy()) actual = scaler_y.inverse_transform(y_test.numpy())","title":"Instantiate the model"},{"location":"AIML/DeepLearning/Pytorch.html#plot-predictions-vs-actual","text":"plt.scatter(actual, predicted, color='blue') plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() from sklearn.metrics import mean_squared_error, r2_score def calculate_accuracy(actual, predicted): # Calculate Mean Squared Error mse = mean_squared_error(actual, predicted) # Calculate R\u00b2 score r2 = r2_score(actual, predicted) print(f'Mean Squared Error (MSE): {mse:.4f}') print(f'R\u00b2 Score: {r2:.4f}') return mse, r2","title":"Plot predictions vs actual"},{"location":"AIML/DeepLearning/Pytorch.html#call-the-function","text":"mse, r2 = calculate_accuracy(actual, predicted) ```","title":"Call the function"},{"location":"AIML/DeepLearning/Reinforcement-Learning.html","text":"","title":"Reinforcement Learning"},{"location":"AIML/DeepLearning/Tensorflow.html","text":"TF Version of the code # import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import MeanSquaredError # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1) y = y.reshape(-1, 1) # Reshape to match output dimensions # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Define the model model = Sequential() model.add(Dense(units=1, input_dim=X_train.shape[1], activation='linear')) # Compile the model model.compile(optimizer=SGD(learning_rate=0.01), loss=MeanSquaredError()) # Train the model history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() Tensorflow version of the code # # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Convert to TensorFlow tensors X_train = tf.constant(X_train, dtype=tf.float32) X_test = tf.constant(X_test, dtype=tf.float32) y_train = tf.constant(y_train, dtype=tf.float32) y_test = tf.constant(y_test, dtype=tf.float32) model = tf.keras.Sequential([ tf.keras.layers.Dense(units=1, input_dim=X_train.shape[1], activation='linear') ]) model.compile(optimizer='sgd', loss='mean_squared_error') history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() # making it deep using TF # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Convert to TensorFlow tensors X_train = tf.constant(X_train, dtype=tf.float32) X_test = tf.constant(X_test, dtype=tf.float32) y_train = tf.constant(y_train, dtype=tf.float32) y_test = tf.constant(y_test, dtype=tf.float32) # Define the model model = tf.keras.Sequential([ tf.keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'), tf.keras.layers.Dense(32, activation='relu'), tf.keras.layers.Dense(16, activation='relu'), tf.keras.layers.Dense(1, activation='linear') ]) model.summary() # Compile the model model.compile(optimizer='adam', loss='mean_squared_error') # Train the model history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1) # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() Refined Intro to TF # import tensorflow as tf string = tf.Variable(\"this is a string\", tf.string) string string = tf.Variable(\"this is a string\", tf.string) number = tf.Variable(324, tf.int16) floating = tf.Variable(3.567, tf.float32) 11*8*8*8*8*8*8*4 rank1_tensor = tf.Variable([\"Test\"], tf.string) rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string) tf.rank(rank1_tensor) #tf.shape(rank2_tensor) t1 = tf.zeros([1,2,3]) t2 = tf.reshape(t1, [2,3,1]) t3 = tf.reshape(t2, [3,-1]) ### Refined Code for TensorFlow 2.x import tensorflow as tf # Tensor Creation Examples string = tf.Variable(\"this is a string\", tf.string) number = tf.Variable(324, tf.int16) floating = tf.Variable(3.567, tf.float32) # Tensor Rank Examples rank1_tensor = tf.Variable([\"Test\"], tf.string) rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string) # Tensor Shape Examples print(tf.rank(rank2_tensor)) print(tf.shape(rank2_tensor)) # Reshaping Tensors t1 = tf.zeros([1,2,3]) t2 = tf.reshape(t1, [2,3,1]) t3 = tf.reshape(t2, [3,-1]) # Placeholder Example (TensorFlow 2.x does not have placeholders) @tf.function def func(x): return x output = func(tf.constant('Hello World')) print(output) # Placeholder with feed_dict Example (Converted to TensorFlow 2.x) @tf.function def func(x, y, z): return x, y, z output_x, output_y, output_z = func(tf.constant('Test String'), tf.constant(123), tf.constant(45.67)) print(output_x) print(output_y) print(output_z) # TensorFlow Math Functions x = tf.add(5, 2) y = tf.subtract(10, 4) z = tf.multiply(2, 5) # Matrix Multiplication x = tf.constant([[1,2], [3,4]]) y = tf.constant([[1,1], [1,1]]) z = tf.matmul(x, y) # Softmax Function x = tf.constant([2.0, 1.0, 0.1]) print(tf.nn.softmax(x)) # Cross Entropy Example softmax_data = [0.7, 0.2, 0.1] one_hot_data = [1.0, 0.0, 0.0] softmax = tf.constant(softmax_data) one_hot = tf.constant(one_hot_data) cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.math.log(softmax))) print(cross_entropy) import tensorflow as tf @tf.function def func(x): return x output = func(tf.constant('Hello World')) print(output) import tensorflow as tf # Tensor Creation Examples string = tf.Variable(\"this is a string\", tf.string) number = tf.Variable(324, tf.int16) floating = tf.Variable(3.567, tf.float32) # Tensor Rank Examples rank1_tensor = tf.Variable([\"Test\"], tf.string) rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string) # Tensor Shape Examples print(tf.rank(rank2_tensor)) print(tf.shape(rank2_tensor)) # Reshaping Tensors t1 = tf.zeros([1,2,3]) t2 = tf.reshape(t1, [2,3,1]) t3 = tf.reshape(t2, [3,-1]) # Placeholder Example (TensorFlow 2.x does not have placeholders) @tf.function def func(x): return x output = func(tf.constant('Hello World')) print(output) # Placeholder with feed_dict Example (Converted to TensorFlow 2.x) @tf.function def func(x, y, z): return x, y, z output_x, output_y, output_z = func(tf.constant('Test String'), tf.constant(123), tf.constant(45.67)) print(output_x) print(output_y) print(output_z) # TensorFlow Math Functions x = tf.add(5, 2) y = tf.subtract(10, 4) z = tf.multiply(2, 5) # Matrix Multiplication x = tf.constant([[1,2], [3,4]]) y = tf.constant([[1,1], [1,1]]) z = tf.matmul(x, y) # Softmax Function x = tf.constant([2.0, 1.0, 0.1]) print(tf.nn.softmax(x)) # Cross Entropy Example softmax_data = [0.7, 0.2, 0.1] one_hot_data = [1.0, 0.0, 0.0] softmax = tf.constant(softmax_data) one_hot = tf.constant(one_hot_data) cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.math.log(softmax))) print(cross_entropy) TF regression # Basic regression: Predict fuel efficiency # In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture). This tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight. # Use seaborn for pairplot. !pip install -q seaborn import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns # Make NumPy printouts easier to read. np.set_printoptions(precision=3, suppress=True) import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers print(tf.__version__) The Auto MPG dataset # The dataset is available from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ Get the data First download and import the dataset using pandas: url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data' column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin'] raw_dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True) dataset = raw_dataset.copy() dataset.tail() Clean the data The dataset contains a few unknown values: dataset.isna().sum() Drop those rows to keep this initial tutorial simple: dataset = dataset.dropna() The \"Origin\" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies. Note: You can set up the tf.keras.Model to do this kind of transformation for you but that's beyond the scope of this tutorial. dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'}) dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='') dataset.tail() Split the data into training and test sets: Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models. train_dataset = dataset.sample(frac=0.8, random_state=0) test_dataset = dataset.drop(train_dataset.index) Inspect the data Review the joint distribution of a few pairs of columns from the training set. The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other. sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde') train_dataset.describe().transpose() Split features from labels Separate the target value\u2014the \"label\"\u2014from the features. This label is the value that you will train the model to predict. train_features = train_dataset.copy() test_features = test_dataset.copy() train_labels = train_features.pop('MPG') test_labels = test_features.pop('MPG') Normalization In the table of statistics it's easy to see how different the ranges of each feature are: train_dataset.describe().transpose()[['mean', 'std']] It is good practice to normalize features that use different scales and ranges. One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs. Although a model might converge without feature normalization, normalization makes training much more stable. Note: There is no advantage to normalizing the one-hot features\u2014it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the Working with preprocessing layers guide and the Classify structured data using Keras preprocessing layers tutorial. The Normalization layer The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model. The first step is to create the layer: normalizer = tf.keras.layers.Normalization(axis=-1) Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt: normalizer.adapt(np.array(train_features)) Calculate the mean and variance, and store them in the layer: print(normalizer.mean.numpy()) When the layer is called, it returns the input data, with each feature independently normalized: first = np.array(train_features[:1]) with np.printoptions(precision=2, suppress=True): print('First example:', first) print() print('Normalized:', normalizer(first).numpy()) Linear regression # Before building a deep neural network model, start with linear regression using one and several variables. Linear regression with one variable Begin with a single-variable linear regression to predict 'MPG' from 'Horsepower'. Training a model with tf.keras typically starts by defining the model architecture. Use a tf.keras.Sequential model, which represents a sequence of steps. There are two steps in your single-variable linear regression model: Normalize the 'Horsepower' input features using the tf.keras.layers.Normalization preprocessing layer. Apply a linear transformation () to produce 1 output using a linear layer (tf.keras.layers.Dense). The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time. First, create a NumPy array made of the 'Horsepower' features. Then, instantiate the tf.keras.layers.Normalization and fit its state to the horsepower data: horsepower = np.array(train_features['Horsepower']) horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None) horsepower_normalizer.adapt(horsepower) Build the Keras Sequential model: horsepower_model = tf.keras.Sequential([ horsepower_normalizer, layers.Dense(units=1) ]) horsepower_model.summary() This model will predict 'MPG' from 'Horsepower' Run the untrained model on the first 10 'Horsepower' values. The output won't be good, but notice that it has the expected shape of (10, 1): Once the model is built, configure the training procedure using the Keras Model.compile method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the tf.keras.optimizers.Adam). horsepower_model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error') Use Keras Model.fit to execute the training for 100 epochs: %%time history = horsepower_model.fit( train_features['Horsepower'], train_labels, epochs=100, # Suppress logging. verbose=0, # Calculate validation results on 20% of the training data. validation_split = 0.2) Visualize the model's training progress using the stats stored in the history object: hist = pd.DataFrame(history.history) hist['epoch'] = history.epoch hist.tail() def plot_loss(history): plt.plot(history.history['loss'], label='loss') plt.plot(history.history['val_loss'], label='val_loss') plt.ylim([0, 10]) plt.xlabel('Epoch') plt.ylabel('Error [MPG]') plt.legend() plt.grid(True) plot_loss(history) Collect the results on the test set for later: test_results = {} test_results['horsepower_model'] = horsepower_model.evaluate( test_features['Horsepower'], test_labels, verbose=0) x = tf.linspace(0.0, 250, 251) y = horsepower_model.predict(x) def plot_horsepower(x, y): plt.scatter(train_features['Horsepower'], train_labels, label='Data') plt.plot(x, y, color='k', label='Predictions') plt.xlabel('Horsepower') plt.ylabel('MPG') plt.legend() Since this is a single variable regression, it's easy to view the model's predictions as a function of the input: def plot_horsepower(x, y): plt.scatter(train_features['Horsepower'], train_labels, label='Data') plt.plot(x, y, color='k', label='Predictions') plt.xlabel('Horsepower') plt.ylabel('MPG') plt.legend() plot_horsepower(x, y) Linear regression with multiple inputs # You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same y = mx + b except that m is a matrix and x is a vector. Create a two-step Keras Sequential model again with the first layer being normalizer (tf.keras.layers.Normalization(axis=-1)) you defined earlier and adapted to the whole dataset: linear_model = tf.keras.Sequential([ normalizer, layers.Dense(units=1) ]) When you call Model.predict on a batch of inputs, it produces units=1 outputs for each example: linear_model.predict(train_features[:10]) When you call the model, its weight matrices will be built\u2014check that the kernel weights (the m in y = mx + b) have a shape (9, 1) linear_model.layers[1].kernel Configure the model with Keras Model.compile and train with Model.fit for 100 epochs: linear_model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error') %%time history = linear_model.fit( train_features, train_labels, epochs=100, # Suppress logging. verbose=0, # Calculate validation results on 20% of the training data. validation_split = 0.2) Using all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input: plot_loss(history)","title":"Tensorflow"},{"location":"AIML/DeepLearning/Tensorflow.html#tf-version-of-the-code","text":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import MeanSquaredError # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1) y = y.reshape(-1, 1) # Reshape to match output dimensions # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Define the model model = Sequential() model.add(Dense(units=1, input_dim=X_train.shape[1], activation='linear')) # Compile the model model.compile(optimizer=SGD(learning_rate=0.01), loss=MeanSquaredError()) # Train the model history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"TF Version of the code"},{"location":"AIML/DeepLearning/Tensorflow.html#tensorflow-version-of-the-code","text":"# Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Convert to TensorFlow tensors X_train = tf.constant(X_train, dtype=tf.float32) X_test = tf.constant(X_test, dtype=tf.float32) y_train = tf.constant(y_train, dtype=tf.float32) y_test = tf.constant(y_test, dtype=tf.float32) model = tf.keras.Sequential([ tf.keras.layers.Dense(units=1, input_dim=X_train.shape[1], activation='linear') ]) model.compile(optimizer='sgd', loss='mean_squared_error') history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1) y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show() # making it deep using TF # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler() scaler_y = StandardScaler() X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test) # Convert to TensorFlow tensors X_train = tf.constant(X_train, dtype=tf.float32) X_test = tf.constant(X_test, dtype=tf.float32) y_train = tf.constant(y_train, dtype=tf.float32) y_test = tf.constant(y_test, dtype=tf.float32) # Define the model model = tf.keras.Sequential([ tf.keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'), tf.keras.layers.Dense(32, activation='relu'), tf.keras.layers.Dense(16, activation='relu'), tf.keras.layers.Dense(1, activation='linear') ]) model.summary() # Compile the model model.compile(optimizer='adam', loss='mean_squared_error') # Train the model history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1) # Evaluate the model y_pred = model.predict(X_test) y_pred = scaler_y.inverse_transform(y_pred) y_test_inverse = scaler_y.inverse_transform(y_test) # Plot predictions vs actual plt.scatter(y_test_inverse, y_pred, color='blue') plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()","title":"Tensorflow version of the code"},{"location":"AIML/DeepLearning/Tensorflow.html#refined-intro-to-tf","text":"import tensorflow as tf string = tf.Variable(\"this is a string\", tf.string) string string = tf.Variable(\"this is a string\", tf.string) number = tf.Variable(324, tf.int16) floating = tf.Variable(3.567, tf.float32) 11*8*8*8*8*8*8*4 rank1_tensor = tf.Variable([\"Test\"], tf.string) rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string) tf.rank(rank1_tensor) #tf.shape(rank2_tensor) t1 = tf.zeros([1,2,3]) t2 = tf.reshape(t1, [2,3,1]) t3 = tf.reshape(t2, [3,-1]) ### Refined Code for TensorFlow 2.x import tensorflow as tf # Tensor Creation Examples string = tf.Variable(\"this is a string\", tf.string) number = tf.Variable(324, tf.int16) floating = tf.Variable(3.567, tf.float32) # Tensor Rank Examples rank1_tensor = tf.Variable([\"Test\"], tf.string) rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string) # Tensor Shape Examples print(tf.rank(rank2_tensor)) print(tf.shape(rank2_tensor)) # Reshaping Tensors t1 = tf.zeros([1,2,3]) t2 = tf.reshape(t1, [2,3,1]) t3 = tf.reshape(t2, [3,-1]) # Placeholder Example (TensorFlow 2.x does not have placeholders) @tf.function def func(x): return x output = func(tf.constant('Hello World')) print(output) # Placeholder with feed_dict Example (Converted to TensorFlow 2.x) @tf.function def func(x, y, z): return x, y, z output_x, output_y, output_z = func(tf.constant('Test String'), tf.constant(123), tf.constant(45.67)) print(output_x) print(output_y) print(output_z) # TensorFlow Math Functions x = tf.add(5, 2) y = tf.subtract(10, 4) z = tf.multiply(2, 5) # Matrix Multiplication x = tf.constant([[1,2], [3,4]]) y = tf.constant([[1,1], [1,1]]) z = tf.matmul(x, y) # Softmax Function x = tf.constant([2.0, 1.0, 0.1]) print(tf.nn.softmax(x)) # Cross Entropy Example softmax_data = [0.7, 0.2, 0.1] one_hot_data = [1.0, 0.0, 0.0] softmax = tf.constant(softmax_data) one_hot = tf.constant(one_hot_data) cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.math.log(softmax))) print(cross_entropy) import tensorflow as tf @tf.function def func(x): return x output = func(tf.constant('Hello World')) print(output) import tensorflow as tf # Tensor Creation Examples string = tf.Variable(\"this is a string\", tf.string) number = tf.Variable(324, tf.int16) floating = tf.Variable(3.567, tf.float32) # Tensor Rank Examples rank1_tensor = tf.Variable([\"Test\"], tf.string) rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string) # Tensor Shape Examples print(tf.rank(rank2_tensor)) print(tf.shape(rank2_tensor)) # Reshaping Tensors t1 = tf.zeros([1,2,3]) t2 = tf.reshape(t1, [2,3,1]) t3 = tf.reshape(t2, [3,-1]) # Placeholder Example (TensorFlow 2.x does not have placeholders) @tf.function def func(x): return x output = func(tf.constant('Hello World')) print(output) # Placeholder with feed_dict Example (Converted to TensorFlow 2.x) @tf.function def func(x, y, z): return x, y, z output_x, output_y, output_z = func(tf.constant('Test String'), tf.constant(123), tf.constant(45.67)) print(output_x) print(output_y) print(output_z) # TensorFlow Math Functions x = tf.add(5, 2) y = tf.subtract(10, 4) z = tf.multiply(2, 5) # Matrix Multiplication x = tf.constant([[1,2], [3,4]]) y = tf.constant([[1,1], [1,1]]) z = tf.matmul(x, y) # Softmax Function x = tf.constant([2.0, 1.0, 0.1]) print(tf.nn.softmax(x)) # Cross Entropy Example softmax_data = [0.7, 0.2, 0.1] one_hot_data = [1.0, 0.0, 0.0] softmax = tf.constant(softmax_data) one_hot = tf.constant(one_hot_data) cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.math.log(softmax))) print(cross_entropy)","title":"Refined Intro to TF"},{"location":"AIML/DeepLearning/Tensorflow.html#tf-regression","text":"","title":"TF regression"},{"location":"AIML/DeepLearning/Tensorflow.html#basic-regression-predict-fuel-efficiency","text":"In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture). This tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight. # Use seaborn for pairplot. !pip install -q seaborn import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns # Make NumPy printouts easier to read. np.set_printoptions(precision=3, suppress=True) import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers print(tf.__version__)","title":"Basic regression: Predict fuel efficiency"},{"location":"AIML/DeepLearning/Tensorflow.html#the-auto-mpg-dataset","text":"The dataset is available from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ Get the data First download and import the dataset using pandas: url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data' column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin'] raw_dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True) dataset = raw_dataset.copy() dataset.tail() Clean the data The dataset contains a few unknown values: dataset.isna().sum() Drop those rows to keep this initial tutorial simple: dataset = dataset.dropna() The \"Origin\" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies. Note: You can set up the tf.keras.Model to do this kind of transformation for you but that's beyond the scope of this tutorial. dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'}) dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='') dataset.tail() Split the data into training and test sets: Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models. train_dataset = dataset.sample(frac=0.8, random_state=0) test_dataset = dataset.drop(train_dataset.index) Inspect the data Review the joint distribution of a few pairs of columns from the training set. The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other. sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde') train_dataset.describe().transpose() Split features from labels Separate the target value\u2014the \"label\"\u2014from the features. This label is the value that you will train the model to predict. train_features = train_dataset.copy() test_features = test_dataset.copy() train_labels = train_features.pop('MPG') test_labels = test_features.pop('MPG') Normalization In the table of statistics it's easy to see how different the ranges of each feature are: train_dataset.describe().transpose()[['mean', 'std']] It is good practice to normalize features that use different scales and ranges. One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs. Although a model might converge without feature normalization, normalization makes training much more stable. Note: There is no advantage to normalizing the one-hot features\u2014it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the Working with preprocessing layers guide and the Classify structured data using Keras preprocessing layers tutorial. The Normalization layer The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model. The first step is to create the layer: normalizer = tf.keras.layers.Normalization(axis=-1) Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt: normalizer.adapt(np.array(train_features)) Calculate the mean and variance, and store them in the layer: print(normalizer.mean.numpy()) When the layer is called, it returns the input data, with each feature independently normalized: first = np.array(train_features[:1]) with np.printoptions(precision=2, suppress=True): print('First example:', first) print() print('Normalized:', normalizer(first).numpy())","title":"The Auto MPG dataset"},{"location":"AIML/DeepLearning/Tensorflow.html#linear-regression","text":"Before building a deep neural network model, start with linear regression using one and several variables. Linear regression with one variable Begin with a single-variable linear regression to predict 'MPG' from 'Horsepower'. Training a model with tf.keras typically starts by defining the model architecture. Use a tf.keras.Sequential model, which represents a sequence of steps. There are two steps in your single-variable linear regression model: Normalize the 'Horsepower' input features using the tf.keras.layers.Normalization preprocessing layer. Apply a linear transformation () to produce 1 output using a linear layer (tf.keras.layers.Dense). The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time. First, create a NumPy array made of the 'Horsepower' features. Then, instantiate the tf.keras.layers.Normalization and fit its state to the horsepower data: horsepower = np.array(train_features['Horsepower']) horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None) horsepower_normalizer.adapt(horsepower) Build the Keras Sequential model: horsepower_model = tf.keras.Sequential([ horsepower_normalizer, layers.Dense(units=1) ]) horsepower_model.summary() This model will predict 'MPG' from 'Horsepower' Run the untrained model on the first 10 'Horsepower' values. The output won't be good, but notice that it has the expected shape of (10, 1): Once the model is built, configure the training procedure using the Keras Model.compile method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the tf.keras.optimizers.Adam). horsepower_model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error') Use Keras Model.fit to execute the training for 100 epochs: %%time history = horsepower_model.fit( train_features['Horsepower'], train_labels, epochs=100, # Suppress logging. verbose=0, # Calculate validation results on 20% of the training data. validation_split = 0.2) Visualize the model's training progress using the stats stored in the history object: hist = pd.DataFrame(history.history) hist['epoch'] = history.epoch hist.tail() def plot_loss(history): plt.plot(history.history['loss'], label='loss') plt.plot(history.history['val_loss'], label='val_loss') plt.ylim([0, 10]) plt.xlabel('Epoch') plt.ylabel('Error [MPG]') plt.legend() plt.grid(True) plot_loss(history) Collect the results on the test set for later: test_results = {} test_results['horsepower_model'] = horsepower_model.evaluate( test_features['Horsepower'], test_labels, verbose=0) x = tf.linspace(0.0, 250, 251) y = horsepower_model.predict(x) def plot_horsepower(x, y): plt.scatter(train_features['Horsepower'], train_labels, label='Data') plt.plot(x, y, color='k', label='Predictions') plt.xlabel('Horsepower') plt.ylabel('MPG') plt.legend() Since this is a single variable regression, it's easy to view the model's predictions as a function of the input: def plot_horsepower(x, y): plt.scatter(train_features['Horsepower'], train_labels, label='Data') plt.plot(x, y, color='k', label='Predictions') plt.xlabel('Horsepower') plt.ylabel('MPG') plt.legend() plot_horsepower(x, y)","title":"Linear regression"},{"location":"AIML/DeepLearning/Tensorflow.html#linear-regression-with-multiple-inputs","text":"You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same y = mx + b except that m is a matrix and x is a vector. Create a two-step Keras Sequential model again with the first layer being normalizer (tf.keras.layers.Normalization(axis=-1)) you defined earlier and adapted to the whole dataset: linear_model = tf.keras.Sequential([ normalizer, layers.Dense(units=1) ]) When you call Model.predict on a batch of inputs, it produces units=1 outputs for each example: linear_model.predict(train_features[:10]) When you call the model, its weight matrices will be built\u2014check that the kernel weights (the m in y = mx + b) have a shape (9, 1) linear_model.layers[1].kernel Configure the model with Keras Model.compile and train with Model.fit for 100 epochs: linear_model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error') %%time history = linear_model.fit( train_features, train_labels, epochs=100, # Suppress logging. verbose=0, # Calculate validation results on 20% of the training data. validation_split = 0.2) Using all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input: plot_loss(history)","title":"Linear regression with multiple inputs"},{"location":"AIML/RAG/Retrieval_augmented_generation.html","text":"Retrieval Augmented Generation (RAG) # But we have another approach where we can augment the knowledge of LLMs and retrieve information from custom content.It is called Retrieval Augmented Generation (RAG). Utility tools like ChatPDF have been popular Generative AI tools. The PDF document is connected as an external data source and we can interact with it as we are assisted by an LLM. What we do in RAG is inserting additional data into the context (prompt) of a model at inference time. That helps the LLM get more precise and relevant content for our queries when compared to zero-shot prompting. Another way of looking at it is in the context of a doctor and patient. A doctor\u2019s diagnosis can be significantly more precise and accurate when they have access to the patient\u2019s test results and charts, as opposed to relying solely on symptomatic observations. The workflow of the RAG based LLM application will be as follows: Receive query from the user. Convert it to an embedded query vector preserving the semantics, using an embedding model. Retrieve the top-k relevant content from the vector database by computing similarity between the query embedding and the content embedding in the database. Pass the retrieved content and query as a prompt to an LLM. The LLM gives the required response. Download Ollama(Macos) https://ollama.com/download https://github.com/ollama/ollama Unzip the zip file Move to Application (base) gggggggg:~ ganesh$ ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \"ollama [command] --help\" for more information about a command. (base) ggggggggg:~ ganesh$ Run the llama2 (base) gggggg:~ ganesh$ ollama run llama2 pulling manifest pulling 8934d96d3f08... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 3.8 GB pulling 8c17c2ebb0ea... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.0 KB pulling 7c23fb36d801... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.8 KB pulling 2e0493f67d0c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 59 B pulling fa304d675061... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 91 B pulling 42ba7f8a01dd... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 557 B verifying sha256 digest writing manifest success >>> >>> Send a message (/? for help) RAG Pipeline with Vector DataBase # ## Data Ingetion #pip install pypdf #pip install langchain_community from langchain_community.document_loaders import TextLoader from langchain_community.document_loaders import PyPDFLoader from langchain_community.document_loaders import WebBaseLoader from langchain_community.document_loaders import CSVLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.schema import Document #from bs4 import SoupStrainer import bs4 import os from dotenv import load_dotenv load_dotenv() os.environ['OPENAPI_API_KEY']=os.getenv(\"OPENAPI_API_KEY\") Text reader loader = TextLoader('Speech.txt') docs = loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` #Web based loader #Load, chunk and Index the content of HTML page loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\ bs_kwargs=dict(parse_only=bs4.SoupStrainer( \\ class_=(\"post-title\",\"post-content\",\"post-header\") ))) docs=loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` # PDF Reader loader = PyPDFLoader('docs/Ganesh_Kinkar_Giri_DevSecOps.pdf') docs = loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` #CSV Loader loader = CSVLoader(\"issue.csv\") docs = loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` Chunk & Split the text data # text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50) chunk_documents = text_splitter.split_documents(docs) chunk_documents Vector Embedding and Vector Store # Either you can use Embeddings model OpenAI or ollama # from langchain_openai import OpenAIEmbeddings from langchain_community.embeddings import OpenAIEmbeddings from langchain_community.embeddings import OllamaEmbeddings from langchain_community.vectorstores import FAISS db=FAISS.from_documents(chunk_documents,OllamaEmbeddings()) db Query # query = \"unable to connect VM/ VM not accessible\" retrive_result = db.similarity_search(query) print(retrive_result[0].page_content) Retirever and Chain with Langchain # ## Data Ingetion #pip install pypdf #pip install langchain_community from langchain_community.document_loaders import TextLoader from langchain_community.document_loaders import PyPDFLoader from langchain_community.document_loaders import WebBaseLoader from langchain_community.document_loaders import CSVLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.schema import Document #from bs4 import SoupStrainer import bs4 import os from dotenv import load_dotenv load_dotenv() os.environ['OPENAPI_API_KEY']=os.getenv(\"OPENAPI_API_KEY\") #Text Loader loader = TextLoader(\"issue.txt\") docs = loader.load() docs Chunk & Split the text data # text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50) chunk_documents = text_splitter.split_documents(docs) chunk_documents Vector Embedding and Vector Store # Either you can use Embeddings model OpenAI or ollama # from langchain_openai import OpenAIEmbeddings from langchain_community.embeddings import OpenAIEmbeddings from langchain_community.embeddings import OllamaEmbeddings from langchain_community.vectorstores import FAISS db=FAISS.from_documents(chunk_documents,OllamaEmbeddings()) db Design ChatPrompt Template # from langchain_core.prompts import ChatPromptTemplate prompt=ChatPromptTemplate.from_template(\"\"\" Answer the following question based only on the provided context. Think step by step before providing a detailed answer. <context> {context} </context> Question: {input} \"\"\") To integrate with LLM - here will use opensource LLM(Ollama) model llama2 # from langchain_community.llms import Ollama llm=Ollama(model=\"llama2\") llm Chain # from langchain.chains.combine_documents import create_stuff_documents_chain document_chain = create_stuff_documents_chain(llm, prompt) document_chain Retrievers # retrievers = db.as_retriever() retrievers Retrievers chain # from langchain.chains import create_retrieval_chain retrieval_chain = create_retrieval_chain(retrievers, document_chain) import textwrap response = retrieval_chain.invoke({\"input\":\"Low disk space\"}) formatted_answer = textwrap.fill(response['answer'], width=80) #wrapped_text = \"\\n\\n\".join(textwrap.fill(paragraph, width=80) for paragraph in formatted_answer.split(\"\\n\\n\")) #print(wrapped_text) print(formatted_answer) Ans: # Based on the provided context, the answer to the question \"Low disk space\" is: To resolve low disk space issues, the team should first check which application is using the most disk space by running the command `df -h` in the terminal. This will display a list of all mounted file systems and their usage in percentage. The application that is using the most disk space should be identified and the team should then take steps to free up space for that application. One possible solution is to increase the SKU size for the application by running the command `sudoedit /etc/systemd/multi- user/sockets.d/skusize` and updating the value of `SKU_SIZE` to a larger value. This will allow the application to use more disk space without running out of space. Another solution is to clean up any unnecessary files or data that are taking up space on the system by using the command `gzip -d <filename>` to decompress files and then removing them with the command `rm <filename>`. This will free up space on the disk without affecting the application's functionality. The team should also consider optimizing the application's code to reduce its dependencies on disk space, such as by using caching or other memory-based storage solutions. This may involve working with the application's developers to make changes to the application's architecture. Finally, the team can check if there are any other applications that are using excessive amounts of disk space and take steps to free up space for those applications as well.","title":"Retrieval augmented generation"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#retrieval-augmented-generation-rag","text":"But we have another approach where we can augment the knowledge of LLMs and retrieve information from custom content.It is called Retrieval Augmented Generation (RAG). Utility tools like ChatPDF have been popular Generative AI tools. The PDF document is connected as an external data source and we can interact with it as we are assisted by an LLM. What we do in RAG is inserting additional data into the context (prompt) of a model at inference time. That helps the LLM get more precise and relevant content for our queries when compared to zero-shot prompting. Another way of looking at it is in the context of a doctor and patient. A doctor\u2019s diagnosis can be significantly more precise and accurate when they have access to the patient\u2019s test results and charts, as opposed to relying solely on symptomatic observations. The workflow of the RAG based LLM application will be as follows: Receive query from the user. Convert it to an embedded query vector preserving the semantics, using an embedding model. Retrieve the top-k relevant content from the vector database by computing similarity between the query embedding and the content embedding in the database. Pass the retrieved content and query as a prompt to an LLM. The LLM gives the required response. Download Ollama(Macos) https://ollama.com/download https://github.com/ollama/ollama Unzip the zip file Move to Application (base) gggggggg:~ ganesh$ ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \"ollama [command] --help\" for more information about a command. (base) ggggggggg:~ ganesh$ Run the llama2 (base) gggggg:~ ganesh$ ollama run llama2 pulling manifest pulling 8934d96d3f08... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 3.8 GB pulling 8c17c2ebb0ea... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.0 KB pulling 7c23fb36d801... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.8 KB pulling 2e0493f67d0c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 59 B pulling fa304d675061... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 91 B pulling 42ba7f8a01dd... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 557 B verifying sha256 digest writing manifest success >>> >>> Send a message (/? for help)","title":"Retrieval Augmented Generation (RAG)"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#rag-pipeline-with-vector-database","text":"## Data Ingetion #pip install pypdf #pip install langchain_community from langchain_community.document_loaders import TextLoader from langchain_community.document_loaders import PyPDFLoader from langchain_community.document_loaders import WebBaseLoader from langchain_community.document_loaders import CSVLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.schema import Document #from bs4 import SoupStrainer import bs4 import os from dotenv import load_dotenv load_dotenv() os.environ['OPENAPI_API_KEY']=os.getenv(\"OPENAPI_API_KEY\") Text reader loader = TextLoader('Speech.txt') docs = loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` #Web based loader #Load, chunk and Index the content of HTML page loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\ bs_kwargs=dict(parse_only=bs4.SoupStrainer( \\ class_=(\"post-title\",\"post-content\",\"post-header\") ))) docs=loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` # PDF Reader loader = PyPDFLoader('docs/Ganesh_Kinkar_Giri_DevSecOps.pdf') docs = loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata` #CSV Loader loader = CSVLoader(\"issue.csv\") docs = loader.load() for doc in docs: print(doc.page_content) # or any other attribute like `metadata`","title":"RAG Pipeline with Vector DataBase"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#chunk-split-the-text-data","text":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50) chunk_documents = text_splitter.split_documents(docs) chunk_documents","title":"Chunk &amp; Split the text data"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#vector-embedding-and-vector-store","text":"","title":"Vector Embedding and Vector Store"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#either-you-can-use-embeddings-model-openai-or-ollama","text":"from langchain_openai import OpenAIEmbeddings from langchain_community.embeddings import OpenAIEmbeddings from langchain_community.embeddings import OllamaEmbeddings from langchain_community.vectorstores import FAISS db=FAISS.from_documents(chunk_documents,OllamaEmbeddings()) db","title":"Either you can use Embeddings model OpenAI or ollama"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#query","text":"query = \"unable to connect VM/ VM not accessible\" retrive_result = db.similarity_search(query) print(retrive_result[0].page_content)","title":"Query"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#retirever-and-chain-with-langchain","text":"## Data Ingetion #pip install pypdf #pip install langchain_community from langchain_community.document_loaders import TextLoader from langchain_community.document_loaders import PyPDFLoader from langchain_community.document_loaders import WebBaseLoader from langchain_community.document_loaders import CSVLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.schema import Document #from bs4 import SoupStrainer import bs4 import os from dotenv import load_dotenv load_dotenv() os.environ['OPENAPI_API_KEY']=os.getenv(\"OPENAPI_API_KEY\") #Text Loader loader = TextLoader(\"issue.txt\") docs = loader.load() docs","title":"Retirever and Chain with Langchain"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#chunk-split-the-text-data_1","text":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50) chunk_documents = text_splitter.split_documents(docs) chunk_documents","title":"Chunk &amp; Split the text data"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#vector-embedding-and-vector-store_1","text":"","title":"Vector Embedding and Vector Store"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#either-you-can-use-embeddings-model-openai-or-ollama_1","text":"from langchain_openai import OpenAIEmbeddings from langchain_community.embeddings import OpenAIEmbeddings from langchain_community.embeddings import OllamaEmbeddings from langchain_community.vectorstores import FAISS db=FAISS.from_documents(chunk_documents,OllamaEmbeddings()) db","title":"Either you can use Embeddings model OpenAI or ollama"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#design-chatprompt-template","text":"from langchain_core.prompts import ChatPromptTemplate prompt=ChatPromptTemplate.from_template(\"\"\" Answer the following question based only on the provided context. Think step by step before providing a detailed answer. <context> {context} </context> Question: {input} \"\"\")","title":"Design ChatPrompt Template"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#to-integrate-with-llm-here-will-use-opensource-llmollama-model-llama2","text":"from langchain_community.llms import Ollama llm=Ollama(model=\"llama2\") llm","title":"To integrate with LLM - here will use opensource LLM(Ollama) model llama2"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#chain","text":"from langchain.chains.combine_documents import create_stuff_documents_chain document_chain = create_stuff_documents_chain(llm, prompt) document_chain","title":"Chain"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#retrievers","text":"retrievers = db.as_retriever() retrievers","title":"Retrievers"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#retrievers-chain","text":"from langchain.chains import create_retrieval_chain retrieval_chain = create_retrieval_chain(retrievers, document_chain) import textwrap response = retrieval_chain.invoke({\"input\":\"Low disk space\"}) formatted_answer = textwrap.fill(response['answer'], width=80) #wrapped_text = \"\\n\\n\".join(textwrap.fill(paragraph, width=80) for paragraph in formatted_answer.split(\"\\n\\n\")) #print(wrapped_text) print(formatted_answer)","title":"Retrievers chain"},{"location":"AIML/RAG/Retrieval_augmented_generation.html#ans","text":"Based on the provided context, the answer to the question \"Low disk space\" is: To resolve low disk space issues, the team should first check which application is using the most disk space by running the command `df -h` in the terminal. This will display a list of all mounted file systems and their usage in percentage. The application that is using the most disk space should be identified and the team should then take steps to free up space for that application. One possible solution is to increase the SKU size for the application by running the command `sudoedit /etc/systemd/multi- user/sockets.d/skusize` and updating the value of `SKU_SIZE` to a larger value. This will allow the application to use more disk space without running out of space. Another solution is to clean up any unnecessary files or data that are taking up space on the system by using the command `gzip -d <filename>` to decompress files and then removing them with the command `rm <filename>`. This will free up space on the disk without affecting the application's functionality. The team should also consider optimizing the application's code to reduce its dependencies on disk space, such as by using caching or other memory-based storage solutions. This may involve working with the application's developers to make changes to the application's architecture. Finally, the team can check if there are any other applications that are using excessive amounts of disk space and take steps to free up space for those applications as well.","title":"Ans:"},{"location":"AIML/RAG/vector_database.html","text":"What is vector database? # A vector database is a type of database optimized for storing, indexing, and searching high-dimensional vectors, which are mathematical representations of data points (like words, images, or other objects) in vector space. Vector databases are especially useful for handling data from applications in AI and machine learning, particularly in tasks involving similarity search and recommendation engines. A Vector Database, at its essence, is a relational database system specifically designed to process vectorized data. Unlike conventional databases that contain information in tables, rows, and columns, vector databases work with vectors\u2013arrays of numerical values that signify points in multidimensional space. Vectors, in turn, are everywhere and are commonly used in, for instance, machine learning, artificial intelligence, genomics, and geospatial analysis. At these datasets, there are frequently high-dimensional vectors where each dimension represents a particular attribute or feature. Such data place a heavy burden on traditional databases as they are tabular in form and do not allow efficiency in the storage and retrieval of such data and there comes the bottleneck in the performance of the database. What is a Vector? # Vector in the field of mathematics and data science refers to a serial arrangement of numerical values. It is a node in a many-dimensional space where one weight from each vector corresponds to a specific dimension. In the domain of vector databases, such arrays of numerical values, thus, turn into primitive concepts of information, making it possible for the store and processing of data in high dimensions. How Vector Databases Work? # Vector Database is a type of database that is used in various machine learning use cases. They are specialized for the storage and retrieval of vector data. What are embeddings? Embedding is a data like words that have been converted into an array of numbers known as a vector that contains patterns of relationships the combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity. The combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity. Let\u2019s see an example describe a 2d graph the words dog and puppy are often used in similar situations. So in a word embedding they would be represented by vectors that are close together. Well this is a simple 2D example of a single dimension in reality the vector has hundreds of Dimensions that cover the rich multi-dimensional complex relationship between words. Example Images can also be turned into vectors. Google does similar images searches and the image sections are broken down into arrays of numbers allowing you to find patterns of similarity for those with closely resembling vectors. Once an embedding is created it can be stored in a database and a database full of these is considered as a vector database. Vector database can be used in several ways, searching where results are ranked by relevance to a query string or clustering where text strings are grouped by similarity and recommendations where items with related text strings are recommended also classification where text strings are classified by their most similar label. Key Components of Vector Databases # Embedding Storage: Stores high-dimensional vectors. Indexing and Search: Uses specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or FAISS (Facebook AI Similarity Search) for fast similarity search across massive vector datasets. Scalability: Optimized for handling millions to billions of vectors. Metadata Support: Allows for filtering and adding metadata alongside vectors for refined search capabilities. Popular Use Cases # Semantic Search: Finds documents, images, or videos similar in meaning to a search query, even if they don\u2019t share keywords. Recommendation Systems: Suggests items similar to user preferences (like items with similar vector embeddings). Anomaly Detection: Identifies outliers in datasets, as anomalies often have unique embeddings. Image and Video Recognition: Matches similar images or videos based on visual features encoded in vectors. Examples of Vector Databases # Weaviate: An open-source vector search engine with strong support for text, image, and video embeddings. Pinecone: A managed vector database with a focus on scalable similarity search and recommendations. FAISS: Primarily a library developed by Facebook AI for fast nearest neighbor search but also used in building vector search systems. Milvus: Another open-source vector database that provides highly scalable, low-latency vector searches. Why Vector Databases Are Important # With the rise of LLMs (large language models) and computer vision, the demand for efficiently storing and retrieving high-dimensional embeddings has grown. Traditional databases like SQL and NoSQL are not optimized for these kinds of searches. Vector databases are specifically designed to handle such data, making them a crucial component in building advanced, intelligent applications in AI, recommendation systems, and search. faiss # !pip install faiss-cpu !pip install sentence-transformers import faiss import numpy as np from sentence_transformers import SentenceTransformer # Initialize embedding model model = SentenceTransformer('all-MiniLM-L6-v2') # Sample data documents = [\"This is document 1\", \"This is document 2\", \"Document 3 content\"] # Generate embeddings embeddings = model.encode(documents) dimension = embeddings.shape[1] # Create FAISS index index = faiss.IndexFlatL2(dimension) # L2 distance index index.add(np.array(embeddings)) # Add embeddings to the index query = \"What is document retrieval?\" query_embedding = model.encode([query]) # Search for top 3 nearest neighbors D, I = index.search(np.array(query_embedding), k=2) print(\"Top documents:\", [documents[i] for i in I[0]]) weaviate # docker run -d -p 8080:8080 semitechnologies/weaviate http://localhost:8080/v1 import weaviate # Connect to your Weaviate instance client = weaviate.Client(\"http://localhost:8080\") # Define a schema for your documents client.schema.create_class({ \"class\": \"Document\", \"properties\": [ { \"name\": \"text\", \"dataType\": [\"text\"] }, { \"name\": \"embedding\", \"dataType\": [\"number[]\"] } ] }) # Sample data and embeddings documents = [\"This is document 1\", \"This is document 2\", \"Document 3 content\"] embeddings = model.encode(documents) # Upload documents with embeddings for i, (doc, embedding) in enumerate(zip(documents, embeddings)): client.data_object.create( { \"text\": doc, \"embedding\": embedding.tolist() }, \"Document\" ) query_embedding = model.encode([\"What is document retrieval?\"]) result = client.query.get(\"Document\", [\"text\"]) \\ .with_near_vector({\"vector\": query_embedding.tolist()}) \\ .with_limit(3) \\ .do() print(\"Top results:\", result) Get a single collection schema Get a list of objects Get an object https://weaviate.io/developers/weaviate/api/rest#description/introduction","title":"Vector Database"},{"location":"AIML/RAG/vector_database.html#what-is-vector-database","text":"A vector database is a type of database optimized for storing, indexing, and searching high-dimensional vectors, which are mathematical representations of data points (like words, images, or other objects) in vector space. Vector databases are especially useful for handling data from applications in AI and machine learning, particularly in tasks involving similarity search and recommendation engines. A Vector Database, at its essence, is a relational database system specifically designed to process vectorized data. Unlike conventional databases that contain information in tables, rows, and columns, vector databases work with vectors\u2013arrays of numerical values that signify points in multidimensional space. Vectors, in turn, are everywhere and are commonly used in, for instance, machine learning, artificial intelligence, genomics, and geospatial analysis. At these datasets, there are frequently high-dimensional vectors where each dimension represents a particular attribute or feature. Such data place a heavy burden on traditional databases as they are tabular in form and do not allow efficiency in the storage and retrieval of such data and there comes the bottleneck in the performance of the database.","title":"What is vector database?"},{"location":"AIML/RAG/vector_database.html#what-is-a-vector","text":"Vector in the field of mathematics and data science refers to a serial arrangement of numerical values. It is a node in a many-dimensional space where one weight from each vector corresponds to a specific dimension. In the domain of vector databases, such arrays of numerical values, thus, turn into primitive concepts of information, making it possible for the store and processing of data in high dimensions.","title":"What is a Vector?"},{"location":"AIML/RAG/vector_database.html#how-vector-databases-work","text":"Vector Database is a type of database that is used in various machine learning use cases. They are specialized for the storage and retrieval of vector data. What are embeddings? Embedding is a data like words that have been converted into an array of numbers known as a vector that contains patterns of relationships the combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity. The combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity. Let\u2019s see an example describe a 2d graph the words dog and puppy are often used in similar situations. So in a word embedding they would be represented by vectors that are close together. Well this is a simple 2D example of a single dimension in reality the vector has hundreds of Dimensions that cover the rich multi-dimensional complex relationship between words. Example Images can also be turned into vectors. Google does similar images searches and the image sections are broken down into arrays of numbers allowing you to find patterns of similarity for those with closely resembling vectors. Once an embedding is created it can be stored in a database and a database full of these is considered as a vector database. Vector database can be used in several ways, searching where results are ranked by relevance to a query string or clustering where text strings are grouped by similarity and recommendations where items with related text strings are recommended also classification where text strings are classified by their most similar label.","title":"How Vector Databases Work?"},{"location":"AIML/RAG/vector_database.html#key-components-of-vector-databases","text":"Embedding Storage: Stores high-dimensional vectors. Indexing and Search: Uses specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or FAISS (Facebook AI Similarity Search) for fast similarity search across massive vector datasets. Scalability: Optimized for handling millions to billions of vectors. Metadata Support: Allows for filtering and adding metadata alongside vectors for refined search capabilities.","title":"Key Components of Vector Databases"},{"location":"AIML/RAG/vector_database.html#popular-use-cases","text":"Semantic Search: Finds documents, images, or videos similar in meaning to a search query, even if they don\u2019t share keywords. Recommendation Systems: Suggests items similar to user preferences (like items with similar vector embeddings). Anomaly Detection: Identifies outliers in datasets, as anomalies often have unique embeddings. Image and Video Recognition: Matches similar images or videos based on visual features encoded in vectors.","title":"Popular Use Cases"},{"location":"AIML/RAG/vector_database.html#examples-of-vector-databases","text":"Weaviate: An open-source vector search engine with strong support for text, image, and video embeddings. Pinecone: A managed vector database with a focus on scalable similarity search and recommendations. FAISS: Primarily a library developed by Facebook AI for fast nearest neighbor search but also used in building vector search systems. Milvus: Another open-source vector database that provides highly scalable, low-latency vector searches.","title":"Examples of Vector Databases"},{"location":"AIML/RAG/vector_database.html#why-vector-databases-are-important","text":"With the rise of LLMs (large language models) and computer vision, the demand for efficiently storing and retrieving high-dimensional embeddings has grown. Traditional databases like SQL and NoSQL are not optimized for these kinds of searches. Vector databases are specifically designed to handle such data, making them a crucial component in building advanced, intelligent applications in AI, recommendation systems, and search.","title":"Why Vector Databases Are Important"},{"location":"AIML/RAG/vector_database.html#faiss","text":"!pip install faiss-cpu !pip install sentence-transformers import faiss import numpy as np from sentence_transformers import SentenceTransformer # Initialize embedding model model = SentenceTransformer('all-MiniLM-L6-v2') # Sample data documents = [\"This is document 1\", \"This is document 2\", \"Document 3 content\"] # Generate embeddings embeddings = model.encode(documents) dimension = embeddings.shape[1] # Create FAISS index index = faiss.IndexFlatL2(dimension) # L2 distance index index.add(np.array(embeddings)) # Add embeddings to the index query = \"What is document retrieval?\" query_embedding = model.encode([query]) # Search for top 3 nearest neighbors D, I = index.search(np.array(query_embedding), k=2) print(\"Top documents:\", [documents[i] for i in I[0]])","title":"faiss"},{"location":"AIML/RAG/vector_database.html#weaviate","text":"docker run -d -p 8080:8080 semitechnologies/weaviate http://localhost:8080/v1 import weaviate # Connect to your Weaviate instance client = weaviate.Client(\"http://localhost:8080\") # Define a schema for your documents client.schema.create_class({ \"class\": \"Document\", \"properties\": [ { \"name\": \"text\", \"dataType\": [\"text\"] }, { \"name\": \"embedding\", \"dataType\": [\"number[]\"] } ] }) # Sample data and embeddings documents = [\"This is document 1\", \"This is document 2\", \"Document 3 content\"] embeddings = model.encode(documents) # Upload documents with embeddings for i, (doc, embedding) in enumerate(zip(documents, embeddings)): client.data_object.create( { \"text\": doc, \"embedding\": embedding.tolist() }, \"Document\" ) query_embedding = model.encode([\"What is document retrieval?\"]) result = client.query.get(\"Document\", [\"text\"]) \\ .with_near_vector({\"vector\": query_embedding.tolist()}) \\ .with_limit(3) \\ .do() print(\"Top results:\", result) Get a single collection schema Get a list of objects Get an object https://weaviate.io/developers/weaviate/api/rest#description/introduction","title":"weaviate"},{"location":"AIML/Supervised/Supervised-overview.html","text":"Supervised Learning # ** What is Supervised Learning?** # In supervised learning, sample labeled data are provided to the machine learning system for training, and the system then predicts the output based on the training data. The system uses labeled data to build a model that understands the datasets and learns about each one. After the training and processing are done, we test the model with sample data to see if it can accurately predict the output. The mapping of the input data to the output data is the objective of supervised learning. Supervised learning can be grouped further in two categories of algorithms: # Classification # Regression #","title":"Supervised Overview"},{"location":"AIML/Supervised/Supervised-overview.html#supervised-learning","text":"","title":"Supervised Learning"},{"location":"AIML/Supervised/Supervised-overview.html#what-is-supervised-learning","text":"In supervised learning, sample labeled data are provided to the machine learning system for training, and the system then predicts the output based on the training data. The system uses labeled data to build a model that understands the datasets and learns about each one. After the training and processing are done, we test the model with sample data to see if it can accurately predict the output. The mapping of the input data to the output data is the objective of supervised learning.","title":"** What is Supervised Learning?**"},{"location":"AIML/Supervised/Supervised-overview.html#supervised-learning-can-be-grouped-further-in-two-categories-of-algorithms","text":"","title":"Supervised learning can be grouped further in two categories of algorithms:"},{"location":"AIML/Supervised/Supervised-overview.html#classification","text":"","title":"Classification"},{"location":"AIML/Supervised/Supervised-overview.html#regression","text":"","title":"Regression"},{"location":"AIML/Supervised/Classification/Classification-overview.html","text":"","title":"Classification overview"},{"location":"AIML/Supervised/Classification/KNN.html","text":"","title":"KNN"},{"location":"AIML/Supervised/Classification/Logistic-Regression.html","text":"","title":"Logistic Regression"},{"location":"AIML/Supervised/Regression/Decision-Tree.html","text":"","title":"Decision Tree"},{"location":"AIML/Supervised/Regression/Gradient-Boosting.html","text":"","title":"Gradient Boosting"},{"location":"AIML/Supervised/Regression/Linear-Regression.html","text":"","title":"Linear Regression"},{"location":"AIML/Supervised/Regression/Naive-Bayes.html","text":"","title":"Naive Bayes"},{"location":"AIML/Supervised/Regression/Random-Forest.html","text":"","title":"Random Forest"},{"location":"AIML/Supervised/Regression/SVM.html","text":"","title":"SVM"},{"location":"AIML/Unsupervised/ARIMA.html","text":"","title":"ARIMA"},{"location":"AIML/Unsupervised/K-means.html","text":"","title":"K-means"},{"location":"AIML/Unsupervised/PCA.html","text":"","title":"PCA"},{"location":"AIML/Unsupervised/SARIMA.html","text":"","title":"SARIMA"},{"location":"AIML/Unsupervised/SOM.html","text":"","title":"SOM"},{"location":"AIML/Unsupervised/Unsupervised-overview.html","text":"","title":"Unsupervised Overview"},{"location":"AIML/Unsupervised/Unsupervised.html","text":"Contents # Business Case ML metrics WSSE Explained variance Feature engineering ML - Algos Kmeans PCA Evaluation Extensions","title":"Unsupervised"},{"location":"AIML/Unsupervised/Unsupervised.html#contents","text":"Business Case ML metrics WSSE Explained variance Feature engineering ML - Algos Kmeans PCA Evaluation Extensions","title":"Contents"},{"location":"AIML/Unsupervised/dddd.html","text":"","title":"Dddd"},{"location":"Cloud/Aws/aws.html","text":"","title":"Aws"},{"location":"Cloud/Azure/azure.html","text":"","title":"Azure"},{"location":"Cloud/Azure/AZ-305/Analytics.html","text":"","title":"Analytics"},{"location":"Cloud/Azure/AZ-305/Azure-all-services.html","text":"*Azure all services Categories # Compute Containers Analytics Databases DevOps Hybrid & Multicloud Identity Integration Internet of Things(IoT) Management and governance Migration Mixed reality Monitor Networking Security Storage Web & Mobile General","title":"*Azure all services Categories"},{"location":"Cloud/Azure/AZ-305/Azure-all-services.html#azure-all-services-categories","text":"Compute Containers Analytics Databases DevOps Hybrid & Multicloud Identity Integration Internet of Things(IoT) Management and governance Migration Mixed reality Monitor Networking Security Storage Web & Mobile General","title":"*Azure all services Categories"},{"location":"Cloud/Azure/AZ-305/Compute.html","text":"","title":"Compute"},{"location":"Cloud/Azure/AZ-305/Containers.html","text":"","title":"Containers"},{"location":"Cloud/Azure/AZ-305/Databases.html","text":"","title":"Databases"},{"location":"Cloud/Azure/AZ-305/DevOps.html","text":"","title":"DevOps"},{"location":"Cloud/Azure/AZ-305/General.html","text":"","title":"General"},{"location":"Cloud/Azure/AZ-305/Hybrid-Multicloud.html","text":"","title":"Hybrid Multicloud"},{"location":"Cloud/Azure/AZ-305/Identity.html","text":"","title":"Identity"},{"location":"Cloud/Azure/AZ-305/Integration.html","text":"","title":"Integration"},{"location":"Cloud/Azure/AZ-305/Internet-of-Things.html","text":"","title":"Internet of Things"},{"location":"Cloud/Azure/AZ-305/Management-governance.html","text":"","title":"Management governance"},{"location":"Cloud/Azure/AZ-305/Migration.html","text":"","title":"Migration"},{"location":"Cloud/Azure/AZ-305/Mixed-reality.html","text":"","title":"Mixed reality"},{"location":"Cloud/Azure/AZ-305/Monitor.html","text":"","title":"Monitor"},{"location":"Cloud/Azure/AZ-305/Networking.html","text":"","title":"Networking"},{"location":"Cloud/Azure/AZ-305/Security.html","text":"","title":"Security"},{"location":"Cloud/Azure/AZ-305/Storage.html","text":"","title":"Storage"},{"location":"Cloud/Azure/AZ-305/Web-Mobile.html","text":"","title":"Web Mobile"},{"location":"Cloud/Azure/AZ-500/Azure-Security-Technologies.html","text":"Get tips and tricks for teaching AZ-500 Microsoft Azure Security Technologies # Introduction Virtual Educator Prep Sessions (VEPS) are part of the Microsoft Learn for Educators (MSLE) program, and they're a critical piece of preparing to teach Microsoft technical skills as a part of our program. MSLE is anchored in the belief that higher education institutions and faculty members play a pivotal role in empowering students for future success. We at Microsoft are committed to supporting students, faculty members, and higher education institutions with free curriculum training and tools for teaching technical skills. In this module, you'll learn tips and tricks from Microsoft technical trainers about preparing for your first teaching session. You'll learn which labs to use, and how to introduce technical topics. You'll also discover other resources and materials that may help your students succeed on their Microsoft certification exams. Understand the objective domains In the AZ-500: Microsoft Azure Security Technologies course, all of the topics that are covered are related to Azure infrastructure and map directly to Exam AZ-500: Microsoft Azure Security Technologies. Percentages indicate the relative weight of each area on the exam. The higher the percentage, the more questions you're likely to see in that area. For the latest breakdown, select the AZ-500 study guide hyperlink in the Tip section of the AZ-500: Microsoft Azure Security Technologies certification page. Become familiar with the objective domains, which are a comprehensive set of specific and measurable knowledge, skills, and abilities that are the basis for the development of both this course and the certification exam. Module 1: Identity and Access Module 2: Implement Platform Protection Module 3: Data and Application Security Module 4: Security Operations Remember that each module points back to the core theme of Azure security and how security and Azure and Cloud technologies are vitally important. Details for each module are included in the corresponding module best practice sections below. Learn module best practices As an educator, you play a pivotal role in empowering students for future success. Skilling our future generations is critical work. The AZ-500: Microsoft Azure Security Technologies course exposes students to the Azure security engineer role. The Azure security engineer is skilled at performing the following tasks: Managing the security posture Identifying and remediating vulnerabilities Performing threat modeling Implementing threat protection Responding to security incident escalations Azure security engineers often serve as part of a larger team to plan and implement cloud-based management and security. Students preparing for this exam should have practical experience in administration of Azure and hybrid environments, along with experience with: Infrastructure as code Security operations processes Cloud capabilities Azure services Before we begin, make sure you're using the most up-to-date version of the Microsoft Official Curriculum (MOC). It's recommended that you log into the Learning Download Center (LDC) at least quarterly. If you haven't already done so, please download the course materials from the LDC before continuing with this module. Now that you have the updated materials, you\u2019re ready to prepare for your class. Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 1 As you're preparing to teach this module, get familiar with what the students will learn during the module \u2013 the concepts and methodologies they'll be working through. In this module, students will explore how identity is the starting point for all security and learn to authenticate and authorize users and apps with Azure Active Directory (Azure AD). Students will also explore creating and managing user and group accounts in Azure AD. This module consists of five lessons: Lesson 1 \u2013 Secure Azure solutions with Azure AD Lesson 2 \u2013 Implement Hybrid identity Lesson 3 \u2013 Deploy Azure AD identity protection Lesson 4 \u2013 Configure Azure AD privileged identity management Lesson 5 \u2013 Design an enterprise governance strategy Module 1 Tips and Tricks Use real-world applications of Azure AD Emphasize the difference between Active Directory and Azure AD Explain that Azure AD is an internet service Utilize Microsoft Whiteboard during instruction Clarify tenant, directory, and subscription Consider using real-word examples Consider using Microsoft customer stories Consider presenting the labs as demonstrations\u202f Students will need access to the Azure portal and credentials that need to be validated to complete the labs Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 2 As you're preparing to teach this module, get familiar with what the students will learn during the module. In this module, students will learn about how security has to happen at all levels and how to lock down the infrastructure and network resources that are running in an Azure environment. This module consists of four lessons: Lesson 1 \u2013 Implement perimeter security Lesson 2 \u2013 Configure network security Lesson 3 \u2013 Configure and manage host security Lesson 4 \u2013 Enable Containers security Module 2 Tips and Tricks Consider starting the conversation on the concepts of Defense in Depth \u2013 layers of security Explain the host and Container security focus on securing the VM Consider presenting the labs as demonstrations\u202f Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 3 As you're preparing to teach this module, get familiar with what the students will learn during the module. In this module, students will learn that the applications that run within Azure and access confidential data will need to be locked down. Students will learn how to secure applications, storage, databases, and key vaults. This module consists of four lessons: Lesson 1 \u2013 Deploy and secure Azure Key Vault Lesson 2 \u2013 Configure application security features Lesson 3 \u2013 Implement storage security Lesson 4 \u2013 Configure and manage SQL database security Module 3 Tips and Tricks Explain how Key Vault and Managed IDs are the center of application security, so password and access credentials don\u2019t need to be included in code. Differentiate between managed ID vs. service principals Emphasize how different skill levels and tool knowledge can be applied in Azure Ensure your students are comfortable with Certificates and Public Key Infrastructure (PKI) Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 4 As you're preparing to teach this module, get familiar with what the students will learn during the module. In this module, students will learn to monitor, operate, and continuously improve the security of solutions once the Azure environment is deployed and secured. This module consists of three lessons: Lesson 1 \u2013 Configure and manage Azure Monitor Lesson 2 \u2013 Enable and manage Microsoft Defender for Cloud Lesson 3 \u2013 Configure and monitor Microsoft Sentinel Module 4 Tips and Tricks Illustrate the relationship of the tools Outline Security Information and Event Management (SIEM) Emphasize Security Orchestration, Automation, and Response (SOAR) Students need to know what Sentinel is and how it works, they aren't responsible for actual threat analysis for this exam Microsoft Defender for Cloud is the new name for Azure Security Center Consider presenting the labs as demonstrations Allow plenty of time to cover the materials Bring it all together As an educator, you play a pivotal role in empowering students for future success. Your journey in teaching MOC to your students will help ensure your students are prepared for future jobs across a range of disciplines and industries in need of talent with Azure skills. Further, you're preparing your students for the AZ-500: Microsoft Azure Security Technologies Certification exam. We've provided you with many learning resources that not only prepare you as an instructor but also give your students the confidence they need to tackle the exam. Now that you've all the necessary tools, you're ready to go. Knowledge check How often should you download content from the Learning Download Center (LDC) to ensure you have the most up to date course materials? Every three years Once a year Quarterly ( Ans ) Which teaching tool is great for giving students a visual representation of topics in the AZ-500: Microsoft Azure Security Technologies course? The Textbook Microsoft Whiteboard ( Ans ) Flash Cards Which teaching tool allows students to build a strong foundation of concepts before beginning to work independently in the AZ-500: Microsoft Azure Security Technologies course? Accessibility Tools Azure Blog Instructor-led lab experiences ( Ans )","title":"Get tips and tricks for teaching AZ-500 Microsoft Azure Security Technologies"},{"location":"Cloud/Azure/AZ-500/Azure-Security-Technologies.html#get-tips-and-tricks-for-teaching-az-500-microsoft-azure-security-technologies","text":"Introduction Virtual Educator Prep Sessions (VEPS) are part of the Microsoft Learn for Educators (MSLE) program, and they're a critical piece of preparing to teach Microsoft technical skills as a part of our program. MSLE is anchored in the belief that higher education institutions and faculty members play a pivotal role in empowering students for future success. We at Microsoft are committed to supporting students, faculty members, and higher education institutions with free curriculum training and tools for teaching technical skills. In this module, you'll learn tips and tricks from Microsoft technical trainers about preparing for your first teaching session. You'll learn which labs to use, and how to introduce technical topics. You'll also discover other resources and materials that may help your students succeed on their Microsoft certification exams. Understand the objective domains In the AZ-500: Microsoft Azure Security Technologies course, all of the topics that are covered are related to Azure infrastructure and map directly to Exam AZ-500: Microsoft Azure Security Technologies. Percentages indicate the relative weight of each area on the exam. The higher the percentage, the more questions you're likely to see in that area. For the latest breakdown, select the AZ-500 study guide hyperlink in the Tip section of the AZ-500: Microsoft Azure Security Technologies certification page. Become familiar with the objective domains, which are a comprehensive set of specific and measurable knowledge, skills, and abilities that are the basis for the development of both this course and the certification exam. Module 1: Identity and Access Module 2: Implement Platform Protection Module 3: Data and Application Security Module 4: Security Operations Remember that each module points back to the core theme of Azure security and how security and Azure and Cloud technologies are vitally important. Details for each module are included in the corresponding module best practice sections below. Learn module best practices As an educator, you play a pivotal role in empowering students for future success. Skilling our future generations is critical work. The AZ-500: Microsoft Azure Security Technologies course exposes students to the Azure security engineer role. The Azure security engineer is skilled at performing the following tasks: Managing the security posture Identifying and remediating vulnerabilities Performing threat modeling Implementing threat protection Responding to security incident escalations Azure security engineers often serve as part of a larger team to plan and implement cloud-based management and security. Students preparing for this exam should have practical experience in administration of Azure and hybrid environments, along with experience with: Infrastructure as code Security operations processes Cloud capabilities Azure services Before we begin, make sure you're using the most up-to-date version of the Microsoft Official Curriculum (MOC). It's recommended that you log into the Learning Download Center (LDC) at least quarterly. If you haven't already done so, please download the course materials from the LDC before continuing with this module. Now that you have the updated materials, you\u2019re ready to prepare for your class. Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 1 As you're preparing to teach this module, get familiar with what the students will learn during the module \u2013 the concepts and methodologies they'll be working through. In this module, students will explore how identity is the starting point for all security and learn to authenticate and authorize users and apps with Azure Active Directory (Azure AD). Students will also explore creating and managing user and group accounts in Azure AD. This module consists of five lessons: Lesson 1 \u2013 Secure Azure solutions with Azure AD Lesson 2 \u2013 Implement Hybrid identity Lesson 3 \u2013 Deploy Azure AD identity protection Lesson 4 \u2013 Configure Azure AD privileged identity management Lesson 5 \u2013 Design an enterprise governance strategy Module 1 Tips and Tricks Use real-world applications of Azure AD Emphasize the difference between Active Directory and Azure AD Explain that Azure AD is an internet service Utilize Microsoft Whiteboard during instruction Clarify tenant, directory, and subscription Consider using real-word examples Consider using Microsoft customer stories Consider presenting the labs as demonstrations\u202f Students will need access to the Azure portal and credentials that need to be validated to complete the labs Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 2 As you're preparing to teach this module, get familiar with what the students will learn during the module. In this module, students will learn about how security has to happen at all levels and how to lock down the infrastructure and network resources that are running in an Azure environment. This module consists of four lessons: Lesson 1 \u2013 Implement perimeter security Lesson 2 \u2013 Configure network security Lesson 3 \u2013 Configure and manage host security Lesson 4 \u2013 Enable Containers security Module 2 Tips and Tricks Consider starting the conversation on the concepts of Defense in Depth \u2013 layers of security Explain the host and Container security focus on securing the VM Consider presenting the labs as demonstrations\u202f Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 3 As you're preparing to teach this module, get familiar with what the students will learn during the module. In this module, students will learn that the applications that run within Azure and access confidential data will need to be locked down. Students will learn how to secure applications, storage, databases, and key vaults. This module consists of four lessons: Lesson 1 \u2013 Deploy and secure Azure Key Vault Lesson 2 \u2013 Configure application security features Lesson 3 \u2013 Implement storage security Lesson 4 \u2013 Configure and manage SQL database security Module 3 Tips and Tricks Explain how Key Vault and Managed IDs are the center of application security, so password and access credentials don\u2019t need to be included in code. Differentiate between managed ID vs. service principals Emphasize how different skill levels and tool knowledge can be applied in Azure Ensure your students are comfortable with Certificates and Public Key Infrastructure (PKI) Explore the best practices for AZ-500 Microsoft Azure Security Technologies Module 4 As you're preparing to teach this module, get familiar with what the students will learn during the module. In this module, students will learn to monitor, operate, and continuously improve the security of solutions once the Azure environment is deployed and secured. This module consists of three lessons: Lesson 1 \u2013 Configure and manage Azure Monitor Lesson 2 \u2013 Enable and manage Microsoft Defender for Cloud Lesson 3 \u2013 Configure and monitor Microsoft Sentinel Module 4 Tips and Tricks Illustrate the relationship of the tools Outline Security Information and Event Management (SIEM) Emphasize Security Orchestration, Automation, and Response (SOAR) Students need to know what Sentinel is and how it works, they aren't responsible for actual threat analysis for this exam Microsoft Defender for Cloud is the new name for Azure Security Center Consider presenting the labs as demonstrations Allow plenty of time to cover the materials Bring it all together As an educator, you play a pivotal role in empowering students for future success. Your journey in teaching MOC to your students will help ensure your students are prepared for future jobs across a range of disciplines and industries in need of talent with Azure skills. Further, you're preparing your students for the AZ-500: Microsoft Azure Security Technologies Certification exam. We've provided you with many learning resources that not only prepare you as an instructor but also give your students the confidence they need to tackle the exam. Now that you've all the necessary tools, you're ready to go. Knowledge check How often should you download content from the Learning Download Center (LDC) to ensure you have the most up to date course materials? Every three years Once a year Quarterly ( Ans ) Which teaching tool is great for giving students a visual representation of topics in the AZ-500: Microsoft Azure Security Technologies course? The Textbook Microsoft Whiteboard ( Ans ) Flash Cards Which teaching tool allows students to build a strong foundation of concepts before beginning to work independently in the AZ-500: Microsoft Azure Security Technologies course? Accessibility Tools Azure Blog Instructor-led lab experiences ( Ans )","title":"Get tips and tricks for teaching AZ-500 Microsoft Azure Security Technologies"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html","text":"AZ-500: Microsoft Certified: Azure Security Engineer Associate # Implement platform protection # Implement perimeter security Introduction Define defense in depth Explore virtual network security Enable Distributed Denial of Service (DDoS) Protection Configure a distributed denial of service protection implementation Explore Azure Firewall features Deploy an Azure Firewall implementation Configure VPN forced tunneling Create User Defined Routes and Network Virtual Appliances Explore hub and spoke topology Perform try-this exercises Knowledge check Summary Configure network security Introduction Explore Network Security Groups (NSG) Deploy a Network Security Groups implementation Create Application Security Groups Enable service endpoints Configure service endpoint services Deploy private links Implement an Azure application gateway Deploy a web application firewall Configure and manage Azure front door Review ExpressRoute Perform try-this exercises Knowledge check Summary 3 Configure and manage host security Introduction Enable endpoint protection Define a privileged access device strategy Deploy privileged access workstations Create virtual machine templates Enable and secure remote access management Configure update management Deploy disk encryption Managed disk encryption options Deploy and configure Windows Defender Microsoft cloud security benchmark in Defender for Cloud Explore Microsoft Defender for Cloud recommendations Perform Try-This exercises Knowledge check Summary 4 Enable Containers security Introduction Explore containers Configure Azure Container Instances security Manage security for Azure Container Instances (ACI)\u200b Explore the Azure Container Registry (ACR) Enable Azure Container Registry authentication Review Azure Kubernetes Service (AKS) Implement an Azure Kubernetes Service architecture\u200b Configure Azure Kubernetes Service networking Deploy Azure Kubernetes Service storage\u200b Secure authentication to Azure Kubernetes Service with Active Directory Manage access to Azure Kubernetes Service using Azure role-based access controls Knowledge check Summary Introduction # Think of security in layers. You need to lock down the perimeter first, and then work your way inward locking down each layer. The more work and cost a bad-actor has to invest to get to valuable information the more likely they are to leave you alone. Scenario # A security engineer uses perimeter security features to block network traffic away from your primary network resources; you will work on such tasks as: Setup DDoS (denial-of-service) Protection. Configure and maintain firewalls. Use dedicated routes and network appliance to protect your network. Define defense in depth # The Defense in depth approach includes other controls in the design to mitigate risk to the organization in the event a primary security control fails. This design should consider how likely the primary control is to fail, the potential organizational risk if it does, and the effectiveness of the additional control (especially in the likely cases that would cause the primary control to fail). During this module, we will explore the defense-in-depth design of Azure services and capabilities to help you securely manage and monitor your cloud data and infrastructure as a managed service. Microsoft designs and operates its cloud services with security at the core and provides you built-in controls and tools to meet your security needs. In addition, with Machine Learning (ML) and Microsoft's significant investments in cyber defense you can benefit from unique intelligence and proactive measures to protect you from threats. Azure offers unified security management and advanced threat protection for your resources whether they're in the cloud, your data center, or both. Each Service in Azure is built with security in mind from the ground up to host your infrastructure apps and data. All services are designed and operated to support multiple layers of defense, spanning your data apps, virtual machines, network perimeter-related policies, and physical security within our data centers. Including how the data sensors and systems that run Azure are architected and operated to the controls you can apply as part of your defense in-depth security management. An example strategy is illustrated in the following image. As more of a company\u2019s digital resources reside outside the corporate network, in the cloud and on personal devices, it becomes obvious that a perimeter only based security, i.e. firewalls, DMZ, VNets, are no longer adequate. The adoption of software-defined networking (SDN) and software-defined data center (SDDC) technologies are driving Network Segmentation concepts to be more granular, i.e. Network Micro-Segmentation. Network Micro-Segmentation # Micro-segmentation is a way to create secure zones in data centers and Azure deployments that allow you to isolate workloads and protect them individually. Security policies in a virtual environment can be assigned to virtual connections that can move with an application if the network is reconfigured \u2013 making the security policy persistent. A best practice recommendation is to adopt a Zero Trust strategy based on user, device, and application identities. In contrast to network access controls that are based on elements such as source and destination IP address, protocols, and port numbers, Zero Trust enforces and validates access control at \u201caccess time\u201d. Granting access at access-time avoids the need to play a prediction game for an entire deployment, network, or subnet \u2013 only the destination resource needs to provide the necessary access controls. Azure Network Security Groups can be used for basic layer 3 & 4 access controls between Azure Virtual Networks, their subnets, and the Internet. Application Security Groups enable you to define fine-grained network security policies based on workloads, centralized on applications, instead of explicit IP addresses. Azure Web Application Firewall and the Azure Firewall can be used for more advanced network access controls that require application layer support. Local Admin Password Solution (LAPS) or a third-party Privileged Access Management can set strong local admin passwords and just in time access to them. Additionally, third parties offer micro-segmentation approaches that may enhance your network controls by applying zero trust principles to networks you control with legacy assets on them. Explore virtual network security # Azure Networking Components # The following sections define key terminology for Azure networking. Later, this course will cover each of these areas in more detail. Azure Virtual Networks are a key component of Azure security services. The Azure network infrastructure enables you to securely connect Azure resources to each other with virtual networks (VNets). A VNet is a representation of your own network in the cloud. A VNet is a logical isolation of the Azure cloud network dedicated to your subscription. You can connect VNets to your on-premises networks. Azure supports dedicated WAN link connectivity to your on-premises network and an Azure Virtual Network with ExpressRoute. The link between Azure and your site uses a dedicated connection that does not go over the public Internet. If your Azure application is running in multiple datacenters, you can use Azure Traffic Manager to route requests from users intelligently across instances of the application. You can also route traffic to services not running in Azure if they are accessible from the Internet. Virtual networks # Organizations can use virtual networks to connect resources. Virtual networks in Azure are network overlays that you can use to configure and control the connectivity among Azure resources, such as VMs and load balancers. Azure Virtual Network enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. A virtual network is scoped to a single Azure region. An Azure region is a set of datacenters deployed within a latency-defined perimeter and connected through a dedicated regional low-latency network. Virtual networks are made up of subnets. A subnet is a range of IP addresses within your virtual network. Subnets, like virtual networks, are scoped to a single Azure region. You can implement multiple virtual networks within each Azure subscription and Azure region. Each virtual network is isolated from other virtual networks. For each virtual network you can: Specify a custom private IP address space using public and private addresses. Azure assigns resources in a virtual network a private IP address from the address space that you assign. Segment the virtual network into one or more subnets and allocate a portion of the virtual network's address space to each subnet. Use Azure-provided name resolution, or specify your own DNS server, for use by resources in a virtual network. IP addresses # VMs, Azure load balancers, and application gateways in a single virtual network require unique Internet Protocol (IP) addresses the same way that clients in an on-premises subnet do. This enables these resources to communicate with each other. A virtual network uses two types of IP addresses: Private - A private IP address is dynamically or statically allocated to a VM from the defined scope of IP addresses in the virtual network. VMs use these addresses to communicate with other VMs in the same or connected virtual networks through a gateway / Azure ExpressRoute connection. These private IP addresses, or non-routable IP addresses, conform to RFC 1918. Public - Public IP addresses, which allow Azure resources to communicate with external clients, are assigned directly at the virtual network adapter of the VM or to the load balancer. Public IP address can also be added to Azure-only virtual networks. All IP blocks in the virtual network will be routable only within the customer's network, and they won't be reachable from outside. Virtual network packets travel through the high-speed Azure backplane. You can control the dynamic IP addresses assigned to VMs and cloud services within an Azure virtual network by specifying an IP addressing scheme. Planning an IP addressing scheme within an Azure virtual network is much like planning an IP addressing scheme on-premises. The same ranges are often used, and the same rules applied. However, conditions exist that are unique to Azure virtual networks. Subnets # You can further divide your network by using subnets for the logical and security-related isolation of Azure resources. Each subnet contains a range of IP addresses that fall within the virtual network address space. Subnetting hides the details of internal network organization from external routers. Subnetting also segments the host within the network, making it easier to apply network security at the interconnections between subnets. Network adapters # VMs communicate with other VMs and other resources on the network by using virtual network adapters. Virtual network adapters configure VMs with private and, optionally, public IP address. A VM can have more than one network adapter for different network configurations. Enable Distributed Denial of Service (DDoS) Protection # A denial of service attack (DoS) is an attack that has the goal of preventing access to services or systems. If the attack originates from one location, it is called a DoS. If the attack originates from multiple networks and systems, it is called distributed denial of service (DDoS). Before learning more about DDoS, you need to know what botnets are. Botnets are collections of internet-connected systems that an individual controls and uses without their owners\u2019 knowledge. Botnet owners use them to perform various actions of their choosing. Often, they use them for spamming, data storage, DDoS, or various other actions that are up to the person in control of the botnet. In the past, botnets were made up just of compromised computers, but now, botnets are also made up of Internet of Things (IoT) devices. Malicious hackers can get these poorly secured security cameras, digital video recorders, thermostats, and other internet-connected devices under their control. So, DDoS is a collection of attack types aimed at disrupting the availability of a target. These attacks involve a coordinated effort that uses multiple internet-connected systems to launch many network requests against DNS, web services, email, and more. Pretty much any application that the malicious hacker can access might become the target of a DDoS. The malicious hacker\u2019s goal is to overwhelm system resources on targeted servers so they can no longer process legitimate traffic, effectively making the system inaccessible. A DDoS generally involves many systems sending traffic to targets as part of a botnet. In most cases, the owners of the systems in a botnet don\u2019t know that their devices are compromised and participating in an attack. Botnets are becoming a bigger problem than before because of the increasing numbers of connected devices. Designing and building for DDoS resiliency requires planning and designing for a variety of failure modes. The following table lists the best practices for building DDoS-resilient services in Azure. Best practice 1 # Ensure that security is a priority throughout the entire lifecycle of an application, from design and implementation to deployment and operations. Applications might have bugs that allow a relatively low volume of requests to use a lot of resources, resulting in a service outage. Solution 1 # To help protect a service running in Azure, understand your application architecture, and focus on the five pillars of software quality. They are: Pillar Description Scalability The ability of a system to handle increased load Availability The proportion of time that a system is functional and working Resiliency The ability of a system to recover from failures and continue to function Management Operations processes that keep a system running in production Security Protecting applications and data from threats You should know typical traffic volumes, the connectivity model between the application and other applications, and the service endpoints that are exposed to the public internet. Helping ensure that an application is resilient enough to handle a DoS targeted at the application itself is most important. Security and privacy features are built in to the Azure platform, beginning with the Microsoft Security Development Lifecycle (SDL). The SDL addresses security at every development phase and ensures that Azure is continually updated to make it even more secure. We will look at SDL later in this course. Best practice 2 # Design your applications to scale horizontally to meet the demands of an amplified load\u2014specifically, in the event of a DDoS. If your application depends on a single instance of a service, it creates a single point of failure. Provisioning multiple instances makes your system more resilient and more scalable. Solution 2 # For Azure App Service, select an App Service plan that offers multiple instances. For Azure Cloud Services, configure each of your roles to use multiple instances. For Azure Virtual Machines, ensure that your VM architecture includes more than one VM and that each VM is included in an availability set. We recommend using virtual machine scale sets for autoscaling capabilities. Best practice 3 # Layer security defenses in an application to reduce the chance of a successful attack. Implement security-enhanced designs for your applications by using the built-in capabilities of the Azure platform. Solution 3 # Be aware that the risk of attack increases with the size, or surface area, of the application. You can reduce the surface area by using IP allowlists to close down the exposed IP address space and listening ports that aren\u2019t needed on the load balancers (for Azure Load Balancer and Azure Application Gateway). You can also use NSGs to reduce the attack surface. You can use service tags and application security groups as a natural extension of an application\u2019s structure to minimize complexity for creating security rules and configuring network security. Configure a distributed denial of service protection implementation # Azure Distributed Denial of Service (DDoS) protection, combined with application design best practices, provide defense against DDoS attacks. Azure DDoS protection provides the following service tiers: Basic : Automatically enabled as part of the Azure platform. Always-on traffic monitoring, and real-time mitigation of common network-level attacks, provide the same defenses utilized by Microsoft's online services. The entire scale of Azure's global network can be used to distribute and mitigate attack traffic across regions. Protection is provided for IPv4 and IPv6 Azure public IP addresses. Standard : Provides additional mitigation capabilities over the Basic service tier that are tuned specifically to Azure Virtual Network resources. DDoS Protection Standard is simple to enable, and requires no application changes. Protection policies are tuned through dedicated traffic monitoring and machine learning algorithms. Policies are applied to public IP addresses associated to resources deployed in virtual networks, such as Azure Load Balancer, Azure Application Gateway, and Azure Service Fabric instances, but this protection does not apply to App Service Environments. Real-time telemetry is available through Azure Monitor views during an attack, and for history. Rich attack mitigation analytics are available via diagnostic settings. Application layer protection can be added through the Azure Application Gateway Web Application Firewall or by installing a 3rd party firewall from Azure Marketplace. Protection is provided for IPv4 and IPv6 Azure public IP addresses. How Azure denial-of-service protection works # DDoS Protection Standard monitors actual traffic utilization and constantly compares it against the thresholds defined in the DDoS policy. When the traffic threshold is exceeded, DDoS mitigation is automatically initiated. When traffic returns to a level below the threshold, the mitigation is removed. During mitigation, DDoS Protection redirects traffic sent to the protected resource and performs several checks, including: - Helping ensure that packets conform to internet specifications and aren\u2019t malformed. - Interacting with the client to determine if the traffic might be a spoofed packet (for example, using SYN Auth or SYN Cookie or dropping a packet for the source to retransmit it). - Using rate-limit packets if it can\u2019t perform any other enforcement method. DDoS Protection blocks attack traffic and forwards the remaining traffic to its intended destination. Within a few minutes of attack detection, you\u2019ll be notified with Azure Monitor metrics. By configuring logging on DDoS Protection Standard telemetry, you can write the logs to available options for future analysis. Azure Monitor retains metric data for DDoS Protection Standard for 30 days. Types of denial-of-service attacks that Azure protection mitigates # DDoS Protection Standard can mitigate the following types of attacks: Volumetric attacks : The attack's goal is to flood the network layer with a substantial amount of seemingly legitimate traffic. It includes UDP floods, amplification floods, and other spoofed-packet floods. DDoS Protection Standard mitigates these potential multi-gigabyte attacks by absorbing and scrubbing them, with Azure's global network scale, automatically. Protocol attacks : These attacks render a target inaccessible, by exploiting a weakness in the layer 3 and layer 4 protocol stack. It includes, SYN flood attacks, reflection attacks, and other protocol attacks. DDoS Protection Standard mitigates these attacks, differentiating between malicious and legitimate traffic, by interacting with the client, and blocking malicious traffic. Resource (application) layer attacks : These attacks target web application packets, to disrupt the transmission of data between hosts. The attacks include HTTP protocol violations, SQL injection, cross-site scripting, and other layer 7 attacks. Use a Web Application Firewall, such as the Azure Application Gateway web application firewall, as well as DDoS Protection Standard to provide defense against these attacks. There are also third-party web application firewall offerings available in the Azure Marketplace. ** Important** DDoS Protection Standard protects resources in a virtual network including public IP addresses associated with virtual machines, load balancers, and application gateways. When coupled with the Application Gateway web application firewall, or a third-party web application firewall deployed in a virtual network with a public IP, DDoS Protection Standard can provide full layer 3 to layer 7 mitigation capability. Explore Azure Firewall features # Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It\u2019s a fully stateful firewall-as-a-service with built-in high availability and unrestricted cloud scalability. By default, Azure Firewall blocks traffic. The Azure firewall features include # Built-in high availability - Because high availability is built in, no additional load balancers are required and there\u2019s nothing you need to configure. Unrestricted cloud scalability - Azure Firewall can scale up as much as you need, to accommodate changing network traffic flows so you don't need to budget for your peak traffic. Application Fully Qualified Domain Name (FQDN) filtering rules - You can limit outbound HTTP/S traffic to a specified list of FQDNs, including wild cards. This feature does not require SSL termination. Network traffic filtering rules - You can centrally create allow or deny network filtering rules by source and destination IP address, port, and protocol. Azure Firewall is fully stateful, so it can distinguish legitimate packets for different types of connections. Rules are enforced and logged across multiple subscriptions and virtual networks. Qualified domain tags - Fully Qualified Domain Names (FQDN) tags make it easier for you to allow well known Azure service network traffic through your firewall. For example, say you want to allow Windows Update network traffic through your firewall. You create an application rule and include the Windows Update tag. Now network traffic from Windows Update can flow through your firewall. Outbound Source Network Address Translation (OSNAT) support - All outbound virtual network traffic IP addresses are translated to the Azure Firewall public IP. You can identify and allow traffic originating from your virtual network to remote internet destinations. Inbound Destination Network Address Translation (DNAT) support - Inbound network traffic to your firewall public IP address is translated and filtered to the private IP addresses on your virtual networks. - Azure Monitor logging - All events are integrated with Azure Monitor, allowing you to archive logs to a storage account, stream events to your Event Hub, or send them to Azure Monitor logs. Grouping the features above into logical groups reveals that Azure Firewall has three rule types: NAT rules, network rules, and application rules. The application order precedence for the rules are that network rules are applied first, then application rules. Rules are terminating, which means if a match is found in network rules, then application rules are not processed. If there\u2019s no network rule match, and if the packet protocol is HTTP/HTTPS, the packet is then evaluated by the application rules. If no match continues to be found, then the packet is evaluated against the infrastructure rule collection. If there\u2019s still no match, then the packet is denied by default. NAT rules You can configure inbound connectivity by configuring Destination Network Address Translation (DNAT) as described in: Filter inbound traffic with Azure Firewall DNAT using the Azure portal. DNAT rules are applied first. If a match is found, an implicit corresponding network rule to allow the translated traffic is added. You can override this behavior by explicitly adding a network rule collection with deny rules that match the translated traffic. No application rules are applied for these connections. Firewall rules to secure Azure Storage Azure Storage provides a layered security model, which enables you to secure your storage accounts to a specific set of supported networks. When network rules are configured, only applications requesting data from over the specified set of networks can access a storage account. An application that accesses a storage account when network rules are in effect requires proper authorization on the request. Authorization is supported with Azure AD credentials for blobs and queues, a valid account access key, or a SAS token. By default, storage accounts accept connections from clients on any network. To limit access to selected networks, you must first change the default action. Making changes to network rules can impact your applications' ability to connect to Azure Storage. Setting the default network rule to Deny blocks all access to the data unless specific network rules that grant access are also applied. Be sure to grant access to any allowed networks using network rules before you change the default rule to deny access. Grant access from a virtual network You can configure storage accounts to allow access only from specific VNets. You enable a service endpoint for Azure Storage within the VNet. This endpoint gives traffic an optimal route to the Azure Storage service. The identities of the virtual network and the subnet are also transmitted with each request. Administrators can then configure network rules for the storage account that allow requests to be received from specific subnets in the VNet. Clients granted access via these network rules must continue to meet the authorization requirements of the storage account to access the data. Each storage account supports up to 100 virtual network rules, which could be combined with IP network rules. Controlling outbound and inbound network access is an important part of an overall network security plan. Network traffic is subjected to the configured firewall rules when you route your network traffic to the firewall as the default gateway. Deploy an Azure Firewall implementation # Controlling outbound network access is an important part of an overall network security plan. For example, you may want to limit access to web sites. Or, you may want to limit the outbound IP addresses and ports that can be accessed. One way you can control outbound network access from an Azure subnet is with Azure Firewall. With Azure Firewall, you can configure: Application rules that define fully qualified domain names (FQDNs) that can be accessed from a subnet. Network rules that define source address, protocol, destination port, and destination address. Network traffic is subjected to the configured firewall rules when you route your network traffic to the firewall as the subnet default gateway. Fully Qualified Domain Name (FQDN) tag An FQDN tag represents a group of fully qualified domain names (FQDNs) associated with well known Microsoft services. You can use an FQDN tag in application rules to allow the required outbound network traffic through your firewall. For example, to manually allow Windows Update network traffic through your firewall, you need to create multiple application rules per the Microsoft documentation. Using FQDN tags, you can create an application rule, include the Windows Updates tag, and now network traffic to Microsoft Windows Update endpoints can flow through your firewall. Infrastructure qualified domain names Azure Firewall includes a built-in rule collection for infrastructure FQDNs that are allowed by default. These FQDNs are specific for the platform and can't be used for other purposes. The following services are included in the built-in rule collection: Compute access to storage Platform Image Repository (PIR) Managed disks status storage access Azure Diagnostics and Logging (MDS) Logs and metrics You can monitor Azure Firewall using firewall logs. You can also use activity logs to audit operations on Azure Firewall resources. You can access some of these logs through the portal. Logs can be sent to Azure Monitor logs, Storage, and Event Hubs and analyzed in Azure Monitor logs or by different tools such as Excel and Power BI. Metrics are lightweight and can support near real-time scenarios making them useful for alerting and fast issue detection. Threat intelligence-based filtering Threat intelligence-based filtering can be enabled for your firewall to alert and deny traffic from/to known malicious IP addresses and domains. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed. Intelligent Security Graph powers Microsoft threat intelligence and is used by multiple services including Microsoft Defender for Cloud. If you've enabled threat intelligence-based filtering, the associated rules are processed before any of the NAT rules, network rules, or application rules. You can choose to just log an alert when a rule is triggered, or you can choose alert and deny mode. By default, threat intelligence-based filtering is enabled in alert mode. Rule processing logic You can configure NAT rules, network rules, and applications rules on Azure Firewall. Rule collections are processed according to the rule type in priority order, lower numbers to higher numbers from 100 to 65,000. A rule collection name can have only letters, numbers, underscores, periods, or hyphens. It must begin with a letter or number, and end with a letter, number or underscore. The maximum name length is 80 characters. It's best to initially space your rule collection priority numbers in 100 increments (100, 200, 300, and so on) so you have room to add more rule collections if needed. Service tags A service tag represents a group of IP address prefixes to help minimize complexity for security rule creation. You cannot create your own service tag, nor specify which IP addresses are included within a tag. Microsoft manages the address prefixes encompassed by the service tag, and automatically updates the service tag as addresses change. Azure Firewall service tags can be used in the network rules destination field. You can use them in place of specific IP addresses. Remote work support VDI Work from home policies requires many IT organizations to address fundamental changes in capacity, network, security, and governance. Employees aren't protected by the layered security policies associated with on-premises services while working from home. Virtual Desktop Infrastructure (VDI) deployments on Azure can help organizations rapidly respond to this changing environment. However, you need a way to protect inbound/outbound Internet access to and from these VDI deployments. You can use Azure Firewall DNAT rules along with its threat intelligence-based filtering capabilities to protect your VDI deployments. Virtual Desktop support Azure Virtual Desktop is a comprehensive desktop and app virtualization service running in Azure. It\u2019s the only virtual desktop infrastructure (VDI) that delivers simplified management, multi-session Windows 10, optimizations for Microsoft 365 ProPlus, and support for Remote Desktop Services (RDS) environments. You can deploy and scale your Windows desktops and apps on Azure in minutes and get built-in security and compliance features. Windows Virtual Desktop doesn't require you to open any inbound access to your virtual network. However, you must allow a set of outbound network connections for the Windows Virtual Desktop virtual machines that run in your virtual network. Configure VPN forced tunneling # Why do some cases require forced tunneling? - A virtual private network (VPN) consists of remote peers sending private data securely to one another over an unsecured network, such as the Internet. This is called Internet tunneling. Site-to-site (S2S) VPNs use tunnels to encapsulate data packets within normal IP packets for forwarding over IP-based networks, using encryption to ensure privacy and authentication to ensure integrity of data. Forced tunneling lets you redirect, or force, all internet-bound traffic back to your on-premises location via a site-to-site VPN tunnel for inspection and auditing. This is a critical security requirement for most enterprise IT policies. Without forced tunneling, internet-bound traffic from your VMs in Azure always traverses from the Azure network infrastructure directly to the internet\u2014without the option to allow you to inspect or audit the traffic. Unauthorized internet access potentially leads to information disclosure or other types of security breaches. As stated earlier, Azure currently works with two deployment models: The Resource Manager deployment model and the classic deployment model. The two models aren\u2019t completely compatible with each other. The following exercise goes through configuring tunneling for virtual networks that were created via the Resource Manager deployment model. The following figure depicts how forced tunneling works. In the preceding figure, the front-end subnet doesn\u2019t use forced tunneling. The workloads in the front-end subnet can continue to accept and respond to customer requests that come directly from the internet. The mid-tier and back-end subnets use forced tunneling. Any outbound connections from these two subnets to the internet are forced back to an on-premises site via one of the S2S VPN tunnels. This allows you to restrict and inspect internet access from your VMs or cloud services in Azure while continuing to enable your multi-tier service architecture. If no internet-facing workloads exist in your VMs, you can also apply forced tunneling to the entire virtual network. You configure forced tunneling in Azure via virtual network User Defined Routes (UDR). Redirecting traffic to an on-premises site is expressed as a default route to the Azure VPN gateway. This example uses UDRs to create a routing table to first add a default route and then associate the routing table with your virtual network subnets to enable forced tunneling on those subnets. Create User Defined Routes and Network Virtual Appliances # User Defined Routes A User Defined Routes (UDR) is a custom route in Azure that overrides Azure's default system routes or adds routes to a subnet's route table. In Azure, you create a route table and then associate that route table with zero or more virtual network subnets. Each subnet can have zero or one route table associated with it. If you create a route table and associate it to a subnet, Azure either combines its routes with the default routes that Azure adds to a subnet or overrides those default routes. In this diagram UDRs are used to direct traffic from the Gateway subnet and the Web tier to the Network Virtual Appliance (NVA). Network Virtual Appliances You can deploy an NVA to a perimeter network in many architectures. In the previous diagram, the NVA helps provide a secure network boundary by checking all inbound and outbound network traffic and then passing only the traffic that meets the network security rules. However, the fact that all network traffic passes through the NVA means that the NVA is a single point of failure in the network. If the NVA fails, no other path will exist for network traffic, and all the back-end subnets will become unavailable. To make an NVA highly available, deploy more than one NVA into an availability set. The following figure shows a high-availability architecture that implements an ingress perimeter network behind an internet-facing load balancer. This architecture is designed to provide connectivity to Azure workloads for layer 7 traffic, such as HTTP or HTTPS traffic. The benefit of this architecture is that all NVAs are active, and if one fails, the load balancer directs network traffic to the other NVA. Both NVAs route traffic to the internal load balancer, so if one NVA is active, traffic will continue to flow. The NVAs are required to terminate SSL traffic intended for the web tier VMs. These NVAs can\u2019t be extended to handle on-premises traffic, because on-premises traffic requires another dedicated set of NVAs with their own network. UDRs and NSGs help provide layer 3 and layer 4 (of the OSI model) security. NVAs help provide layer 7, application layer, security. Explore hub and spoke topology # This reference architecture shows how to implement a hub-spoke topology in Azure. The hub is a virtual network in Azure that acts as a central point of connectivity to your on-premises network. The spokes are virtual networks that peer with the hub and can be used to isolate workloads. Traffic flows between the on-premises datacenter and the hub through an ExpressRoute or VPN gateway connection. Typical uses for this architecture include: Workloads deployed in different environments, such as development, testing, and production, that require shared services such as DNS, IDS, NTP, or AD DS. Shared services are placed in the hub virtual network, while each environment is deployed to a spoke to maintain isolation. Workloads that do not require connectivity to each other, but require access to shared services. Enterprises that require central control over security aspects, such as a firewall in the hub as a DMZ, and segregated management for the workloads in each spoke. The architecture consists of the following components. On-premises network - A private local-area network running within an organization. VPN device - A device or service that provides external connectivity to the on-premises network. The VPN device may be a hardware device or a software solution such as the Routing and Remote Access Service (RRAS) in Windows Server 2012. For a list of supported VPN appliances and information on configuring selected VPN appliances for connecting to Azure, see About VPN devices for Site-to-Site VPN Gateway connections. VPN virtual network gateway or ExpressRoute gateway - The virtual network gateway enables the virtual network to connect to the VPN device, or ExpressRoute circuit, used for connectivity with your on-premises network. For more information, see Connect an on-premises network to a Microsoft Azure virtual network. Hub virtual network - The virtual network is used as the hub in the hub-spoke topology. The hub is the central point of connectivity to your on-premises network, and a place to host services that can be consumed by the different workloads hosted in the spoke virtual networks. Gateway subnet - The virtual network gateways are held in the same subnet. Spoke virtual networks - One or more virtual networks that are used as spokes in the hub-spoke topology. Spokes can be used to isolate workloads in their own virtual networks, managed separately from other spokes. Each workload might include multiple tiers, with multiple subnets connected through Azure load balancers. For more information about the application infrastructure, see Running Windows VM workloads and Running Linux VM workloads. Virtual network peering - Two virtual networks can be connected using a peering connection. Peering connections are non-transitive, low latency connections between virtual networks. Once peered, the virtual networks exchange traffic by using the Azure backbone, without the need for a router. In hub-spoke network topology, you use virtual network peering to connect the hub to each spoke. You can peer virtual networks in the same region or different regions. For more information, see Requirements and constraints. Perform try-this exercises # VNet Peering This lab requires two virtual machines. Each virtual machine should be in a different virtual network. For these instructions, we have AZ500vm01, AZ500vm02, AZ500-vnet, AZ500-vnet1, and az500-rg. To save time, you can connect to each virtual machine. Also, it might be helpful to edit the default.htm page on each machine, so the page provides the virtual machine name. For example, This is AZ500vm01. In this demonstration, you will configure and test VNet peering. Review the infrastructure setup In this task, you will review the infrastructure that has been configured for this demonstration. In the Portal, navigate to Virtual Machines. Show there are two virtual machines, AZ500vm01 and AZ500vm02. Select AZ500vm01 and review the IP addresses. Select AZ500vm02 and review the IP addresses. Make a note of the private IP address. Based on the addressing, discuss how each machine is in a different subnet. In the Portal navigate to Virtual networks. Show there are two virtual networks, AZ500-vnet and AZ500-vnet1. Test the virtual machine connections In this task, you will test connecting from AZ500vm01 to AZ500vm02's private IP address. This connection will not work. The virtual machines are in different virtual networks. Use RDP to connect to AZ500vm01. In a browser, view the http://localhost.default.htm This page should display without error. Use RDP to connect to AZ500vm02 In a browser, view the http://localhost.default.htm This page should display without error. The above steps show that IIS is working on the virtual machines. Return to your AZ500vm01 RDP session. We will now try to access AZ500vm02. In a browser, view the http://private_IP_address_of_AZ500vm02/default.htm The page will not display. AZ500vm01 cannot access AZ500vm02 using the private address. Configure VNet peering and test the connections In this task, you will configure VNet peering and test the previous connection. The connection will now work. In the Portal, navigate to the AZ500-vnet virtual network. Under Settings select Peerings. Add a virtual network peering. The page adapts as you make selections. Name of the peering from az500-vnet to remote virtual network: Peering-A-to-B Virtual network: AZ500-vnet1 (az500-rg) Name of the peering from az500-vnet1 to az500-vnet: Peering-B-to-A Discuss the other configuration options. Click OK. Follow the notifications while the virtual network peerings are deployed. Return to your AZ500vm01 RDP session. In the browser, refresh the http://private_IP_address_of_AZ500vm02/default.htm This page should now display. Azure Firewall This task requires a virtual network with two subnets, Subnet1 and Jumpnet. Subnet1 has the 10.0.0.0/24 address range. Jumpnet has the 10.0.1.0/24 address range. Subnet1 includes a Windows virtual machine. Your resource names may be different. Configure the firewall subnet In the Portal, select your virtual network. Under Settings, select Subnets. Click + Subnet to add a new subnet for the firewall. Name: AzureFirewallSubnet Address range: 10.0.2.0/24 There is not need for a NAT Gateway, NSG, Route table, or services. Click Add. Wait for the subnet to deploy. Add and configure the firewall Search for and select Firewalls. Discuss the benefits of a firewall and how it can be used to increase perimeter security. Click + Add. Complete the required configuration information: subscription, resource group, name, and region. Select your Virtual network. Add a new Firewall public IP address. Create the firewall and wait for it to deploy. Navigate to your new firewall. On the Overview blade, locate the Firewall private IP. Copy the address to the clipboard. Create a route table and route that uses the firewall Search for and select Route tables. Add a new route table. Complete the required configuration information: name, subscription, resource group, and location. Disable Virtual network gateway route propagation. Review what this means. Create the route table and wait for it to deploy. Navigate to the new route table. Under Settings, click Routes. Add a new route. This route will ensure traffic goes through the firewall. Discuss the different next hop types. Route name: your choice Address prefix: 0.0.0.0/0/ Next hop type: Virtual appliance Next hope address: Firewall_private_IP_address When finished click Ok and wait for the new route to deploy. Associate the route table with Subnet1 Still in the route table resource, under Settings click Subnets. Associate your virtual network and Subnet1. This will ensure Subnet1 uses the route table. When you are finished click Ok and wait for the association to complete. Test the firewall In the Portal, navigate to a virtual machine in Subnet1. From the Overview blade, ensure the VM is running. Click Connect and RDP into the VM. On the virtual machine, open a browser. Try to access: www.msn.com Notice the error. Action denied. No rule matches. Add a firewall application rule In the Portal, navigate to your firewall. Under Settings select Rules. Select the Application rule selection tab. Click Add application rule collection. Review how application rules work and complete the required information. Name: your choice Priority: 300 Action: Allow Continue completing the rule, under Target FQDNs. This will allow Subnet1 IP address to traverse the firewall. Name: Allow-MSN Source type: IP address Source: 10.0.0.0/24 Protocol:Port: http,https Target FQDNs: www.msn.com Click Add and wait for the firewall to be updated. Test the firewall again In your VM RDP session, refresh the browser page. The MSN.com page should now display. Knowledge check Which of the following features of Azure networking enables the redirect of Internet traffic back to the company's on-premises servers for packet inspection? User Defined Routes ( Ans ) User-defined routes and forced tunneling. Use forced tunneling to redirect internet bound traffic back to the company's on-premises infrastructure. Forced tunneling is commonly used in scenarios where organizations want to implement packet inspection or corporate audits. Forced tunneling in Azure is configured via virtual network user-defined routes (UDR). Cross-premises network connectivity Traffic Manager When configuring Azure Firewall, the organization needs to allow Windows Update network traffic through the firewall. Which of the following rules should be configured? Destination inbound rules NAT rules Application rules( Ans ) Application rules. Application rules define fully qualified domain names (FQDNs) that can be accessed from a subnet. Usage of FQDNs would be appropriate to allow Windows Update network traffic. An organization would like to limit outbound Internet traffic from a subnet, which product should be installed and configured? Azure Web Application Firewall Azure Firewall( Ans ) Azure Firewall. Azure Firewall can limit the outbound IP addresses and ports that can be accessed. Define network rules that assign source address, protocol, destination port, and destination address. Load Balancer An organization has a web application and is concerned about attacks that flood the network layer with a substantial amount of seemingly legitimate traffic, how can this type of attack be blocked? Add a Web Application Firewall Add an Azure Firewall Create a DDoS policy( Ans ) Create a DDoS policy to provide defense against the exhaustion resources. This exhaustion could make an application unavailable to legitimate users for example. Configure network security # Introduction Network security could be defined as the process of protecting resources from unauthorized access or attack by applying controls to network traffic. The goal is to ensure that only legitimate traffic is allowed. Azure includes a robust networking infrastructure to support your application and service connectivity requirements. Network connectivity is possible between resources located in Azure, between on-premises and Azure hosted resources, and to and from the internet and Azure. Scenario A security engineer uses network security features to restrict, monitor, and manage network traffic once it reaches your Azure network; you will work on such tasks as: Setup network security groups and application security groups. Deploy Service Endpoints and Private Links. Configure Front Door and ExpressRoute. Skills measured Securing the Azure platform your cloud solutions run on is a part of Exam AZ-500: Microsoft Azure Security Engineer. Implement platform protection (15-20%) Implement advanced network security - Secure the connectivity of virtual networks (VPN authentication, Express Route encryption) - Configure Network Security Groups (NSGs) and Application Security Groups (ASGs) - Configure Azure Front Door service as an Application Gateway - Configure a Web Application Firewall (WAF) on Azure Application Gateway - Implement Service Endpoints Explore Network Security Groups (NSG) Network traffic can be filtered to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol. VMs that you create via the Resource Manager deployment model can have direct connectivity to the internet by using a public IP address that is directly assigned to the VMs. Only the host firewall configured inside the VMs helps protect these VMs from the internet. VMs that you create by using the classic deployment model communicate with internet resources through the cloud service that is assigned the public IP address, which is also known as the VIP. VMs that reside inside the cloud service share that VIP and establish communication with internet resources by using endpoints. If you remove the VM endpoints that map the public port and public IP address of the cloud service to the private port and private IP address of the VM, the VM becomes unreachable from the internet via the public IP address. Network Security Groups (NSGs) help provide advanced security for the VMs you create via either deployment model (Resource Manager or classic). NSGs control inbound and outbound traffic passing through a network adapter (in the Resource Manager deployment model), a VM (in the classic deployment model), or a subnet (in both deployment models). Network Security Group rules NSGs contain rules that specify whether traffic will be approved or denied. Each rule is based on a source IP address, a source port, a destination IP address, and a destination port. Based on whether the traffic matches this combination, the rule either allows or denies the traffic. Each rule consists of the following properties: Name . This is a unique identifier for the rule. Direction . This specifies whether the traffic is inbound or outbound. Priority . If multiple rules match the traffic, rules with a higher priority apply. Access . This specifies whether the traffic is allowed or denied. Source IP address prefix . This prefix identifies where the traffic originated from. It can be based on a single IP address; a range of IP addresses in Classless Interdomain Routing (CIDR) notation; or the asterisk (*), which is a wildcard that matches all possible IP addresses. Source port range . This specifies source ports by using either a single port number from 1 through 65,535; a range of ports (for example, 200\u2013400); or the asterisk (*) to denote all possible ports. Destination IP address prefix . This identifies the traffic destination based on a single IP address, a range of IP addresses in CIDR notation, or the asterisk (*) to match all possible IP addresses. Destination port range . This specifies destination ports by using either a single port number from 1 through 65,535; a range of ports (for example, 200\u2013400); or the asterisk (*) to denote all possible ports. Protocol . This specifies a protocol that matches the rule. It can be UDP, TCP, or the asterisk (*). Custom Network Security Group rules Predefined default rules exist for inbound and outbound traffic. You can\u2019t delete these rules, but you can override them, because they have the lowest priority. The default rules allow all inbound and outbound traffic within a virtual network, allow outbound traffic towards the internet, and allow inbound traffic to an Azure load balancer. A default rule with the lowest priority also exists in both the inbound and outbound sets of rules that denies all network communication. When you create a custom rule, you can use default tags in the source and destination IP address prefixes to specify predefined categories of IP addresses. These default tags are: Internet. This tag represents internet IP addresses. Virtual_network. This tag identifies all IP addresses that the IP range for the virtual network defines. It also includes IP address ranges from on-premises networks when they are defined as local network to virtual network. Azure_loadbalancer. This tag specifies the default Azure load balancer destination. Planning Network Security Groups You can design NSGs to isolate virtual networks in security zones, like the model used by on-premises infrastructure does. You can apply NSGs to subnets, which allows you to create protected screened subnets, or DMZs, that can restrict traffic flow to all the machines residing within that subnet. With the classic deployment model, you can also assign NSGs to individual computers to control traffic that is both destined for and leaving the VM. With the Resource Manager deployment model, you can assign NSGs to a network adapter so that NSG rules control only the traffic that flows through that network adapter. If the VM has multiple network adapters, NSG rules won\u2019t automatically be applied to traffic that is designated for other network adapters. You create NSGs as resources in a resource group, but you can share them with other resource groups in your subscription. Deploy a Network Security Groups implementation When implementing NSGs, these are the limits to keep in mind: By default, you can create 100 NSGs per region per subscription. You can raise this limit to 400 by contacting Azure support. You can apply only one NSG to a VM, subnet, or network adapter. By default, you can have up to 200 rules in a single NSG. You can raise this limit to 500 by contacting Azure support. You can apply an NSG to multiple resources. An individual subnet can have zero, or one, associated NSG. An individual network interface can also have zero, or one, associated NSG. So, you can effectively have dual traffic restriction for a virtual machine by associating an NSG first to a subnet, and then another NSG to the VM's network interface. The application of NSG rules in this case depends on the direction of traffic and priority of applied security rules. Consider a simple example with one virtual machine as follows: The virtual machine is placed inside the Contoso Subnet. Contoso Subnet is associated with Subnet NSG. The VM network interface is additionally associated with VM NSG. In this example, for inbound traffic, the Subnet NSG is evaluated first. Any traffic allowed through Subnet NSG is then evaluated by VM NSG. The reverse is applicable for outbound traffic, with VM NSG being evaluated first. Any traffic allowed through VM NSG is then evaluated by Subnet NSG. This allows for granular security rule application. For example, you might want to allow inbound internet access to a few application VMs (such as frontend VMs) under a subnet but restrict inbound internet access to other VMs (such as database and other backend VMs). In this case you can have a more lenient rule on the Subnet NSG, allowing internet traffic, and restrict access to specific VMs by denying access on VM NSG. The same can be applied for outbound traffic. To help secure and protect your Azure resources, make sure NSG planning is standard operating procedure (SOP) for your deployments. How traffic is evaluated Several resources from Azure services can be deployed into an Azure virtual network. You can associate zero, or one, network security group to each virtual network subnet and network interface in a virtual machine. The same network security group can be associated to as many subnets and network interfaces as you choose. The following picture illustrates different scenarios for how network security groups might be deployed to allow network traffic to and from the internet over TCP port 80: Reference the above diagram, along with the following text, to understand how Azure processes inbound and outbound rules for network security groups: Inbound traffic For inbound traffic, Azure processes the rules in a network security group associated to a subnet first, if there is one, and then the rules in a network security group associated to the network interface, if there is one. VM1: The security rules in NSG1 are processed, since it is associated to Subnet1 and VM1 is in Subnet1. Unless you've created a rule that allows port 80 inbound, the traffic is denied by the DenyAllInbound default security rule, and never evaluated by NSG2, since NSG2 is associated to the network interface. If NSG1 has a security rule that allows port 80, the traffic is then processed by NSG2. To allow port 80 to the virtual machine, both NSG1 and NSG2 must have a rule that allows port 80 from the internet. VM2: The rules in NSG1 are processed because VM2 is also in Subnet1. Since VM2 does not have a network security group associated to its network interface, it receives all traffic allowed through NSG1 or is denied all traffic denied by NSG1. Traffic is either allowed or denied to all resources in the same subnet when a network security group is associated to a subnet. VM3: Since there is no network security group associated to Subnet2, traffic is allowed into the subnet and processed by NSG2, because NSG2 is associated to the network interface attached to VM3. VM4: Traffic is allowed to VM4, because a network security group isn't associated to Subnet3, or the network interface in the virtual machine. All network traffic is allowed through a subnet and network interface if they don't have a network security group associated to them. Outbound traffic For outbound traffic, Azure processes the rules in a network security group associated to a network interface first, if there is one, and then the rules in a network security group associated to the subnet, if there is one. VM1: The security rules in NSG2 are processed. Unless you create a security rule that denies port 80 outbound to the internet, the traffic is allowed by the AllowInternetOutbound default security rule in both NSG1 and NSG2. If NSG2 has a security rule that denies port 80, the traffic is denied, and never evaluated by NSG1. To deny port 80 from the virtual machine, either, or both of the network security groups must have a rule that denies port 80 to the internet. VM2: All traffic is sent through the network interface to the subnet, since the network interface attached to VM2 does not have a network security group associated to it. The rules in NSG1 are processed. VM3: If NSG2 has a security rule that denies port 80, the traffic is denied. If NSG2 has a security rule that allows port 80, then port 80 is allowed outbound to the internet, since a network security group is not associated to Subnet2. VM4: All network traffic is allowed from VM4, because a network security group isn't associated to the network interface attached to the virtual machine, or to Subnet3. Intra-subnet traffic It's important to note that security rules in an NSG associated to a subnet can affect connectivity between VM's within it. For example, if a rule is added to NSG1 which denies all inbound and outbound traffic, VM1 and VM2 will no longer be able to communicate with each other. Another rule would have to be added specifically to allow this. General guidelines Unless you have a specific reason to, we recommended that you associate a network security group to a subnet, or a network interface, but not both. Since rules in a network security group associated to a subnet can conflict with rules in a network security group associated to a network interface, you can have unexpected communication problems that require troubleshooting. Create Application Security Groups In this topic we look at Application Security Groups (ASGs), which are built on network security groups. A quick review of Security groups reminds us that you can filter network traffic to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol. You can enable network security group flow logs to analyze network traffic to and from resources that have an associated network security group. Application security groups ASGs enable you to configure network security as a natural extension of an application's structure. You then can group VMs and define network security policies based on those groups. You also can reuse your security policy at scale without manual maintenance of explicit IP addresses. The platform manages the complexity of explicit IP addresses and multiple rule sets, allowing you to focus on your business logic. Consider the following illustration. In the illustration, NIC1 and NIC2 are members of the AsgWeb ASG. NIC3 is a member of the AsgLogic ASG. NIC4 is a member of the AsgDb ASG. Though each network interface in this example is a member of only one ASG, a network interface can be a member of multiple ASGs, up to the Azure limits. None of the network interfaces have an associated network security group. NSG1 is associated to both subnets and contains the following rules: Allow-HTTP-Inbound-Internet Deny-Database-All Allow-Database-BusinessLogic The rules that specify an ASG as the source or destination are only applied to the network interfaces that are members of the ASG. If the network interface is not a member of an ASG, the rule is not applied to the network interface even though the network security group is associated to the subnet. Application security groups have the following constraints There are limits to the number of ASGs you can have in a subscription, in addition to other limits related to ASGs. You can specify one ASG as the source and destination in a security rule. You cannot specify multiple ASGs in the source or destination. All network interfaces assigned to an ASG must exist in the same virtual network that the first network interface assigned to the ASG is in. For example, if the first network interface assigned to an ASG named AsgWeb is in the virtual network named VNet1, then all subsequent network interfaces assigned to ASGWeb must exist in VNet1. You cannot add network interfaces from different virtual networks to the same ASG. If you specify an ASG as the source and destination in a security rule, the network interfaces in both ASGs must exist in the same virtual network. For example, if AsgLogic contained network interfaces from VNet1, and AsgDb contained network interfaces from VNet2, you could not assign AsgLogic as the source and AsgDb as the destination in a rule. All network interfaces for both the source and destination ASGs need to exist in the same virtual network. Summary Application Security Groups along with NSGs, have brought multiple benefits on the network security area: A single management experience Increased limits on multiple dimensions A great level of simplification A seamless integration with your architecture Enable service endpoints A virtual network service endpoint provides the identity of your virtual network to the Azure service. Once service endpoints are enabled in your virtual network, you can secure Azure service resources to your virtual network by adding a virtual network rule to the resources. Today, Azure service traffic from a virtual network uses public IP addresses as source IP addresses. With service endpoints, service traffic switches to use virtual network private addresses as the source IP addresses when accessing the Azure service from a virtual network. This switch allows you to access the services without the need for reserved, public IP addresses used in IP firewalls. A common usage case for service endpoints is a virtual machine accessing storage. The storage account restricts access to the virtual machines private IP address. Why use a service endpoint? Improved security for your Azure service resources. VNet private address space can be overlapping and so, cannot be used to uniquely identify traffic originating from your VNet. Service endpoints provide the ability to secure Azure service resources to your virtual network, by extending VNet identity to the service. Once service endpoints are enabled in your virtual network, you can secure Azure service resources to your virtual network by adding a virtual network rule to the resources. This provides improved security by fully removing public Internet access to resources, and allowing traffic only from your virtual network. Optimal routing for Azure service traffic from your virtual network. Today, any routes in your virtual network that force Internet traffic to your premises and/or virtual appliances, known as forced-tunneling, also force Azure service traffic to take the same route as the Internet traffic. Service endpoints provide optimal routing for Azure traffic. Endpoints always take service traffic directly from your virtual network to the service on the Microsoft Azure backbone network. Keeping traffic on the Azure backbone network allows you to continue auditing and monitoring outbound Internet traffic from your virtual networks, through forced-tunneling, without impacting service traffic. Simple to set up with less management overhead. You no longer need reserved, public IP addresses in your virtual networks to secure Azure resources through IP firewall. There are no NAT or gateway devices required to set up the service endpoints. Service endpoints are configured through a simple click on a subnet. There is no additional overhead to maintaining the endpoints. Important: With service endpoints, the source IP addresses of the virtual machines in the subnet for service traffic switches from using public IPv4 addresses to using private IPv4 addresses. Existing Azure service firewall rules using Azure public IP addresses will stop working with this switch. Please ensure Azure service firewall rules allow for this switch before setting up service endpoints. You may also experience temporary interruption to service traffic from this subnet while configuring service endpoints. Configure service endpoint services Service endpoints would provide benefits in the following Scenarios. Scenarios Peered, connected, or multiple virtual networks: To secure Azure services to multiple subnets within a virtual network or across multiple virtual networks, you can enable service endpoints on each of the subnets independently, and secure Azure service resources to all of the subnets. Filtering outbound traffic from a virtual network to Azure services: If you want to inspect or filter the traffic sent to an Azure service from a virtual network, you can deploy a network virtual appliance within the virtual network. You can then apply service endpoints to the subnet where the network virtual appliance is deployed, and secure Azure service resources only to this subnet. This scenario might be helpful if you want use network virtual appliance filtering to restrict Azure service access from your virtual network only to specific Azure resources. Securing Azure resources to services deployed directly into virtual networks: You can directly deploy various Azure services into specific subnets in a virtual network. You can secure Azure service resources to managed service subnets by setting up a service endpoint on the managed service subnet. Disk traffic from an Azure virtual machine: Virtual Machine Disk traffic for managed and unmanaged disks isn't affected by service endpoints routing changes for Azure Storage. This traffic includes diskIO as well as mount and unmount. You can limit REST access to page blobs to select networks through service endpoints and Azure Storage network rules. Deploy private links Azure Private Link works on an approval call flow model wherein the Private Link service consumer can request a connection to the service provider for consuming the service. The service provider can then decide whether to allow the consumer to connect or not. Azure Private Link enables the service providers to manage the private endpoint connection on their resources There are two connection approval methods that a Private Link service consumer can choose from: Automatic: If the service consumer has RBAC permissions on the service provider resource, the consumer can choose the automatic approval method. In this case, when the request reaches the service provider resource, no action is required from the service provider and the connection is automatically approved. Manual: On the contrary, if the service consumer doesn\u2019t have RBAC permissions on the service provider resource, the consumer can choose the manual approval method. In this case, the connection request appears on the service resources as Pending. The service provider has to manually approve the request before connections can be established. In manual cases, service consumer can also specify a message with the request to provide more context to the service provider. The service provider has following options to choose from for all Private Endpoint connections: Approved Reject Remove Manage private endpoint connections on Azure PaaS resources Portal is the preferred method for managing private endpoint connections on Azure PaaS resources. Implement an Azure application gateway Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Traditional load balancers operate at the transport layer (OSI layer 4 - TCP and UDP) and route traffic based on the source IP address and port to a destination IP address and port. Application Gateway can make routing decisions based on additional attributes of an HTTP request, for example, URI path or host headers. For example, you can route traffic based on the incoming URL. So if /images are in the incoming URL, you can route traffic to a specific set of servers (known as a pool) configured for images. If /video is in the URL, that traffic is routed to another pool that's optimized for videos. This type of routing is known as application layer (OSI layer 7) load balancing. Application Gateway includes the following features: Secure Sockets Layer (SSL/TLS) termination - Application gateway supports SSL/TLS termination at the gateway, after which traffic typically flows unencrypted to the backend servers. This feature allows web servers to be unburdened from costly encryption and decryption overhead. Autoscaling - Application Gateway Standard_v2 supports autoscaling and can scale up or down based on changing traffic load patterns. Autoscaling also removes the requirement to choose a deployment size or instance count during provisioning. Zone redundancy - A Standard_v2 Application Gateway can span multiple Availability Zones, offering better fault resiliency and removing the need to provision separate Application Gateways in each zone. Static VIP - The application gateway Standard_v2 SKU supports static VIP type exclusively. This ensures that the VIP associated with application gateway doesn't change even over the lifetime of the Application Gateway. Web Application Firewall - Web Application Firewall (WAF) is a service that provides centralized protection of your web applications from common exploits and vulnerabilities. WAF is based on rules from the OWASP (Open Web Application Security Project) core rule sets 3.1 (WAF_v2 only), 3.0, and 2.2.9. Ingress Controller for AKS - Application Gateway Ingress Controller (AGIC) allows you to use Application Gateway as the ingress for an Azure Kubernetes Service (AKS) cluster. URL-based routing - URL Path Based Routing allows you to route traffic to back-end server pools based on URL Paths of the request. One of the scenarios is to route requests for different content types to different pool. Multiple-site hosting - Multiple-site hosting enables you to configure more than one web site on the same application gateway instance. This feature allows you to configure a more efficient topology for your deployments by adding up to 100 web sites to one Application Gateway (for optimal performance). Redirection - A common scenario for many web applications is to support automatic HTTP to HTTPS redirection to ensure all communication between an application and its users occurs over an encrypted path. Session affinity - The cookie-based session affinity feature is useful when you want to keep a user session on the same server. Websocket and HTTP/2 traffic - Application Gateway provides native support for the WebSocket and HTTP/2 protocols. There's no user-configurable setting to selectively enable or disable WebSocket support. Connection draining - Connection draining helps you achieve graceful removal of backend pool members during planned service updates. Custom error pages - Application Gateway allows you to create custom error pages instead of displaying default error pages. You can use your own branding and layout using a custom error page. Rewrite HTTP headers - HTTP headers allow the client and server to pass additional information with the request or the response. Sizing - Application Gateway Standard_v2 can be configured for autoscaling or fixed size deployments. This SKU doesn't offer different instance sizes. New Application Gateway v1 SKU deployments can take up to 20 minutes to provision. Changes to instance size or count aren't disruptive, and the gateway remains active during this time. Most deployments that use the v2 SKU take around 6 minutes to provision. However it can take longer depending on the type of deployment. For example, deployments across multiple Availability Zones with many instances can take more than 6 minutes. Deploy a web application firewall Web Application Firewall (WAF) provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks. Preventing such attacks in application code is challenging. It can require rigorous maintenance, patching, and monitoring at multiple layers of the application topology. A centralized web application firewall helps make security management much simpler. A WAF also gives application administrators better assurance of protection against threats and intrusions. A WAF solution can react to a security threat faster by centrally patching a known vulnerability, instead of securing each individual web application. Supported service WAF can be deployed with Azure Application Gateway, Azure Front Door, and Azure Content Delivery Network (CDN) service from Microsoft. WAF on Azure CDN is currently under public preview. WAF has features that are customized for each specific service. Configure and manage Azure front door Azure Front Door enables you to define, manage, and monitor the global routing for your web traffic by optimizing for best performance and instant global failover for high availability. With Front Door, you can transform your global (multi-region) consumer and enterprise applications into robust, high-performance personalized modern applications, APIs, and content that reaches a global audience with Azure. Front Door works at Layer 7 or HTTP/HTTPS layer and uses split TCP-based anycast protocol. Front Door ensures that your end users promptly connect to the nearest Front Door POP (Point of Presence). So, per your routing method selection in the configuration, you can ensure that Front Door is routing your client requests to the fastest and most available application backend. An application backend is any Internet-facing service hosted inside or outside of Azure. Front Door provides a range of traffic-routing methods and backend health monitoring options to suit different application needs and automatic failover models. Like Traffic Manager, Front Door is resilient to failures, including the failure of an entire Azure region. The following features are included with Front Door: Accelerate application performance - Using split TCP-based anycast protocol, Front Door ensures that your end users promptly connect to the nearest Front Door POP (Point of Presence). Increase application availability with smart health probes - Front Door delivers high availability for your critical applications using its smart health probes, monitoring your backends for both latency and availability and providing instant automatic failover when a backend goes down. URL-based routing - URL Path Based Routing allows you to route traffic to backend pools based on URL paths of the request. One of the scenarios is to route requests for different content types to different backend pools. Multiple-site hosting - Multiple-site hosting enables you to configure more than one web site on the same Front Door configuration. Session affinity - The cookie-based session affinity feature is useful when you want to keep a user session on the same application backend. TLS termination - Front Door supports TLS termination at the edge that is, individual users can set up a TLS connection with Front Door environments instead of establishing it over long haul connections with the application backend. Custom domains and certificate management - When you use Front Door to deliver content, a custom domain is necessary if you would like your own domain name to be visible in your Front Door URL. Application layer security - Azure Front Door allows you to author custom Web Application Firewall (WAF) rules for access control to protect your HTTP/HTTPS workload from exploitation based on client IP addresses, country code, and http parameters. URL redirection - With the strong industry push on supporting only secure communication, web applications are expected to automatically redirect any HTTP traffic to HTTPS. URL rewrite - Front Door supports URL rewrite by allowing you to configure an optional Custom Forwarding Path to use when constructing the request to forward to the backend. Protocol support - IPv6 and HTTP/2 traffic - Azure Front Door natively supports end-to-end IPv6 connectivity and HTTP/2 protocol. As mentioned above, routing to the Azure Front Door environments leverages Anycast for both DNS (Domain Name System) and HTTP (Hypertext Transfer Protocol) traffic, so user traffic will go to the closest environment in terms of network topology (fewest hops). This architecture typically offers better round-trip times for end users (maximizing the benefits of Split TCP). Front Door organizes its environments into primary and fallback \"rings\". The outer ring has environments that are closer to users, offering lower latencies. The inner ring has environments that can handle the failover for the outer ring environment in case an issue happens. The outer ring is the preferred target for all traffic, but the inner ring is necessary to handle traffic overflow from the outer ring. In terms of VIPs (Virtual Internet Protocol addresses), each frontend host, or domain served by Front Door is assigned a primary VIP, which is announced by environments in both the inner and outer ring, as well as a fallback VIP, which is only announced by environments in the inner ring. This overall strategy ensures that requests from your end users always reach the closest Front Door environment and that even if the preferred Front Door environment is unhealthy then traffic automatically moves to the next closest environment. Review ExpressRoute ExpressRoute is a direct, private connection from your WAN (not over the public Internet) to Microsoft Services, including Azure. Site-to-Site VPN traffic travels encrypted over the public Internet. Being able to configure Site-to-Site VPN and ExpressRoute connections for the same virtual network has several advantages. You can configure a Site-to-Site VPN as a secure failover path for ExpressRoute, or use Site-to-Site VPNs to connect to sites that are not part of your network, but that are connected through ExpressRoute. Notice that this configuration requires two virtual network gateways for the same virtual network, one using the gateway type 'Vpn', and the other using the gateway type 'ExpressRoute'. ExpressRoute encryption IPsec over ExpressRoute for Virtual WAN Azure Virtual WAN uses an Internet Protocol Security (IPsec) Internet Key Exchange (IKE) VPN connection from your on-premises network to Azure over the private peering of an Azure ExpressRoute circuit. This technique can provide an encrypted transit between the on-premises networks and Azure virtual networks over ExpressRoute, without going over the public internet or using public IP addresses. The following diagram shows an example of VPN connectivity over ExpressRoute private peering. The diagram shows a network within the on-premises network connected to the Azure hub VPN gateway over ExpressRoute private peering. The connectivity establishment is straightforward: Establish ExpressRoute connectivity with an ExpressRoute circuit and private peering. Establish the VPN connectivity. An important aspect of this configuration is routing between the on-premises networks and Azure over both the ExpressRoute and VPN paths. ExpressRoute supports a couple of encryption technologies to ensure confidentiality and integrity of the data traversing between your network and Microsoft's network. Point-to-point encryption by MACsec MACsec is an IEEE standard. It encrypts data at the Media Access control (MAC) level or Network Layer 2. You can use MACsec to encrypt the physical links between your network devices and Microsoft's network devices when you connect to Microsoft via ExpressRoute Direct. MACsec is disabled on ExpressRoute Direct ports by default. You bring your own MACsec key for encryption and store it in Azure Key Vault. You decide when to rotate the key. End-to-end encryption by IPsec and MACsec IPsec is an IETF standard. It encrypts data at the Internet Protocol (IP) level or Network Layer 3. You can use IPsec to encrypt an end-to-end connection between your on-premises network and your virtual network (VNET) on Azure. MACsec secures the physical connections between you and Microsoft. IPsec secures the end-to-end connection between you and your virtual networks on Azure. You can enable them independently. ExpressRoute Direct ExpressRoute Direct gives you the ability to connect directly into Microsoft\u2019s global network at peering locations strategically distributed across the world. ExpressRoute Direct provides dual 100 Gbps or 10 Gbps connectivity, which supports Active/Active connectivity at scale Key features that ExpressRoute Direct provides include, but aren't limited to: Massive Data Ingestion into services like Storage and Cosmos DB Physical isolation for industries that are regulated and require dedicated and isolated connectivity like: Banking, Government, and Retail Granular control of circuit distribution based on business unit ExpressRoute Direct supports massive data ingestion scenarios into Azure storage and other big data services. ExpressRoute circuits on 100 Gbps ExpressRoute Direct now also support 40 Gbps and 100 Gbps circuit SKUs. The physical port pairs are 100 or 10 Gbps only and can have multiple virtual circuits. ExpressRoute Direct supports both QinQ and Dot1Q VLAN tagging. QinQ VLAN Tagging allows for isolated routing domains on a per ExpressRoute circuit basis. Azure dynamically allocates an S-Tag at circuit creation and cannot be changed. Each peering on the circuit (Private and Microsoft) will utilize a unique C-Tag as the VLAN. The C-Tag is not required to be unique across circuits on the ExpressRoute Direct ports. Dot1Q VLAN Tagging allows for a single tagged VLAN on a per ExpressRoute Direct port pair basis. A C-Tag used on a peering must be unique across all circuits and peerings on the ExpressRoute Direct port pair. ExpressRoute Direct provides the same enterprise-grade SLA with Active/Active redundant connections into the Microsoft Global Network. ExpressRoute infrastructure is redundant and connectivity into the Microsoft Global Network is redundant and diverse and scales accordingly with customer requirements. Perform try-this exercises Use this short Try-This exercises to get some hands-on experience with using Azure. An individual Azure subscription is required to perform the exercise tasks. To subscribe, browse to https://www.azure.microsoft.com/free. Task 1 - Network security groups This task requires a Windows virtual machine associated with a network security group. The NSG should have an inbound security rule that allows RDP. The virtual machine should be in a running state and have a public IP address. In this task, we will review networking rules, confirm the public IP page does not display, configure an inbound NSG rule, and confirm the public IP page now displays. Review networking rules In the Portal, navigate to your virtual machine. Under Settings, click Networking. Discuss the default inbound and outbound rules. Review the inbound rules and ensure RDP is allowed. Make a note of the public IP address. Connect to the virtual machine and test the public IP address From the Overview blade, click Connect and RDP into the virtual machine. On the virtual machine, open a browser. Test the default localhost IIS HTML page: http://localhost/default.htm. This page should appear. Test the default public IP IIS HTML page: http://public_IP_address/default.htm. This page should not display. Configure an inbound rule to allow public access on port 80 Return to the Portal and the Networking blade. Make a note of the virtual machine's private IP address. On the Inbound port rules tab, click Add inbound port rule. This rule will only allow certain IP addresses on port 80. As you go through the configuration settings, be sure to discuss each one. Source: Service Tag Source service tag: Internet Destination: IP addresses Destination IP addresses/CIDR range: private_IP_address/32 Destination port range: 80 Protocol: TCP Action: Allow Name: Allow_Port_80 Click Add Wait for your new inbound rule to be added. Retest the public IP address On the virtual machine, return to the browser. Refresh the default public IP IIS HTML page: http://public_IP_address/default.htm. This page should now display. Task 2 - Application service groups This task requires a Windows virtual machine with IIS installed. These steps use VM1. Your machine name may be different. In this task, we will connect to a virtual machine, create an inbound deny rule, configure an application security group, and test connectivity. Connect to the virtual machine In the Portal, navigate to VM1. On the Networking blade, make a note of the private IP address. Ensure there is an Inbound port rule that allows RDP. From the Overview blade, ensure VM1 is running. Click Connect and RDP into the VM1. On VM1, open a browser. Ensure the default IIS page display for the private IP address: http://private_IP_address/default.htm. Add an inbound deny rule and test the rule Continue in the Portal from the Networking blade. On the Inbound port rules tab, click Add inbound port rule. Add a rule that denies all inbound traffic. Destination port ranges: * Action: Deny Name: Deny_All Click Add Wait for your new inbound rule to be added. On VM1, refresh the browser page: http://private_IP_address/default.htm. Verify that the page does not display. Configure an application security group In the Portal, search for and select Application security groups. Create a new Application security group. Provide the required information: subscription, resource group, name, and region. Wait for the ASG to deploy. In the Portal, return to VM1. On the Networking blade, select the Application security groups tab. Click Configure the application security groups. Select your new application security group, and Save your changes. From the Inbound port rules tab, click Add inbound rule. This will allow the ASG. Source: Application security group Source application security group: your_ASG Destination: IP addresses Destination IP addresses: private_IP_address/32 Destination port range: 80 Priority: 250 Name: Allow_ASG Click Add Wait for your new inbound rule to be added. Test the application security group On VM1, refresh the browser page: http://private_IP_address/default.htm. Verify that the page now displays. Task 3 - Storage endpoints (you could do this in the storage lesson) This task requires a storage account and virtual network with a subnet. Storage Explorer is also required. In this task, we will secure a storage endpoint. In the Portal. Locate your storage account. Create a file share, and upload a file. Use the Shared Access Signature blade to Generate SAS and connection string. Use Storage Explorer and the connection string to access the file share. Ensure you can view your uploaded file. Locate your virtual network, and then select a subnet in the virtual network. Under Service Endpoints, view the Services drop-down and the different services that can be secured with an endpoint. Check the Microsoft.Storage option. Save your changes. Return to your storage account. Select Firewalls and virtual networks. Change to Selected networks. Add your virtual network and verify your subnet with the new service endpoint is listed. Save your changes. Return to the Storage Explorer. Refresh the storage account. Verify you can no longer access the file share. Knowledge check When deploying the Azure Application Gateway and there's a need to ensure incoming requests are checked for common security threats like cross-site scripting and crawlers; how can this concern be addressed? Install a load balancer Install Azure Firewall Install the Web Application Firewall( Ans ) Install the Web Application Firewall. The web application firewall (WAF) is an optional component that handles incoming requests before they reach a listener. The web application firewall checks each request for many common threats, based on the Open Web Application Security Project (OWASP). Which services below are features of Azure Application Gateway? Authentication Layer 7 load balancing( Ans ) Layer 7 load balancing, Offloading of CPUT intensive SSL termination, Round robin distribution of incoming traffic. Azure Application Gateway is a dedicated virtual offering various layer 7 load balancing capabilities for your application. It lets customers optimize web farm productivity by offloading CPU intensive SSL termination to the application gateway, round robin distribution of incoming traffic, cookie-based session affinity, URL path-based routing, and the ability to host multiple websites behind a single Application Gateway. Vulnerability assessments A company is configuring a Network Security Group. To configure the group to allow traffic from public sources, what rule needs to be added to the default rules? Allow all virtual networks inbound and outbound Allow Azure load balancer inbound Allow Internet inbound( Ans ) Allow Internet inbound. NSGs have default inbound and outbound rules. There is a default allow Internet outbound rule, but not an allow Internet inbound rule. An organization has web servers in different regions and this organization wants to optimize the availability of the servers. Which of the network security is best suited for this purpose? Azure Application Gateway Azure Front Door( Ans ) Azure Front Door. Azure Front Door grants the ability to define, manage, and monitor the global routing for web traffic by optimizing for best performance and instant global failover for high availability. Custom routing Configure and manage host security # Introduction Once you have your network and perimeter secure, you need to lock down the host machines where your applications run. Installing updates, using jump boxes to only access servers, and following Microsoft Defender for Cloud recommendations are great tools to keep your hosts secure. Scenario A security engineer uses host security tools like privilege access workstation and virtual machine templates to keep application host machines secure; you will work on such tasks as: Deploy endpoint protection. Create and use a privileged access workstation. Set up protection tools like disk encryption and Windows Defender. Enable endpoint protection Microsoft Defender for Endpoint is an enterprise endpoint security platform designed to help enterprise networks prevent, detect, investigate, and respond to advanced threats. Defender for Endpoint uses the following combination of technology built into Windows 10 and Microsoft's robust cloud service: Endpoint behavioral sensors: Embedded in Windows 10, these sensors collect and process behavioral signals from the operating system and send this sensor data to your private, isolated cloud instance of Microsoft Defender for Endpoint. Cloud security analytics: Leveraging big data, device learning, and unique Microsoft optics across the Windows ecosystem, enterprise cloud products (such as Office 365), and online assets, behavioral signals are translated into insights, detections, and recommended responses to advanced threats. Threat intelligence: Generated by Microsoft hunters, security teams, and augmented by threat intelligence provided by partners, threat intelligence enables Defender for Endpoint to identify attacker tools, techniques, and procedures and generate alerts when they are observed in collected sensor data. Important The capabilities on non-Windows platforms may be different from the ones for Windows. Core Defender Vulnerability Management Built-in core vulnerability management capabilities use a modern risk-based approach to the discovery, assessment, prioritization, and remediation of endpoint vulnerabilities and misconfigurations. Attack surface reduction The attack surface reduction set of capabilities provides the first line of defense in the stack. By ensuring configuration settings are properly set and exploit mitigation techniques are applied, the capabilities resist attacks and exploitation. This set of capabilities also includes network protection and web protection, which regulate access to malicious IP addresses, domains, and URLs. Next-generation protection To further reinforce the security perimeter of your network, Microsoft Defender for Endpoint uses next-generation protection designed to catch all types of emerging threats. Endpoint detection and response Endpoint detection and response capabilities are put in place to detect, investigate, and respond to advanced threats that may have made it past the first two security pillars. Advanced hunting provides a query-based threat-hunting tool that lets you proactively find breaches and create custom detections. Automated investigation and remediation In conjunction with being able to quickly respond to advanced attacks, Microsoft Defender for Endpoint offers automatic investigation and remediation capabilities that help reduce the volume of alerts in minutes at scale. Microsoft Secure Score for Devices Defender for Endpoint includes Microsoft Secure Score for Devices to help you dynamically assess the security state of your enterprise network, identify unprotected systems, and take recommended actions to improve the overall security of your organization. Microsoft Threat Experts Microsoft Defender for Endpoint's new managed threat hunting service provides proactive hunting, prioritization, and additional context and insights that further empower Security operation centers (SOCs) to identify and respond to threats quickly and accurately. Important Defender for Endpoint customers need to apply for the Microsoft Threat Experts managed threat hunting service to get proactive Targeted Attack Notifications and to collaborate with experts on demand. Experts on Demand is an add-on service. Targeted Attack Notifications are always included after you have been accepted into Microsoft Threat Experts managed threat hunting service. Centralized configuration and administration, APIs Integrate Microsoft Defender for Endpoint into your existing workflows. Integration with Microsoft solutions Defender for Endpoint directly integrates with various Microsoft solutions, including: Microsoft Defender for Cloud Microsoft Sentinel Intune Microsoft Defender for Cloud Apps Microsoft Defender for Identity Microsoft Defender for Office Skype for Business Microsoft 365 Defender With Microsoft 365 Defender, Defender for Endpoint, and various Microsoft security solutions, form a unified pre- and post-breach enterprise defense suite that natively integrates across endpoint, identity, email, and applications to detect, prevent, investigate, and automatically respond to sophisticated attacks. Define a privileged access device strategy To ensure the most secure conditions for your company to you need to ensure security from the time of purchase of a new device, to its first usage, and beyond. Zero Trust, means that you don't purchase from generic retailers but only supply hardware from an authorized OEM that support Autopilot. For this solution, root of trust will be deployed using Windows Autopilot technology with hardware that meets the modern technical requirements. To secure a workstation, Autopilot lets you leverage Microsoft OEM-optimized Windows 10 devices. These devices come in a known good state from the manufacturer. Instead of reimaging a potentially insecure device, Autopilot can transform a Windows 10 device into a \u201cbusiness-ready\u201d state. It applies settings and policies, installs apps, and even changes the edition of Windows 10. Hardware root-of-trust To have a secured workstation you need to make sure the following security technologies are included on the device: Trusted Platform Module (TPM) 2.0 BitLocker Drive Encryption UEFI Secure Boot Drivers and Firmware Distributed through Windows Update Virtualization and HVCI Enabled Drivers and Apps HVCI-Ready Windows Hello DMA I/O Protection System Guard Modern Standby Levels of device security Device Type Common usage scenario Permitted activities Security guidance Enterprise Device Home users, small business users, general purpose developers, and enterprise Run any application, browse any website Anti-malware and virus protection and policy based security posture for the enterprise. Specialized Device Specialized or secure enterprise users Run approved applications, but cannot install apps. Email and web browsing allowed. No admin controls No self administration of device, no application installation, policy based security, and endpoint management Privileged Device Extremely sensitive roles IT Operations No local admins, no productivity tools, locked down browsing. PAW device This chart shows the level of device security controls based on how the device will be used. Device security controls A secure workstation requires it be part of an end-to-end approach including device security, account security, and security policies applied to the device at all times. Here are some common security measures you should consider implementing based on the users' needs. Using a device with security measures directly aligned to the security needs of it users is the more secure solution. Security Control Enterprise Device Specialized Device Privileged Device Microsoft Endpoint Manager (MEM) managed Yes Yes Yes Deny BYOD Device enrollment No Yes Yes MEM security baseline applied Yes Yes Yes Microsoft Defender for Endpoint Yes Yes Yes Join personal device via Autopilot Yes Yes No URLs restricted to approved list Allow Most Allow Most Deny Default Removal of admin rights Yes Yes Application execution control (AppLocker) Audit -> Enforced Yes Applications installed only by MEM Yes Yes Deploy privileged access workstations Privileged Access Workstations (PAWs) provide a dedicated system for sensitive tasks that is protected from Internet attacks and threat vectors. Separating these sensitive tasks and accounts from the daily use workstations and devices provides very strong protection from phishing attacks, application and OS vulnerabilities, various impersonation attacks, and credential theft attacks such as keystroke logging, Pass-the-Hash, and Pass-The-Ticket. PAW workstations PAW is a hardened and locked down workstation designed to provide high security assurances for sensitive accounts and tasks. PAWs are recommended for administration of identity systems, cloud services, and private cloud fabric as well as sensitive business functions. In order to provide the greatest security, PAWs should always run the most up-to-date and secure operating system available: Microsoft strongly recommends Windows 10 Enterprise, which includes several additional security features not available in other editions (in particular, Credential Guard and Device Guard). The PAW security controls are focused on mitigating high impact and high probability risks of compromise. These include mitigating attacks on the environment and risks that can decrease the effectiveness of PAW controls over time: Internet attacks - Isolating the PAW from the open internet is a key element to ensuring the PAW is not compromised. Usability risk - If a PAW is too difficult to use for daily tasks, administrators will be motivated to create workarounds to make their jobs easier. Environment risks - Minimizing the use of management tools and accounts that have access to the PAWs to secure and monitor these specialized workstations. Supply chain tampering - Taking a few key actions can mitigate critical attack vectors that are readily available to attackers. This includes validating the integrity of all installation media (Clean Source Principle) and using a trusted and reputable supplier for hardware and software. Physical attacks - Because PAWs can be physically mobile and used outside of physically secure facilities, they must be protected against attacks that leverage unauthorized physical access to the computer. Architecture overview The diagram below depicts a separate \"channel\" for administration (a highly sensitive task) that is created by maintaining separate dedicated administrative accounts and workstations. This architectural approach builds on the protections found in the Windows 10 Credential Guard and Device Guard features and goes beyond those protections for sensitive accounts and tasks. This methodology is appropriate for accounts with access to high value assets: Administrative Privileges - PAWs provide increased security for high impact IT administrative roles and tasks. This architecture can be applied to administration of many types of systems including Active Directory Domains and Forests, Microsoft Azure Active Directory tenants, Microsoft 365 tenants, Process Control Networks (PCN), Supervisory Control and Data Acquisition (SCADA) systems, Automated Teller Machines (ATMs), and Point of Sale (PoS) devices. High Sensitivity Information workers - The approach used in a PAW can also provide protection for highly sensitive information worker tasks and personnel such as those involving pre-announcement Merger and Acquisition activity, pre-release financial reports, organizational social media presence, executive communications, unpatented trade secrets, sensitive research, or other proprietary or sensitive data. This guidance does not discuss the configuration of these information worker scenarios in depth or include this scenario in the technical instructions. Securing privileged access is a critical first step to establishing security assurances for business assets in a modern organization. The security of most or all business assets in an IT organization depends on the integrity of the privileged accounts used to administer, manage, and develop. Jump Box Administrative \"Jump Box\" architectures set up a small number administrative console servers and restrict personnel to using them for administrative tasks. This is typically based on remote desktop services, a 3rd-party presentation virtualization solution, or a Virtual Desktop Infrastructure (VDI) technology. This approach is frequently proposed to mitigate risk to administration and does provide some security assurances, but the jump box approach by itself is vulnerable to certain attacks because it violates the clean source principle. The clean source principle requires all security dependencies to be as trustworthy as the object being secured. This figure depicts a simple control relationship. Any subject in control of an object is a security dependency of that object. If an adversary can control a security dependency of a target object (subject), they can control that object. The administrative session on the jump server relies on the integrity of the local computer accessing it. If this computer is a user workstation subject to phishing attacks and other internet-based attack vectors, then the administrative session is also subject to those risks. While some advanced security controls like multifactor authentication can increase the difficulty of an attacker taking over this administrative session from the user workstation, no security feature can fully protect against technical attacks when an attacker has administrative access of the source computer (e.g. injecting illicit commands into a legitimate session, hijacking legitimate processes, and so on.) The default configuration in this PAW guidance installs administrative tools on the PAW, but a jump server architecture can also be added if required. This above figure shows how reversing the control relationship and accessing user apps from an admin workstation gives the attacker no path to the targeted object. The user jump box is still exposed to risk so appropriate protective controls, detective controls, and response processes should still be applied for that internet-facing computer. Create virtual machine templates Before diving into configuring VM policies and templates, you need to understand the features and functionality of Azure Resource Manager. Resource Manager is the deployment and management service for your Azure subscription. It provides a consistent management layer that allows you to create, update, and delete resources in your Azure subscription. You can use its access control, auditing, and tagging features to help secure and organize your resources after deployment. When you take actions through the Azure portal, Azure PowerShell, the Azure CLI, REST APIs, or client SDKs, the Resource Manager API handles your request. Because the same API handles all requests, you get consistent results and capabilities in all the different tools. Here are some additional terms to know when using Resource Manager: Resource provider. A service that supplies Azure resources. For example, a common resource provider is Microsoft.Compute, which supplies the VM resource. Microsoft.Storage is another common resource provider. Resource Manager template. A JSON file that defines one or more resources to deploy to a resource group or subscription. You can use the template to consistently and repeatedly deploy the resources. Declarative syntax. Syntax that lets you state, \"Here\u2019s what I intend to create\" without having to write the sequence of programming commands to create it. The Resource Manager template is an example of declarative syntax. In the file, you define the properties for the infrastructure to deploy to Azure. You can use the Resource Manager template to define your VMs. After they are defined you can easily deploy and redeploy them. We recommend periodically redeploying your VMs to force the deployment of a freshly updated and security-enhanced VM OS. Template design How you define templates and resource groups is entirely up to you and how you want to manage your solution. For example, you can deploy your three tier application through a single template to a single resource group. But, you don't have to define your entire infrastructure in a single template. Often, it makes sense to divide your deployment requirements into a set of targeted, purpose-specific templates. You can easily reuse these templates for different solutions. To deploy a particular solution, you create a master template that links all the required templates. If you envision your tiers having separate lifecycles, you can deploy your three tiers to separate resource groups. Notice the resources can still be linked to resources in other resource groups. Important When you deploy a template, Resource Manager converts the template into REST API operations. Enable and secure remote access management This topic explains how to connect to and sign into the virtual machines (VMs) you created on Azure. Once you've successfully connected, you can work with the VM as if you were locally logged on to its host server. Connect to a Windows VM The most common way to connect to a Windows based VM running in Azure is by using Remote Desktop Protocol (RDP). Most versions of Windows natively contain support for the remote desktop protocol (RDP). If you are connecting to a Windows VM from a Mac, you will need to install an RDP client for Mac. If you are using PowerShell and have the Azure PowerShell module installed you may also connect using the Get-AzRemoteDesktopFile cmdlet. Connect to a Linux-based VM To connect the Linux-based VM, you need a secure shell protocol (SSH) client. The most used free tool is PuTTY SSH terminal. The following shows the PuTTY configuration dialog. Azure Bastion The Azure Bastion service is a fully platform-managed PaaS service that you provision inside your virtual network. It provides secure and seamless RDP/SSH connectivity to your virtual machines directly in the Azure portal over TLS. When you connect using Azure Bastion, your virtual machines do not need a public IP address. Bastion provides secure RDP and SSH connectivity to all the VMs in the virtual network in which it is provisioned. Using Azure Bastion protects your virtual machines from exposing RDP/SSH ports to the outside world, while still providing secure access using RDP/SSH. With Azure Bastion, you connect to the virtual machine directly from the Azure portal. Architecture Azure Bastion is deployed to a virtual network and supports virtual network peering. Specifically, Azure Bastion manages RDP/SSH connectivity to VMs created in the local or peered virtual networks. RDP and SSH are some of the fundamental means through which you can connect to your workloads running in Azure. Exposing RDP/SSH ports over the Internet isn't desired and is seen as a significant threat surface. This is often due to protocol vulnerabilities. To contain this threat surface, you can deploy bastion hosts (also known as jump-servers) at the public side of your perimeter network. Bastion host servers are designed and configured to withstand attacks. Bastion servers also provide RDP and SSH connectivity to the workloads sitting behind the bastion, as well as further inside the network. This figure shows the architecture of an Azure Bastion deployment. In this diagram: The Bastion host is deployed in the virtual network. The user connects to the Azure portal using any HTML5 browser. The user selects the virtual machine to connect to. With a single click, the RDP/SSH session opens in the browser. No public IP is required on the Azure VM. Key features The following features are available: RDP and SSH directly in Azure portal: You can directly get to the RDP and SSH session directly in the Azure portal using a single click seamless experience. Remote Session over TLS and firewall traversal for RDP/SSH: Azure Bastion uses an HTML5 based web client that is automatically streamed to your local device, so that you get your RDP/SSH session over TLS on port 443 enabling you to traverse corporate firewalls securely. No Public IP required on the Azure VM: Azure Bastion opens the RDP/SSH connection to your Azure virtual machine using private IP on your VM. You don't need a public IP on your virtual machine. No hassle of managing NSGs: Azure Bastion is a fully managed platform PaaS service from Azure that is hardened internally to provide you secure RDP/SSH connectivity. You don't need to apply any NSGs on Azure Bastion subnet. Because Azure Bastion connects to your virtual machines over private IP, you can configure your NSGs to allow RDP/SSH from Azure Bastion only. Protection against port scanning: Because you do not need to expose your virtual machines to public Internet, your VMs are protected against port scanning by rogue and malicious users located outside your virtual network. Protect against zero-day exploits. Hardening in one place only: Azure Bastion is a fully platform-managed PaaS service. Because it sits at the perimeter of your virtual network, you don\u2019t need to worry about hardening each of the virtual machines in your virtual network. Configure update management Azure Update Management is a service included as part of your Azure subscription. With Update Management, you can assess your update status across your environment and manage your Windows Server and Linux server updates from a single location\u2014for both your on-premises and Azure environments. Update Management is available at no additional cost (you pay only for the log data that Azure Log Analytics stores), and you can easily enable it for Azure and on-premises VMs. To try it, navigate to your VM tab in Azure, and then enable Update Management for one or more of your VMs. You can also enable Update Management for VMs directly from your Azure Automation account. Making updates easy, is one of the key factors in maintaining good security hygiene. Azure Update Management overview Computers that Update Management manages use the following configurations to perform assessment and update deployments: Microsoft Monitoring Agent (MMA) for Windows or Linux Desired State Configuration (DSC) in Windows PowerShell for Linux Hybrid Runbook Worker in Azure Automation Microsoft Update or Windows Server Update Services (WSUS) for Windows computers Azure Automation uses runbooks to install updates. You can't view these runbooks, and they don\u2019t require any configuration. When an update deployment is created, it creates a schedule that starts a master update runbook at the specified time for the included computers. The master runbook starts a child runbook on each agent to install the required updates. The following diagram is a conceptual depiction of the behavior and data flow together with how the solution assesses and applies security updates to all connected Windows Server and Linux computers in a workspace. Manage updates for multiple machines You can use the Update Management solution to manage updates and patches for your Windows and Linux virtual machines. From your Azure Automation account, you can: Onboard virtual machines Assess the status of available updates Schedule installation of required updates Review deployment results to verify that updates were applied successfully to all virtual machines for which Update Management is enabled The Log Analytics agent for Windows and Linux needs to be installed on the VMs that are running on your corporate network or other cloud environment in order to enable them with Update Management. After you enable Update Management for your machines, you can view machine information by selecting Computers. You can view information about machine name, compliance status, environment, OS type, critical and security updates installed, other updates installed, and update agent readiness for your computers. Computers that have recently been enabled for Update Management might not have been assessed yet. The compliance state status for those computers is Not assessed. Update inclusion Azure Update Management provides the ability to deploy patches based on classifications. However, there are scenarios where you may want to explicitly list the exact set of patches. Common scenarios include allowing specific patches after canary environment testing and zero-day patch rollouts. With update inclusion lists you can choose exactly which patches you want to deploy instead of relying on patch classifications. Deploy disk encryption Azure Disk Encryption for Windows VMs helps protect and safeguard your data to meet your organizational security and compliance commitments. It uses the BitLocker feature of Windows to provide volume encryption for the OS and data disks of Azure virtual machines (VMs), and is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. If you use Microsoft Defender for Cloud, you'll be alerted if you have VMs that aren't encrypted. The alerts show as High Severity and the recommendation is to encrypt these VMs. Azure Disk Encryption is zone resilient, the same way as Virtual Machines. Supported VMs and operating systems Supported VMs Windows VMs are available in a range of sizes. Azure Disk Encryption is supported on Generation 1 and Generation 2 VMs. Azure Disk Encryption is also available for VMs with premium storage. Azure Disk Encryption is not available on Basic, A-series VMs, or on virtual machines with less than 2 GB of memory. Supported operating systems - Windows client: Windows 8 and later. - Windows Server: Windows Server 2008 R2 and later. - Windows 10 Enterprise multi-session. Networking requirements To enable Azure Disk Encryption, the VMs must meet the following network endpoint configuration requirements: To get a token to connect to your key vault, the Windows VM must be able to connect to an Azure Active Directory endpoint, [login.microsoftonline.com]. To write the encryption keys to your key vault, the Windows VM must be able to connect to the key vault endpoint. The Windows VM must be able to connect to an Azure storage endpoint that hosts the Azure extension repository and an Azure storage account that hosts the VHD files. If your security policy limits access from Azure VMs to the Internet, you can resolve the preceding URI and configure a specific rule to allow outbound connectivity to the IPs. Group Policy requirements Azure Disk Encryption uses the BitLocker external key protector for Windows VMs. For domain joined VMs, don't push any group policies that enforce TPM protectors. BitLocker policy on domain joined virtual machines with custom group policy must include the following setting: Configure user storage of BitLocker recovery information -> Allow 256-bit recovery key. Azure Disk Encryption will fail when custom group policy settings for BitLocker are incompatible. On machines that didn't have the correct policy setting, apply the new policy, force the new policy to update (gpupdate.exe /force), and then restarting may be required. Azure Disk Encryption will fail if domain level group policy blocks the AES-CBC algorithm, which is used by BitLocker. Encryption key storage requirements Azure Disk Encryption requires an Azure Key Vault to control and manage disk encryption keys and secrets. Your key vault and VMs must reside in the same Azure region and subscription. Azure Disk Encryption for Linux VMs Azure Disk Encryption helps protect and safeguard your data to meet your organizational security and compliance commitments. It uses the DM-Crypt feature of Linux to provide volume encryption for the OS and data disks of Azure virtual machines (VMs) and is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. As for Windows VMs, if you use Microsoft Defender for Cloud, you're alerted if you have VMs that aren't encrypted. The alerts show as High Severity and the recommendation is to encrypt these VMs. Supported VMs and operating systems Supported VMs Linux VMs are available in a range of sizes. Azure Disk Encryption is supported on Generation 1 and Generation 2 VMs. Azure Disk Encryption is also available for VMs with premium storage. Note: Azure Disk Encryption is not available on Basic, A-series VMs, or on virtual machines that do not meet these minimum memory requirements: Virtual machine Minimum memory requirement Linux VMs when only encrypting data volumes 2 GB Linux VMs when encrypting both data and OS volumes, and where the root (/) file system usage is 4GB or less 8 GB Linux VMs when encrypting both data and OS volumes, and where the root (/) file system usage is greater than 4GB The root file system usage * 2. For instance, a 16 GB of root file system usage requires at least 32GB of RAM Once the OS disk encryption process is complete on Linux virtual machines, the VM can be configured to run with less memory. Azure Disk Encryption requires the dm-crypt and vfat modules to be present on the system. Removing or disabling vfat from the default image will prevent the system from reading the key volume and obtaining the key needed to unlock the disks on subsequent reboots. System hardening steps that remove the vfat module from the system are not compatible with Azure Disk Encryption Managed disk encryption options Managed disk Encryption Options There are several types of encryption available for your managed disks, including Azure Disk Encryption (ADE), Server-Side Encryption (SSE), and encryption at the host. Azure Disk Encryption helps protect and safeguard your data to meet organizational security and compliance commitments. ADE encrypts the OS and data disks of Azure virtual machines (VMs) inside your VMs by using the device mapper DM-Crypt feature of Linux or the BitLocker feature of Windows. Azure Disk Encryption (ADE) is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. Azure Disk Storage Server-Side Encryption (also referred to as encryption-at-rest or Azure Storage encryption) automatically encrypts data stored on Azure-managed disks (OS and data disks) when persisting on the Storage Clusters. When configured with a Disk Encryption Set (DES), it supports customer-managed keys as well. Encryption at the host ensures that data stored on the VM host hosting your VM is encrypted at rest and flows encrypted to the Storage clusters. Confidential disk encryption binds disk encryption keys to the virtual machine's TPM (Trusted Platform Module) and makes the protected disk content accessible only to the VM. The TPM and VM guest state is always encrypted in attested code using keys released by a secure protocol that bypasses the hypervisor and host operating system. Currently only available for the OS disk. Encryption at the host may be used for other disks on a Confidential VM in addition to Confidential Disk Encryption. Encryption is part of a layered approach to security and should be used with other recommendations to secure Virtual Machines and their disks. Comparison The following is a comparison of Storage Server-Side Encryption (SSE), Azure Disk Encryption (ADE), encryption at the host, and Confidential disk encryption. Important For Encryption at the host and Confidential disk encryption, Microsoft Defender for Cloud does not detect the encryption state. We are in the process of updating Microsoft Defender. Deploy and configure Windows Defender Windows 10, Windows Server 2019, and Windows Server 2016 include key security features. They are Windows Defender Credential Guard, Windows Defender Device Guard, and Windows Defender Application Control. Windows Defender Credential Guard Introduced in Windows 10 Enterprise and Windows Server 2016, Windows Defender Credential Guard uses virtualization-based security enhancement to isolate secrets so that only privileged system software can access them. Unauthorized access to these secrets might lead to credential theft attacks, such as Pass-the-Hash or pass-the-ticket attacks. Windows Defender Credential Guard helps prevent these attacks by helping protect Integrated Windows Authentication (NTLM) password hashes, Kerberos authentication ticket-granting tickets, and credentials that applications store as domain credentials. By enabling Windows Defender Credential Guard, you get the following features and solutions: Hardware security enhancement. NTLM, Kerberos, and Credential Manager take advantage of platform security features, including Secure Boot and virtualization, to help protect credentials. Virtualization-based security enhancement. NTLM-derived credentials, Kerberos-derived credentials, and other secrets run in a protected environment that is isolated from the running operating system. Better protection against advanced persistent threats. When virtualization-based security enhancement helps protect Credential Manager domain credentials, NTLM-derived credentials, and Kerberos-derived credentials, the credential theft attack techniques and tools that many targeted attacks use are blocked. Malware running in the OS with administrative privileges can\u2019t extract secrets that virtualization-based security helps protect. Although Windows Defender Credential Guard provides powerful mitigation, persistent threat attacks will likely shift to new attack techniques, so you should also incorporate Windows Defender Device Guard and other security strategies and architectures. Windows Defender Device Guard and Windows Defender Application Control The configuration state of Windows Defender Device Guard was originally designed with a specific security idea in mind. Although no direct dependencies existed between the two main OS features of the Windows Defender Device Guard configuration\u2014that is, between configurable code integrity and Hypervisor-protected code integrity (HVCI)\u2014the discussion intentionally focused on the Windows Defender Device Guard lockdown state that can be achieved when they\u2019re deployed together. However, the use of the term device guard to describe this configuration state has unintentionally left many IT pros with the impression that the two features are inexorably linked and can\u2019t be separately deployed. Additionally, because HVCI relies on security based on Windows virtualization, it comes with additional hardware, firmware, and kernel driver compatibility requirements that some older systems can\u2019t meet. As a result, many IT pros assumed that because some systems couldn't use HVCI, they couldn\u2019t use configurable code integrity, either. But configurable code integrity has no specific hardware or software requirements other than running Windows 10, which means that many IT pros were wrongly denied the benefits of this powerful application control capability. Since the initial release of Windows 10, the world has witnessed numerous hacking and malware attacks where application control alone might have prevented the attack altogether. Configurable code integrity is now documented as an independent technology within the Microsoft security stack and given a name of its own: Windows Defender Application Control. Application control is a crucial line of defense for helping protect enterprises given today\u2019s threat landscape, and it has an inherent advantage over traditional antivirus solutions. Specifically, application control moves away from the traditional application trust model, in which all applications are assumed trustworthy by default, to one where applications must earn trust to run. Many organizations understand this and frequently cite application control as one of the most effective means for addressing the threat of malware based on executable files (such as .exe and .dll files). Windows Defender Application Control helps mitigate these types of threats by restricting the applications that users can run and the code that runs in the system core, or kernel. Policies in Windows Defender Application Control also block unsigned scripts and MSIs, and Windows PowerShell runs in Constrained language mode. Does this mean the Windows Defender Device Guard configuration state is going away? Not at all. The term device guard will continue to describe the fully locked down state achieved using Windows Defender Application Control, HVCI, and hardware and firmware security features. It will also allow Microsoft to work with its original equipment manufacturer (OEM) partners to identify specifications for devices that are device guard capable\u2014so that joint customers can easily purchase devices that meet all the hardware and firmware requirements of the original locked down scenario of Windows Defender Device Guard for Windows 10 devices. Microsoft Defender for Endpoint - Supported Operating Systems Microsoft cloud security benchmark in Defender for Cloud The Microsoft cloud security benchmark (MCSB) provides prescriptive best practices and recommendations to help improve the security of workloads, data, and services on Azure and your multicloud environment. This benchmark focuses on cloud-centric control areas with input from a set of holistic Microsoft and industry security guidance that includes: Cloud Adoption Framework: Guidance on security, including strategy, roles and responsibilities, Azure Top 10 Security Best Practices, and reference implementation. Azure Well-Architected Framework: Guidance on securing your workloads on Azure. The Chief Information Security Officer (CISO) Workshop: Program guidance and reference strategies to accelerate security modernization using Zero Trust principles. Other industry and cloud service provider's security best practice standards and framework: Examples include the Amazon Web Services (AWS) Well-Architected Framework, Center for Internet Security (CIS) Controls, National Institute of Standards and Technology (NIST), and Payment Card Industry Data Security Standard (PCI-DSS). Microsoft cloud security benchmark features Comprehensive multicloud security framework: Organizations often have to build an internal security standard to reconcile security controls across multiple cloud platforms to meet security and compliance requirements on each of them. This often requires security teams to repeat the same implementation, monitoring, and assessment across the different cloud environments (often for different compliance standards). This creates unnecessary overhead, cost, and effort. To address this concern, we enhanced the Azure Security Benchmark (ASB) to the Microsoft cloud security benchmark (MCSB) to help you quickly work with different clouds by: Providing a single control framework to easily meet the security controls across clouds Providing consistent user experience for monitoring and enforcing the multicloud security benchmark in Defender for Cloud Staying aligned with Industry Standards (e.g., Center for Internet Security, National Institute of Standards and Technology, Payment Card Industry) Automated control monitoring for AWS in Microsoft Defender for Cloud: You can use Microsoft Defender for Cloud Regulatory Compliance Dashboard to monitor your AWS environment against Microsoft cloud security benchmark (MCSB), just like how you monitor your Azure environment. We developed approximately 180 AWS checks for the new AWS security guidance in MCSB, allowing you to monitor your AWS environment and resources in Microsoft Defender for Cloud. Example: Microsoft Defender for Cloud - Regulatory compliance dashboard Azure guidance and security principles: Azure security guidance, security principles, features, and capabilities. Controls Control Domains Description Network security (NS) Network Security covers controls to secure and protect networks, including securing virtual networks, establishing private connections, preventing and mitigating external attacks, and securing Domain Name System (DNS). Identity Management (IM) Identity Management covers controls to establish a secure identity and access controls using identity and access management systems, including the use of single sign-on, strong authentications, managed identities (and service principals) for applications, conditional access, and account anomalies monitoring. Privileged Access (PA) Privileged Access covers controls to protect privileged access to your tenant and resources, including a range of controls to protect your administrative model, administrative accounts, and privileged access workstations against deliberate and inadvertent risk. Data Protection (DP) Data Protection covers control of data protection at rest, in transit, and via authorized access mechanisms, including discover, classify, protect, and monitoring sensitive data assets using access control, encryption, key management, and certificate management. Asset Management (AM) Asset Management covers controls to ensure security visibility and governance over your resources, including recommendations on permissions for security personnel, security access to asset inventory and managing approvals for services and resources (inventory, track, and correct). Logging and Threat Detection (LT) Logging and Threat Detection covers controls for detecting threats on the cloud and enabling, collecting, and storing audit logs for cloud services, including enabling detection, investigation, and remediation processes with controls to generate high-quality alerts with native threat detection in cloud services; it also includes collecting logs with a cloud monitoring service, centralizing security analysis with a security event management (SEM), time synchronization, and log retention. Incident Response (IR) Incident Response covers controls in the incident response life cycle - preparation, detection and analysis, containment, and post-incident activities, including using Azure services (such as Microsoft Defender for Cloud and Sentinel) and/or other cloud services to automate the incident response process. Posture and Vulnerability Management (PV) Posture and Vulnerability Management focuses on controls for assessing and improving the cloud security posture, including vulnerability scanning, penetration testing, and remediation, as well as security configuration tracking, reporting, and correction in cloud resources. Endpoint Security (ES) Endpoint Security covers controls in endpoint detection and response, including the use of endpoint detection and response (EDR) and anti-malware service for endpoints in cloud environments. Backup and Recovery (BR) Backup and Recovery covers controls to ensure that data and configuration backups at the different service tiers are performed, validated, and protected. DevOps Security (DS) DevOps Security covers the controls related to the security engineering and operations in the DevOps processes, including deployment of critical security checks (such as static application security testing and vulnerability management) prior to the deployment phase to ensure the security throughout the DevOps process; it also includes common topics such as threat modeling and software supply security. Governance and Strategy (GS) Governance and Strategy provides guidance for ensuring a coherent security strategy and documented governance approach to guide and sustain security assurance, including establishing roles and responsibilities for the different cloud security functions, unified technical strategy, and supporting policies and standards. Explore Microsoft Defender for Cloud recommendations What is a security recommendation? Using the policies, Defender for Cloud periodically analyzes the compliance status of your resources to identify potential security misconfigurations and weaknesses. It then provides you with recommendations on how to remediate those issues. Recommendations result from assessing your resources against the relevant policies and identifying resources that aren't meeting your defined requirements. Defender for Cloud makes its security recommendations based on your chosen initiatives. When a policy from your initiative is compared against your resources and finds one or more that aren't compliant, it is presented as a recommendation in Defender for Cloud. Example: Microsoft Defender for Cloud - All recommendations Recommendations are actions for you to take to secure and harden your resources. Each recommendation provides you with the following information: A short description of the issue The remediation steps to carry out in order to implement the recommendation The affected resources In practice, it works like this: Azure Security Benchmark is an initiative that contains requirements. For example, Azure Storage accounts must restrict network access to reduce their attack surface. The initiative includes multiple policies, each requiring a specific resource type. These policies enforce the requirements in the initiative. To continue the example, the storage requirement is enforced with the policy \"Storage accounts should restrict network access using virtual network rules.\" Microsoft Defender for Cloud continually assesses your connected subscriptions. If it finds a resource that doesn't satisfy a policy, it displays a recommendation to fix that situation and harden the security of resources that aren't meeting your security requirements. For example, if an Azure Storage account on your protected subscriptions isn't protected with virtual network rules, you'll see the recommendation to harden those resources. So, (1) an initiative includes (2) policies that generate (3) environment-specific recommendations. Perform Try-This exercises Use this Try-This exercises to get some hands-on experience with Azure. In this demonstration, we'll configure the Bastion service, virtual machine updates, virtual machine extensions, and disk encryption. Optionally, we'll use RDP to connect to a Windows virtual machine and SSH to connect to a Linux machine. Task 1 - Use the Bastion service Note This task requires a virtual machine. If you are doing the next task, virtual machine updates, use a Windows virtual machine and keep the session running. In this task, we'll configure the Bastion service and connect to a virtual machine with service. Configure the Bastion service In the Portal navigate to your Windows virtual machine. Ensure the virtual machine is Running. Click Connect and select Bastion. Click Use Bastion. Note installing the service is only required once. Because you are creating the Bastion service from the target virtual machine, mention that most of the networking information has automatically been filled in. Note the Bastion service will be assigned a public IP address. To create the Bastion subnet in the virtual network, click Manage subnet configuration. On the virtual network subnet blade, click + Subnet. On the Add subnet page, type AzureBastionSubnet as the subnet name. Note this name can't be changed. Specify the address range in CIDR notation. For example, 10.1.1.0/27. Click Ok, then click Create. It will take a few minutes for the service to deploy. Connect to the virtual machine using Bastion From the target virtual machine\u2019s Overview blade, select Connect and then Bastion On the Connect to Bastion page, enter the virtual machine login credentials. Notice the checkbox to open the session in a new window. Click Connect. If you receive a message that popup windows are blocked, allow the session. Once your session is connected, launch the Bastion clipboard access tool palette by selecting the two arrows. The arrows are located on the left center of the session. Explain this copy and paste feature. In the Portal, navigate to the Bastion host and under Settings select Sessions. Review the session management experience and the ability to delete a session. As you have time, review the Bastion components and how this provides a secure way to access your virtual machines. Task 2 - Virtual Machine Updates Note This task requires a virtual machine in the running state. You may want to enable Update management prior to this lesson. In this task, we'll review virtual machine update management. In the Portal, navigate to your virtual machine. Under Operations select Update management. Select the Azure Log Analytics workspace and Automation account, and then click Enable. Wait for update management to deploy. It can take up to 15 minutes for the deployment and longer for results to be provided. Select Missing Updates and use the Information link to open the support article for the update. Select Schedule update deployment. Review the various options including maintenance windows, reboot options, scheduling, classifications, kbs to include and exclude. You can view the status for the deployment on the Update deployments tab. The available values aren't attempted, succeeded, and failed. Task 3 - Virtual Machine Extensions In this task, we'll install the IaaSAntimalware extension. In the Portal, select your virtual machine. Under Settings, click Extensions. Review how extensions are used. On the Extensions page, click + Add. Scroll through the available extensions and review what extensions are available. Select Microsoft Antimalware. Discuss the features of this extension. Click Create. On the Install extension page use the informational icons to explain Excluded files and locations, Excluded file extensions, and Excluded processes. Review Real-time protection and Run a scheduled scan. Review other options of interest. After the extension is deployed, the extensions page will show the IaaSAntimalware extension. Task 4 - Disk Encryption Note This task requires a storage account. In this task, we'll enable disk encryption for a storage account. Review encryption key options In the Portal, access your storage account. Under Security + networking, select Encryption. Review Storage Service Encryption and why it is used. Review the two types of keys: Microsoft Managed Keys and Customer Managed Keys. Select Customer Managed Keys. Create the customer managed key For Encryption key choose Select from key vault. Click Select a key vault and key. You will now create a new key vault. If you already had a key vault you could use that. For Key vault select Create new. Notice the key vault will be created in the same region as the storage account. Give your key vault a name. Click Review + create. Once the validation passes, click Create. Wait for the key vault to be created. You will now create a key in the key vault. If you already had a key you could use that. On the Select key from Azure key vault page, for Key select Create new. Review the options for creating a key. Give your key a name. Notice the activation and expiration options. Click Create. Now that you have created a key vault and key, Select the key vault and key. Save your changes on the Encryption page. Review the information that is now available: Current key, Automated key rotation, and Key version in use. Review the key options Return to the resource group that includes your storage account. Refresh the page and ensure your new key vault is listed as a resource. Select the key vault. Under Settings click Keys. Ensure your new key is Enabled. Notice the ability to regenerate the key. Select the key and review the current version information. Return to the key vault page. Under Settings select Access policies. Under Current access policies your storage account will be listed. Notice the drop-downs for Key Permissions, Secret Permissions, and Certificate Permissions. Select Key Permissions and notice the properties that are checked (Get, Unwrap key, and Wrap key). Task 5 - Use RDP to connect to a Windows VM (optional) Note This task requires a Windows VM with a public IP address. You also need the login credentials for the machine. In this task, we'll use RDP to connect to a Windows virtual machine. In the Portal navigate to your Windows virtual machine. Ensure the virtual machine is Running. From the Overview blade select Connect and then RDP. In the Connect to virtual machine page, keep the default options to connect by DNS name over port 3389 and click Download RDP file. Mention that if the VM has a just-in-time policy set, you first need to select the Request access button to request access before you can download the RDP file. Open the downloaded RDP file and then click Connect. In the Windows Security window, select More choices and then Use a different account. Type the username as localhost\\username, enter password you created for the virtual machine, and then select OK. You may receive a certificate warning during the sign-in process. Select Yes or Continue to create the connection. Explain how RDP is different from the Bastion service. Task 6 - Use SSH to connect to a Linux VM (optional) Note This task requires a Linux VM. Ensure port 22 is open. In this task, we'll create an SSH private key with PuTTYgen, and then use SSH to connect to a Linux virtual machine. Create the SSH Keys Download the PuTTY tool. This will include PuTTYgen - https://putty.org/ Once installed, locate and open the PuTTYgen program. In the Parameters option group choose RSA. Click the Generate button. Move your mouse around the blank area in the window to generate some randomness. Copy the text of the Public key for pasting into authorized keys file. Optionally you can specify a Key passphrase and then Confirm passphrase. You will be prompted for the passphrase when you authenticate to the VM with your private SSH key. Without a passphrase, if someone obtains your private key, they can sign in to any VM or service that uses that key. We recommend you create a passphrase. However, if you forget the passphrase, there is no way to recover it. Click Save private key. Choose a location and filename and click Save. You'll need this file to access the VM. Create the Linux machine and assign the public SSH key In the portal navigate to your Linux machine. Choose SSH Public Key for the Authentication type (instead of Password ). Provide a Username. Paste the public SSH key from PuTTY into the SSH public key text area. Ensure the key validates with a checkmark. Create the VM. Wait for it to deploy. Access the running VM. From the Overview blade, click Connect. Make a note of your login information including user and public IP address. Access the server using SSH Open the PuTTY tool. Enter username@publicIpAddress where username is the value you assigned when creating the VM and publicIpAddress is the value you obtained from the Azure portal. Specify 22 for the Port. Choose SSH in the Connection Type option group. Navigate to SSH in the Category panel, then click Auth. Click the Browse button next to Private key file for authentication. Navigate to the private key file saved when you generated the SSH keys and click Open. From the main PuTTY screen click Open. You will now be connected to your server command line. Explain how SSH is different from the Bastion service. Knowledge check An organization has a security policy that prohibits exposing SSH ports to the outside world. You need to connect to an Azure Linux virtual machine to install software. What should you do? Configure the Bastion service( Ans ) Configure a Guest configuration on the virtual machine Create a custom script extension What type of disk encryption is used for Linux disks? BitLocker DM-Crypt( Ans ) FileVault A company with both Azure and on-premises virtual machines needs to ensure virtual machines are up to date with security patches. Update Management is the Azure tool they will use. Which of the following would enable the company to assess the status of available updates and manage the installation of required updates on virtual machines? The Microsoft Monitoring Agent must be installed for both Windows and Linux virtual machines on-premises. Both the Update Management feature and the log data storage are free for the customer. Update Management in Azure Automation collects information about Windows and Linux virtual machines and manages operating system updates.( Ans ) Which of the following recommendations from Security Center is a medium-severity recommendation for virtual machines and servers? Disk encryption should be applied on virtual machines. Install endpoint protection solution on virtual machines.( Ans ) System updates should be installed on your machines. Which attacks will using a Privileged access workstations protect companies from? Protects against attackers who have gained administrative access. Protects against server-based phishing attacks, various impersonation attacks, and credential theft attacks such as keystroke logging. Protects high impact IT administrative roles and tasks.( Ans ) Enable Containers security # Introduction Containers are a great choice for running applications in Azure. Just like virtual machines, those containers must be locked to protect your data and business. Use the built-in Azure container tools to keep your container-based solutions secure. Scenario A security engineer uses Azure Container Instances, the Container Registry, and access controls to protect your containers; you will work on such tasks as: Deploy containers from the Container Registry into Azure Container Instances. Use RBAC and Conditional Access to control access. Use proper architecture, storage, and network design to optimize container security. Explore containers A container is an isolated, lightweight silo for running an application on the host operating system. Containers build on top of the host operating system's kernel (which can be thought of as the buried plumbing of the operating system), and contain only apps and some lightweight operating system APIs and services that run in user mode. While a container shares the host operating system's kernel, the container doesn't get unfettered access to it. Instead, the container gets an isolated\u2013and in some cases virtualized\u2013view of the system. For example, a container can access a virtualized version of the file system and registry, but any changes affect only the container and are discarded when it stops. To save data, the container can mount persistent storage such as an Azure Disk or a file share (including Azure Files). You need Docker in order to work with Windows Containers. Docker consists of the Docker Engine (dockerd.exe), and the Docker client (docker.exe). How it works A container builds on top of the kernel, but the kernel doesn't provide all of the APIs and services an app needs to run\u2013most of these are provided by system files (libraries) that run above the kernel in user mode. Because a container is isolated from the host's user mode environment, the container needs its own copy of these user mode system files, which are packaged into something known as a base image. The base image serves as the foundational layer upon which your container is built, providing it with operating system services not provided by the kernel. Because containers require far fewer resources (for example, they don't need a full OS), they're easy to deploy and they start fast. This allows you to have higher density, meaning that it allows you to run more services on the same hardware unit, thereby reducing costs. As a side effect of running on the same kernel, you get less isolation than VMs. Features of Containers Features Description Isolation Typically provides lightweight isolation from the host and other containers, but doesn't provide as strong a security boundary as a VM. (You can increase the security by using Hyper-V isolation mode to isolate each container in a lightweight VM). Operating System Runs the user mode portion of an operating system, and can be tailored to contain just the needed services for your app, using fewer system resources. Deployment Deploy individual containers by using Docker via command line; deploy multiple containers by using an orchestrator such as Azure Kubernetes Service. Persistent storage Use Azure Disks for local storage for a single node, or Azure Files (SMB shares) for storage shared by multiple nodes or servers. Fault tolerance If a cluster node fails, any containers running on it are rapidly recreated by the orchestrator on another cluster node. Networking Uses an isolated view of a virtual network adapter, providing a little less virtualization\u2013the host's firewall is shared with containers\u2013while using less resources. In Docker, each layer is the resulting set of changes that happen to the filesystem after executing a command, such as, installing a program. So, when you view the filesystem after the layer has been copied, you can view all the files, including the layer when the program was installed. You can think of an image as an auxiliary read-only hard disk ready to be installed in a \"computer\" where the operating system is already installed. Similarly, you can think of a container as the \"computer\" with the image hard disk installed. The container, just like a computer, can be powered on or off. Configure Azure Container Instances security Use a private registry Containers are built from images that are stored in one or more repositories. These repositories can belong to a public registry, like Docker Hub, or to a private registry. An example of a private registry is the Docker Trusted Registry, which can be installed on-premises or in a virtual private cloud. You can also use cloud-based private container registry services, including Azure Container Registry. A publicly available container image does not guarantee security. Container images consist of multiple software layers, and each software layer might have vulnerabilities. To help reduce the threat of attacks, you should store and retrieve images from a private registry, such as Azure Container Registry or Docker Trusted Registry. In addition to providing a managed private registry, Azure Container Registry supports service principal-based authentication through Azure Active Directory for basic authentication flows. This authentication includes role-based access for read-only (pull), write (push), and other permissions. Monitor and scan container images continuously Take advantage of solutions to scan container images in a private registry and identify potential vulnerabilities. It\u2019s important to understand the depth of threat detection that the different solutions provide. For example, Azure Container Registry optionally integrates with Microsoft Defender for Cloud to automatically scan all Linux images pushed to a registry. Microsoft Defender for Cloud integrated Qualys scanner detects image vulnerabilities, classifies them, and provides remediation guidance. Protect credentials Containers can spread across several clusters and Azure regions. So, you must secure credentials required for logins or API access, such as passwords or tokens. Ensure that only privileged users can access those containers in transit and at rest. Inventory all credential secrets, and then require developers to use emerging secrets-management tools that are designed for container platforms. Make sure that your solution includes encrypted databases, TLS encryption for secrets data in transit, and least-privilege role-based access control. Azure Key Vault is a cloud service that safeguards encryption keys and secrets (such as certificates, connection strings, and passwords) for containerized applications. Because this data is sensitive and business critical, secure access to your key vaults so that only authorized applications and users can access them. Use vulnerability management as part of your container development lifecycle By using effective vulnerability management throughout the container development lifecycle, you improve the odds that you identify and resolve security concerns before they become a more serious problem. Scan for vulnerabilities New vulnerabilities are discovered all the time, so scanning for and identifying vulnerabilities is a continuous process. Incorporate vulnerability scanning throughout the container lifecycle: As a final check in your development pipeline, you should perform a vulnerability scan on containers before pushing the images to a public or private registry. Continue to scan container images in the registry both to identify any flaws that were somehow missed during development and to address any newly discovered vulnerabilities that might exist in the code used in the container images. Ensure that only approved images are used in your environment There\u2019s enough change and volatility in a container ecosystem without allowing unknown containers as well. Allow only approved container images. Have tools and processes in place to monitor for and prevent the use of unapproved container images. An effective way of reducing the attack surface and preventing developers from making critical security mistakes is to control the flow of container images into your development environment. For example, you might sanction a single Linux distribution as a base image, preferably one that is lean (Alpine or CoreOS rather than Ubuntu), to minimize the surface for potential attacks. Image signing or fingerprinting can provide a chain of custody that enables you to verify the integrity of the containers. For example, Azure Container Registry supports Docker's content trust model, which allows image publishers to sign images that are pushed to a registry, and image consumers to pull only signed images. Enforce least privileges in runtime The concept of least privileges is a basic security best practice that also applies to containers. When a vulnerability is exploited, it generally gives the attacker access and privileges equal to those of the compromised application or process. Ensuring that containers operate with the lowest privileges and access required to get the job done reduces your exposure to risk. Reduce the container attack surface by removing unneeded privileges You can also minimize the potential attack surface by removing any unused or unnecessary processes or privileges from the container runtime. Privileged containers run as root. If a malicious user or workload escapes in a privileged container, the container will then run as root on that system. Log all container administrative user access for auditing Maintain an accurate audit trail of administrative access to your container ecosystem, including your Kubernetes cluster, container registry, and container images. These logs might be necessary for auditing purposes and will be useful as forensic evidence after any security incident. Azure solutions include: Integration of Azure Kubernetes Service with Microsoft Defender for Cloud to monitor the security configuration of the cluster environment and generate security recommendations Azure Container Monitoring solution Resource logs for Azure Container Instances and Azure Container Registry Manage security for Azure Container Instances (ACI) Azure Container Instances (ACI), is a PaaS service for scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, we recommend Azure Kubernetes Service (which will be covered later on in this lesson). Features of ACI Fast startup times Containers offer significant startup benefits over virtual machines (VMs). Azure Container Instances can start containers in Azure in seconds, without the need to provision and manage VMs. Container access Azure Container Instances enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN). When you create a container instance, you can specify a custom DNS name label so your application is reachable at customlabel.azureregion.azurecontainer.io. Azure Container Instances also supports executing a command in a running container by providing an interactive shell to help with application development and troubleshooting. Access takes places over HTTPS, using TLS to secure client connections. Container deployment Deploy containers from DockerHub or Azure Container Registry. Hypervisor-level security Historically, containers have offered application dependency isolation and resource governance but have not been considered sufficiently hardened for hostile multi-tenant usage. Azure Container Instances guarantees your application is as isolated in a container as it would be in a VM. Custom sizes Containers are typically optimized to run just a single application, but the exact needs of those applications can differ greatly. Azure Container Instances provides optimum utilization by allowing exact specifications of CPU cores and memory. You pay based on what you need and get billed by the second, so you can fine-tune your spending based on actual need. For compute-intensive jobs such as machine learning, Azure Container Instances can schedule Linux containers to use NVIDIA Tesla GPU resources. Persistent storage To retrieve and persist state with Azure Container Instances, we offer direct mounting of Azure Files shares backed by Azure Storage. Flexible billing Supports per-GB, per-CPU, and per-second billing. Linux and Windows containers Azure Container Instances can schedule both Windows and Linux containers with the same API. Simply specify the OS type when you create your container groups. Some features are currently restricted to Linux containers: Multiple containers per container group Volume mounting (Azure Files, emptyDir, GitRepo, secret) Resource usage metrics with Azure Monitor Virtual network deployment GPU resources (preview) For Windows container deployments, use images based on common Windows base images. Co-scheduled groups Azure Container Instances supports scheduling of multi-container groups that share a host machine, local network, storage, and lifecycle. This enables you to combine your main application container with other supporting role containers, such as logging sidecars. Virtual network deployment Currently available for production workloads in a subset of Azure regions, this feature of Azure Container Instances enables deployment of container instances into an Azure virtual network. By deploying container instances into a subnet within your virtual network, they can communicate securely with other resources in the virtual network, including those that are on premises (through VPN gateway or ExpressRoute). Explore the Azure Container Registry (ACR) A container registry is a service that stores and distributes container images. Docker Hub is a public container registry that supports the open source community and serves as a general catalog of images. Azure Container Registry provides users with direct control of their images, with integrated authentication, geo-replication supporting global distribution and reliability for network-close deployments, virtual network and firewall configuration, tag locking, and many other enhanced features. In addition to Docker container images, Azure Container Registry supports related content artifacts including Open Container Initiative (OCI) image formats. Security and access You log in to a registry using the Azure CLI or the standard docker login command. Azure Container Registry transfers container images over HTTPS, and supports TLS to secure client connections. Azure Container Registry requires all secure connections from servers and applications to use TLS 1.2. Enable TLS 1.2 by using any recent docker client (version 18.03.0 or later). You control access to a container registry using an Azure identity, an Azure Active Directory-backed service principal, or a provided admin account. Use role-based access control (RBAC) to assign users or systems fine-grained permissions to a registry. Security features of the Premium SKU include content trust for image tag signing, and firewalls and virtual networks to restrict access to the registry. Microsoft Defender for Cloud optionally integrates with Azure Container Registry to scan images whenever an image is pushed to a registry. Repository Container registries manage repositories, collections of container images or other artifacts with the same name, but different tags. For example, the following three images are in the \"acr-helloworld\" repository: acr-helloworld:latest acr-helloworld:v1 acr-helloworld:v2 Image A container image or other artifact within a registry is associated with one or more tags, has one or more layers, and is identified by a manifest. Understanding how these components relate to each other can help you manage your registry effectively. Monitor container activity and user access As with any IT environment, you should consistently monitor activity and user access to your container ecosystem to quickly identify any suspicious or malicious activity. The container monitoring solution in Log Analytics can help you view and manage your Docker and Windows container hosts in a single location. By using Log Analytics, you can: View detailed audit information that shows commands used with containers. Troubleshoot containers by viewing and searching centralized logs without having to remotely view Docker or Windows hosts. Find containers that may be noisy and consuming excess resources on a host. View centralized CPU, memory, storage, and network usage and performance information for containers. On computers running Windows, you can centralize and compare logs from Windows Server, Hyper-V, and Docker containers. The solution supports container orchestrators such as Docker Swarm, DC/OS, Kubernetes, Service Fabric, and Red Hat OpenShift. Container technology is causing a structural change in the cloud-computing world. Containers make it possible to run multiple instances of an application on a single instance of an operating system, thereby using resources more efficiently. Containers give organizations consistency and flexibility. They enable continuous deployment because the application can be developed on a desktop, tested in a virtual machine, and then deployed for production in the cloud. Containers provide agility, streamlined operations, scalability, and reduced costs due to resource optimization. Enable Azure Container Registry authentication There are several ways to authenticate with an Azure container registry, each of which is applicable to one or more registry usage scenarios. Recommended ways include authenticating to a registry directly via individual login, or your applications and container orchestrators can perform unattended, or \"headless,\" authentication by using an Azure Active Directory (Azure AD) service principal. Authentication options The following table lists available authentication methods and recommended scenarios. Identity Usage scenario Details Azure AD identities including user and service principals Unattended push from DevOps, unattended pull to Azure or external services Role-based access control - Reader, Contributor, Owner Individual AD Identity Interactive push/pull by developers and testers Admin user Interactive push/pull by individual developers and testers By default, disabled. Individual login with Azure AD When working with your registry directly, such as pulling images to and pushing images from a development workstation, authenticate by using the az acr login command in the Azure CLI. When you log in with az acr login, the CLI uses the token created when you executed az login to seamlessly authenticate your session with your registry. To complete the authentication flow, Docker must be installed and running in your environment. az acr login uses the Docker client to set an Azure Active Directory token in the docker.config file. Once you've logged in this way, your credentials are cached, and subsequent docker commands in your session do not require a username or password. Service principal If you assign a service principal to your registry, your application or service can use it for headless authentication. Service principals allow role-based access to a registry, and you can assign multiple service principals to a registry. Multiple service principals allow you to define different access for different applications. The available roles for a container registry include: AcrPull: pull AcrPush: pull and push Owner: pull, push, and assign roles to other users Admin account Each container registry includes an admin user account, which is disabled by default. You can enable the admin user and manage its credentials in the Azure portal, or by using the Azure CLI or other Azure tools. The admin account is provided with two passwords, both of which can be regenerated. Two passwords allow you to maintain connection to the registry by using one password while you regenerate the other. If the admin account is enabled, you can pass the username and either password to the docker login command when prompted for basic authentication to the registry. Review Azure Kubernetes Service (AKS) As application development moves towards a container-based approach, the need to orchestrate and manage resources is important. Kubernetes is the leading platform that provides the ability to provide reliable scheduling of fault-tolerant application workloads. Azure Kubernetes Service (AKS) is a managed Kubernetes offering that further simplifies container-based application deployment and management. Kubernetes is ... Kubernetes is a rapidly evolving platform that manages container-based applications and their associated networking and storage components. The focus is on the application workloads, not the underlying infrastructure components. Kubernetes provides a declarative approach to deployments, backed by a robust set of APIs for management operations. You can build and run modern, portable, microservices-based applications that benefit from Kubernetes orchestrating and managing the availability of those application components. Kubernetes supports both stateless and stateful applications as teams progress through the adoption of microservices-based applications. Deployment of containers As an open platform, Kubernetes allows you to build your applications with your preferred programming language, OS, libraries, or messaging bus. Existing continuous integration and continuous delivery (CI/CD) tools can integrate with Kubernetes to schedule and deploy releases. Azure Kubernetes Service (AKS) provides a managed Kubernetes service that reduces the complexity for deployment and core management tasks, including coordinating upgrades. The AKS control plane is managed by the Azure platform, and you only pay for the AKS nodes that run your applications. AKS is built on top of the open-source Azure Kubernetes Service Engine (aks-engine). Kubernetes cluster architecture A Kubernetes cluster is divided into two components: Control plane nodes provide the core Kubernetes services and orchestration of application workloads. Nodes run your application workloads. Features of Azure Kubernetes Service Fully managed Public IP and FQDN (Private IP option) Accessed with RBAC or Azure AD Deployment of containers Dynamic scale containers Automation of rolling updates and rollbacks of containers Management of storage, network traffic, and sensitive information Implement an Azure Kubernetes Service architecture Kubernetes cluster architecture is a set of design recommendations for deploying your containers in a secure and managed configuration. Cluster master When you create an AKS cluster, a cluster master is automatically created and configured. This cluster master is provided as a managed Azure resource abstracted from the user. There is no cost for the cluster master, only the nodes that are part of the AKS cluster. The cluster master includes the following core Kubernetes components: kube-apiserver - The API server is how the underlying Kubernetes APIs are exposed. This component provides the interaction for management tools, such as kubectl or the Kubernetes dashboard. etcd - To maintain the state of your Kubernetes cluster and configuration, the highly available etcd is a key value store within Kubernetes. kube-scheduler - When you create or scale applications, the Scheduler determines what nodes can run the workload and starts them. kube-controller-manager - The Controller Manager oversees a number of smaller Controllers that perform actions such as replicating pods and handling node operations. AKS provides a single-tenant cluster master, with a dedicated API server, Scheduler, etc. You define the number and size of the nodes, and the Azure platform configures the secure communication between the cluster master and nodes. Interaction with the cluster master occurs through Kubernetes APIs, such as kubectl or the Kubernetes dashboard. This managed cluster master means that you do not need to configure components like a highly available store, but it also means that you cannot access the cluster master directly. Upgrades to Kubernetes are orchestrated through the Azure CLI or Azure portal, which upgrades the cluster master and then the nodes. To troubleshoot possible issues, you can review the cluster master logs through Azure Log Analytics. If you need to configure the cluster master in a particular way or need direct access to them, you can deploy your own Kubernetes cluster using aks-engine. Nodes and node pools To run your applications and supporting services, you need a Kubernetes node. An AKS cluster has one or more nodes, which is an Azure virtual machine (VM) that runs the Kubernetes node components and container runtime: The kubelet is the Kubernetes agent that processes the orchestration requests from the control plane and scheduling of running the requested containers. Virtual networking is handled by the kube-proxy on each node. The proxy routes network traffic and manages IP addressing for services and pods. The container runtime is the component that allows containerized applications to run and interact with additional resources such as the virtual network and storage. In AKS, Moby is used as the container runtime. The Azure VM size for your nodes defines how many CPUs, how much memory, and the size and type of storage available (such as high-performance SSD or regular HDD). If you anticipate a need for applications that require large amounts of CPU and memory or high-performance storage, plan the node size accordingly. You can also scale out the number of nodes in your AKS cluster to meet demand. In AKS, the VM image for the nodes in your cluster is currently based on Ubuntu Linux or Windows Server 2019. When you create an AKS cluster or scale out the number of nodes, the Azure platform creates the requested number of VMs and configures them. There's no manual configuration for you to perform. Agent nodes are billed as standard virtual machines, so any discounts you have on the VM size you're using (including Azure reservations) are automatically applied. If you need to use a different host OS, container runtime, or include custom packages, you can deploy your own Kubernetes cluster using aks-engine. The upstream aks-engine releases features and provides configuration options before they are officially supported in AKS clusters. For example, if you wish to use a container runtime other than Moby, you can use aks-engine to configure and deploy a Kubernetes cluster that meets your current needs. Cluster master nodes provide the core Kubernetes services and orchestration of application workloads. Nodes (virtual machines) run your application workloads. AKS Terminology Term Description Pools Group of nodes with identical configuration Node Individual VM running containerized applications Pods Single instance of an application. A pod can contain multiple containers Deployment One or more identical pods managed by Kubernetes Manifest YAML file describing a deployment Master security In AKS, the Kubernetes master components are part of the managed service provided by Microsoft. Each AKS cluster has its own single-tenanted, dedicated Kubernetes master to provide the API Server, Scheduler, etc. This master is managed and maintained by Microsoft. By default, the Kubernetes API server uses a public IP address and a fully qualified domain name (FQDN). You can control access to the API server using Kubernetes role-based access controls and Azure Active Directory. Node security AKS nodes are Azure virtual machines that you manage and maintain. Linux nodes run an optimized Ubuntu distribution using the Moby container runtime. Windows Server nodes run an optimized Windows Server 2019 release and also use the Moby container runtime. When an AKS cluster is created or scaled up, the nodes are automatically deployed with the latest OS security updates and configurations. The Azure platform automatically applies OS security patches to Linux nodes on a nightly basis. If a Linux OS security update requires a host reboot, that reboot is not automatically performed. You can manually reboot the Linux nodes, or a common approach is to use Kured, an open-source reboot daemon for Kubernetes. Kured runs as a DaemonSet and monitors each node for the presence of a file indicating that a reboot is required. Reboots are managed across the cluster using the same cordon and drain process as a cluster upgrade. For Windows Server nodes, Windows Update does not automatically run and apply the latest updates. On a regular schedule around the Windows Update release cycle and your own validation process, you should perform an upgrade on the Windows Server node pool(s) in your AKS cluster. This upgrade process creates nodes that run the latest Windows Server image and patches, then removes the older nodes. Nodes are deployed into a private virtual network subnet, with no public IP addresses assigned. For troubleshooting and management purposes, SSH is enabled by default. This SSH access is only available using the internal IP address. To provide storage, the nodes use Azure Managed Disks. For most VM node sizes, these are Premium disks backed by high-performance SSDs. The data stored on managed disks is automatically encrypted at rest within the Azure platform. To improve redundancy, these disks are also securely replicated within the Azure datacenter. Kubernetes environments, in AKS or elsewhere, currently aren't completely safe for hostile multi-tenant usage. Additional security features such as Pod Security Policies or more fine-grained role-based access controls (RBAC) for nodes make exploits more difficult. However, for true security when running hostile multi-tenant workloads, a hypervisor is the only level of security that you should trust. The security domain for Kubernetes becomes the entire cluster, not an individual node. For these types of hostile multi-tenant workloads, you should use physically isolated clusters. To protect your customer data as you run application workloads in Azure Kubernetes Service (AKS), the security of your cluster is a key consideration. Configure Azure Kubernetes Service networking To allow access to your applications, or for application components to communicate with each other, Kubernetes provides an abstraction layer to virtual networking. Kubernetes nodes are connected to a virtual network, and can provide inbound and outbound connectivity for pods. The kube-proxy component runs on each node to provide these network features. In Kubernetes, Services logically group pods to allow for direct access via an IP address or DNS name and on a specific port. You can also distribute traffic using a load balancer. More complex routing of application traffic can also be achieved with Ingress Controllers. Security and filtering of the network traffic for pods is possible with Kubernetes network policies. The Azure platform also helps to simplify virtual networking for AKS clusters. When you create a Kubernetes load balancer, the underlying Azure load balancer resource is created and configured. As you open network ports to pods, the corresponding Azure network security group rules are configured. For HTTP application routing, Azure can also configure external DNS as new ingress routes are configured. Services To simplify the network configuration for application workloads, Kubernetes uses Services to logically group a set of pods together and provide network connectivity. The following Service types are available: Cluster IP - Creates an internal IP address for use within the AKS cluster. Good for internal-only applications that support other workloads within the cluster. NodePort - Creates a port mapping on the underlying node that allows the application to be accessed directly with the node IP address and port. LoadBalancer - Creates an Azure load balancer resource, configures an external IP address, and connects the requested pods to the load balancer backend pool. To allow customers' traffic to reach the application, load balancing rules are created on the desired ports. ExternalName - Creates a specific DNS entry for easier application access. When you run modern, microservices-based applications in Kubernetes, you often want to control which components can communicate with each other. The principle of least privilege should be applied to how traffic can flow between pods in an Azure Kubernetes Service (AKS) cluster. Let's say you likely want to block traffic directly to back-end applications. The Network Policy feature in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster. Deploy Azure Kubernetes Service storage Applications that run in Azure Kubernetes Service (AKS) may need to store and retrieve data. For some application workloads, this data storage can use local, fast storage on the node that is no longer needed when the pods are deleted. Other application workloads may require storage that persists on more regular data volumes within the Azure platform. Multiple pods may need to share the same data volumes, or reattach data volumes if the pod is rescheduled on a different node. Finally, you may need to inject sensitive data or application configuration information into pods. Volumes Applications often need to be able to store and retrieve data. As Kubernetes typically treats individual pods as ephemeral, disposable resources, different approaches are available for applications to use and persist data as necessary. A volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. Traditional volumes to store and retrieve data are created as Kubernetes resources backed by Azure Storage. You can manually create these data volumes to be assigned to pods directly, or have Kubernetes automatically create them. These data volumes can use Azure Disks or Azure Files: Azure Disks can be used to create a Kubernetes DataDisk resource. Disks can use Azure Premium storage, backed by high-performance SSDs, or Azure Standard storage, backed by regular HDDs. For most production and development workloads, use Premium storage. Azure Disks are mounted as ReadWriteOnce, so are only available to a single pod. For storage volumes that can be accessed by multiple pods simultaneously, use Azure Files. Azure Files can be used to mount an SMB 3.0 share backed by an Azure Storage account to pods. Files let you share data across multiple nodes and pods. Files can use Azure Standard storage backed by regular HDDs, or Azure Premium storage, backed by high-performance SSDs. Persistent volumes Volumes that are defined and created as part of the pod lifecycle only exist until the pod is deleted. Pods often expect their storage to remain if a pod is rescheduled on a different host during a maintenance event, especially in StatefulSets. A persistent volume (PV) is a storage resource created and managed by the Kubernetes API that can exist beyond the lifetime of an individual pod. Azure Disks or Files are used to provide the PersistentVolume. As noted in the previous section on Volumes, the choice of Disks or Files is often determined by the need for concurrent access to the data or the performance tier. A PersistentVolume can be statically created by a cluster administrator, or dynamically created by the Kubernetes API server. If a pod is scheduled and requests storage that is not currently available, Kubernetes can create the underlying Azure Disk or Files storage and attach it to the pod. Dynamic provisioning uses a StorageClass to identify what type of Azure storage needs to be created Storage classes To define different tiers of storage, such as Premium and Standard, you can create a StorageClass. The StorageClass also defines the reclaimPolicy. This reclaimPolicy controls the behavior of the underlying Azure storage resource when the pod is deleted and the persistent volume may no longer be required. The underlying storage resource can be deleted, or retained for use with a future pod. In AKS, two initial StorageClasses are created: default - Uses Azure Standard storage to create a Managed Disk. The reclaim policy indicates that the underlying Azure Disk is deleted when the persistent volume that used it is deleted. managed-premium - Uses Azure Premium storage to create Managed Disk. The reclaim policy again indicates that the underlying Azure Disk is deleted when the persistent volume that used it is deleted. If no StorageClass is specified for a persistent volume, the default StorageClass is used. Persistent volume claims A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. The Kubernetes API server can dynamically provision the underlying storage resource in Azure if there is no existing resource to fulfill the claim based on the defined StorageClass. The pod definition includes the volume mount once the volume has been connected to the pod. A PersistentVolume is bound to a PersistentVolumeClaim once an available storage resource has been assigned to the pod requesting it. There is a 1:1 mapping of persistent volumes to claims. Secure authentication to Azure Kubernetes Service with Active Directory\u200b There are different ways to authenticate with and secure Kubernetes clusters. Using role-based access controls (RBAC), you can grant users or groups access to only the resources they need. With Azure Kubernetes Service (AKS), you can further enhance the security and permissions structure by using Azure Active Directory. These approaches help you secure your application workloads and customer data. Kubernetes service accounts One of the primary user types in Kubernetes is a service account. A service account exists and is managed by, the Kubernetes API. The credentials for service accounts are stored as Kubernetes secrets, which allows them to be used by authorized pods to communicate with the API Server. Most API requests provide an authentication token for a service account or a normal user account. Normal user accounts allow more traditional access for human administrators or developers, not just services and processes. Kubernetes itself doesn't provide an identity management solution where regular user accounts and passwords are stored. Instead, external identity solutions can be integrated into Kubernetes. For AKS clusters, this integrated identity solution is Azure Active Directory. Azure Active Directory integration The security of AKS clusters can be enhanced with the integration of Azure Active Directory (AD). Built on decades of enterprise identity management, Azure AD is a multi-tenant, cloud-based directory, and identity management service that combines core directory services, application access management, and identity protection. With Azure AD, you can integrate on-premises identities into AKS clusters to provide a single source for account management and security. With Azure AD-integrated AKS clusters, you can grant users or groups access to Kubernetes resources within a namespace or across the cluster. To obtain a Kubectl configuration context, a user can run the az aks get-credentials command. When a user then interacts with the AKS cluster with kubectl, they are prompted to sign in with their Azure AD credentials. This approach provides a single source for user account management and password credentials. The user can only access the resources as defined by the cluster administrator. Azure AD authentication in AKS clusters uses OpenID Connect, an identity layer built on top of the OAuth 2.0 protocol. OAuth 2.0 defines mechanisms to obtain and use access tokens to access protected resources, and OpenID Connect implements authentication as an extension to the OAuth 2.0 authorization process. Manage access to Azure Kubernetes Service using Azure role-based access controls One additional mechanism for controlling access to resources is Azure role-based access controls (RBAC). Kubernetes RBAC is designed to work on resources within your AKS cluster, and Azure RBAC is designed to work on resources within your Azure subscription. With Azure RBAC, you create a role definition that outlines the permissions to be applied. A user or group is then assigned this role definition for a particular scope, which could be an individual resource, a resource group, or across the subscription. Roles and ClusterRoles Before you assign permissions to users with Kubernetes RBAC, you first define those permissions as a Role. Kubernetes roles grant permissions. There is no concept of a deny permission. Roles are used to grant permissions within a namespace. If you need to grant permissions across the entire cluster, or to cluster resources outside a given namespace, you can instead use ClusterRoles. A ClusterRole works in the same way to grant permissions to resources, but can be applied to resources across the entire cluster, not a specific namespace. RoleBindings and ClusterRoleBindings Once roles are defined to grant permissions to resources, you assign those Kubernetes RBAC permissions with a RoleBinding. If your AKS cluster integrates with Azure Active Directory, bindings are how those Azure AD users are granted permissions to perform actions within the cluster. Role bindings are used to assign roles for a given namespace. This approach lets you logically segregate a single AKS cluster, with users only able to access the application resources in their assigned namespace. If you need to bind roles across the entire cluster, or to cluster resources outside a given namespace, you can instead use ClusterRoleBindings. A ClusterRoleBinding works in the same way to bind roles to users, but can be applied to resources across the entire cluster, not a specific namespace. This approach lets you grant administrators or support engineers access to all resources in the AKS cluster. Kubernetes Secrets A Kubernetes Secret is used to inject sensitive data into pods, such as access credentials or keys. You first create a Secret using the Kubernetes API. When you define your pod or deployment, a specific Secret can be requested. Secrets are only provided to nodes that have a scheduled pod that requires it, and the Secret is stored in tmpfs, not written to disk. When the last pod on a node that requires a Secret is deleted, the Secret is deleted from the node's tmpfs. Secrets are stored within a given namespace and can only be accessed by pods within the same namespace. The use of Secrets reduces the sensitive information that is defined in the pod or service YAML manifest. Instead, you request the Secret stored in Kubernetes API Server as part of your YAML manifest. This approach only provides the specific pod access to the Secret. Please note: the raw secret manifest files contains the secret data in base64 format. Therefore, this file should be treated as sensitive information, and never committed to source control. Windows containers Secrets are written in clear text on the node\u2019s volume (as compared to tmpfs/in-memory on linux). This means customers have to do two things Use file ACLs to secure the secrets file location Use volume-level encryption using BitLocker Knowledge check To interact with Azure APIs, an Azure Kubernetes Service (AKS) cluster requires which of following? AKS contributor Azure AD service principal( Ans ) Global Administrator permissions When using Azure Kubernetes Service (AKS) and there is a need to control the flow of traffic between pods and block traffic directly to the backend application; what is the best way to configure this? Create an AKS network policy( Ans ) Create an application gateway Create an Azure firewall The organization is defining RBAC rules for the Azure Kubernetes security team. What is the best solution to grant permissions across the entire cluster? ClusterRoles and RoleBindings ClusterRoles and ClusterRoleBindings( Ans ) Roles and RoleBindings","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#az-500-microsoft-certified-azure-security-engineer-associate","text":"","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#implement-platform-protection","text":"Implement perimeter security Introduction Define defense in depth Explore virtual network security Enable Distributed Denial of Service (DDoS) Protection Configure a distributed denial of service protection implementation Explore Azure Firewall features Deploy an Azure Firewall implementation Configure VPN forced tunneling Create User Defined Routes and Network Virtual Appliances Explore hub and spoke topology Perform try-this exercises Knowledge check Summary Configure network security Introduction Explore Network Security Groups (NSG) Deploy a Network Security Groups implementation Create Application Security Groups Enable service endpoints Configure service endpoint services Deploy private links Implement an Azure application gateway Deploy a web application firewall Configure and manage Azure front door Review ExpressRoute Perform try-this exercises Knowledge check Summary 3 Configure and manage host security Introduction Enable endpoint protection Define a privileged access device strategy Deploy privileged access workstations Create virtual machine templates Enable and secure remote access management Configure update management Deploy disk encryption Managed disk encryption options Deploy and configure Windows Defender Microsoft cloud security benchmark in Defender for Cloud Explore Microsoft Defender for Cloud recommendations Perform Try-This exercises Knowledge check Summary 4 Enable Containers security Introduction Explore containers Configure Azure Container Instances security Manage security for Azure Container Instances (ACI)\u200b Explore the Azure Container Registry (ACR) Enable Azure Container Registry authentication Review Azure Kubernetes Service (AKS) Implement an Azure Kubernetes Service architecture\u200b Configure Azure Kubernetes Service networking Deploy Azure Kubernetes Service storage\u200b Secure authentication to Azure Kubernetes Service with Active Directory Manage access to Azure Kubernetes Service using Azure role-based access controls Knowledge check Summary","title":"Implement platform protection"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#introduction","text":"Think of security in layers. You need to lock down the perimeter first, and then work your way inward locking down each layer. The more work and cost a bad-actor has to invest to get to valuable information the more likely they are to leave you alone.","title":"Introduction"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#scenario","text":"A security engineer uses perimeter security features to block network traffic away from your primary network resources; you will work on such tasks as: Setup DDoS (denial-of-service) Protection. Configure and maintain firewalls. Use dedicated routes and network appliance to protect your network.","title":"Scenario"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#define-defense-in-depth","text":"The Defense in depth approach includes other controls in the design to mitigate risk to the organization in the event a primary security control fails. This design should consider how likely the primary control is to fail, the potential organizational risk if it does, and the effectiveness of the additional control (especially in the likely cases that would cause the primary control to fail). During this module, we will explore the defense-in-depth design of Azure services and capabilities to help you securely manage and monitor your cloud data and infrastructure as a managed service. Microsoft designs and operates its cloud services with security at the core and provides you built-in controls and tools to meet your security needs. In addition, with Machine Learning (ML) and Microsoft's significant investments in cyber defense you can benefit from unique intelligence and proactive measures to protect you from threats. Azure offers unified security management and advanced threat protection for your resources whether they're in the cloud, your data center, or both. Each Service in Azure is built with security in mind from the ground up to host your infrastructure apps and data. All services are designed and operated to support multiple layers of defense, spanning your data apps, virtual machines, network perimeter-related policies, and physical security within our data centers. Including how the data sensors and systems that run Azure are architected and operated to the controls you can apply as part of your defense in-depth security management. An example strategy is illustrated in the following image. As more of a company\u2019s digital resources reside outside the corporate network, in the cloud and on personal devices, it becomes obvious that a perimeter only based security, i.e. firewalls, DMZ, VNets, are no longer adequate. The adoption of software-defined networking (SDN) and software-defined data center (SDDC) technologies are driving Network Segmentation concepts to be more granular, i.e. Network Micro-Segmentation.","title":"Define defense in depth"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#network-micro-segmentation","text":"Micro-segmentation is a way to create secure zones in data centers and Azure deployments that allow you to isolate workloads and protect them individually. Security policies in a virtual environment can be assigned to virtual connections that can move with an application if the network is reconfigured \u2013 making the security policy persistent. A best practice recommendation is to adopt a Zero Trust strategy based on user, device, and application identities. In contrast to network access controls that are based on elements such as source and destination IP address, protocols, and port numbers, Zero Trust enforces and validates access control at \u201caccess time\u201d. Granting access at access-time avoids the need to play a prediction game for an entire deployment, network, or subnet \u2013 only the destination resource needs to provide the necessary access controls. Azure Network Security Groups can be used for basic layer 3 & 4 access controls between Azure Virtual Networks, their subnets, and the Internet. Application Security Groups enable you to define fine-grained network security policies based on workloads, centralized on applications, instead of explicit IP addresses. Azure Web Application Firewall and the Azure Firewall can be used for more advanced network access controls that require application layer support. Local Admin Password Solution (LAPS) or a third-party Privileged Access Management can set strong local admin passwords and just in time access to them. Additionally, third parties offer micro-segmentation approaches that may enhance your network controls by applying zero trust principles to networks you control with legacy assets on them.","title":"Network Micro-Segmentation"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#explore-virtual-network-security","text":"","title":"Explore virtual network security"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#azure-networking-components","text":"The following sections define key terminology for Azure networking. Later, this course will cover each of these areas in more detail. Azure Virtual Networks are a key component of Azure security services. The Azure network infrastructure enables you to securely connect Azure resources to each other with virtual networks (VNets). A VNet is a representation of your own network in the cloud. A VNet is a logical isolation of the Azure cloud network dedicated to your subscription. You can connect VNets to your on-premises networks. Azure supports dedicated WAN link connectivity to your on-premises network and an Azure Virtual Network with ExpressRoute. The link between Azure and your site uses a dedicated connection that does not go over the public Internet. If your Azure application is running in multiple datacenters, you can use Azure Traffic Manager to route requests from users intelligently across instances of the application. You can also route traffic to services not running in Azure if they are accessible from the Internet.","title":"Azure Networking Components"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#virtual-networks","text":"Organizations can use virtual networks to connect resources. Virtual networks in Azure are network overlays that you can use to configure and control the connectivity among Azure resources, such as VMs and load balancers. Azure Virtual Network enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. A virtual network is scoped to a single Azure region. An Azure region is a set of datacenters deployed within a latency-defined perimeter and connected through a dedicated regional low-latency network. Virtual networks are made up of subnets. A subnet is a range of IP addresses within your virtual network. Subnets, like virtual networks, are scoped to a single Azure region. You can implement multiple virtual networks within each Azure subscription and Azure region. Each virtual network is isolated from other virtual networks. For each virtual network you can: Specify a custom private IP address space using public and private addresses. Azure assigns resources in a virtual network a private IP address from the address space that you assign. Segment the virtual network into one or more subnets and allocate a portion of the virtual network's address space to each subnet. Use Azure-provided name resolution, or specify your own DNS server, for use by resources in a virtual network.","title":"Virtual networks"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#ip-addresses","text":"VMs, Azure load balancers, and application gateways in a single virtual network require unique Internet Protocol (IP) addresses the same way that clients in an on-premises subnet do. This enables these resources to communicate with each other. A virtual network uses two types of IP addresses: Private - A private IP address is dynamically or statically allocated to a VM from the defined scope of IP addresses in the virtual network. VMs use these addresses to communicate with other VMs in the same or connected virtual networks through a gateway / Azure ExpressRoute connection. These private IP addresses, or non-routable IP addresses, conform to RFC 1918. Public - Public IP addresses, which allow Azure resources to communicate with external clients, are assigned directly at the virtual network adapter of the VM or to the load balancer. Public IP address can also be added to Azure-only virtual networks. All IP blocks in the virtual network will be routable only within the customer's network, and they won't be reachable from outside. Virtual network packets travel through the high-speed Azure backplane. You can control the dynamic IP addresses assigned to VMs and cloud services within an Azure virtual network by specifying an IP addressing scheme. Planning an IP addressing scheme within an Azure virtual network is much like planning an IP addressing scheme on-premises. The same ranges are often used, and the same rules applied. However, conditions exist that are unique to Azure virtual networks.","title":"IP addresses"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#subnets","text":"You can further divide your network by using subnets for the logical and security-related isolation of Azure resources. Each subnet contains a range of IP addresses that fall within the virtual network address space. Subnetting hides the details of internal network organization from external routers. Subnetting also segments the host within the network, making it easier to apply network security at the interconnections between subnets.","title":"Subnets"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#network-adapters","text":"VMs communicate with other VMs and other resources on the network by using virtual network adapters. Virtual network adapters configure VMs with private and, optionally, public IP address. A VM can have more than one network adapter for different network configurations.","title":"Network adapters"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#enable-distributed-denial-of-service-ddos-protection","text":"A denial of service attack (DoS) is an attack that has the goal of preventing access to services or systems. If the attack originates from one location, it is called a DoS. If the attack originates from multiple networks and systems, it is called distributed denial of service (DDoS). Before learning more about DDoS, you need to know what botnets are. Botnets are collections of internet-connected systems that an individual controls and uses without their owners\u2019 knowledge. Botnet owners use them to perform various actions of their choosing. Often, they use them for spamming, data storage, DDoS, or various other actions that are up to the person in control of the botnet. In the past, botnets were made up just of compromised computers, but now, botnets are also made up of Internet of Things (IoT) devices. Malicious hackers can get these poorly secured security cameras, digital video recorders, thermostats, and other internet-connected devices under their control. So, DDoS is a collection of attack types aimed at disrupting the availability of a target. These attacks involve a coordinated effort that uses multiple internet-connected systems to launch many network requests against DNS, web services, email, and more. Pretty much any application that the malicious hacker can access might become the target of a DDoS. The malicious hacker\u2019s goal is to overwhelm system resources on targeted servers so they can no longer process legitimate traffic, effectively making the system inaccessible. A DDoS generally involves many systems sending traffic to targets as part of a botnet. In most cases, the owners of the systems in a botnet don\u2019t know that their devices are compromised and participating in an attack. Botnets are becoming a bigger problem than before because of the increasing numbers of connected devices. Designing and building for DDoS resiliency requires planning and designing for a variety of failure modes. The following table lists the best practices for building DDoS-resilient services in Azure.","title":"Enable Distributed Denial of Service (DDoS) Protection"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#best-practice-1","text":"Ensure that security is a priority throughout the entire lifecycle of an application, from design and implementation to deployment and operations. Applications might have bugs that allow a relatively low volume of requests to use a lot of resources, resulting in a service outage.","title":"Best practice 1"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#solution-1","text":"To help protect a service running in Azure, understand your application architecture, and focus on the five pillars of software quality. They are: Pillar Description Scalability The ability of a system to handle increased load Availability The proportion of time that a system is functional and working Resiliency The ability of a system to recover from failures and continue to function Management Operations processes that keep a system running in production Security Protecting applications and data from threats You should know typical traffic volumes, the connectivity model between the application and other applications, and the service endpoints that are exposed to the public internet. Helping ensure that an application is resilient enough to handle a DoS targeted at the application itself is most important. Security and privacy features are built in to the Azure platform, beginning with the Microsoft Security Development Lifecycle (SDL). The SDL addresses security at every development phase and ensures that Azure is continually updated to make it even more secure. We will look at SDL later in this course.","title":"Solution 1"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#best-practice-2","text":"Design your applications to scale horizontally to meet the demands of an amplified load\u2014specifically, in the event of a DDoS. If your application depends on a single instance of a service, it creates a single point of failure. Provisioning multiple instances makes your system more resilient and more scalable.","title":"Best practice 2"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#solution-2","text":"For Azure App Service, select an App Service plan that offers multiple instances. For Azure Cloud Services, configure each of your roles to use multiple instances. For Azure Virtual Machines, ensure that your VM architecture includes more than one VM and that each VM is included in an availability set. We recommend using virtual machine scale sets for autoscaling capabilities.","title":"Solution 2"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#best-practice-3","text":"Layer security defenses in an application to reduce the chance of a successful attack. Implement security-enhanced designs for your applications by using the built-in capabilities of the Azure platform.","title":"Best practice 3"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#solution-3","text":"Be aware that the risk of attack increases with the size, or surface area, of the application. You can reduce the surface area by using IP allowlists to close down the exposed IP address space and listening ports that aren\u2019t needed on the load balancers (for Azure Load Balancer and Azure Application Gateway). You can also use NSGs to reduce the attack surface. You can use service tags and application security groups as a natural extension of an application\u2019s structure to minimize complexity for creating security rules and configuring network security.","title":"Solution 3"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#configure-a-distributed-denial-of-service-protection-implementation","text":"Azure Distributed Denial of Service (DDoS) protection, combined with application design best practices, provide defense against DDoS attacks. Azure DDoS protection provides the following service tiers: Basic : Automatically enabled as part of the Azure platform. Always-on traffic monitoring, and real-time mitigation of common network-level attacks, provide the same defenses utilized by Microsoft's online services. The entire scale of Azure's global network can be used to distribute and mitigate attack traffic across regions. Protection is provided for IPv4 and IPv6 Azure public IP addresses. Standard : Provides additional mitigation capabilities over the Basic service tier that are tuned specifically to Azure Virtual Network resources. DDoS Protection Standard is simple to enable, and requires no application changes. Protection policies are tuned through dedicated traffic monitoring and machine learning algorithms. Policies are applied to public IP addresses associated to resources deployed in virtual networks, such as Azure Load Balancer, Azure Application Gateway, and Azure Service Fabric instances, but this protection does not apply to App Service Environments. Real-time telemetry is available through Azure Monitor views during an attack, and for history. Rich attack mitigation analytics are available via diagnostic settings. Application layer protection can be added through the Azure Application Gateway Web Application Firewall or by installing a 3rd party firewall from Azure Marketplace. Protection is provided for IPv4 and IPv6 Azure public IP addresses.","title":"Configure a distributed denial of service protection implementation"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#how-azure-denial-of-service-protection-works","text":"DDoS Protection Standard monitors actual traffic utilization and constantly compares it against the thresholds defined in the DDoS policy. When the traffic threshold is exceeded, DDoS mitigation is automatically initiated. When traffic returns to a level below the threshold, the mitigation is removed. During mitigation, DDoS Protection redirects traffic sent to the protected resource and performs several checks, including: - Helping ensure that packets conform to internet specifications and aren\u2019t malformed. - Interacting with the client to determine if the traffic might be a spoofed packet (for example, using SYN Auth or SYN Cookie or dropping a packet for the source to retransmit it). - Using rate-limit packets if it can\u2019t perform any other enforcement method. DDoS Protection blocks attack traffic and forwards the remaining traffic to its intended destination. Within a few minutes of attack detection, you\u2019ll be notified with Azure Monitor metrics. By configuring logging on DDoS Protection Standard telemetry, you can write the logs to available options for future analysis. Azure Monitor retains metric data for DDoS Protection Standard for 30 days.","title":"How Azure denial-of-service protection works"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#types-of-denial-of-service-attacks-that-azure-protection-mitigates","text":"DDoS Protection Standard can mitigate the following types of attacks: Volumetric attacks : The attack's goal is to flood the network layer with a substantial amount of seemingly legitimate traffic. It includes UDP floods, amplification floods, and other spoofed-packet floods. DDoS Protection Standard mitigates these potential multi-gigabyte attacks by absorbing and scrubbing them, with Azure's global network scale, automatically. Protocol attacks : These attacks render a target inaccessible, by exploiting a weakness in the layer 3 and layer 4 protocol stack. It includes, SYN flood attacks, reflection attacks, and other protocol attacks. DDoS Protection Standard mitigates these attacks, differentiating between malicious and legitimate traffic, by interacting with the client, and blocking malicious traffic. Resource (application) layer attacks : These attacks target web application packets, to disrupt the transmission of data between hosts. The attacks include HTTP protocol violations, SQL injection, cross-site scripting, and other layer 7 attacks. Use a Web Application Firewall, such as the Azure Application Gateway web application firewall, as well as DDoS Protection Standard to provide defense against these attacks. There are also third-party web application firewall offerings available in the Azure Marketplace. ** Important** DDoS Protection Standard protects resources in a virtual network including public IP addresses associated with virtual machines, load balancers, and application gateways. When coupled with the Application Gateway web application firewall, or a third-party web application firewall deployed in a virtual network with a public IP, DDoS Protection Standard can provide full layer 3 to layer 7 mitigation capability.","title":"Types of denial-of-service attacks that Azure protection mitigates"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#explore-azure-firewall-features","text":"Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It\u2019s a fully stateful firewall-as-a-service with built-in high availability and unrestricted cloud scalability. By default, Azure Firewall blocks traffic.","title":"Explore Azure Firewall features"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#the-azure-firewall-features-include","text":"Built-in high availability - Because high availability is built in, no additional load balancers are required and there\u2019s nothing you need to configure. Unrestricted cloud scalability - Azure Firewall can scale up as much as you need, to accommodate changing network traffic flows so you don't need to budget for your peak traffic. Application Fully Qualified Domain Name (FQDN) filtering rules - You can limit outbound HTTP/S traffic to a specified list of FQDNs, including wild cards. This feature does not require SSL termination. Network traffic filtering rules - You can centrally create allow or deny network filtering rules by source and destination IP address, port, and protocol. Azure Firewall is fully stateful, so it can distinguish legitimate packets for different types of connections. Rules are enforced and logged across multiple subscriptions and virtual networks. Qualified domain tags - Fully Qualified Domain Names (FQDN) tags make it easier for you to allow well known Azure service network traffic through your firewall. For example, say you want to allow Windows Update network traffic through your firewall. You create an application rule and include the Windows Update tag. Now network traffic from Windows Update can flow through your firewall. Outbound Source Network Address Translation (OSNAT) support - All outbound virtual network traffic IP addresses are translated to the Azure Firewall public IP. You can identify and allow traffic originating from your virtual network to remote internet destinations. Inbound Destination Network Address Translation (DNAT) support - Inbound network traffic to your firewall public IP address is translated and filtered to the private IP addresses on your virtual networks. - Azure Monitor logging - All events are integrated with Azure Monitor, allowing you to archive logs to a storage account, stream events to your Event Hub, or send them to Azure Monitor logs. Grouping the features above into logical groups reveals that Azure Firewall has three rule types: NAT rules, network rules, and application rules. The application order precedence for the rules are that network rules are applied first, then application rules. Rules are terminating, which means if a match is found in network rules, then application rules are not processed. If there\u2019s no network rule match, and if the packet protocol is HTTP/HTTPS, the packet is then evaluated by the application rules. If no match continues to be found, then the packet is evaluated against the infrastructure rule collection. If there\u2019s still no match, then the packet is denied by default. NAT rules You can configure inbound connectivity by configuring Destination Network Address Translation (DNAT) as described in: Filter inbound traffic with Azure Firewall DNAT using the Azure portal. DNAT rules are applied first. If a match is found, an implicit corresponding network rule to allow the translated traffic is added. You can override this behavior by explicitly adding a network rule collection with deny rules that match the translated traffic. No application rules are applied for these connections. Firewall rules to secure Azure Storage Azure Storage provides a layered security model, which enables you to secure your storage accounts to a specific set of supported networks. When network rules are configured, only applications requesting data from over the specified set of networks can access a storage account. An application that accesses a storage account when network rules are in effect requires proper authorization on the request. Authorization is supported with Azure AD credentials for blobs and queues, a valid account access key, or a SAS token. By default, storage accounts accept connections from clients on any network. To limit access to selected networks, you must first change the default action. Making changes to network rules can impact your applications' ability to connect to Azure Storage. Setting the default network rule to Deny blocks all access to the data unless specific network rules that grant access are also applied. Be sure to grant access to any allowed networks using network rules before you change the default rule to deny access. Grant access from a virtual network You can configure storage accounts to allow access only from specific VNets. You enable a service endpoint for Azure Storage within the VNet. This endpoint gives traffic an optimal route to the Azure Storage service. The identities of the virtual network and the subnet are also transmitted with each request. Administrators can then configure network rules for the storage account that allow requests to be received from specific subnets in the VNet. Clients granted access via these network rules must continue to meet the authorization requirements of the storage account to access the data. Each storage account supports up to 100 virtual network rules, which could be combined with IP network rules. Controlling outbound and inbound network access is an important part of an overall network security plan. Network traffic is subjected to the configured firewall rules when you route your network traffic to the firewall as the default gateway.","title":"The Azure firewall features include"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#deploy-an-azure-firewall-implementation","text":"Controlling outbound network access is an important part of an overall network security plan. For example, you may want to limit access to web sites. Or, you may want to limit the outbound IP addresses and ports that can be accessed. One way you can control outbound network access from an Azure subnet is with Azure Firewall. With Azure Firewall, you can configure: Application rules that define fully qualified domain names (FQDNs) that can be accessed from a subnet. Network rules that define source address, protocol, destination port, and destination address. Network traffic is subjected to the configured firewall rules when you route your network traffic to the firewall as the subnet default gateway. Fully Qualified Domain Name (FQDN) tag An FQDN tag represents a group of fully qualified domain names (FQDNs) associated with well known Microsoft services. You can use an FQDN tag in application rules to allow the required outbound network traffic through your firewall. For example, to manually allow Windows Update network traffic through your firewall, you need to create multiple application rules per the Microsoft documentation. Using FQDN tags, you can create an application rule, include the Windows Updates tag, and now network traffic to Microsoft Windows Update endpoints can flow through your firewall. Infrastructure qualified domain names Azure Firewall includes a built-in rule collection for infrastructure FQDNs that are allowed by default. These FQDNs are specific for the platform and can't be used for other purposes. The following services are included in the built-in rule collection: Compute access to storage Platform Image Repository (PIR) Managed disks status storage access Azure Diagnostics and Logging (MDS) Logs and metrics You can monitor Azure Firewall using firewall logs. You can also use activity logs to audit operations on Azure Firewall resources. You can access some of these logs through the portal. Logs can be sent to Azure Monitor logs, Storage, and Event Hubs and analyzed in Azure Monitor logs or by different tools such as Excel and Power BI. Metrics are lightweight and can support near real-time scenarios making them useful for alerting and fast issue detection. Threat intelligence-based filtering Threat intelligence-based filtering can be enabled for your firewall to alert and deny traffic from/to known malicious IP addresses and domains. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed. Intelligent Security Graph powers Microsoft threat intelligence and is used by multiple services including Microsoft Defender for Cloud. If you've enabled threat intelligence-based filtering, the associated rules are processed before any of the NAT rules, network rules, or application rules. You can choose to just log an alert when a rule is triggered, or you can choose alert and deny mode. By default, threat intelligence-based filtering is enabled in alert mode. Rule processing logic You can configure NAT rules, network rules, and applications rules on Azure Firewall. Rule collections are processed according to the rule type in priority order, lower numbers to higher numbers from 100 to 65,000. A rule collection name can have only letters, numbers, underscores, periods, or hyphens. It must begin with a letter or number, and end with a letter, number or underscore. The maximum name length is 80 characters. It's best to initially space your rule collection priority numbers in 100 increments (100, 200, 300, and so on) so you have room to add more rule collections if needed. Service tags A service tag represents a group of IP address prefixes to help minimize complexity for security rule creation. You cannot create your own service tag, nor specify which IP addresses are included within a tag. Microsoft manages the address prefixes encompassed by the service tag, and automatically updates the service tag as addresses change. Azure Firewall service tags can be used in the network rules destination field. You can use them in place of specific IP addresses. Remote work support VDI Work from home policies requires many IT organizations to address fundamental changes in capacity, network, security, and governance. Employees aren't protected by the layered security policies associated with on-premises services while working from home. Virtual Desktop Infrastructure (VDI) deployments on Azure can help organizations rapidly respond to this changing environment. However, you need a way to protect inbound/outbound Internet access to and from these VDI deployments. You can use Azure Firewall DNAT rules along with its threat intelligence-based filtering capabilities to protect your VDI deployments. Virtual Desktop support Azure Virtual Desktop is a comprehensive desktop and app virtualization service running in Azure. It\u2019s the only virtual desktop infrastructure (VDI) that delivers simplified management, multi-session Windows 10, optimizations for Microsoft 365 ProPlus, and support for Remote Desktop Services (RDS) environments. You can deploy and scale your Windows desktops and apps on Azure in minutes and get built-in security and compliance features. Windows Virtual Desktop doesn't require you to open any inbound access to your virtual network. However, you must allow a set of outbound network connections for the Windows Virtual Desktop virtual machines that run in your virtual network.","title":"Deploy an Azure Firewall implementation"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#configure-vpn-forced-tunneling","text":"Why do some cases require forced tunneling? - A virtual private network (VPN) consists of remote peers sending private data securely to one another over an unsecured network, such as the Internet. This is called Internet tunneling. Site-to-site (S2S) VPNs use tunnels to encapsulate data packets within normal IP packets for forwarding over IP-based networks, using encryption to ensure privacy and authentication to ensure integrity of data. Forced tunneling lets you redirect, or force, all internet-bound traffic back to your on-premises location via a site-to-site VPN tunnel for inspection and auditing. This is a critical security requirement for most enterprise IT policies. Without forced tunneling, internet-bound traffic from your VMs in Azure always traverses from the Azure network infrastructure directly to the internet\u2014without the option to allow you to inspect or audit the traffic. Unauthorized internet access potentially leads to information disclosure or other types of security breaches. As stated earlier, Azure currently works with two deployment models: The Resource Manager deployment model and the classic deployment model. The two models aren\u2019t completely compatible with each other. The following exercise goes through configuring tunneling for virtual networks that were created via the Resource Manager deployment model. The following figure depicts how forced tunneling works. In the preceding figure, the front-end subnet doesn\u2019t use forced tunneling. The workloads in the front-end subnet can continue to accept and respond to customer requests that come directly from the internet. The mid-tier and back-end subnets use forced tunneling. Any outbound connections from these two subnets to the internet are forced back to an on-premises site via one of the S2S VPN tunnels. This allows you to restrict and inspect internet access from your VMs or cloud services in Azure while continuing to enable your multi-tier service architecture. If no internet-facing workloads exist in your VMs, you can also apply forced tunneling to the entire virtual network. You configure forced tunneling in Azure via virtual network User Defined Routes (UDR). Redirecting traffic to an on-premises site is expressed as a default route to the Azure VPN gateway. This example uses UDRs to create a routing table to first add a default route and then associate the routing table with your virtual network subnets to enable forced tunneling on those subnets.","title":"Configure VPN forced tunneling"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#create-user-defined-routes-and-network-virtual-appliances","text":"User Defined Routes A User Defined Routes (UDR) is a custom route in Azure that overrides Azure's default system routes or adds routes to a subnet's route table. In Azure, you create a route table and then associate that route table with zero or more virtual network subnets. Each subnet can have zero or one route table associated with it. If you create a route table and associate it to a subnet, Azure either combines its routes with the default routes that Azure adds to a subnet or overrides those default routes. In this diagram UDRs are used to direct traffic from the Gateway subnet and the Web tier to the Network Virtual Appliance (NVA). Network Virtual Appliances You can deploy an NVA to a perimeter network in many architectures. In the previous diagram, the NVA helps provide a secure network boundary by checking all inbound and outbound network traffic and then passing only the traffic that meets the network security rules. However, the fact that all network traffic passes through the NVA means that the NVA is a single point of failure in the network. If the NVA fails, no other path will exist for network traffic, and all the back-end subnets will become unavailable. To make an NVA highly available, deploy more than one NVA into an availability set. The following figure shows a high-availability architecture that implements an ingress perimeter network behind an internet-facing load balancer. This architecture is designed to provide connectivity to Azure workloads for layer 7 traffic, such as HTTP or HTTPS traffic. The benefit of this architecture is that all NVAs are active, and if one fails, the load balancer directs network traffic to the other NVA. Both NVAs route traffic to the internal load balancer, so if one NVA is active, traffic will continue to flow. The NVAs are required to terminate SSL traffic intended for the web tier VMs. These NVAs can\u2019t be extended to handle on-premises traffic, because on-premises traffic requires another dedicated set of NVAs with their own network. UDRs and NSGs help provide layer 3 and layer 4 (of the OSI model) security. NVAs help provide layer 7, application layer, security.","title":"Create User Defined Routes and Network Virtual Appliances"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#explore-hub-and-spoke-topology","text":"This reference architecture shows how to implement a hub-spoke topology in Azure. The hub is a virtual network in Azure that acts as a central point of connectivity to your on-premises network. The spokes are virtual networks that peer with the hub and can be used to isolate workloads. Traffic flows between the on-premises datacenter and the hub through an ExpressRoute or VPN gateway connection. Typical uses for this architecture include: Workloads deployed in different environments, such as development, testing, and production, that require shared services such as DNS, IDS, NTP, or AD DS. Shared services are placed in the hub virtual network, while each environment is deployed to a spoke to maintain isolation. Workloads that do not require connectivity to each other, but require access to shared services. Enterprises that require central control over security aspects, such as a firewall in the hub as a DMZ, and segregated management for the workloads in each spoke. The architecture consists of the following components. On-premises network - A private local-area network running within an organization. VPN device - A device or service that provides external connectivity to the on-premises network. The VPN device may be a hardware device or a software solution such as the Routing and Remote Access Service (RRAS) in Windows Server 2012. For a list of supported VPN appliances and information on configuring selected VPN appliances for connecting to Azure, see About VPN devices for Site-to-Site VPN Gateway connections. VPN virtual network gateway or ExpressRoute gateway - The virtual network gateway enables the virtual network to connect to the VPN device, or ExpressRoute circuit, used for connectivity with your on-premises network. For more information, see Connect an on-premises network to a Microsoft Azure virtual network. Hub virtual network - The virtual network is used as the hub in the hub-spoke topology. The hub is the central point of connectivity to your on-premises network, and a place to host services that can be consumed by the different workloads hosted in the spoke virtual networks. Gateway subnet - The virtual network gateways are held in the same subnet. Spoke virtual networks - One or more virtual networks that are used as spokes in the hub-spoke topology. Spokes can be used to isolate workloads in their own virtual networks, managed separately from other spokes. Each workload might include multiple tiers, with multiple subnets connected through Azure load balancers. For more information about the application infrastructure, see Running Windows VM workloads and Running Linux VM workloads. Virtual network peering - Two virtual networks can be connected using a peering connection. Peering connections are non-transitive, low latency connections between virtual networks. Once peered, the virtual networks exchange traffic by using the Azure backbone, without the need for a router. In hub-spoke network topology, you use virtual network peering to connect the hub to each spoke. You can peer virtual networks in the same region or different regions. For more information, see Requirements and constraints.","title":"Explore hub and spoke topology"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#perform-try-this-exercises","text":"VNet Peering This lab requires two virtual machines. Each virtual machine should be in a different virtual network. For these instructions, we have AZ500vm01, AZ500vm02, AZ500-vnet, AZ500-vnet1, and az500-rg. To save time, you can connect to each virtual machine. Also, it might be helpful to edit the default.htm page on each machine, so the page provides the virtual machine name. For example, This is AZ500vm01. In this demonstration, you will configure and test VNet peering. Review the infrastructure setup In this task, you will review the infrastructure that has been configured for this demonstration. In the Portal, navigate to Virtual Machines. Show there are two virtual machines, AZ500vm01 and AZ500vm02. Select AZ500vm01 and review the IP addresses. Select AZ500vm02 and review the IP addresses. Make a note of the private IP address. Based on the addressing, discuss how each machine is in a different subnet. In the Portal navigate to Virtual networks. Show there are two virtual networks, AZ500-vnet and AZ500-vnet1. Test the virtual machine connections In this task, you will test connecting from AZ500vm01 to AZ500vm02's private IP address. This connection will not work. The virtual machines are in different virtual networks. Use RDP to connect to AZ500vm01. In a browser, view the http://localhost.default.htm This page should display without error. Use RDP to connect to AZ500vm02 In a browser, view the http://localhost.default.htm This page should display without error. The above steps show that IIS is working on the virtual machines. Return to your AZ500vm01 RDP session. We will now try to access AZ500vm02. In a browser, view the http://private_IP_address_of_AZ500vm02/default.htm The page will not display. AZ500vm01 cannot access AZ500vm02 using the private address. Configure VNet peering and test the connections In this task, you will configure VNet peering and test the previous connection. The connection will now work. In the Portal, navigate to the AZ500-vnet virtual network. Under Settings select Peerings. Add a virtual network peering. The page adapts as you make selections. Name of the peering from az500-vnet to remote virtual network: Peering-A-to-B Virtual network: AZ500-vnet1 (az500-rg) Name of the peering from az500-vnet1 to az500-vnet: Peering-B-to-A Discuss the other configuration options. Click OK. Follow the notifications while the virtual network peerings are deployed. Return to your AZ500vm01 RDP session. In the browser, refresh the http://private_IP_address_of_AZ500vm02/default.htm This page should now display. Azure Firewall This task requires a virtual network with two subnets, Subnet1 and Jumpnet. Subnet1 has the 10.0.0.0/24 address range. Jumpnet has the 10.0.1.0/24 address range. Subnet1 includes a Windows virtual machine. Your resource names may be different. Configure the firewall subnet In the Portal, select your virtual network. Under Settings, select Subnets. Click + Subnet to add a new subnet for the firewall. Name: AzureFirewallSubnet Address range: 10.0.2.0/24 There is not need for a NAT Gateway, NSG, Route table, or services. Click Add. Wait for the subnet to deploy. Add and configure the firewall Search for and select Firewalls. Discuss the benefits of a firewall and how it can be used to increase perimeter security. Click + Add. Complete the required configuration information: subscription, resource group, name, and region. Select your Virtual network. Add a new Firewall public IP address. Create the firewall and wait for it to deploy. Navigate to your new firewall. On the Overview blade, locate the Firewall private IP. Copy the address to the clipboard. Create a route table and route that uses the firewall Search for and select Route tables. Add a new route table. Complete the required configuration information: name, subscription, resource group, and location. Disable Virtual network gateway route propagation. Review what this means. Create the route table and wait for it to deploy. Navigate to the new route table. Under Settings, click Routes. Add a new route. This route will ensure traffic goes through the firewall. Discuss the different next hop types. Route name: your choice Address prefix: 0.0.0.0/0/ Next hop type: Virtual appliance Next hope address: Firewall_private_IP_address When finished click Ok and wait for the new route to deploy. Associate the route table with Subnet1 Still in the route table resource, under Settings click Subnets. Associate your virtual network and Subnet1. This will ensure Subnet1 uses the route table. When you are finished click Ok and wait for the association to complete. Test the firewall In the Portal, navigate to a virtual machine in Subnet1. From the Overview blade, ensure the VM is running. Click Connect and RDP into the VM. On the virtual machine, open a browser. Try to access: www.msn.com Notice the error. Action denied. No rule matches. Add a firewall application rule In the Portal, navigate to your firewall. Under Settings select Rules. Select the Application rule selection tab. Click Add application rule collection. Review how application rules work and complete the required information. Name: your choice Priority: 300 Action: Allow Continue completing the rule, under Target FQDNs. This will allow Subnet1 IP address to traverse the firewall. Name: Allow-MSN Source type: IP address Source: 10.0.0.0/24 Protocol:Port: http,https Target FQDNs: www.msn.com Click Add and wait for the firewall to be updated. Test the firewall again In your VM RDP session, refresh the browser page. The MSN.com page should now display. Knowledge check Which of the following features of Azure networking enables the redirect of Internet traffic back to the company's on-premises servers for packet inspection? User Defined Routes ( Ans ) User-defined routes and forced tunneling. Use forced tunneling to redirect internet bound traffic back to the company's on-premises infrastructure. Forced tunneling is commonly used in scenarios where organizations want to implement packet inspection or corporate audits. Forced tunneling in Azure is configured via virtual network user-defined routes (UDR). Cross-premises network connectivity Traffic Manager When configuring Azure Firewall, the organization needs to allow Windows Update network traffic through the firewall. Which of the following rules should be configured? Destination inbound rules NAT rules Application rules( Ans ) Application rules. Application rules define fully qualified domain names (FQDNs) that can be accessed from a subnet. Usage of FQDNs would be appropriate to allow Windows Update network traffic. An organization would like to limit outbound Internet traffic from a subnet, which product should be installed and configured? Azure Web Application Firewall Azure Firewall( Ans ) Azure Firewall. Azure Firewall can limit the outbound IP addresses and ports that can be accessed. Define network rules that assign source address, protocol, destination port, and destination address. Load Balancer An organization has a web application and is concerned about attacks that flood the network layer with a substantial amount of seemingly legitimate traffic, how can this type of attack be blocked? Add a Web Application Firewall Add an Azure Firewall Create a DDoS policy( Ans ) Create a DDoS policy to provide defense against the exhaustion resources. This exhaustion could make an application unavailable to legitimate users for example.","title":"Perform try-this exercises"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#configure-network-security","text":"Introduction Network security could be defined as the process of protecting resources from unauthorized access or attack by applying controls to network traffic. The goal is to ensure that only legitimate traffic is allowed. Azure includes a robust networking infrastructure to support your application and service connectivity requirements. Network connectivity is possible between resources located in Azure, between on-premises and Azure hosted resources, and to and from the internet and Azure. Scenario A security engineer uses network security features to restrict, monitor, and manage network traffic once it reaches your Azure network; you will work on such tasks as: Setup network security groups and application security groups. Deploy Service Endpoints and Private Links. Configure Front Door and ExpressRoute. Skills measured Securing the Azure platform your cloud solutions run on is a part of Exam AZ-500: Microsoft Azure Security Engineer. Implement platform protection (15-20%) Implement advanced network security - Secure the connectivity of virtual networks (VPN authentication, Express Route encryption) - Configure Network Security Groups (NSGs) and Application Security Groups (ASGs) - Configure Azure Front Door service as an Application Gateway - Configure a Web Application Firewall (WAF) on Azure Application Gateway - Implement Service Endpoints Explore Network Security Groups (NSG) Network traffic can be filtered to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol. VMs that you create via the Resource Manager deployment model can have direct connectivity to the internet by using a public IP address that is directly assigned to the VMs. Only the host firewall configured inside the VMs helps protect these VMs from the internet. VMs that you create by using the classic deployment model communicate with internet resources through the cloud service that is assigned the public IP address, which is also known as the VIP. VMs that reside inside the cloud service share that VIP and establish communication with internet resources by using endpoints. If you remove the VM endpoints that map the public port and public IP address of the cloud service to the private port and private IP address of the VM, the VM becomes unreachable from the internet via the public IP address. Network Security Groups (NSGs) help provide advanced security for the VMs you create via either deployment model (Resource Manager or classic). NSGs control inbound and outbound traffic passing through a network adapter (in the Resource Manager deployment model), a VM (in the classic deployment model), or a subnet (in both deployment models). Network Security Group rules NSGs contain rules that specify whether traffic will be approved or denied. Each rule is based on a source IP address, a source port, a destination IP address, and a destination port. Based on whether the traffic matches this combination, the rule either allows or denies the traffic. Each rule consists of the following properties: Name . This is a unique identifier for the rule. Direction . This specifies whether the traffic is inbound or outbound. Priority . If multiple rules match the traffic, rules with a higher priority apply. Access . This specifies whether the traffic is allowed or denied. Source IP address prefix . This prefix identifies where the traffic originated from. It can be based on a single IP address; a range of IP addresses in Classless Interdomain Routing (CIDR) notation; or the asterisk (*), which is a wildcard that matches all possible IP addresses. Source port range . This specifies source ports by using either a single port number from 1 through 65,535; a range of ports (for example, 200\u2013400); or the asterisk (*) to denote all possible ports. Destination IP address prefix . This identifies the traffic destination based on a single IP address, a range of IP addresses in CIDR notation, or the asterisk (*) to match all possible IP addresses. Destination port range . This specifies destination ports by using either a single port number from 1 through 65,535; a range of ports (for example, 200\u2013400); or the asterisk (*) to denote all possible ports. Protocol . This specifies a protocol that matches the rule. It can be UDP, TCP, or the asterisk (*). Custom Network Security Group rules Predefined default rules exist for inbound and outbound traffic. You can\u2019t delete these rules, but you can override them, because they have the lowest priority. The default rules allow all inbound and outbound traffic within a virtual network, allow outbound traffic towards the internet, and allow inbound traffic to an Azure load balancer. A default rule with the lowest priority also exists in both the inbound and outbound sets of rules that denies all network communication. When you create a custom rule, you can use default tags in the source and destination IP address prefixes to specify predefined categories of IP addresses. These default tags are: Internet. This tag represents internet IP addresses. Virtual_network. This tag identifies all IP addresses that the IP range for the virtual network defines. It also includes IP address ranges from on-premises networks when they are defined as local network to virtual network. Azure_loadbalancer. This tag specifies the default Azure load balancer destination. Planning Network Security Groups You can design NSGs to isolate virtual networks in security zones, like the model used by on-premises infrastructure does. You can apply NSGs to subnets, which allows you to create protected screened subnets, or DMZs, that can restrict traffic flow to all the machines residing within that subnet. With the classic deployment model, you can also assign NSGs to individual computers to control traffic that is both destined for and leaving the VM. With the Resource Manager deployment model, you can assign NSGs to a network adapter so that NSG rules control only the traffic that flows through that network adapter. If the VM has multiple network adapters, NSG rules won\u2019t automatically be applied to traffic that is designated for other network adapters. You create NSGs as resources in a resource group, but you can share them with other resource groups in your subscription. Deploy a Network Security Groups implementation When implementing NSGs, these are the limits to keep in mind: By default, you can create 100 NSGs per region per subscription. You can raise this limit to 400 by contacting Azure support. You can apply only one NSG to a VM, subnet, or network adapter. By default, you can have up to 200 rules in a single NSG. You can raise this limit to 500 by contacting Azure support. You can apply an NSG to multiple resources. An individual subnet can have zero, or one, associated NSG. An individual network interface can also have zero, or one, associated NSG. So, you can effectively have dual traffic restriction for a virtual machine by associating an NSG first to a subnet, and then another NSG to the VM's network interface. The application of NSG rules in this case depends on the direction of traffic and priority of applied security rules. Consider a simple example with one virtual machine as follows: The virtual machine is placed inside the Contoso Subnet. Contoso Subnet is associated with Subnet NSG. The VM network interface is additionally associated with VM NSG. In this example, for inbound traffic, the Subnet NSG is evaluated first. Any traffic allowed through Subnet NSG is then evaluated by VM NSG. The reverse is applicable for outbound traffic, with VM NSG being evaluated first. Any traffic allowed through VM NSG is then evaluated by Subnet NSG. This allows for granular security rule application. For example, you might want to allow inbound internet access to a few application VMs (such as frontend VMs) under a subnet but restrict inbound internet access to other VMs (such as database and other backend VMs). In this case you can have a more lenient rule on the Subnet NSG, allowing internet traffic, and restrict access to specific VMs by denying access on VM NSG. The same can be applied for outbound traffic. To help secure and protect your Azure resources, make sure NSG planning is standard operating procedure (SOP) for your deployments. How traffic is evaluated Several resources from Azure services can be deployed into an Azure virtual network. You can associate zero, or one, network security group to each virtual network subnet and network interface in a virtual machine. The same network security group can be associated to as many subnets and network interfaces as you choose. The following picture illustrates different scenarios for how network security groups might be deployed to allow network traffic to and from the internet over TCP port 80: Reference the above diagram, along with the following text, to understand how Azure processes inbound and outbound rules for network security groups: Inbound traffic For inbound traffic, Azure processes the rules in a network security group associated to a subnet first, if there is one, and then the rules in a network security group associated to the network interface, if there is one. VM1: The security rules in NSG1 are processed, since it is associated to Subnet1 and VM1 is in Subnet1. Unless you've created a rule that allows port 80 inbound, the traffic is denied by the DenyAllInbound default security rule, and never evaluated by NSG2, since NSG2 is associated to the network interface. If NSG1 has a security rule that allows port 80, the traffic is then processed by NSG2. To allow port 80 to the virtual machine, both NSG1 and NSG2 must have a rule that allows port 80 from the internet. VM2: The rules in NSG1 are processed because VM2 is also in Subnet1. Since VM2 does not have a network security group associated to its network interface, it receives all traffic allowed through NSG1 or is denied all traffic denied by NSG1. Traffic is either allowed or denied to all resources in the same subnet when a network security group is associated to a subnet. VM3: Since there is no network security group associated to Subnet2, traffic is allowed into the subnet and processed by NSG2, because NSG2 is associated to the network interface attached to VM3. VM4: Traffic is allowed to VM4, because a network security group isn't associated to Subnet3, or the network interface in the virtual machine. All network traffic is allowed through a subnet and network interface if they don't have a network security group associated to them. Outbound traffic For outbound traffic, Azure processes the rules in a network security group associated to a network interface first, if there is one, and then the rules in a network security group associated to the subnet, if there is one. VM1: The security rules in NSG2 are processed. Unless you create a security rule that denies port 80 outbound to the internet, the traffic is allowed by the AllowInternetOutbound default security rule in both NSG1 and NSG2. If NSG2 has a security rule that denies port 80, the traffic is denied, and never evaluated by NSG1. To deny port 80 from the virtual machine, either, or both of the network security groups must have a rule that denies port 80 to the internet. VM2: All traffic is sent through the network interface to the subnet, since the network interface attached to VM2 does not have a network security group associated to it. The rules in NSG1 are processed. VM3: If NSG2 has a security rule that denies port 80, the traffic is denied. If NSG2 has a security rule that allows port 80, then port 80 is allowed outbound to the internet, since a network security group is not associated to Subnet2. VM4: All network traffic is allowed from VM4, because a network security group isn't associated to the network interface attached to the virtual machine, or to Subnet3. Intra-subnet traffic It's important to note that security rules in an NSG associated to a subnet can affect connectivity between VM's within it. For example, if a rule is added to NSG1 which denies all inbound and outbound traffic, VM1 and VM2 will no longer be able to communicate with each other. Another rule would have to be added specifically to allow this. General guidelines Unless you have a specific reason to, we recommended that you associate a network security group to a subnet, or a network interface, but not both. Since rules in a network security group associated to a subnet can conflict with rules in a network security group associated to a network interface, you can have unexpected communication problems that require troubleshooting. Create Application Security Groups In this topic we look at Application Security Groups (ASGs), which are built on network security groups. A quick review of Security groups reminds us that you can filter network traffic to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol. You can enable network security group flow logs to analyze network traffic to and from resources that have an associated network security group. Application security groups ASGs enable you to configure network security as a natural extension of an application's structure. You then can group VMs and define network security policies based on those groups. You also can reuse your security policy at scale without manual maintenance of explicit IP addresses. The platform manages the complexity of explicit IP addresses and multiple rule sets, allowing you to focus on your business logic. Consider the following illustration. In the illustration, NIC1 and NIC2 are members of the AsgWeb ASG. NIC3 is a member of the AsgLogic ASG. NIC4 is a member of the AsgDb ASG. Though each network interface in this example is a member of only one ASG, a network interface can be a member of multiple ASGs, up to the Azure limits. None of the network interfaces have an associated network security group. NSG1 is associated to both subnets and contains the following rules: Allow-HTTP-Inbound-Internet Deny-Database-All Allow-Database-BusinessLogic The rules that specify an ASG as the source or destination are only applied to the network interfaces that are members of the ASG. If the network interface is not a member of an ASG, the rule is not applied to the network interface even though the network security group is associated to the subnet. Application security groups have the following constraints There are limits to the number of ASGs you can have in a subscription, in addition to other limits related to ASGs. You can specify one ASG as the source and destination in a security rule. You cannot specify multiple ASGs in the source or destination. All network interfaces assigned to an ASG must exist in the same virtual network that the first network interface assigned to the ASG is in. For example, if the first network interface assigned to an ASG named AsgWeb is in the virtual network named VNet1, then all subsequent network interfaces assigned to ASGWeb must exist in VNet1. You cannot add network interfaces from different virtual networks to the same ASG. If you specify an ASG as the source and destination in a security rule, the network interfaces in both ASGs must exist in the same virtual network. For example, if AsgLogic contained network interfaces from VNet1, and AsgDb contained network interfaces from VNet2, you could not assign AsgLogic as the source and AsgDb as the destination in a rule. All network interfaces for both the source and destination ASGs need to exist in the same virtual network. Summary Application Security Groups along with NSGs, have brought multiple benefits on the network security area: A single management experience Increased limits on multiple dimensions A great level of simplification A seamless integration with your architecture Enable service endpoints A virtual network service endpoint provides the identity of your virtual network to the Azure service. Once service endpoints are enabled in your virtual network, you can secure Azure service resources to your virtual network by adding a virtual network rule to the resources. Today, Azure service traffic from a virtual network uses public IP addresses as source IP addresses. With service endpoints, service traffic switches to use virtual network private addresses as the source IP addresses when accessing the Azure service from a virtual network. This switch allows you to access the services without the need for reserved, public IP addresses used in IP firewalls. A common usage case for service endpoints is a virtual machine accessing storage. The storage account restricts access to the virtual machines private IP address. Why use a service endpoint? Improved security for your Azure service resources. VNet private address space can be overlapping and so, cannot be used to uniquely identify traffic originating from your VNet. Service endpoints provide the ability to secure Azure service resources to your virtual network, by extending VNet identity to the service. Once service endpoints are enabled in your virtual network, you can secure Azure service resources to your virtual network by adding a virtual network rule to the resources. This provides improved security by fully removing public Internet access to resources, and allowing traffic only from your virtual network. Optimal routing for Azure service traffic from your virtual network. Today, any routes in your virtual network that force Internet traffic to your premises and/or virtual appliances, known as forced-tunneling, also force Azure service traffic to take the same route as the Internet traffic. Service endpoints provide optimal routing for Azure traffic. Endpoints always take service traffic directly from your virtual network to the service on the Microsoft Azure backbone network. Keeping traffic on the Azure backbone network allows you to continue auditing and monitoring outbound Internet traffic from your virtual networks, through forced-tunneling, without impacting service traffic. Simple to set up with less management overhead. You no longer need reserved, public IP addresses in your virtual networks to secure Azure resources through IP firewall. There are no NAT or gateway devices required to set up the service endpoints. Service endpoints are configured through a simple click on a subnet. There is no additional overhead to maintaining the endpoints. Important: With service endpoints, the source IP addresses of the virtual machines in the subnet for service traffic switches from using public IPv4 addresses to using private IPv4 addresses. Existing Azure service firewall rules using Azure public IP addresses will stop working with this switch. Please ensure Azure service firewall rules allow for this switch before setting up service endpoints. You may also experience temporary interruption to service traffic from this subnet while configuring service endpoints. Configure service endpoint services Service endpoints would provide benefits in the following Scenarios. Scenarios Peered, connected, or multiple virtual networks: To secure Azure services to multiple subnets within a virtual network or across multiple virtual networks, you can enable service endpoints on each of the subnets independently, and secure Azure service resources to all of the subnets. Filtering outbound traffic from a virtual network to Azure services: If you want to inspect or filter the traffic sent to an Azure service from a virtual network, you can deploy a network virtual appliance within the virtual network. You can then apply service endpoints to the subnet where the network virtual appliance is deployed, and secure Azure service resources only to this subnet. This scenario might be helpful if you want use network virtual appliance filtering to restrict Azure service access from your virtual network only to specific Azure resources. Securing Azure resources to services deployed directly into virtual networks: You can directly deploy various Azure services into specific subnets in a virtual network. You can secure Azure service resources to managed service subnets by setting up a service endpoint on the managed service subnet. Disk traffic from an Azure virtual machine: Virtual Machine Disk traffic for managed and unmanaged disks isn't affected by service endpoints routing changes for Azure Storage. This traffic includes diskIO as well as mount and unmount. You can limit REST access to page blobs to select networks through service endpoints and Azure Storage network rules. Deploy private links Azure Private Link works on an approval call flow model wherein the Private Link service consumer can request a connection to the service provider for consuming the service. The service provider can then decide whether to allow the consumer to connect or not. Azure Private Link enables the service providers to manage the private endpoint connection on their resources There are two connection approval methods that a Private Link service consumer can choose from: Automatic: If the service consumer has RBAC permissions on the service provider resource, the consumer can choose the automatic approval method. In this case, when the request reaches the service provider resource, no action is required from the service provider and the connection is automatically approved. Manual: On the contrary, if the service consumer doesn\u2019t have RBAC permissions on the service provider resource, the consumer can choose the manual approval method. In this case, the connection request appears on the service resources as Pending. The service provider has to manually approve the request before connections can be established. In manual cases, service consumer can also specify a message with the request to provide more context to the service provider. The service provider has following options to choose from for all Private Endpoint connections: Approved Reject Remove Manage private endpoint connections on Azure PaaS resources Portal is the preferred method for managing private endpoint connections on Azure PaaS resources. Implement an Azure application gateway Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Traditional load balancers operate at the transport layer (OSI layer 4 - TCP and UDP) and route traffic based on the source IP address and port to a destination IP address and port. Application Gateway can make routing decisions based on additional attributes of an HTTP request, for example, URI path or host headers. For example, you can route traffic based on the incoming URL. So if /images are in the incoming URL, you can route traffic to a specific set of servers (known as a pool) configured for images. If /video is in the URL, that traffic is routed to another pool that's optimized for videos. This type of routing is known as application layer (OSI layer 7) load balancing. Application Gateway includes the following features: Secure Sockets Layer (SSL/TLS) termination - Application gateway supports SSL/TLS termination at the gateway, after which traffic typically flows unencrypted to the backend servers. This feature allows web servers to be unburdened from costly encryption and decryption overhead. Autoscaling - Application Gateway Standard_v2 supports autoscaling and can scale up or down based on changing traffic load patterns. Autoscaling also removes the requirement to choose a deployment size or instance count during provisioning. Zone redundancy - A Standard_v2 Application Gateway can span multiple Availability Zones, offering better fault resiliency and removing the need to provision separate Application Gateways in each zone. Static VIP - The application gateway Standard_v2 SKU supports static VIP type exclusively. This ensures that the VIP associated with application gateway doesn't change even over the lifetime of the Application Gateway. Web Application Firewall - Web Application Firewall (WAF) is a service that provides centralized protection of your web applications from common exploits and vulnerabilities. WAF is based on rules from the OWASP (Open Web Application Security Project) core rule sets 3.1 (WAF_v2 only), 3.0, and 2.2.9. Ingress Controller for AKS - Application Gateway Ingress Controller (AGIC) allows you to use Application Gateway as the ingress for an Azure Kubernetes Service (AKS) cluster. URL-based routing - URL Path Based Routing allows you to route traffic to back-end server pools based on URL Paths of the request. One of the scenarios is to route requests for different content types to different pool. Multiple-site hosting - Multiple-site hosting enables you to configure more than one web site on the same application gateway instance. This feature allows you to configure a more efficient topology for your deployments by adding up to 100 web sites to one Application Gateway (for optimal performance). Redirection - A common scenario for many web applications is to support automatic HTTP to HTTPS redirection to ensure all communication between an application and its users occurs over an encrypted path. Session affinity - The cookie-based session affinity feature is useful when you want to keep a user session on the same server. Websocket and HTTP/2 traffic - Application Gateway provides native support for the WebSocket and HTTP/2 protocols. There's no user-configurable setting to selectively enable or disable WebSocket support. Connection draining - Connection draining helps you achieve graceful removal of backend pool members during planned service updates. Custom error pages - Application Gateway allows you to create custom error pages instead of displaying default error pages. You can use your own branding and layout using a custom error page. Rewrite HTTP headers - HTTP headers allow the client and server to pass additional information with the request or the response. Sizing - Application Gateway Standard_v2 can be configured for autoscaling or fixed size deployments. This SKU doesn't offer different instance sizes. New Application Gateway v1 SKU deployments can take up to 20 minutes to provision. Changes to instance size or count aren't disruptive, and the gateway remains active during this time. Most deployments that use the v2 SKU take around 6 minutes to provision. However it can take longer depending on the type of deployment. For example, deployments across multiple Availability Zones with many instances can take more than 6 minutes. Deploy a web application firewall Web Application Firewall (WAF) provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks. Preventing such attacks in application code is challenging. It can require rigorous maintenance, patching, and monitoring at multiple layers of the application topology. A centralized web application firewall helps make security management much simpler. A WAF also gives application administrators better assurance of protection against threats and intrusions. A WAF solution can react to a security threat faster by centrally patching a known vulnerability, instead of securing each individual web application. Supported service WAF can be deployed with Azure Application Gateway, Azure Front Door, and Azure Content Delivery Network (CDN) service from Microsoft. WAF on Azure CDN is currently under public preview. WAF has features that are customized for each specific service. Configure and manage Azure front door Azure Front Door enables you to define, manage, and monitor the global routing for your web traffic by optimizing for best performance and instant global failover for high availability. With Front Door, you can transform your global (multi-region) consumer and enterprise applications into robust, high-performance personalized modern applications, APIs, and content that reaches a global audience with Azure. Front Door works at Layer 7 or HTTP/HTTPS layer and uses split TCP-based anycast protocol. Front Door ensures that your end users promptly connect to the nearest Front Door POP (Point of Presence). So, per your routing method selection in the configuration, you can ensure that Front Door is routing your client requests to the fastest and most available application backend. An application backend is any Internet-facing service hosted inside or outside of Azure. Front Door provides a range of traffic-routing methods and backend health monitoring options to suit different application needs and automatic failover models. Like Traffic Manager, Front Door is resilient to failures, including the failure of an entire Azure region. The following features are included with Front Door: Accelerate application performance - Using split TCP-based anycast protocol, Front Door ensures that your end users promptly connect to the nearest Front Door POP (Point of Presence). Increase application availability with smart health probes - Front Door delivers high availability for your critical applications using its smart health probes, monitoring your backends for both latency and availability and providing instant automatic failover when a backend goes down. URL-based routing - URL Path Based Routing allows you to route traffic to backend pools based on URL paths of the request. One of the scenarios is to route requests for different content types to different backend pools. Multiple-site hosting - Multiple-site hosting enables you to configure more than one web site on the same Front Door configuration. Session affinity - The cookie-based session affinity feature is useful when you want to keep a user session on the same application backend. TLS termination - Front Door supports TLS termination at the edge that is, individual users can set up a TLS connection with Front Door environments instead of establishing it over long haul connections with the application backend. Custom domains and certificate management - When you use Front Door to deliver content, a custom domain is necessary if you would like your own domain name to be visible in your Front Door URL. Application layer security - Azure Front Door allows you to author custom Web Application Firewall (WAF) rules for access control to protect your HTTP/HTTPS workload from exploitation based on client IP addresses, country code, and http parameters. URL redirection - With the strong industry push on supporting only secure communication, web applications are expected to automatically redirect any HTTP traffic to HTTPS. URL rewrite - Front Door supports URL rewrite by allowing you to configure an optional Custom Forwarding Path to use when constructing the request to forward to the backend. Protocol support - IPv6 and HTTP/2 traffic - Azure Front Door natively supports end-to-end IPv6 connectivity and HTTP/2 protocol. As mentioned above, routing to the Azure Front Door environments leverages Anycast for both DNS (Domain Name System) and HTTP (Hypertext Transfer Protocol) traffic, so user traffic will go to the closest environment in terms of network topology (fewest hops). This architecture typically offers better round-trip times for end users (maximizing the benefits of Split TCP). Front Door organizes its environments into primary and fallback \"rings\". The outer ring has environments that are closer to users, offering lower latencies. The inner ring has environments that can handle the failover for the outer ring environment in case an issue happens. The outer ring is the preferred target for all traffic, but the inner ring is necessary to handle traffic overflow from the outer ring. In terms of VIPs (Virtual Internet Protocol addresses), each frontend host, or domain served by Front Door is assigned a primary VIP, which is announced by environments in both the inner and outer ring, as well as a fallback VIP, which is only announced by environments in the inner ring. This overall strategy ensures that requests from your end users always reach the closest Front Door environment and that even if the preferred Front Door environment is unhealthy then traffic automatically moves to the next closest environment. Review ExpressRoute ExpressRoute is a direct, private connection from your WAN (not over the public Internet) to Microsoft Services, including Azure. Site-to-Site VPN traffic travels encrypted over the public Internet. Being able to configure Site-to-Site VPN and ExpressRoute connections for the same virtual network has several advantages. You can configure a Site-to-Site VPN as a secure failover path for ExpressRoute, or use Site-to-Site VPNs to connect to sites that are not part of your network, but that are connected through ExpressRoute. Notice that this configuration requires two virtual network gateways for the same virtual network, one using the gateway type 'Vpn', and the other using the gateway type 'ExpressRoute'. ExpressRoute encryption IPsec over ExpressRoute for Virtual WAN Azure Virtual WAN uses an Internet Protocol Security (IPsec) Internet Key Exchange (IKE) VPN connection from your on-premises network to Azure over the private peering of an Azure ExpressRoute circuit. This technique can provide an encrypted transit between the on-premises networks and Azure virtual networks over ExpressRoute, without going over the public internet or using public IP addresses. The following diagram shows an example of VPN connectivity over ExpressRoute private peering. The diagram shows a network within the on-premises network connected to the Azure hub VPN gateway over ExpressRoute private peering. The connectivity establishment is straightforward: Establish ExpressRoute connectivity with an ExpressRoute circuit and private peering. Establish the VPN connectivity. An important aspect of this configuration is routing between the on-premises networks and Azure over both the ExpressRoute and VPN paths. ExpressRoute supports a couple of encryption technologies to ensure confidentiality and integrity of the data traversing between your network and Microsoft's network. Point-to-point encryption by MACsec MACsec is an IEEE standard. It encrypts data at the Media Access control (MAC) level or Network Layer 2. You can use MACsec to encrypt the physical links between your network devices and Microsoft's network devices when you connect to Microsoft via ExpressRoute Direct. MACsec is disabled on ExpressRoute Direct ports by default. You bring your own MACsec key for encryption and store it in Azure Key Vault. You decide when to rotate the key. End-to-end encryption by IPsec and MACsec IPsec is an IETF standard. It encrypts data at the Internet Protocol (IP) level or Network Layer 3. You can use IPsec to encrypt an end-to-end connection between your on-premises network and your virtual network (VNET) on Azure. MACsec secures the physical connections between you and Microsoft. IPsec secures the end-to-end connection between you and your virtual networks on Azure. You can enable them independently. ExpressRoute Direct ExpressRoute Direct gives you the ability to connect directly into Microsoft\u2019s global network at peering locations strategically distributed across the world. ExpressRoute Direct provides dual 100 Gbps or 10 Gbps connectivity, which supports Active/Active connectivity at scale Key features that ExpressRoute Direct provides include, but aren't limited to: Massive Data Ingestion into services like Storage and Cosmos DB Physical isolation for industries that are regulated and require dedicated and isolated connectivity like: Banking, Government, and Retail Granular control of circuit distribution based on business unit ExpressRoute Direct supports massive data ingestion scenarios into Azure storage and other big data services. ExpressRoute circuits on 100 Gbps ExpressRoute Direct now also support 40 Gbps and 100 Gbps circuit SKUs. The physical port pairs are 100 or 10 Gbps only and can have multiple virtual circuits. ExpressRoute Direct supports both QinQ and Dot1Q VLAN tagging. QinQ VLAN Tagging allows for isolated routing domains on a per ExpressRoute circuit basis. Azure dynamically allocates an S-Tag at circuit creation and cannot be changed. Each peering on the circuit (Private and Microsoft) will utilize a unique C-Tag as the VLAN. The C-Tag is not required to be unique across circuits on the ExpressRoute Direct ports. Dot1Q VLAN Tagging allows for a single tagged VLAN on a per ExpressRoute Direct port pair basis. A C-Tag used on a peering must be unique across all circuits and peerings on the ExpressRoute Direct port pair. ExpressRoute Direct provides the same enterprise-grade SLA with Active/Active redundant connections into the Microsoft Global Network. ExpressRoute infrastructure is redundant and connectivity into the Microsoft Global Network is redundant and diverse and scales accordingly with customer requirements. Perform try-this exercises Use this short Try-This exercises to get some hands-on experience with using Azure. An individual Azure subscription is required to perform the exercise tasks. To subscribe, browse to https://www.azure.microsoft.com/free. Task 1 - Network security groups This task requires a Windows virtual machine associated with a network security group. The NSG should have an inbound security rule that allows RDP. The virtual machine should be in a running state and have a public IP address. In this task, we will review networking rules, confirm the public IP page does not display, configure an inbound NSG rule, and confirm the public IP page now displays. Review networking rules In the Portal, navigate to your virtual machine. Under Settings, click Networking. Discuss the default inbound and outbound rules. Review the inbound rules and ensure RDP is allowed. Make a note of the public IP address. Connect to the virtual machine and test the public IP address From the Overview blade, click Connect and RDP into the virtual machine. On the virtual machine, open a browser. Test the default localhost IIS HTML page: http://localhost/default.htm. This page should appear. Test the default public IP IIS HTML page: http://public_IP_address/default.htm. This page should not display. Configure an inbound rule to allow public access on port 80 Return to the Portal and the Networking blade. Make a note of the virtual machine's private IP address. On the Inbound port rules tab, click Add inbound port rule. This rule will only allow certain IP addresses on port 80. As you go through the configuration settings, be sure to discuss each one. Source: Service Tag Source service tag: Internet Destination: IP addresses Destination IP addresses/CIDR range: private_IP_address/32 Destination port range: 80 Protocol: TCP Action: Allow Name: Allow_Port_80 Click Add Wait for your new inbound rule to be added. Retest the public IP address On the virtual machine, return to the browser. Refresh the default public IP IIS HTML page: http://public_IP_address/default.htm. This page should now display. Task 2 - Application service groups This task requires a Windows virtual machine with IIS installed. These steps use VM1. Your machine name may be different. In this task, we will connect to a virtual machine, create an inbound deny rule, configure an application security group, and test connectivity. Connect to the virtual machine In the Portal, navigate to VM1. On the Networking blade, make a note of the private IP address. Ensure there is an Inbound port rule that allows RDP. From the Overview blade, ensure VM1 is running. Click Connect and RDP into the VM1. On VM1, open a browser. Ensure the default IIS page display for the private IP address: http://private_IP_address/default.htm. Add an inbound deny rule and test the rule Continue in the Portal from the Networking blade. On the Inbound port rules tab, click Add inbound port rule. Add a rule that denies all inbound traffic. Destination port ranges: * Action: Deny Name: Deny_All Click Add Wait for your new inbound rule to be added. On VM1, refresh the browser page: http://private_IP_address/default.htm. Verify that the page does not display. Configure an application security group In the Portal, search for and select Application security groups. Create a new Application security group. Provide the required information: subscription, resource group, name, and region. Wait for the ASG to deploy. In the Portal, return to VM1. On the Networking blade, select the Application security groups tab. Click Configure the application security groups. Select your new application security group, and Save your changes. From the Inbound port rules tab, click Add inbound rule. This will allow the ASG. Source: Application security group Source application security group: your_ASG Destination: IP addresses Destination IP addresses: private_IP_address/32 Destination port range: 80 Priority: 250 Name: Allow_ASG Click Add Wait for your new inbound rule to be added. Test the application security group On VM1, refresh the browser page: http://private_IP_address/default.htm. Verify that the page now displays. Task 3 - Storage endpoints (you could do this in the storage lesson) This task requires a storage account and virtual network with a subnet. Storage Explorer is also required. In this task, we will secure a storage endpoint. In the Portal. Locate your storage account. Create a file share, and upload a file. Use the Shared Access Signature blade to Generate SAS and connection string. Use Storage Explorer and the connection string to access the file share. Ensure you can view your uploaded file. Locate your virtual network, and then select a subnet in the virtual network. Under Service Endpoints, view the Services drop-down and the different services that can be secured with an endpoint. Check the Microsoft.Storage option. Save your changes. Return to your storage account. Select Firewalls and virtual networks. Change to Selected networks. Add your virtual network and verify your subnet with the new service endpoint is listed. Save your changes. Return to the Storage Explorer. Refresh the storage account. Verify you can no longer access the file share. Knowledge check When deploying the Azure Application Gateway and there's a need to ensure incoming requests are checked for common security threats like cross-site scripting and crawlers; how can this concern be addressed? Install a load balancer Install Azure Firewall Install the Web Application Firewall( Ans ) Install the Web Application Firewall. The web application firewall (WAF) is an optional component that handles incoming requests before they reach a listener. The web application firewall checks each request for many common threats, based on the Open Web Application Security Project (OWASP). Which services below are features of Azure Application Gateway? Authentication Layer 7 load balancing( Ans ) Layer 7 load balancing, Offloading of CPUT intensive SSL termination, Round robin distribution of incoming traffic. Azure Application Gateway is a dedicated virtual offering various layer 7 load balancing capabilities for your application. It lets customers optimize web farm productivity by offloading CPU intensive SSL termination to the application gateway, round robin distribution of incoming traffic, cookie-based session affinity, URL path-based routing, and the ability to host multiple websites behind a single Application Gateway. Vulnerability assessments A company is configuring a Network Security Group. To configure the group to allow traffic from public sources, what rule needs to be added to the default rules? Allow all virtual networks inbound and outbound Allow Azure load balancer inbound Allow Internet inbound( Ans ) Allow Internet inbound. NSGs have default inbound and outbound rules. There is a default allow Internet outbound rule, but not an allow Internet inbound rule. An organization has web servers in different regions and this organization wants to optimize the availability of the servers. Which of the network security is best suited for this purpose? Azure Application Gateway Azure Front Door( Ans ) Azure Front Door. Azure Front Door grants the ability to define, manage, and monitor the global routing for web traffic by optimizing for best performance and instant global failover for high availability. Custom routing","title":"Configure network security"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#configure-and-manage-host-security","text":"Introduction Once you have your network and perimeter secure, you need to lock down the host machines where your applications run. Installing updates, using jump boxes to only access servers, and following Microsoft Defender for Cloud recommendations are great tools to keep your hosts secure. Scenario A security engineer uses host security tools like privilege access workstation and virtual machine templates to keep application host machines secure; you will work on such tasks as: Deploy endpoint protection. Create and use a privileged access workstation. Set up protection tools like disk encryption and Windows Defender. Enable endpoint protection Microsoft Defender for Endpoint is an enterprise endpoint security platform designed to help enterprise networks prevent, detect, investigate, and respond to advanced threats. Defender for Endpoint uses the following combination of technology built into Windows 10 and Microsoft's robust cloud service: Endpoint behavioral sensors: Embedded in Windows 10, these sensors collect and process behavioral signals from the operating system and send this sensor data to your private, isolated cloud instance of Microsoft Defender for Endpoint. Cloud security analytics: Leveraging big data, device learning, and unique Microsoft optics across the Windows ecosystem, enterprise cloud products (such as Office 365), and online assets, behavioral signals are translated into insights, detections, and recommended responses to advanced threats. Threat intelligence: Generated by Microsoft hunters, security teams, and augmented by threat intelligence provided by partners, threat intelligence enables Defender for Endpoint to identify attacker tools, techniques, and procedures and generate alerts when they are observed in collected sensor data. Important The capabilities on non-Windows platforms may be different from the ones for Windows. Core Defender Vulnerability Management Built-in core vulnerability management capabilities use a modern risk-based approach to the discovery, assessment, prioritization, and remediation of endpoint vulnerabilities and misconfigurations. Attack surface reduction The attack surface reduction set of capabilities provides the first line of defense in the stack. By ensuring configuration settings are properly set and exploit mitigation techniques are applied, the capabilities resist attacks and exploitation. This set of capabilities also includes network protection and web protection, which regulate access to malicious IP addresses, domains, and URLs. Next-generation protection To further reinforce the security perimeter of your network, Microsoft Defender for Endpoint uses next-generation protection designed to catch all types of emerging threats. Endpoint detection and response Endpoint detection and response capabilities are put in place to detect, investigate, and respond to advanced threats that may have made it past the first two security pillars. Advanced hunting provides a query-based threat-hunting tool that lets you proactively find breaches and create custom detections. Automated investigation and remediation In conjunction with being able to quickly respond to advanced attacks, Microsoft Defender for Endpoint offers automatic investigation and remediation capabilities that help reduce the volume of alerts in minutes at scale. Microsoft Secure Score for Devices Defender for Endpoint includes Microsoft Secure Score for Devices to help you dynamically assess the security state of your enterprise network, identify unprotected systems, and take recommended actions to improve the overall security of your organization. Microsoft Threat Experts Microsoft Defender for Endpoint's new managed threat hunting service provides proactive hunting, prioritization, and additional context and insights that further empower Security operation centers (SOCs) to identify and respond to threats quickly and accurately. Important Defender for Endpoint customers need to apply for the Microsoft Threat Experts managed threat hunting service to get proactive Targeted Attack Notifications and to collaborate with experts on demand. Experts on Demand is an add-on service. Targeted Attack Notifications are always included after you have been accepted into Microsoft Threat Experts managed threat hunting service. Centralized configuration and administration, APIs Integrate Microsoft Defender for Endpoint into your existing workflows. Integration with Microsoft solutions Defender for Endpoint directly integrates with various Microsoft solutions, including: Microsoft Defender for Cloud Microsoft Sentinel Intune Microsoft Defender for Cloud Apps Microsoft Defender for Identity Microsoft Defender for Office Skype for Business Microsoft 365 Defender With Microsoft 365 Defender, Defender for Endpoint, and various Microsoft security solutions, form a unified pre- and post-breach enterprise defense suite that natively integrates across endpoint, identity, email, and applications to detect, prevent, investigate, and automatically respond to sophisticated attacks. Define a privileged access device strategy To ensure the most secure conditions for your company to you need to ensure security from the time of purchase of a new device, to its first usage, and beyond. Zero Trust, means that you don't purchase from generic retailers but only supply hardware from an authorized OEM that support Autopilot. For this solution, root of trust will be deployed using Windows Autopilot technology with hardware that meets the modern technical requirements. To secure a workstation, Autopilot lets you leverage Microsoft OEM-optimized Windows 10 devices. These devices come in a known good state from the manufacturer. Instead of reimaging a potentially insecure device, Autopilot can transform a Windows 10 device into a \u201cbusiness-ready\u201d state. It applies settings and policies, installs apps, and even changes the edition of Windows 10. Hardware root-of-trust To have a secured workstation you need to make sure the following security technologies are included on the device: Trusted Platform Module (TPM) 2.0 BitLocker Drive Encryption UEFI Secure Boot Drivers and Firmware Distributed through Windows Update Virtualization and HVCI Enabled Drivers and Apps HVCI-Ready Windows Hello DMA I/O Protection System Guard Modern Standby Levels of device security Device Type Common usage scenario Permitted activities Security guidance Enterprise Device Home users, small business users, general purpose developers, and enterprise Run any application, browse any website Anti-malware and virus protection and policy based security posture for the enterprise. Specialized Device Specialized or secure enterprise users Run approved applications, but cannot install apps. Email and web browsing allowed. No admin controls No self administration of device, no application installation, policy based security, and endpoint management Privileged Device Extremely sensitive roles IT Operations No local admins, no productivity tools, locked down browsing. PAW device This chart shows the level of device security controls based on how the device will be used. Device security controls A secure workstation requires it be part of an end-to-end approach including device security, account security, and security policies applied to the device at all times. Here are some common security measures you should consider implementing based on the users' needs. Using a device with security measures directly aligned to the security needs of it users is the more secure solution. Security Control Enterprise Device Specialized Device Privileged Device Microsoft Endpoint Manager (MEM) managed Yes Yes Yes Deny BYOD Device enrollment No Yes Yes MEM security baseline applied Yes Yes Yes Microsoft Defender for Endpoint Yes Yes Yes Join personal device via Autopilot Yes Yes No URLs restricted to approved list Allow Most Allow Most Deny Default Removal of admin rights Yes Yes Application execution control (AppLocker) Audit -> Enforced Yes Applications installed only by MEM Yes Yes Deploy privileged access workstations Privileged Access Workstations (PAWs) provide a dedicated system for sensitive tasks that is protected from Internet attacks and threat vectors. Separating these sensitive tasks and accounts from the daily use workstations and devices provides very strong protection from phishing attacks, application and OS vulnerabilities, various impersonation attacks, and credential theft attacks such as keystroke logging, Pass-the-Hash, and Pass-The-Ticket. PAW workstations PAW is a hardened and locked down workstation designed to provide high security assurances for sensitive accounts and tasks. PAWs are recommended for administration of identity systems, cloud services, and private cloud fabric as well as sensitive business functions. In order to provide the greatest security, PAWs should always run the most up-to-date and secure operating system available: Microsoft strongly recommends Windows 10 Enterprise, which includes several additional security features not available in other editions (in particular, Credential Guard and Device Guard). The PAW security controls are focused on mitigating high impact and high probability risks of compromise. These include mitigating attacks on the environment and risks that can decrease the effectiveness of PAW controls over time: Internet attacks - Isolating the PAW from the open internet is a key element to ensuring the PAW is not compromised. Usability risk - If a PAW is too difficult to use for daily tasks, administrators will be motivated to create workarounds to make their jobs easier. Environment risks - Minimizing the use of management tools and accounts that have access to the PAWs to secure and monitor these specialized workstations. Supply chain tampering - Taking a few key actions can mitigate critical attack vectors that are readily available to attackers. This includes validating the integrity of all installation media (Clean Source Principle) and using a trusted and reputable supplier for hardware and software. Physical attacks - Because PAWs can be physically mobile and used outside of physically secure facilities, they must be protected against attacks that leverage unauthorized physical access to the computer. Architecture overview The diagram below depicts a separate \"channel\" for administration (a highly sensitive task) that is created by maintaining separate dedicated administrative accounts and workstations. This architectural approach builds on the protections found in the Windows 10 Credential Guard and Device Guard features and goes beyond those protections for sensitive accounts and tasks. This methodology is appropriate for accounts with access to high value assets: Administrative Privileges - PAWs provide increased security for high impact IT administrative roles and tasks. This architecture can be applied to administration of many types of systems including Active Directory Domains and Forests, Microsoft Azure Active Directory tenants, Microsoft 365 tenants, Process Control Networks (PCN), Supervisory Control and Data Acquisition (SCADA) systems, Automated Teller Machines (ATMs), and Point of Sale (PoS) devices. High Sensitivity Information workers - The approach used in a PAW can also provide protection for highly sensitive information worker tasks and personnel such as those involving pre-announcement Merger and Acquisition activity, pre-release financial reports, organizational social media presence, executive communications, unpatented trade secrets, sensitive research, or other proprietary or sensitive data. This guidance does not discuss the configuration of these information worker scenarios in depth or include this scenario in the technical instructions. Securing privileged access is a critical first step to establishing security assurances for business assets in a modern organization. The security of most or all business assets in an IT organization depends on the integrity of the privileged accounts used to administer, manage, and develop. Jump Box Administrative \"Jump Box\" architectures set up a small number administrative console servers and restrict personnel to using them for administrative tasks. This is typically based on remote desktop services, a 3rd-party presentation virtualization solution, or a Virtual Desktop Infrastructure (VDI) technology. This approach is frequently proposed to mitigate risk to administration and does provide some security assurances, but the jump box approach by itself is vulnerable to certain attacks because it violates the clean source principle. The clean source principle requires all security dependencies to be as trustworthy as the object being secured. This figure depicts a simple control relationship. Any subject in control of an object is a security dependency of that object. If an adversary can control a security dependency of a target object (subject), they can control that object. The administrative session on the jump server relies on the integrity of the local computer accessing it. If this computer is a user workstation subject to phishing attacks and other internet-based attack vectors, then the administrative session is also subject to those risks. While some advanced security controls like multifactor authentication can increase the difficulty of an attacker taking over this administrative session from the user workstation, no security feature can fully protect against technical attacks when an attacker has administrative access of the source computer (e.g. injecting illicit commands into a legitimate session, hijacking legitimate processes, and so on.) The default configuration in this PAW guidance installs administrative tools on the PAW, but a jump server architecture can also be added if required. This above figure shows how reversing the control relationship and accessing user apps from an admin workstation gives the attacker no path to the targeted object. The user jump box is still exposed to risk so appropriate protective controls, detective controls, and response processes should still be applied for that internet-facing computer. Create virtual machine templates Before diving into configuring VM policies and templates, you need to understand the features and functionality of Azure Resource Manager. Resource Manager is the deployment and management service for your Azure subscription. It provides a consistent management layer that allows you to create, update, and delete resources in your Azure subscription. You can use its access control, auditing, and tagging features to help secure and organize your resources after deployment. When you take actions through the Azure portal, Azure PowerShell, the Azure CLI, REST APIs, or client SDKs, the Resource Manager API handles your request. Because the same API handles all requests, you get consistent results and capabilities in all the different tools. Here are some additional terms to know when using Resource Manager: Resource provider. A service that supplies Azure resources. For example, a common resource provider is Microsoft.Compute, which supplies the VM resource. Microsoft.Storage is another common resource provider. Resource Manager template. A JSON file that defines one or more resources to deploy to a resource group or subscription. You can use the template to consistently and repeatedly deploy the resources. Declarative syntax. Syntax that lets you state, \"Here\u2019s what I intend to create\" without having to write the sequence of programming commands to create it. The Resource Manager template is an example of declarative syntax. In the file, you define the properties for the infrastructure to deploy to Azure. You can use the Resource Manager template to define your VMs. After they are defined you can easily deploy and redeploy them. We recommend periodically redeploying your VMs to force the deployment of a freshly updated and security-enhanced VM OS. Template design How you define templates and resource groups is entirely up to you and how you want to manage your solution. For example, you can deploy your three tier application through a single template to a single resource group. But, you don't have to define your entire infrastructure in a single template. Often, it makes sense to divide your deployment requirements into a set of targeted, purpose-specific templates. You can easily reuse these templates for different solutions. To deploy a particular solution, you create a master template that links all the required templates. If you envision your tiers having separate lifecycles, you can deploy your three tiers to separate resource groups. Notice the resources can still be linked to resources in other resource groups. Important When you deploy a template, Resource Manager converts the template into REST API operations. Enable and secure remote access management This topic explains how to connect to and sign into the virtual machines (VMs) you created on Azure. Once you've successfully connected, you can work with the VM as if you were locally logged on to its host server. Connect to a Windows VM The most common way to connect to a Windows based VM running in Azure is by using Remote Desktop Protocol (RDP). Most versions of Windows natively contain support for the remote desktop protocol (RDP). If you are connecting to a Windows VM from a Mac, you will need to install an RDP client for Mac. If you are using PowerShell and have the Azure PowerShell module installed you may also connect using the Get-AzRemoteDesktopFile cmdlet. Connect to a Linux-based VM To connect the Linux-based VM, you need a secure shell protocol (SSH) client. The most used free tool is PuTTY SSH terminal. The following shows the PuTTY configuration dialog. Azure Bastion The Azure Bastion service is a fully platform-managed PaaS service that you provision inside your virtual network. It provides secure and seamless RDP/SSH connectivity to your virtual machines directly in the Azure portal over TLS. When you connect using Azure Bastion, your virtual machines do not need a public IP address. Bastion provides secure RDP and SSH connectivity to all the VMs in the virtual network in which it is provisioned. Using Azure Bastion protects your virtual machines from exposing RDP/SSH ports to the outside world, while still providing secure access using RDP/SSH. With Azure Bastion, you connect to the virtual machine directly from the Azure portal. Architecture Azure Bastion is deployed to a virtual network and supports virtual network peering. Specifically, Azure Bastion manages RDP/SSH connectivity to VMs created in the local or peered virtual networks. RDP and SSH are some of the fundamental means through which you can connect to your workloads running in Azure. Exposing RDP/SSH ports over the Internet isn't desired and is seen as a significant threat surface. This is often due to protocol vulnerabilities. To contain this threat surface, you can deploy bastion hosts (also known as jump-servers) at the public side of your perimeter network. Bastion host servers are designed and configured to withstand attacks. Bastion servers also provide RDP and SSH connectivity to the workloads sitting behind the bastion, as well as further inside the network. This figure shows the architecture of an Azure Bastion deployment. In this diagram: The Bastion host is deployed in the virtual network. The user connects to the Azure portal using any HTML5 browser. The user selects the virtual machine to connect to. With a single click, the RDP/SSH session opens in the browser. No public IP is required on the Azure VM. Key features The following features are available: RDP and SSH directly in Azure portal: You can directly get to the RDP and SSH session directly in the Azure portal using a single click seamless experience. Remote Session over TLS and firewall traversal for RDP/SSH: Azure Bastion uses an HTML5 based web client that is automatically streamed to your local device, so that you get your RDP/SSH session over TLS on port 443 enabling you to traverse corporate firewalls securely. No Public IP required on the Azure VM: Azure Bastion opens the RDP/SSH connection to your Azure virtual machine using private IP on your VM. You don't need a public IP on your virtual machine. No hassle of managing NSGs: Azure Bastion is a fully managed platform PaaS service from Azure that is hardened internally to provide you secure RDP/SSH connectivity. You don't need to apply any NSGs on Azure Bastion subnet. Because Azure Bastion connects to your virtual machines over private IP, you can configure your NSGs to allow RDP/SSH from Azure Bastion only. Protection against port scanning: Because you do not need to expose your virtual machines to public Internet, your VMs are protected against port scanning by rogue and malicious users located outside your virtual network. Protect against zero-day exploits. Hardening in one place only: Azure Bastion is a fully platform-managed PaaS service. Because it sits at the perimeter of your virtual network, you don\u2019t need to worry about hardening each of the virtual machines in your virtual network. Configure update management Azure Update Management is a service included as part of your Azure subscription. With Update Management, you can assess your update status across your environment and manage your Windows Server and Linux server updates from a single location\u2014for both your on-premises and Azure environments. Update Management is available at no additional cost (you pay only for the log data that Azure Log Analytics stores), and you can easily enable it for Azure and on-premises VMs. To try it, navigate to your VM tab in Azure, and then enable Update Management for one or more of your VMs. You can also enable Update Management for VMs directly from your Azure Automation account. Making updates easy, is one of the key factors in maintaining good security hygiene. Azure Update Management overview Computers that Update Management manages use the following configurations to perform assessment and update deployments: Microsoft Monitoring Agent (MMA) for Windows or Linux Desired State Configuration (DSC) in Windows PowerShell for Linux Hybrid Runbook Worker in Azure Automation Microsoft Update or Windows Server Update Services (WSUS) for Windows computers Azure Automation uses runbooks to install updates. You can't view these runbooks, and they don\u2019t require any configuration. When an update deployment is created, it creates a schedule that starts a master update runbook at the specified time for the included computers. The master runbook starts a child runbook on each agent to install the required updates. The following diagram is a conceptual depiction of the behavior and data flow together with how the solution assesses and applies security updates to all connected Windows Server and Linux computers in a workspace. Manage updates for multiple machines You can use the Update Management solution to manage updates and patches for your Windows and Linux virtual machines. From your Azure Automation account, you can: Onboard virtual machines Assess the status of available updates Schedule installation of required updates Review deployment results to verify that updates were applied successfully to all virtual machines for which Update Management is enabled The Log Analytics agent for Windows and Linux needs to be installed on the VMs that are running on your corporate network or other cloud environment in order to enable them with Update Management. After you enable Update Management for your machines, you can view machine information by selecting Computers. You can view information about machine name, compliance status, environment, OS type, critical and security updates installed, other updates installed, and update agent readiness for your computers. Computers that have recently been enabled for Update Management might not have been assessed yet. The compliance state status for those computers is Not assessed. Update inclusion Azure Update Management provides the ability to deploy patches based on classifications. However, there are scenarios where you may want to explicitly list the exact set of patches. Common scenarios include allowing specific patches after canary environment testing and zero-day patch rollouts. With update inclusion lists you can choose exactly which patches you want to deploy instead of relying on patch classifications. Deploy disk encryption Azure Disk Encryption for Windows VMs helps protect and safeguard your data to meet your organizational security and compliance commitments. It uses the BitLocker feature of Windows to provide volume encryption for the OS and data disks of Azure virtual machines (VMs), and is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. If you use Microsoft Defender for Cloud, you'll be alerted if you have VMs that aren't encrypted. The alerts show as High Severity and the recommendation is to encrypt these VMs. Azure Disk Encryption is zone resilient, the same way as Virtual Machines. Supported VMs and operating systems Supported VMs Windows VMs are available in a range of sizes. Azure Disk Encryption is supported on Generation 1 and Generation 2 VMs. Azure Disk Encryption is also available for VMs with premium storage. Azure Disk Encryption is not available on Basic, A-series VMs, or on virtual machines with less than 2 GB of memory. Supported operating systems - Windows client: Windows 8 and later. - Windows Server: Windows Server 2008 R2 and later. - Windows 10 Enterprise multi-session. Networking requirements To enable Azure Disk Encryption, the VMs must meet the following network endpoint configuration requirements: To get a token to connect to your key vault, the Windows VM must be able to connect to an Azure Active Directory endpoint, [login.microsoftonline.com]. To write the encryption keys to your key vault, the Windows VM must be able to connect to the key vault endpoint. The Windows VM must be able to connect to an Azure storage endpoint that hosts the Azure extension repository and an Azure storage account that hosts the VHD files. If your security policy limits access from Azure VMs to the Internet, you can resolve the preceding URI and configure a specific rule to allow outbound connectivity to the IPs. Group Policy requirements Azure Disk Encryption uses the BitLocker external key protector for Windows VMs. For domain joined VMs, don't push any group policies that enforce TPM protectors. BitLocker policy on domain joined virtual machines with custom group policy must include the following setting: Configure user storage of BitLocker recovery information -> Allow 256-bit recovery key. Azure Disk Encryption will fail when custom group policy settings for BitLocker are incompatible. On machines that didn't have the correct policy setting, apply the new policy, force the new policy to update (gpupdate.exe /force), and then restarting may be required. Azure Disk Encryption will fail if domain level group policy blocks the AES-CBC algorithm, which is used by BitLocker. Encryption key storage requirements Azure Disk Encryption requires an Azure Key Vault to control and manage disk encryption keys and secrets. Your key vault and VMs must reside in the same Azure region and subscription. Azure Disk Encryption for Linux VMs Azure Disk Encryption helps protect and safeguard your data to meet your organizational security and compliance commitments. It uses the DM-Crypt feature of Linux to provide volume encryption for the OS and data disks of Azure virtual machines (VMs) and is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. As for Windows VMs, if you use Microsoft Defender for Cloud, you're alerted if you have VMs that aren't encrypted. The alerts show as High Severity and the recommendation is to encrypt these VMs. Supported VMs and operating systems Supported VMs Linux VMs are available in a range of sizes. Azure Disk Encryption is supported on Generation 1 and Generation 2 VMs. Azure Disk Encryption is also available for VMs with premium storage. Note: Azure Disk Encryption is not available on Basic, A-series VMs, or on virtual machines that do not meet these minimum memory requirements: Virtual machine Minimum memory requirement Linux VMs when only encrypting data volumes 2 GB Linux VMs when encrypting both data and OS volumes, and where the root (/) file system usage is 4GB or less 8 GB Linux VMs when encrypting both data and OS volumes, and where the root (/) file system usage is greater than 4GB The root file system usage * 2. For instance, a 16 GB of root file system usage requires at least 32GB of RAM Once the OS disk encryption process is complete on Linux virtual machines, the VM can be configured to run with less memory. Azure Disk Encryption requires the dm-crypt and vfat modules to be present on the system. Removing or disabling vfat from the default image will prevent the system from reading the key volume and obtaining the key needed to unlock the disks on subsequent reboots. System hardening steps that remove the vfat module from the system are not compatible with Azure Disk Encryption Managed disk encryption options Managed disk Encryption Options There are several types of encryption available for your managed disks, including Azure Disk Encryption (ADE), Server-Side Encryption (SSE), and encryption at the host. Azure Disk Encryption helps protect and safeguard your data to meet organizational security and compliance commitments. ADE encrypts the OS and data disks of Azure virtual machines (VMs) inside your VMs by using the device mapper DM-Crypt feature of Linux or the BitLocker feature of Windows. Azure Disk Encryption (ADE) is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. Azure Disk Storage Server-Side Encryption (also referred to as encryption-at-rest or Azure Storage encryption) automatically encrypts data stored on Azure-managed disks (OS and data disks) when persisting on the Storage Clusters. When configured with a Disk Encryption Set (DES), it supports customer-managed keys as well. Encryption at the host ensures that data stored on the VM host hosting your VM is encrypted at rest and flows encrypted to the Storage clusters. Confidential disk encryption binds disk encryption keys to the virtual machine's TPM (Trusted Platform Module) and makes the protected disk content accessible only to the VM. The TPM and VM guest state is always encrypted in attested code using keys released by a secure protocol that bypasses the hypervisor and host operating system. Currently only available for the OS disk. Encryption at the host may be used for other disks on a Confidential VM in addition to Confidential Disk Encryption. Encryption is part of a layered approach to security and should be used with other recommendations to secure Virtual Machines and their disks. Comparison The following is a comparison of Storage Server-Side Encryption (SSE), Azure Disk Encryption (ADE), encryption at the host, and Confidential disk encryption. Important For Encryption at the host and Confidential disk encryption, Microsoft Defender for Cloud does not detect the encryption state. We are in the process of updating Microsoft Defender. Deploy and configure Windows Defender Windows 10, Windows Server 2019, and Windows Server 2016 include key security features. They are Windows Defender Credential Guard, Windows Defender Device Guard, and Windows Defender Application Control. Windows Defender Credential Guard Introduced in Windows 10 Enterprise and Windows Server 2016, Windows Defender Credential Guard uses virtualization-based security enhancement to isolate secrets so that only privileged system software can access them. Unauthorized access to these secrets might lead to credential theft attacks, such as Pass-the-Hash or pass-the-ticket attacks. Windows Defender Credential Guard helps prevent these attacks by helping protect Integrated Windows Authentication (NTLM) password hashes, Kerberos authentication ticket-granting tickets, and credentials that applications store as domain credentials. By enabling Windows Defender Credential Guard, you get the following features and solutions: Hardware security enhancement. NTLM, Kerberos, and Credential Manager take advantage of platform security features, including Secure Boot and virtualization, to help protect credentials. Virtualization-based security enhancement. NTLM-derived credentials, Kerberos-derived credentials, and other secrets run in a protected environment that is isolated from the running operating system. Better protection against advanced persistent threats. When virtualization-based security enhancement helps protect Credential Manager domain credentials, NTLM-derived credentials, and Kerberos-derived credentials, the credential theft attack techniques and tools that many targeted attacks use are blocked. Malware running in the OS with administrative privileges can\u2019t extract secrets that virtualization-based security helps protect. Although Windows Defender Credential Guard provides powerful mitigation, persistent threat attacks will likely shift to new attack techniques, so you should also incorporate Windows Defender Device Guard and other security strategies and architectures. Windows Defender Device Guard and Windows Defender Application Control The configuration state of Windows Defender Device Guard was originally designed with a specific security idea in mind. Although no direct dependencies existed between the two main OS features of the Windows Defender Device Guard configuration\u2014that is, between configurable code integrity and Hypervisor-protected code integrity (HVCI)\u2014the discussion intentionally focused on the Windows Defender Device Guard lockdown state that can be achieved when they\u2019re deployed together. However, the use of the term device guard to describe this configuration state has unintentionally left many IT pros with the impression that the two features are inexorably linked and can\u2019t be separately deployed. Additionally, because HVCI relies on security based on Windows virtualization, it comes with additional hardware, firmware, and kernel driver compatibility requirements that some older systems can\u2019t meet. As a result, many IT pros assumed that because some systems couldn't use HVCI, they couldn\u2019t use configurable code integrity, either. But configurable code integrity has no specific hardware or software requirements other than running Windows 10, which means that many IT pros were wrongly denied the benefits of this powerful application control capability. Since the initial release of Windows 10, the world has witnessed numerous hacking and malware attacks where application control alone might have prevented the attack altogether. Configurable code integrity is now documented as an independent technology within the Microsoft security stack and given a name of its own: Windows Defender Application Control. Application control is a crucial line of defense for helping protect enterprises given today\u2019s threat landscape, and it has an inherent advantage over traditional antivirus solutions. Specifically, application control moves away from the traditional application trust model, in which all applications are assumed trustworthy by default, to one where applications must earn trust to run. Many organizations understand this and frequently cite application control as one of the most effective means for addressing the threat of malware based on executable files (such as .exe and .dll files). Windows Defender Application Control helps mitigate these types of threats by restricting the applications that users can run and the code that runs in the system core, or kernel. Policies in Windows Defender Application Control also block unsigned scripts and MSIs, and Windows PowerShell runs in Constrained language mode. Does this mean the Windows Defender Device Guard configuration state is going away? Not at all. The term device guard will continue to describe the fully locked down state achieved using Windows Defender Application Control, HVCI, and hardware and firmware security features. It will also allow Microsoft to work with its original equipment manufacturer (OEM) partners to identify specifications for devices that are device guard capable\u2014so that joint customers can easily purchase devices that meet all the hardware and firmware requirements of the original locked down scenario of Windows Defender Device Guard for Windows 10 devices. Microsoft Defender for Endpoint - Supported Operating Systems Microsoft cloud security benchmark in Defender for Cloud The Microsoft cloud security benchmark (MCSB) provides prescriptive best practices and recommendations to help improve the security of workloads, data, and services on Azure and your multicloud environment. This benchmark focuses on cloud-centric control areas with input from a set of holistic Microsoft and industry security guidance that includes: Cloud Adoption Framework: Guidance on security, including strategy, roles and responsibilities, Azure Top 10 Security Best Practices, and reference implementation. Azure Well-Architected Framework: Guidance on securing your workloads on Azure. The Chief Information Security Officer (CISO) Workshop: Program guidance and reference strategies to accelerate security modernization using Zero Trust principles. Other industry and cloud service provider's security best practice standards and framework: Examples include the Amazon Web Services (AWS) Well-Architected Framework, Center for Internet Security (CIS) Controls, National Institute of Standards and Technology (NIST), and Payment Card Industry Data Security Standard (PCI-DSS). Microsoft cloud security benchmark features Comprehensive multicloud security framework: Organizations often have to build an internal security standard to reconcile security controls across multiple cloud platforms to meet security and compliance requirements on each of them. This often requires security teams to repeat the same implementation, monitoring, and assessment across the different cloud environments (often for different compliance standards). This creates unnecessary overhead, cost, and effort. To address this concern, we enhanced the Azure Security Benchmark (ASB) to the Microsoft cloud security benchmark (MCSB) to help you quickly work with different clouds by: Providing a single control framework to easily meet the security controls across clouds Providing consistent user experience for monitoring and enforcing the multicloud security benchmark in Defender for Cloud Staying aligned with Industry Standards (e.g., Center for Internet Security, National Institute of Standards and Technology, Payment Card Industry) Automated control monitoring for AWS in Microsoft Defender for Cloud: You can use Microsoft Defender for Cloud Regulatory Compliance Dashboard to monitor your AWS environment against Microsoft cloud security benchmark (MCSB), just like how you monitor your Azure environment. We developed approximately 180 AWS checks for the new AWS security guidance in MCSB, allowing you to monitor your AWS environment and resources in Microsoft Defender for Cloud. Example: Microsoft Defender for Cloud - Regulatory compliance dashboard Azure guidance and security principles: Azure security guidance, security principles, features, and capabilities. Controls Control Domains Description Network security (NS) Network Security covers controls to secure and protect networks, including securing virtual networks, establishing private connections, preventing and mitigating external attacks, and securing Domain Name System (DNS). Identity Management (IM) Identity Management covers controls to establish a secure identity and access controls using identity and access management systems, including the use of single sign-on, strong authentications, managed identities (and service principals) for applications, conditional access, and account anomalies monitoring. Privileged Access (PA) Privileged Access covers controls to protect privileged access to your tenant and resources, including a range of controls to protect your administrative model, administrative accounts, and privileged access workstations against deliberate and inadvertent risk. Data Protection (DP) Data Protection covers control of data protection at rest, in transit, and via authorized access mechanisms, including discover, classify, protect, and monitoring sensitive data assets using access control, encryption, key management, and certificate management. Asset Management (AM) Asset Management covers controls to ensure security visibility and governance over your resources, including recommendations on permissions for security personnel, security access to asset inventory and managing approvals for services and resources (inventory, track, and correct). Logging and Threat Detection (LT) Logging and Threat Detection covers controls for detecting threats on the cloud and enabling, collecting, and storing audit logs for cloud services, including enabling detection, investigation, and remediation processes with controls to generate high-quality alerts with native threat detection in cloud services; it also includes collecting logs with a cloud monitoring service, centralizing security analysis with a security event management (SEM), time synchronization, and log retention. Incident Response (IR) Incident Response covers controls in the incident response life cycle - preparation, detection and analysis, containment, and post-incident activities, including using Azure services (such as Microsoft Defender for Cloud and Sentinel) and/or other cloud services to automate the incident response process. Posture and Vulnerability Management (PV) Posture and Vulnerability Management focuses on controls for assessing and improving the cloud security posture, including vulnerability scanning, penetration testing, and remediation, as well as security configuration tracking, reporting, and correction in cloud resources. Endpoint Security (ES) Endpoint Security covers controls in endpoint detection and response, including the use of endpoint detection and response (EDR) and anti-malware service for endpoints in cloud environments. Backup and Recovery (BR) Backup and Recovery covers controls to ensure that data and configuration backups at the different service tiers are performed, validated, and protected. DevOps Security (DS) DevOps Security covers the controls related to the security engineering and operations in the DevOps processes, including deployment of critical security checks (such as static application security testing and vulnerability management) prior to the deployment phase to ensure the security throughout the DevOps process; it also includes common topics such as threat modeling and software supply security. Governance and Strategy (GS) Governance and Strategy provides guidance for ensuring a coherent security strategy and documented governance approach to guide and sustain security assurance, including establishing roles and responsibilities for the different cloud security functions, unified technical strategy, and supporting policies and standards. Explore Microsoft Defender for Cloud recommendations What is a security recommendation? Using the policies, Defender for Cloud periodically analyzes the compliance status of your resources to identify potential security misconfigurations and weaknesses. It then provides you with recommendations on how to remediate those issues. Recommendations result from assessing your resources against the relevant policies and identifying resources that aren't meeting your defined requirements. Defender for Cloud makes its security recommendations based on your chosen initiatives. When a policy from your initiative is compared against your resources and finds one or more that aren't compliant, it is presented as a recommendation in Defender for Cloud. Example: Microsoft Defender for Cloud - All recommendations Recommendations are actions for you to take to secure and harden your resources. Each recommendation provides you with the following information: A short description of the issue The remediation steps to carry out in order to implement the recommendation The affected resources In practice, it works like this: Azure Security Benchmark is an initiative that contains requirements. For example, Azure Storage accounts must restrict network access to reduce their attack surface. The initiative includes multiple policies, each requiring a specific resource type. These policies enforce the requirements in the initiative. To continue the example, the storage requirement is enforced with the policy \"Storage accounts should restrict network access using virtual network rules.\" Microsoft Defender for Cloud continually assesses your connected subscriptions. If it finds a resource that doesn't satisfy a policy, it displays a recommendation to fix that situation and harden the security of resources that aren't meeting your security requirements. For example, if an Azure Storage account on your protected subscriptions isn't protected with virtual network rules, you'll see the recommendation to harden those resources. So, (1) an initiative includes (2) policies that generate (3) environment-specific recommendations. Perform Try-This exercises Use this Try-This exercises to get some hands-on experience with Azure. In this demonstration, we'll configure the Bastion service, virtual machine updates, virtual machine extensions, and disk encryption. Optionally, we'll use RDP to connect to a Windows virtual machine and SSH to connect to a Linux machine. Task 1 - Use the Bastion service Note This task requires a virtual machine. If you are doing the next task, virtual machine updates, use a Windows virtual machine and keep the session running. In this task, we'll configure the Bastion service and connect to a virtual machine with service. Configure the Bastion service In the Portal navigate to your Windows virtual machine. Ensure the virtual machine is Running. Click Connect and select Bastion. Click Use Bastion. Note installing the service is only required once. Because you are creating the Bastion service from the target virtual machine, mention that most of the networking information has automatically been filled in. Note the Bastion service will be assigned a public IP address. To create the Bastion subnet in the virtual network, click Manage subnet configuration. On the virtual network subnet blade, click + Subnet. On the Add subnet page, type AzureBastionSubnet as the subnet name. Note this name can't be changed. Specify the address range in CIDR notation. For example, 10.1.1.0/27. Click Ok, then click Create. It will take a few minutes for the service to deploy. Connect to the virtual machine using Bastion From the target virtual machine\u2019s Overview blade, select Connect and then Bastion On the Connect to Bastion page, enter the virtual machine login credentials. Notice the checkbox to open the session in a new window. Click Connect. If you receive a message that popup windows are blocked, allow the session. Once your session is connected, launch the Bastion clipboard access tool palette by selecting the two arrows. The arrows are located on the left center of the session. Explain this copy and paste feature. In the Portal, navigate to the Bastion host and under Settings select Sessions. Review the session management experience and the ability to delete a session. As you have time, review the Bastion components and how this provides a secure way to access your virtual machines. Task 2 - Virtual Machine Updates Note This task requires a virtual machine in the running state. You may want to enable Update management prior to this lesson. In this task, we'll review virtual machine update management. In the Portal, navigate to your virtual machine. Under Operations select Update management. Select the Azure Log Analytics workspace and Automation account, and then click Enable. Wait for update management to deploy. It can take up to 15 minutes for the deployment and longer for results to be provided. Select Missing Updates and use the Information link to open the support article for the update. Select Schedule update deployment. Review the various options including maintenance windows, reboot options, scheduling, classifications, kbs to include and exclude. You can view the status for the deployment on the Update deployments tab. The available values aren't attempted, succeeded, and failed. Task 3 - Virtual Machine Extensions In this task, we'll install the IaaSAntimalware extension. In the Portal, select your virtual machine. Under Settings, click Extensions. Review how extensions are used. On the Extensions page, click + Add. Scroll through the available extensions and review what extensions are available. Select Microsoft Antimalware. Discuss the features of this extension. Click Create. On the Install extension page use the informational icons to explain Excluded files and locations, Excluded file extensions, and Excluded processes. Review Real-time protection and Run a scheduled scan. Review other options of interest. After the extension is deployed, the extensions page will show the IaaSAntimalware extension. Task 4 - Disk Encryption Note This task requires a storage account. In this task, we'll enable disk encryption for a storage account. Review encryption key options In the Portal, access your storage account. Under Security + networking, select Encryption. Review Storage Service Encryption and why it is used. Review the two types of keys: Microsoft Managed Keys and Customer Managed Keys. Select Customer Managed Keys. Create the customer managed key For Encryption key choose Select from key vault. Click Select a key vault and key. You will now create a new key vault. If you already had a key vault you could use that. For Key vault select Create new. Notice the key vault will be created in the same region as the storage account. Give your key vault a name. Click Review + create. Once the validation passes, click Create. Wait for the key vault to be created. You will now create a key in the key vault. If you already had a key you could use that. On the Select key from Azure key vault page, for Key select Create new. Review the options for creating a key. Give your key a name. Notice the activation and expiration options. Click Create. Now that you have created a key vault and key, Select the key vault and key. Save your changes on the Encryption page. Review the information that is now available: Current key, Automated key rotation, and Key version in use. Review the key options Return to the resource group that includes your storage account. Refresh the page and ensure your new key vault is listed as a resource. Select the key vault. Under Settings click Keys. Ensure your new key is Enabled. Notice the ability to regenerate the key. Select the key and review the current version information. Return to the key vault page. Under Settings select Access policies. Under Current access policies your storage account will be listed. Notice the drop-downs for Key Permissions, Secret Permissions, and Certificate Permissions. Select Key Permissions and notice the properties that are checked (Get, Unwrap key, and Wrap key). Task 5 - Use RDP to connect to a Windows VM (optional) Note This task requires a Windows VM with a public IP address. You also need the login credentials for the machine. In this task, we'll use RDP to connect to a Windows virtual machine. In the Portal navigate to your Windows virtual machine. Ensure the virtual machine is Running. From the Overview blade select Connect and then RDP. In the Connect to virtual machine page, keep the default options to connect by DNS name over port 3389 and click Download RDP file. Mention that if the VM has a just-in-time policy set, you first need to select the Request access button to request access before you can download the RDP file. Open the downloaded RDP file and then click Connect. In the Windows Security window, select More choices and then Use a different account. Type the username as localhost\\username, enter password you created for the virtual machine, and then select OK. You may receive a certificate warning during the sign-in process. Select Yes or Continue to create the connection. Explain how RDP is different from the Bastion service. Task 6 - Use SSH to connect to a Linux VM (optional) Note This task requires a Linux VM. Ensure port 22 is open. In this task, we'll create an SSH private key with PuTTYgen, and then use SSH to connect to a Linux virtual machine. Create the SSH Keys Download the PuTTY tool. This will include PuTTYgen - https://putty.org/ Once installed, locate and open the PuTTYgen program. In the Parameters option group choose RSA. Click the Generate button. Move your mouse around the blank area in the window to generate some randomness. Copy the text of the Public key for pasting into authorized keys file. Optionally you can specify a Key passphrase and then Confirm passphrase. You will be prompted for the passphrase when you authenticate to the VM with your private SSH key. Without a passphrase, if someone obtains your private key, they can sign in to any VM or service that uses that key. We recommend you create a passphrase. However, if you forget the passphrase, there is no way to recover it. Click Save private key. Choose a location and filename and click Save. You'll need this file to access the VM. Create the Linux machine and assign the public SSH key In the portal navigate to your Linux machine. Choose SSH Public Key for the Authentication type (instead of Password ). Provide a Username. Paste the public SSH key from PuTTY into the SSH public key text area. Ensure the key validates with a checkmark. Create the VM. Wait for it to deploy. Access the running VM. From the Overview blade, click Connect. Make a note of your login information including user and public IP address. Access the server using SSH Open the PuTTY tool. Enter username@publicIpAddress where username is the value you assigned when creating the VM and publicIpAddress is the value you obtained from the Azure portal. Specify 22 for the Port. Choose SSH in the Connection Type option group. Navigate to SSH in the Category panel, then click Auth. Click the Browse button next to Private key file for authentication. Navigate to the private key file saved when you generated the SSH keys and click Open. From the main PuTTY screen click Open. You will now be connected to your server command line. Explain how SSH is different from the Bastion service. Knowledge check An organization has a security policy that prohibits exposing SSH ports to the outside world. You need to connect to an Azure Linux virtual machine to install software. What should you do? Configure the Bastion service( Ans ) Configure a Guest configuration on the virtual machine Create a custom script extension What type of disk encryption is used for Linux disks? BitLocker DM-Crypt( Ans ) FileVault A company with both Azure and on-premises virtual machines needs to ensure virtual machines are up to date with security patches. Update Management is the Azure tool they will use. Which of the following would enable the company to assess the status of available updates and manage the installation of required updates on virtual machines? The Microsoft Monitoring Agent must be installed for both Windows and Linux virtual machines on-premises. Both the Update Management feature and the log data storage are free for the customer. Update Management in Azure Automation collects information about Windows and Linux virtual machines and manages operating system updates.( Ans ) Which of the following recommendations from Security Center is a medium-severity recommendation for virtual machines and servers? Disk encryption should be applied on virtual machines. Install endpoint protection solution on virtual machines.( Ans ) System updates should be installed on your machines. Which attacks will using a Privileged access workstations protect companies from? Protects against attackers who have gained administrative access. Protects against server-based phishing attacks, various impersonation attacks, and credential theft attacks such as keystroke logging. Protects high impact IT administrative roles and tasks.( Ans )","title":"Configure and manage host security"},{"location":"Cloud/Azure/AZ-500/Implement-platform-protection.html#enable-containers-security","text":"Introduction Containers are a great choice for running applications in Azure. Just like virtual machines, those containers must be locked to protect your data and business. Use the built-in Azure container tools to keep your container-based solutions secure. Scenario A security engineer uses Azure Container Instances, the Container Registry, and access controls to protect your containers; you will work on such tasks as: Deploy containers from the Container Registry into Azure Container Instances. Use RBAC and Conditional Access to control access. Use proper architecture, storage, and network design to optimize container security. Explore containers A container is an isolated, lightweight silo for running an application on the host operating system. Containers build on top of the host operating system's kernel (which can be thought of as the buried plumbing of the operating system), and contain only apps and some lightweight operating system APIs and services that run in user mode. While a container shares the host operating system's kernel, the container doesn't get unfettered access to it. Instead, the container gets an isolated\u2013and in some cases virtualized\u2013view of the system. For example, a container can access a virtualized version of the file system and registry, but any changes affect only the container and are discarded when it stops. To save data, the container can mount persistent storage such as an Azure Disk or a file share (including Azure Files). You need Docker in order to work with Windows Containers. Docker consists of the Docker Engine (dockerd.exe), and the Docker client (docker.exe). How it works A container builds on top of the kernel, but the kernel doesn't provide all of the APIs and services an app needs to run\u2013most of these are provided by system files (libraries) that run above the kernel in user mode. Because a container is isolated from the host's user mode environment, the container needs its own copy of these user mode system files, which are packaged into something known as a base image. The base image serves as the foundational layer upon which your container is built, providing it with operating system services not provided by the kernel. Because containers require far fewer resources (for example, they don't need a full OS), they're easy to deploy and they start fast. This allows you to have higher density, meaning that it allows you to run more services on the same hardware unit, thereby reducing costs. As a side effect of running on the same kernel, you get less isolation than VMs. Features of Containers Features Description Isolation Typically provides lightweight isolation from the host and other containers, but doesn't provide as strong a security boundary as a VM. (You can increase the security by using Hyper-V isolation mode to isolate each container in a lightweight VM). Operating System Runs the user mode portion of an operating system, and can be tailored to contain just the needed services for your app, using fewer system resources. Deployment Deploy individual containers by using Docker via command line; deploy multiple containers by using an orchestrator such as Azure Kubernetes Service. Persistent storage Use Azure Disks for local storage for a single node, or Azure Files (SMB shares) for storage shared by multiple nodes or servers. Fault tolerance If a cluster node fails, any containers running on it are rapidly recreated by the orchestrator on another cluster node. Networking Uses an isolated view of a virtual network adapter, providing a little less virtualization\u2013the host's firewall is shared with containers\u2013while using less resources. In Docker, each layer is the resulting set of changes that happen to the filesystem after executing a command, such as, installing a program. So, when you view the filesystem after the layer has been copied, you can view all the files, including the layer when the program was installed. You can think of an image as an auxiliary read-only hard disk ready to be installed in a \"computer\" where the operating system is already installed. Similarly, you can think of a container as the \"computer\" with the image hard disk installed. The container, just like a computer, can be powered on or off. Configure Azure Container Instances security Use a private registry Containers are built from images that are stored in one or more repositories. These repositories can belong to a public registry, like Docker Hub, or to a private registry. An example of a private registry is the Docker Trusted Registry, which can be installed on-premises or in a virtual private cloud. You can also use cloud-based private container registry services, including Azure Container Registry. A publicly available container image does not guarantee security. Container images consist of multiple software layers, and each software layer might have vulnerabilities. To help reduce the threat of attacks, you should store and retrieve images from a private registry, such as Azure Container Registry or Docker Trusted Registry. In addition to providing a managed private registry, Azure Container Registry supports service principal-based authentication through Azure Active Directory for basic authentication flows. This authentication includes role-based access for read-only (pull), write (push), and other permissions. Monitor and scan container images continuously Take advantage of solutions to scan container images in a private registry and identify potential vulnerabilities. It\u2019s important to understand the depth of threat detection that the different solutions provide. For example, Azure Container Registry optionally integrates with Microsoft Defender for Cloud to automatically scan all Linux images pushed to a registry. Microsoft Defender for Cloud integrated Qualys scanner detects image vulnerabilities, classifies them, and provides remediation guidance. Protect credentials Containers can spread across several clusters and Azure regions. So, you must secure credentials required for logins or API access, such as passwords or tokens. Ensure that only privileged users can access those containers in transit and at rest. Inventory all credential secrets, and then require developers to use emerging secrets-management tools that are designed for container platforms. Make sure that your solution includes encrypted databases, TLS encryption for secrets data in transit, and least-privilege role-based access control. Azure Key Vault is a cloud service that safeguards encryption keys and secrets (such as certificates, connection strings, and passwords) for containerized applications. Because this data is sensitive and business critical, secure access to your key vaults so that only authorized applications and users can access them. Use vulnerability management as part of your container development lifecycle By using effective vulnerability management throughout the container development lifecycle, you improve the odds that you identify and resolve security concerns before they become a more serious problem. Scan for vulnerabilities New vulnerabilities are discovered all the time, so scanning for and identifying vulnerabilities is a continuous process. Incorporate vulnerability scanning throughout the container lifecycle: As a final check in your development pipeline, you should perform a vulnerability scan on containers before pushing the images to a public or private registry. Continue to scan container images in the registry both to identify any flaws that were somehow missed during development and to address any newly discovered vulnerabilities that might exist in the code used in the container images. Ensure that only approved images are used in your environment There\u2019s enough change and volatility in a container ecosystem without allowing unknown containers as well. Allow only approved container images. Have tools and processes in place to monitor for and prevent the use of unapproved container images. An effective way of reducing the attack surface and preventing developers from making critical security mistakes is to control the flow of container images into your development environment. For example, you might sanction a single Linux distribution as a base image, preferably one that is lean (Alpine or CoreOS rather than Ubuntu), to minimize the surface for potential attacks. Image signing or fingerprinting can provide a chain of custody that enables you to verify the integrity of the containers. For example, Azure Container Registry supports Docker's content trust model, which allows image publishers to sign images that are pushed to a registry, and image consumers to pull only signed images. Enforce least privileges in runtime The concept of least privileges is a basic security best practice that also applies to containers. When a vulnerability is exploited, it generally gives the attacker access and privileges equal to those of the compromised application or process. Ensuring that containers operate with the lowest privileges and access required to get the job done reduces your exposure to risk. Reduce the container attack surface by removing unneeded privileges You can also minimize the potential attack surface by removing any unused or unnecessary processes or privileges from the container runtime. Privileged containers run as root. If a malicious user or workload escapes in a privileged container, the container will then run as root on that system. Log all container administrative user access for auditing Maintain an accurate audit trail of administrative access to your container ecosystem, including your Kubernetes cluster, container registry, and container images. These logs might be necessary for auditing purposes and will be useful as forensic evidence after any security incident. Azure solutions include: Integration of Azure Kubernetes Service with Microsoft Defender for Cloud to monitor the security configuration of the cluster environment and generate security recommendations Azure Container Monitoring solution Resource logs for Azure Container Instances and Azure Container Registry Manage security for Azure Container Instances (ACI) Azure Container Instances (ACI), is a PaaS service for scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, we recommend Azure Kubernetes Service (which will be covered later on in this lesson). Features of ACI Fast startup times Containers offer significant startup benefits over virtual machines (VMs). Azure Container Instances can start containers in Azure in seconds, without the need to provision and manage VMs. Container access Azure Container Instances enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN). When you create a container instance, you can specify a custom DNS name label so your application is reachable at customlabel.azureregion.azurecontainer.io. Azure Container Instances also supports executing a command in a running container by providing an interactive shell to help with application development and troubleshooting. Access takes places over HTTPS, using TLS to secure client connections. Container deployment Deploy containers from DockerHub or Azure Container Registry. Hypervisor-level security Historically, containers have offered application dependency isolation and resource governance but have not been considered sufficiently hardened for hostile multi-tenant usage. Azure Container Instances guarantees your application is as isolated in a container as it would be in a VM. Custom sizes Containers are typically optimized to run just a single application, but the exact needs of those applications can differ greatly. Azure Container Instances provides optimum utilization by allowing exact specifications of CPU cores and memory. You pay based on what you need and get billed by the second, so you can fine-tune your spending based on actual need. For compute-intensive jobs such as machine learning, Azure Container Instances can schedule Linux containers to use NVIDIA Tesla GPU resources. Persistent storage To retrieve and persist state with Azure Container Instances, we offer direct mounting of Azure Files shares backed by Azure Storage. Flexible billing Supports per-GB, per-CPU, and per-second billing. Linux and Windows containers Azure Container Instances can schedule both Windows and Linux containers with the same API. Simply specify the OS type when you create your container groups. Some features are currently restricted to Linux containers: Multiple containers per container group Volume mounting (Azure Files, emptyDir, GitRepo, secret) Resource usage metrics with Azure Monitor Virtual network deployment GPU resources (preview) For Windows container deployments, use images based on common Windows base images. Co-scheduled groups Azure Container Instances supports scheduling of multi-container groups that share a host machine, local network, storage, and lifecycle. This enables you to combine your main application container with other supporting role containers, such as logging sidecars. Virtual network deployment Currently available for production workloads in a subset of Azure regions, this feature of Azure Container Instances enables deployment of container instances into an Azure virtual network. By deploying container instances into a subnet within your virtual network, they can communicate securely with other resources in the virtual network, including those that are on premises (through VPN gateway or ExpressRoute). Explore the Azure Container Registry (ACR) A container registry is a service that stores and distributes container images. Docker Hub is a public container registry that supports the open source community and serves as a general catalog of images. Azure Container Registry provides users with direct control of their images, with integrated authentication, geo-replication supporting global distribution and reliability for network-close deployments, virtual network and firewall configuration, tag locking, and many other enhanced features. In addition to Docker container images, Azure Container Registry supports related content artifacts including Open Container Initiative (OCI) image formats. Security and access You log in to a registry using the Azure CLI or the standard docker login command. Azure Container Registry transfers container images over HTTPS, and supports TLS to secure client connections. Azure Container Registry requires all secure connections from servers and applications to use TLS 1.2. Enable TLS 1.2 by using any recent docker client (version 18.03.0 or later). You control access to a container registry using an Azure identity, an Azure Active Directory-backed service principal, or a provided admin account. Use role-based access control (RBAC) to assign users or systems fine-grained permissions to a registry. Security features of the Premium SKU include content trust for image tag signing, and firewalls and virtual networks to restrict access to the registry. Microsoft Defender for Cloud optionally integrates with Azure Container Registry to scan images whenever an image is pushed to a registry. Repository Container registries manage repositories, collections of container images or other artifacts with the same name, but different tags. For example, the following three images are in the \"acr-helloworld\" repository: acr-helloworld:latest acr-helloworld:v1 acr-helloworld:v2 Image A container image or other artifact within a registry is associated with one or more tags, has one or more layers, and is identified by a manifest. Understanding how these components relate to each other can help you manage your registry effectively. Monitor container activity and user access As with any IT environment, you should consistently monitor activity and user access to your container ecosystem to quickly identify any suspicious or malicious activity. The container monitoring solution in Log Analytics can help you view and manage your Docker and Windows container hosts in a single location. By using Log Analytics, you can: View detailed audit information that shows commands used with containers. Troubleshoot containers by viewing and searching centralized logs without having to remotely view Docker or Windows hosts. Find containers that may be noisy and consuming excess resources on a host. View centralized CPU, memory, storage, and network usage and performance information for containers. On computers running Windows, you can centralize and compare logs from Windows Server, Hyper-V, and Docker containers. The solution supports container orchestrators such as Docker Swarm, DC/OS, Kubernetes, Service Fabric, and Red Hat OpenShift. Container technology is causing a structural change in the cloud-computing world. Containers make it possible to run multiple instances of an application on a single instance of an operating system, thereby using resources more efficiently. Containers give organizations consistency and flexibility. They enable continuous deployment because the application can be developed on a desktop, tested in a virtual machine, and then deployed for production in the cloud. Containers provide agility, streamlined operations, scalability, and reduced costs due to resource optimization. Enable Azure Container Registry authentication There are several ways to authenticate with an Azure container registry, each of which is applicable to one or more registry usage scenarios. Recommended ways include authenticating to a registry directly via individual login, or your applications and container orchestrators can perform unattended, or \"headless,\" authentication by using an Azure Active Directory (Azure AD) service principal. Authentication options The following table lists available authentication methods and recommended scenarios. Identity Usage scenario Details Azure AD identities including user and service principals Unattended push from DevOps, unattended pull to Azure or external services Role-based access control - Reader, Contributor, Owner Individual AD Identity Interactive push/pull by developers and testers Admin user Interactive push/pull by individual developers and testers By default, disabled. Individual login with Azure AD When working with your registry directly, such as pulling images to and pushing images from a development workstation, authenticate by using the az acr login command in the Azure CLI. When you log in with az acr login, the CLI uses the token created when you executed az login to seamlessly authenticate your session with your registry. To complete the authentication flow, Docker must be installed and running in your environment. az acr login uses the Docker client to set an Azure Active Directory token in the docker.config file. Once you've logged in this way, your credentials are cached, and subsequent docker commands in your session do not require a username or password. Service principal If you assign a service principal to your registry, your application or service can use it for headless authentication. Service principals allow role-based access to a registry, and you can assign multiple service principals to a registry. Multiple service principals allow you to define different access for different applications. The available roles for a container registry include: AcrPull: pull AcrPush: pull and push Owner: pull, push, and assign roles to other users Admin account Each container registry includes an admin user account, which is disabled by default. You can enable the admin user and manage its credentials in the Azure portal, or by using the Azure CLI or other Azure tools. The admin account is provided with two passwords, both of which can be regenerated. Two passwords allow you to maintain connection to the registry by using one password while you regenerate the other. If the admin account is enabled, you can pass the username and either password to the docker login command when prompted for basic authentication to the registry. Review Azure Kubernetes Service (AKS) As application development moves towards a container-based approach, the need to orchestrate and manage resources is important. Kubernetes is the leading platform that provides the ability to provide reliable scheduling of fault-tolerant application workloads. Azure Kubernetes Service (AKS) is a managed Kubernetes offering that further simplifies container-based application deployment and management. Kubernetes is ... Kubernetes is a rapidly evolving platform that manages container-based applications and their associated networking and storage components. The focus is on the application workloads, not the underlying infrastructure components. Kubernetes provides a declarative approach to deployments, backed by a robust set of APIs for management operations. You can build and run modern, portable, microservices-based applications that benefit from Kubernetes orchestrating and managing the availability of those application components. Kubernetes supports both stateless and stateful applications as teams progress through the adoption of microservices-based applications. Deployment of containers As an open platform, Kubernetes allows you to build your applications with your preferred programming language, OS, libraries, or messaging bus. Existing continuous integration and continuous delivery (CI/CD) tools can integrate with Kubernetes to schedule and deploy releases. Azure Kubernetes Service (AKS) provides a managed Kubernetes service that reduces the complexity for deployment and core management tasks, including coordinating upgrades. The AKS control plane is managed by the Azure platform, and you only pay for the AKS nodes that run your applications. AKS is built on top of the open-source Azure Kubernetes Service Engine (aks-engine). Kubernetes cluster architecture A Kubernetes cluster is divided into two components: Control plane nodes provide the core Kubernetes services and orchestration of application workloads. Nodes run your application workloads. Features of Azure Kubernetes Service Fully managed Public IP and FQDN (Private IP option) Accessed with RBAC or Azure AD Deployment of containers Dynamic scale containers Automation of rolling updates and rollbacks of containers Management of storage, network traffic, and sensitive information Implement an Azure Kubernetes Service architecture Kubernetes cluster architecture is a set of design recommendations for deploying your containers in a secure and managed configuration. Cluster master When you create an AKS cluster, a cluster master is automatically created and configured. This cluster master is provided as a managed Azure resource abstracted from the user. There is no cost for the cluster master, only the nodes that are part of the AKS cluster. The cluster master includes the following core Kubernetes components: kube-apiserver - The API server is how the underlying Kubernetes APIs are exposed. This component provides the interaction for management tools, such as kubectl or the Kubernetes dashboard. etcd - To maintain the state of your Kubernetes cluster and configuration, the highly available etcd is a key value store within Kubernetes. kube-scheduler - When you create or scale applications, the Scheduler determines what nodes can run the workload and starts them. kube-controller-manager - The Controller Manager oversees a number of smaller Controllers that perform actions such as replicating pods and handling node operations. AKS provides a single-tenant cluster master, with a dedicated API server, Scheduler, etc. You define the number and size of the nodes, and the Azure platform configures the secure communication between the cluster master and nodes. Interaction with the cluster master occurs through Kubernetes APIs, such as kubectl or the Kubernetes dashboard. This managed cluster master means that you do not need to configure components like a highly available store, but it also means that you cannot access the cluster master directly. Upgrades to Kubernetes are orchestrated through the Azure CLI or Azure portal, which upgrades the cluster master and then the nodes. To troubleshoot possible issues, you can review the cluster master logs through Azure Log Analytics. If you need to configure the cluster master in a particular way or need direct access to them, you can deploy your own Kubernetes cluster using aks-engine. Nodes and node pools To run your applications and supporting services, you need a Kubernetes node. An AKS cluster has one or more nodes, which is an Azure virtual machine (VM) that runs the Kubernetes node components and container runtime: The kubelet is the Kubernetes agent that processes the orchestration requests from the control plane and scheduling of running the requested containers. Virtual networking is handled by the kube-proxy on each node. The proxy routes network traffic and manages IP addressing for services and pods. The container runtime is the component that allows containerized applications to run and interact with additional resources such as the virtual network and storage. In AKS, Moby is used as the container runtime. The Azure VM size for your nodes defines how many CPUs, how much memory, and the size and type of storage available (such as high-performance SSD or regular HDD). If you anticipate a need for applications that require large amounts of CPU and memory or high-performance storage, plan the node size accordingly. You can also scale out the number of nodes in your AKS cluster to meet demand. In AKS, the VM image for the nodes in your cluster is currently based on Ubuntu Linux or Windows Server 2019. When you create an AKS cluster or scale out the number of nodes, the Azure platform creates the requested number of VMs and configures them. There's no manual configuration for you to perform. Agent nodes are billed as standard virtual machines, so any discounts you have on the VM size you're using (including Azure reservations) are automatically applied. If you need to use a different host OS, container runtime, or include custom packages, you can deploy your own Kubernetes cluster using aks-engine. The upstream aks-engine releases features and provides configuration options before they are officially supported in AKS clusters. For example, if you wish to use a container runtime other than Moby, you can use aks-engine to configure and deploy a Kubernetes cluster that meets your current needs. Cluster master nodes provide the core Kubernetes services and orchestration of application workloads. Nodes (virtual machines) run your application workloads. AKS Terminology Term Description Pools Group of nodes with identical configuration Node Individual VM running containerized applications Pods Single instance of an application. A pod can contain multiple containers Deployment One or more identical pods managed by Kubernetes Manifest YAML file describing a deployment Master security In AKS, the Kubernetes master components are part of the managed service provided by Microsoft. Each AKS cluster has its own single-tenanted, dedicated Kubernetes master to provide the API Server, Scheduler, etc. This master is managed and maintained by Microsoft. By default, the Kubernetes API server uses a public IP address and a fully qualified domain name (FQDN). You can control access to the API server using Kubernetes role-based access controls and Azure Active Directory. Node security AKS nodes are Azure virtual machines that you manage and maintain. Linux nodes run an optimized Ubuntu distribution using the Moby container runtime. Windows Server nodes run an optimized Windows Server 2019 release and also use the Moby container runtime. When an AKS cluster is created or scaled up, the nodes are automatically deployed with the latest OS security updates and configurations. The Azure platform automatically applies OS security patches to Linux nodes on a nightly basis. If a Linux OS security update requires a host reboot, that reboot is not automatically performed. You can manually reboot the Linux nodes, or a common approach is to use Kured, an open-source reboot daemon for Kubernetes. Kured runs as a DaemonSet and monitors each node for the presence of a file indicating that a reboot is required. Reboots are managed across the cluster using the same cordon and drain process as a cluster upgrade. For Windows Server nodes, Windows Update does not automatically run and apply the latest updates. On a regular schedule around the Windows Update release cycle and your own validation process, you should perform an upgrade on the Windows Server node pool(s) in your AKS cluster. This upgrade process creates nodes that run the latest Windows Server image and patches, then removes the older nodes. Nodes are deployed into a private virtual network subnet, with no public IP addresses assigned. For troubleshooting and management purposes, SSH is enabled by default. This SSH access is only available using the internal IP address. To provide storage, the nodes use Azure Managed Disks. For most VM node sizes, these are Premium disks backed by high-performance SSDs. The data stored on managed disks is automatically encrypted at rest within the Azure platform. To improve redundancy, these disks are also securely replicated within the Azure datacenter. Kubernetes environments, in AKS or elsewhere, currently aren't completely safe for hostile multi-tenant usage. Additional security features such as Pod Security Policies or more fine-grained role-based access controls (RBAC) for nodes make exploits more difficult. However, for true security when running hostile multi-tenant workloads, a hypervisor is the only level of security that you should trust. The security domain for Kubernetes becomes the entire cluster, not an individual node. For these types of hostile multi-tenant workloads, you should use physically isolated clusters. To protect your customer data as you run application workloads in Azure Kubernetes Service (AKS), the security of your cluster is a key consideration. Configure Azure Kubernetes Service networking To allow access to your applications, or for application components to communicate with each other, Kubernetes provides an abstraction layer to virtual networking. Kubernetes nodes are connected to a virtual network, and can provide inbound and outbound connectivity for pods. The kube-proxy component runs on each node to provide these network features. In Kubernetes, Services logically group pods to allow for direct access via an IP address or DNS name and on a specific port. You can also distribute traffic using a load balancer. More complex routing of application traffic can also be achieved with Ingress Controllers. Security and filtering of the network traffic for pods is possible with Kubernetes network policies. The Azure platform also helps to simplify virtual networking for AKS clusters. When you create a Kubernetes load balancer, the underlying Azure load balancer resource is created and configured. As you open network ports to pods, the corresponding Azure network security group rules are configured. For HTTP application routing, Azure can also configure external DNS as new ingress routes are configured. Services To simplify the network configuration for application workloads, Kubernetes uses Services to logically group a set of pods together and provide network connectivity. The following Service types are available: Cluster IP - Creates an internal IP address for use within the AKS cluster. Good for internal-only applications that support other workloads within the cluster. NodePort - Creates a port mapping on the underlying node that allows the application to be accessed directly with the node IP address and port. LoadBalancer - Creates an Azure load balancer resource, configures an external IP address, and connects the requested pods to the load balancer backend pool. To allow customers' traffic to reach the application, load balancing rules are created on the desired ports. ExternalName - Creates a specific DNS entry for easier application access. When you run modern, microservices-based applications in Kubernetes, you often want to control which components can communicate with each other. The principle of least privilege should be applied to how traffic can flow between pods in an Azure Kubernetes Service (AKS) cluster. Let's say you likely want to block traffic directly to back-end applications. The Network Policy feature in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster. Deploy Azure Kubernetes Service storage Applications that run in Azure Kubernetes Service (AKS) may need to store and retrieve data. For some application workloads, this data storage can use local, fast storage on the node that is no longer needed when the pods are deleted. Other application workloads may require storage that persists on more regular data volumes within the Azure platform. Multiple pods may need to share the same data volumes, or reattach data volumes if the pod is rescheduled on a different node. Finally, you may need to inject sensitive data or application configuration information into pods. Volumes Applications often need to be able to store and retrieve data. As Kubernetes typically treats individual pods as ephemeral, disposable resources, different approaches are available for applications to use and persist data as necessary. A volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. Traditional volumes to store and retrieve data are created as Kubernetes resources backed by Azure Storage. You can manually create these data volumes to be assigned to pods directly, or have Kubernetes automatically create them. These data volumes can use Azure Disks or Azure Files: Azure Disks can be used to create a Kubernetes DataDisk resource. Disks can use Azure Premium storage, backed by high-performance SSDs, or Azure Standard storage, backed by regular HDDs. For most production and development workloads, use Premium storage. Azure Disks are mounted as ReadWriteOnce, so are only available to a single pod. For storage volumes that can be accessed by multiple pods simultaneously, use Azure Files. Azure Files can be used to mount an SMB 3.0 share backed by an Azure Storage account to pods. Files let you share data across multiple nodes and pods. Files can use Azure Standard storage backed by regular HDDs, or Azure Premium storage, backed by high-performance SSDs. Persistent volumes Volumes that are defined and created as part of the pod lifecycle only exist until the pod is deleted. Pods often expect their storage to remain if a pod is rescheduled on a different host during a maintenance event, especially in StatefulSets. A persistent volume (PV) is a storage resource created and managed by the Kubernetes API that can exist beyond the lifetime of an individual pod. Azure Disks or Files are used to provide the PersistentVolume. As noted in the previous section on Volumes, the choice of Disks or Files is often determined by the need for concurrent access to the data or the performance tier. A PersistentVolume can be statically created by a cluster administrator, or dynamically created by the Kubernetes API server. If a pod is scheduled and requests storage that is not currently available, Kubernetes can create the underlying Azure Disk or Files storage and attach it to the pod. Dynamic provisioning uses a StorageClass to identify what type of Azure storage needs to be created Storage classes To define different tiers of storage, such as Premium and Standard, you can create a StorageClass. The StorageClass also defines the reclaimPolicy. This reclaimPolicy controls the behavior of the underlying Azure storage resource when the pod is deleted and the persistent volume may no longer be required. The underlying storage resource can be deleted, or retained for use with a future pod. In AKS, two initial StorageClasses are created: default - Uses Azure Standard storage to create a Managed Disk. The reclaim policy indicates that the underlying Azure Disk is deleted when the persistent volume that used it is deleted. managed-premium - Uses Azure Premium storage to create Managed Disk. The reclaim policy again indicates that the underlying Azure Disk is deleted when the persistent volume that used it is deleted. If no StorageClass is specified for a persistent volume, the default StorageClass is used. Persistent volume claims A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. The Kubernetes API server can dynamically provision the underlying storage resource in Azure if there is no existing resource to fulfill the claim based on the defined StorageClass. The pod definition includes the volume mount once the volume has been connected to the pod. A PersistentVolume is bound to a PersistentVolumeClaim once an available storage resource has been assigned to the pod requesting it. There is a 1:1 mapping of persistent volumes to claims. Secure authentication to Azure Kubernetes Service with Active Directory\u200b There are different ways to authenticate with and secure Kubernetes clusters. Using role-based access controls (RBAC), you can grant users or groups access to only the resources they need. With Azure Kubernetes Service (AKS), you can further enhance the security and permissions structure by using Azure Active Directory. These approaches help you secure your application workloads and customer data. Kubernetes service accounts One of the primary user types in Kubernetes is a service account. A service account exists and is managed by, the Kubernetes API. The credentials for service accounts are stored as Kubernetes secrets, which allows them to be used by authorized pods to communicate with the API Server. Most API requests provide an authentication token for a service account or a normal user account. Normal user accounts allow more traditional access for human administrators or developers, not just services and processes. Kubernetes itself doesn't provide an identity management solution where regular user accounts and passwords are stored. Instead, external identity solutions can be integrated into Kubernetes. For AKS clusters, this integrated identity solution is Azure Active Directory. Azure Active Directory integration The security of AKS clusters can be enhanced with the integration of Azure Active Directory (AD). Built on decades of enterprise identity management, Azure AD is a multi-tenant, cloud-based directory, and identity management service that combines core directory services, application access management, and identity protection. With Azure AD, you can integrate on-premises identities into AKS clusters to provide a single source for account management and security. With Azure AD-integrated AKS clusters, you can grant users or groups access to Kubernetes resources within a namespace or across the cluster. To obtain a Kubectl configuration context, a user can run the az aks get-credentials command. When a user then interacts with the AKS cluster with kubectl, they are prompted to sign in with their Azure AD credentials. This approach provides a single source for user account management and password credentials. The user can only access the resources as defined by the cluster administrator. Azure AD authentication in AKS clusters uses OpenID Connect, an identity layer built on top of the OAuth 2.0 protocol. OAuth 2.0 defines mechanisms to obtain and use access tokens to access protected resources, and OpenID Connect implements authentication as an extension to the OAuth 2.0 authorization process. Manage access to Azure Kubernetes Service using Azure role-based access controls One additional mechanism for controlling access to resources is Azure role-based access controls (RBAC). Kubernetes RBAC is designed to work on resources within your AKS cluster, and Azure RBAC is designed to work on resources within your Azure subscription. With Azure RBAC, you create a role definition that outlines the permissions to be applied. A user or group is then assigned this role definition for a particular scope, which could be an individual resource, a resource group, or across the subscription. Roles and ClusterRoles Before you assign permissions to users with Kubernetes RBAC, you first define those permissions as a Role. Kubernetes roles grant permissions. There is no concept of a deny permission. Roles are used to grant permissions within a namespace. If you need to grant permissions across the entire cluster, or to cluster resources outside a given namespace, you can instead use ClusterRoles. A ClusterRole works in the same way to grant permissions to resources, but can be applied to resources across the entire cluster, not a specific namespace. RoleBindings and ClusterRoleBindings Once roles are defined to grant permissions to resources, you assign those Kubernetes RBAC permissions with a RoleBinding. If your AKS cluster integrates with Azure Active Directory, bindings are how those Azure AD users are granted permissions to perform actions within the cluster. Role bindings are used to assign roles for a given namespace. This approach lets you logically segregate a single AKS cluster, with users only able to access the application resources in their assigned namespace. If you need to bind roles across the entire cluster, or to cluster resources outside a given namespace, you can instead use ClusterRoleBindings. A ClusterRoleBinding works in the same way to bind roles to users, but can be applied to resources across the entire cluster, not a specific namespace. This approach lets you grant administrators or support engineers access to all resources in the AKS cluster. Kubernetes Secrets A Kubernetes Secret is used to inject sensitive data into pods, such as access credentials or keys. You first create a Secret using the Kubernetes API. When you define your pod or deployment, a specific Secret can be requested. Secrets are only provided to nodes that have a scheduled pod that requires it, and the Secret is stored in tmpfs, not written to disk. When the last pod on a node that requires a Secret is deleted, the Secret is deleted from the node's tmpfs. Secrets are stored within a given namespace and can only be accessed by pods within the same namespace. The use of Secrets reduces the sensitive information that is defined in the pod or service YAML manifest. Instead, you request the Secret stored in Kubernetes API Server as part of your YAML manifest. This approach only provides the specific pod access to the Secret. Please note: the raw secret manifest files contains the secret data in base64 format. Therefore, this file should be treated as sensitive information, and never committed to source control. Windows containers Secrets are written in clear text on the node\u2019s volume (as compared to tmpfs/in-memory on linux). This means customers have to do two things Use file ACLs to secure the secrets file location Use volume-level encryption using BitLocker Knowledge check To interact with Azure APIs, an Azure Kubernetes Service (AKS) cluster requires which of following? AKS contributor Azure AD service principal( Ans ) Global Administrator permissions When using Azure Kubernetes Service (AKS) and there is a need to control the flow of traffic between pods and block traffic directly to the backend application; what is the best way to configure this? Create an AKS network policy( Ans ) Create an application gateway Create an Azure firewall The organization is defining RBAC rules for the Azure Kubernetes security team. What is the best solution to grant permissions across the entire cluster? ClusterRoles and RoleBindings ClusterRoles and ClusterRoleBindings( Ans ) Roles and RoleBindings","title":"Enable Containers security"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html","text":"AZ-500: Microsoft Certified: Azure Security Engineer Associate # Manage Identity and Access # Secure Azure solutions with Azure Active Directory Introduction Explore Azure Active Directory features Self-managed Active Directory Domain Services, Azure Active Directory, and managed Azure Active Directory Domain Services Azure AD DS and self-managed AD DS Azure AD DS and Azure AD Investigate roles in Azure AD Azure AD built-in roles Deploy Azure AD Domain Services Create and manage Azure AD users Manage users with Azure AD groups Configure Azure AD administrative units Implement passwordless authentication Exercises Implement Hybrid identity Introduction Deploy Azure AD connect Explore authentication options Configure Password Hash Synchronization (PHS) Implement Pass-through Authentication (PTA) Deploy Federation with Azure AD Explore the authentication decision tree Configure password writeback Deploy Azure AD identity protection Introduction Explore Azure AD identity protection Configure risk event detections Implement user risk policy Implement sign-in risk policy Deploy multifactor authentication in Azure Explore multifactor authentication settings Enable multifactor authentication Implement Azure AD conditional access Configure conditional access conditions Implement access reviews Exercises Configure Azure AD privileged identity management Introduction Explore the zero trust model Review the evolution of identity management Deploy Azure AD privileged identity management Configure privileged identity management scope Implement privileged identity management onboarding Explore privileged identity management configuration settings Implement a privileged identity management workflow Exercises Design an enterprise governance strategy Introduction Review the shared responsibility model Review Azure hierarchy of systems Configure Azure policies Enable Azure role-based access control (RBAC) Compare and contrast Azure RBAC vs Azure policies Configure built-in roles Enable resource locks Deploy Azure blueprints Design an Azure subscription management plan Exercises The Azure security engineer implements, manages, and monitors security for resources in Azure, multi-cloud, and hybrid environments as part of an end-to-end infrastructure. They recommend security components and configurations to protect identity & access, data, applications, and networks. Responsibilities for an Azure security engineer include managing the security posture, identifying and remediating vulnerabilities, performing threat modelling, and implementing threat protection. They may also participate in responding to security incidents. Azure security engineers work with architects, administrators, and developers to plan and implement solutions that meet security and compliance requirements. The Azure security engineer should have practical experience in administration of Microsoft Azure and hybrid environments. The Azure security engineer should have a strong familiarity with compute, network, and storage in Azure, as well as Azure Active Directory, part of Microsoft Entra. AZ-500: Manage Identity and Access # Explore how identity is the starting point for all security within your company. Learn to authenticate and authorize users and apps with Azure Active Directory. Introduction # Azure Active Directory (Azure AD) is Microsoft\u2019s cloud-based identity and access management service, which helps your employee's sign in and access resources in: External resources, such as Microsoft 365, the Azure portal, and thousands of other SaaS applications. Internal resources, such as apps on your corporate network and intranet, along with any cloud apps developed by your own organization. Scenario # A security engineer uses Azure Active Directory's identity and access management services to execute and facilitate the following tasks: Create users, groups, and administrative units to securely access resources and services. Configure access to systems to be passwordless. Define a strategy for using Azure AD and Azure AD Domain Services to lock down access to your solutions. Skills measured # Azure Active Directory is a part of Exam AZ-500: Microsoft Azure Security Engineer. Manage identity and access (30-35%) Manage Azure AD identities Manage Azure AD directory groups Manage Azure AD users Manage administrative unit Learning objectives # Compare and contrast Azure AD versus on-premises directory services with Azure AD Domain Services. Configure and deploy users, groups, and administrative units to securely access resources in your tenant. Deploy a passwordless login solution for your Azure users and resources. Explore Azure Active Directory features # Azure Active Directory (Azure AD) is a cloud-based identity and access management service. This service helps employees access external resources, such as Microsoft 365, the Azure portal, and thousands of other software-as-a-service (SaaS) applications. Azure Active Directory also helps them access internal resources like apps on your corporate intranet network, along with any cloud apps developed for your organization. Who uses Azure AD? # Azure AD is intended for: IT admins: As an IT admin, use Azure AD to control access to your apps and your app resources based on your business requirements. For example, you can use Azure AD to require multi-factor authentication when accessing important organizational resources. You can also use Azure AD to automate user provisioning between your existing Windows Server AD and your cloud apps, including Microsoft 365. Finally, Azure AD gives you powerful tools to automatically help protect user identities and credentials and to meet your access governance requirements. App developers: As an app developer, you can use Azure AD as a standards-based approach for adding single sign-on (SSO) to your app, allowing it to work with a user's pre-existing credentials. Azure AD also provides APIs that can help you build personalized app experiences using existing organizational data. Microsoft 365, Office 365, Azure, or Dynamics Customer relationship management (CRM) Online subscribers: As a subscriber, you're already using Azure AD. Each Microsoft 365, Office 365, Azure, and Dynamics CRM Online tenant is automatically an Azure AD tenant. You can immediately start to manage access to your integrated cloud apps. What are the Azure AD licenses? # Microsoft Online business services, such as Microsoft 365 or Microsoft Azure, require Azure AD for sign-in activities and to help with identity protection. If you subscribe to any Microsoft Online business service, you automatically get Azure AD with access to all the free features. To enhance your Azure AD implementation, you can also add paid capabilities by upgrading to Azure Active Directory Premium P1 or Premium P2 licenses. Azure AD paid licenses are built on top of your existing free directory. The licenses provide self-service, enhanced monitoring, security reporting, and secure access for your mobile users. Azure Active Directory Free: Provides user and group management, on-premises directory synchronization, basic reports, self-service password change for cloud users, and single sign-on across Azure, Microsoft 365, and many popular SaaS apps. Azure Active Directory Premium P1: In addition to the Free features, P1 lets your hybrid users access both on-premises and cloud resources. It also supports advanced administration, such as dynamic groups, self-service group management, Microsoft Identity Manager, and cloud write-back capabilities, which allow self-service password reset for your on-premises users. Azure Active Directory Premium P2: In addition to the Free and P1 features, P2 also offers Azure Active Directory Identity Protection to help provide risk-based Conditional Access to your apps and critical company data and Privileged Identity Management to help discover, restrict, and monitor administrators and their access to resources and to provide just-in-time access when needed. \"Pay as you go\" feature licenses: You also get additional feature licenses, such as Azure Active Directory Business-to-Customer (B2C). B2C can help you provide identity and access management solutions for your customer-facing apps. Which features work in Azure AD? # After you choose your Azure AD license, you'll get access to some or all of the following features: Category Description Application management Manage your cloud and on-premises apps using Application Proxy, single sign-on, the My Apps portal, and Software as a Service (SaaS) apps. Authentication Manage Azure Active Directory self-service password reset, Multi-Factor Authentication, custom banned password list, and smart lockout. Azure Active Directory for developers Build apps that sign in all Microsoft identities, and get tokens to call Microsoft Graph, other Microsoft APIs, or custom APIs. Business-to-Business (B2B) Manages your guest users and external partners while maintaining control over your own corporate data. Business-to-Customer (B2C) Manages your guest users and external partners while maintaining control over your own corporate data. Conditional Access Manage access to your cloud apps. Device Management Manage how your cloud or on-premises devices access your corporate data. Domain services Join Azure virtual machines to a domain without using domain controllers. Enterprise users Manage license assignments, access to apps, and set up delegates using groups and administrator roles. Hybrid identity Use Azure Active Directory Connect and Connect Health to provide a single user identity for authentication and authorization to all resources, regardless of location (cloud or on-premises). Identity governance Manage your organization's identity through employee, business partner, vendor, service, and app access controls. You can also perform access reviews. Identity protection Detect potential vulnerabilities affecting your organization's identities, configure policies to respond to suspicious actions, and then take appropriate action to resolve them. Managed identities for Azure resources Provide your Azure services with an automatically managed identity in Azure AD that can authenticate any Azure AD-supported authentication service, including Key Vault. Privileged identity management (PIM) Manages, controls, and monitors access within your organization. This feature includes access to resources in Azure AD and Azure, and other Microsoft Online Services, like Microsoft 365 or Intune. Reports and monitoring Gain insights into the security and usage patterns in your environment. Self-managed Active Directory Domain Services, Azure Active Directory, and managed Azure Active Directory Domain Services # To provide applications, services, or devices access to a central identity, there are three common ways to use Active Directory-based services in Azure. This choice in identity solutions gives you the flexibility to use the most appropriate directory for your organization's needs. For example, if you mostly manage cloud-only users that run mobile devices, it may not make sense to build and run your own Active Directory Domain Services (AD DS) identity solution. Instead, you could use Azure Active Directory. Although the three Active Directory-based identity solutions share a common name and technology, they're designed to provide services that meet different customer demands. At a high level, these identity solutions and feature sets are: Azure Active Directory (Azure AD) - Cloud-based identity and mobile device management that provides user account and authentication services for resources such as Microsoft 365, the Azure portal, or SaaS applications. Azure AD can be synchronized with an on-premises AD DS environment to provide a single identity to users that works natively in the cloud. Active Directory Domain Services (AD DS) - Enterprise-ready lightweight directory access protocol (LDAP) server that provides key features such as identity and authentication, computer object management, group policy, and trusts. AD DS is a central component in many organizations with an on-premises IT environment and provides core user account authentication and computer management features. Azure Active Directory Domain Services (Azure AD DS) - Provides managed domain services with a subset of fully compatible traditional AD DS features such as domain join, group policy, LDAP, and Kerberos / New Technology LAN Manager (NTLM) authentication. Azure AD DS integrates with Azure AD, which can synchronize with an on-premises AD DS environment. This ability extends central identity use cases to traditional web applications that run in Azure as part of a lift-and-shift strategy. Azure AD DS and self-managed AD DS # If you have applications and services that need access to traditional authentication mechanisms such as Kerberos or NTLM, there are two ways to provide Active Directory Domain Services in the cloud: A managed domain that you create using Azure Active Directory Domain Services (Azure AD DS). Microsoft creates and manages the required resources. A self-managed domain that you create and configure using traditional resources such as virtual machines (VMs), Windows Server guest OS, and Active Directory Domain Services (AD DS). You then continue to administer these resources. With Azure AD DS, the core service components are deployed and maintained for you by Microsoft as a managed domain experience. You don't deploy, manage, patch, and secure the AD DS infrastructure for components like the VMs, Windows Server OS, or domain controllers (DCs). Azure AD DS provides a smaller subset of features to traditional self-managed AD DS environment, which reduces some of the design and management complexity. For example, there are no AD forests, domains, sites, and replication links to design and maintain. For applications and services that run in the cloud and need access to traditional authentication mechanisms such as Kerberos or NTLM, Azure AD DS provides a managed domain experience with a minimal amount of administrative overhead. When you deploy and run a self-managed AD DS environment, you must maintain all of the associated infrastructure and directory components. There's additional maintenance overhead with a self-managed AD DS environment, but you're then able to do additional tasks, such as extending the schema or create forest trusts. Common deployment models for a self-managed AD DS environment that provides identity to applications and services in the cloud include the following: Standalone cloud-only AD DS - Azure VMs are configured as domain controllers, and a separate, cloud-only AD DS environment is created. This AD DS environment doesn't integrate with an on-premises AD DS environment. A different set of credentials is used to sign in and administer VMs in the cloud. Resource forest deployment - Azure VMs are configured as domain controllers, and an AD DS domain that's part of an existing forest is created. A trust relationship is then configured to an on-premises AD DS environment. Other Azure VMs can domain-join this resource forest in the cloud. User authentication runs over a VPN / ExpressRoute connection to the on-premises AD DS environment. Extend on-premises domain to Azure - An Azure virtual network connects to an on-premises network using a VPN / ExpressRoute connection. Azure VMs connect to this Azure virtual network, which lets them domain-join to the on-premises AD DS environment. An alternative is to create Azure VMs and promote them as replica domain controllers from the on-premises AD DS domain. These domain controllers replicate over a VPN / ExpressRoute connection to the on-premises AD DS environment. The on-premises AD DS domain is effectively extended into Azure. The following table outlines some of the features you may need for your organization and the differences between a managed Azure AD DS domain or a self-managed AD DS domain: Feature Azure Active Directory Services (Azure AD DS) Self-managed AD DS Managed service \u2713 \u2715 Secure deployments \u2713 The administrator secures the deployment Domain Name System (DNS) server \u2713 (managed service) \u2713 Domain or Enterprise administrator privileges \u2715 \u2713 Domain join \u2713 \u2713 Domain authentication using New Technology LAN Manager (NTLM) and Kerberos \u2713 \u2713 Kerberos constrained delegation Resource-based Resource-based & account-based Custom organizational unit (OU) structure \u2713 \u2713 Group Policy \u2713 \u2713 Schema extensions \u2715 \u2713 Active Directory domain/forest trusts \u2713 (one-way outbound forest trusts only) \u2713 Secure Lightweight Directory Access Protocols (LDAPs) \u2713 \u2713 Lightweight Directory Access Protocol (LDAP) read \u2713 \u2713 Lightweight Directory Access Protocol (LDAP) write \u2713 (within the managed domain) \u2713 Geographical-distributed (Geo-distributed) deployments \u2713 \u2713 Azure AD DS and Azure AD # Azure AD lets you manage the identity of devices used by the organization and control access to corporate resources from those devices. Users can also register their personal device (a bring-your-own (BYO) model) with Azure AD, which provides the device with an identity. Azure AD then authenticates the device when a user signs in to Azure AD and uses the device to access secured resources. The device can be managed using Mobile Device Management (MDM) software like Microsoft Intune. This management ability lets you restrict access to sensitive resources to managed and policy-compliant devices. Traditional computers and laptops can also join Azure AD. This mechanism offers the same benefits of registering a personal device with Azure AD, such as allowing users to sign in to the device using their corporate credentials. Azure AD joined devices give you the following benefits: Single sign-on (SSO) to applications secured by Azure AD. Enterprise policy-compliant roaming of user settings across devices. Access to the Windows Store for Business using corporate credentials. Windows Hello for Business. Restricted access to apps and resources from devices compliant with corporate policy. Devices can be joined to Azure AD with or without a hybrid deployment that includes an on-premises AD DS environment. The following table outlines common device ownership models and how they would typically be joined to a domain: Type of device Device platforms Mechanism Personal devices Windows 10, iOS, Android, macOS Azure AD registered Organization-owned device not joined to on-premises AD DS Windows 10 Azure AD joined Organization-owned device joined to an on-premises AD DS Windows 10 Hybrid Azure AD joined On an Azure AD-joined or registered device, user authentication happens using modern OAuth / OpenID Connect-based protocols. These protocols are designed to work over the internet, so are great for mobile scenarios where users access corporate resources from anywhere. With Azure AD DS-joined devices, applications can use the Kerberos and New Technology LAN Manager (NTLM) protocols for authentication, so can support legacy applications migrated to run on Azure VMs as part of a lift-and-shift strategy. The following table outlines differences in how the devices are represented and can authenticate themselves against the directory: Aspect Azure AD-joined Azure AD DS-joined Device controlled by Azure AD Azure AD Domain Services managed domain Representation in the directory Device objects in the Azure AD directory Computer objects in the Azure AD DS managed domain Authentication Open Authorization OAuth / OpenID Connect-based protocols Kerberos and NTLM protocols Management Mobile Device Management (MDM) software like Intune Group Policy Networking Works over the internet Must be connected to, or peered with, the virtual network where the managed domain is deployed Great for... End-user mobile or desktop devices Server VMs deployed in Azure If on-premises AD DS and Azure AD are configured for federated authentication using Active Directory Federation Services (ADFS), then there's no (current/valid) password hash available in Azure DS. Azure AD user accounts created before fed auth was implemented might have an old password hash that doesn't match a hash of their on-premises password. Hence Azure AD DS won't validate the user's credentials. Investigate roles in Azure AD # Categories of Azure AD roles # Azure AD built-in roles differ in where they can be used, which fall into the following three broad categories. Azure AD-specific roles: These roles grant permissions to manage resources within Azure AD only. For example, User Administrator, Application Administrator, and Groups Administrator all grant permissions to manage resources that live in Azure AD. Service-specific roles: For major Microsoft 365 services (non-Azure AD), we have built service-specific roles that grant permissions to manage all features within the service. For example, Exchange Administrator, Intune Administrator, SharePoint Administrator, and Teams Administrator roles can manage features with their respective services. Exchange Administrator can manage mailboxes, Intune Administrator can manage device policies, SharePoint Administrator can manage site collections, Teams Administrator can manage call qualities, and so on. Cross-service roles: There are some roles that span services. We have two global roles - Global Administrator and Global Reader. All Microsoft 365 services honor these two roles. Also, there are some security-related roles like Security Administrator and Security Reader that grant access across multiple security services within Microsoft 365. For example, using Security Administrator roles in Azure AD, you can manage Microsoft 365 Defender portal, Microsoft Defender Advanced Threat Protection, and Microsoft Defender for Cloud Apps. Similarly, in the Compliance Administrator role, you can manage Compliance-related settings in the Compliance portal, Exchange, and so on. The following table is offered as an aid to understanding these role categories. The categories are named arbitrarily and aren't intended to imply any other capabilities beyond the documented Azure AD role permissions. Category Role Azure AD-specific roles Application Administrator Application Developer Authentication Administrator Business to consumer (B2C) Identity Experience Framework (IEF) Keyset Administrator Business to consumer (B2C) Identity Experience Framework (IEF) Policy Administrator Cloud Application Administrator Cloud Device Administrator Conditional Access Administrator Device Administrators Directory Readers Directory Synchronization Accounts Directory Writers External ID User Flow Administrator External ID User Flow Attribute Administrator External Identity Provider Administrator Groups Administrator Guest Inviter Helpdesk Administrator Hybrid Identity Administrator License Administrator Partner Tier1 Support Partner Tier2 Support Password Administrator Privileged Authentication Administrator Privileged Role Administrator Reports Reader User Administrator Cross-service roles Global Administrator Compliance Administrator Compliance Data Administrator Global Reader Security Administrator Security Operator Security Reader Service Support Administrator Service-specific roles Azure DevOps Administrator Azure Information Protection Administrator Billing Administrator Customer relationship management (CRM) Service Administrator Customer Lockbox Access Approver Desktop Analytics Administrator Exchange Service Administrator Insights Administrator Insights Business Leader Intune Service Administrator Kaizala Administrator Lync Service Administrator Message Center Privacy Reader Message Center Reader Modern Commerce User Network Administrator Office Apps Administrator Power BI Service Administrator Power Platform Administrator Printer Administrator Printer Technician Search Administrator Search Editor SharePoint Service Administrator Teams Communications Administrator Teams Communications Support Engineer Teams Communications Support Specialist Teams Devices Administrator Teams Administrator Azure AD built-in roles # In Azure Active Directory (Azure AD), if another administrator or nonadministrator needs to manage Azure AD resources, you assign them an Azure AD role that provides the permissions they need. For example, you can assign roles to allow adding or changing users, resetting user passwords, managing user licenses, or managing domain names. All Roles # Role Description Application Administrator Users in this role can create and manage all aspects of enterprise applications, application registrations, and application proxy settings. Users assigned to this role aren't added as owners when creating new application registrations or enterprise applications. This role also grants the ability to consent for delegated permissions and application permissions, except for application permissions for Microsoft Graph. Application Developer Can create application registrations independent of the Users can register applications setting. Attack Payload Author Users in this role can create attack payloads but not actually launch or schedule them. Attack payloads are then available to all administrators in the tenant, who can use them to create a simulation. Attack Simulation Administrator Users in this role can create and manage all aspects of attack simulation creation, launch/scheduling of a simulation, and the review of simulation results. Members of this role have this access for all simulations in the tenant. Attribute Assignment Administrator Users with this role can assign and remove custom security attribute keys and values for supported Azure AD objects such as users, service principals, and devices. By default, Global Administrator and other administrator roles don't have permissions to read, define, or assign custom security attributes. To work with custom security attributes, you must be assigned one of the custom security attribute roles. Attribute Assignment Reader Users with this role can read custom security attribute keys and values for supported Azure AD objects. By default, Global Administrator and other administrator roles don't have permissions to read, define, or assign custom security attributes. You must be assigned one of the custom security attribute roles to work with custom security attributes. Attribute Definition Administrator Users with this role can define a valid set of custom security attributes that can be assigned to supported Azure AD objects. This role can also activate and deactivate custom security attributes. By default, Global Administrator and other administrator roles don't have permissions to read, define, or assign custom security attributes. To work with custom security attributes, you must be assigned one of the custom security attribute roles. Authentication Administrator Assign the Authentication Administrator role to users who need to do the following: -Set or reset any authentication method (including passwords) for nonadministrators and some roles. -Require users who are nonadministrators or assigned to some roles to re-register against existing nonpassword credentials (for example, Multifactor authentication (MFA) or Fast ID Online (FIDO), and can also revoke remember MFA on the device, which prompts for MFA on the next sign-in. -Perform sensitive actions for some users. -Create and manage support tickets in Azure and the Microsoft 365 admin center. Users with this role can't do the following tasks: -Can't change the credentials or reset MFA for members and owners of a role-assignable group. -Can't manage MFA settings in the legacy MFA management portal or Hardware OATH tokens. The same functions can be accomplished using the Set-MsolUser commandlet Azure AD PowerShell module. Authentication Policy Administrator Assign the Authentication Policy Administrator role to users who need to do the following: -Configure the authentication methods policy, tenant-wide MFA settings, and password protection policy that determine which methods each user can register and use. -Manage Password Protection settings: smart lockout configurations and updating the custom banned passwords list. -Create and manage verifiable credentials. -Create and manage Azure support tickets. Users with this role can't do the following tasks: -Can't update sensitive properties. -Can't delete or restore users. -Can't manage MFA settings in the legacy MFA management portal or Hardware OATH tokens. Azure AD Joined Device Local Administrator This role is available for assignment only as another local administrator in Device settings. Users with this role become local machine administrators on all Windows 10 devices that are joined to Azure Active Directory. They don't have the ability to manage device objects in Azure Active Directory. Azure DevOps Administrator Users with this role can manage all enterprise Azure DevOps policies applicable to all Azure DevOps organizations backed by the Azure AD. Users in this role can manage these policies by navigating to any Azure DevOps organization that is backed by the company's Azure AD. Users in this role can claim ownership of orphaned Azure DevOps organizations. This role grants no other Azure DevOps-specific permissions (for example, Project Collection Administrators) inside any of the Azure DevOps organizations backed by the company's Azure AD organization. Azure Information Protection Administrator Users with this role have all permissions in the Azure Information Protection service. This role allows configuring labels for the Azure Information Protection policy, managing protection templates, and activating protection. This role doesn't grant any permissions in Identity Protection Center, Privileged Identity Management, Monitor Microsoft 365 Service Health, or Office 365 Security and compliance center. Business-to-Consumer (B2C) Identity Experience Framework (IEF) Keyset Administrator Users can create and manage policy keys and secrets for token encryption, token signatures, and claim encryption/decryption. By adding new keys to existing key containers, this limited administrator can roll over secrets as needed without impacting existing applications. This user can see the full content of these secrets and their expiration dates even after their creation. Business-to-Consumer (B2C) Identity Experience Framework (IEF) Policy Administrator Users in this role have the ability to create, read, update, and delete all custom policies in Azure AD B2C and therefore have full control over the Identity Experience Framework in the relevant Azure AD B2C organization. By editing policies, this user can establish direct federation with external identity providers, change the directory schema, change all user-facing content HyperText Markup Language (HTML), Cascading Style Sheets (CSS), JavaScript), change the requirements to complete authentication, create new users, send user data to external systems including full migrations, and edit all user information including sensitive fields like passwords and phone numbers. Conversely, this role can't change the encryption keys or edit the secrets used for federation in the organization. Billing Administrator Makes purchases, manages subscriptions, manages support tickets, and monitors service health. Cloud App Security Administrator Users with this role have full permissions in Defender for Cloud Apps. They can add administrators, add Microsoft Defender for Cloud Apps policies and settings, upload logs, and perform governance actions. Cloud Application Administrator Users in this role have the same permissions as the Application Administrator role, excluding the ability to manage application proxy. This role grants the ability to create and manage all aspects of enterprise applications and application registrations. Users assigned to this role aren't added as owners when creating new application registrations or enterprise applications. This role also grants the ability to consent for delegated permissions and application permissions, except for application permissions for Microsoft Graph. Cloud Device Administrator Users in this role can enable, disable, and delete devices in Azure AD and read Windows 10 BitLocker keys (if present) in the Azure portal. The role doesn't grant permissions to manage any other properties on the device. Compliance Administrator Users with this role have permissions to manage compliance-related features in the Microsoft Purview compliance portal, Microsoft 365 admin center, Azure, and Office 365 Security and compliance center. Assignees can also manage all features within the Exchange admin center and create support tickets for Azure and Microsoft 365. Compliance Data Administrator Users with this role have permissions to track data in the Microsoft Purview compliance portal, Microsoft 365 admin center, and Azure. Users can also track compliance data within the Exchange admin center, Compliance Manager, and Teams and Skype for Business admin center and create support tickets for Azure and Microsoft 365. Conditional Access Administrator Users with this role have the ability to manage Azure Active Directory Conditional Access settings. Customer Lockbox Access Approver Manages Microsoft Purview Customer Lockbox requests in your organization. They receive email notifications for Customer Lockbox requests and can approve and deny requests from the Microsoft 365 admin center. They can also turn the Customer Lockbox feature on or off. Only Global Administrators can reset the passwords of people assigned to this role. Desktop Analytics Administrator Users in this role can manage the Desktop Analytics service, including viewing asset inventory, creating deployment plans, and viewing deployment and health status. Directory Readers Users in this role can read basic directory information. This role should be used for: -Granting a specific set of guest users read access instead of granting it to all guest users. -Granting a specific set of nonadmin users access to the Azure portal when \"Restrict access to Azure AD portal to admins only\" is set to \"Yes\". -Granting service principals access to the directory where Directory.Read.All isn't an option. Directory Synchronization Accounts Don't use. This role is automatically assigned to the Azure AD Connect service and isn't intended or supported for any other use. Directory Writers Users in this role can read and update basic information of users, groups, and service principals. Domain Name Administrator Users with this role can manage (read, add, verify, update, and delete) domain names. They can also read directory information about users, groups, and applications, as these objects possess domain dependencies. For on-premises environments, users with this role can configure domain names for federation so that associated users are always authenticated on-premises. These users can then sign into Azure AD-based services with their on-premises passwords via single sign-on. Federation settings need to be synced via Azure AD Connect so users also have permissions to manage Azure AD Connect. Dynamics 365 Administrator Users with this role have global permissions within Microsoft Dynamics 365 Online when the service is present, and the ability to manage support tickets and monitor service health. Edge Administrator Users in this role can create and manage the enterprise site list required for Internet Explorer mode on Microsoft Edge. This role grants permissions to create, edit, and publish the site list and additionally allows access to manage support tickets. Exchange Administrator Users with this role have global permissions within Microsoft Exchange Online, when the service is present. Also has the ability to create and manage all Microsoft 365 groups, manage support tickets, and monitor service health. Exchange Recipient Administrator Users with this role have read access to recipients and write access to the attributes of those recipients in Exchange Online. External ID User Flow Administrator Users with this role can create and manage user flows (also called \"built-in\" policies) in the Azure portal. These users can customize HTML/CSS/JavaScript content, change MFA requirements, select claims in the token, manage API connectors and their credentials, and configure session settings for all user flows in the Azure AD organization. On the other hand, this role doesn't include the ability to review user data or make changes to the attributes that are included in the organization schema. Changes to Identity Experience Framework policies (also known as custom policies) are also outside the scope of this role. External ID User Flow Attribute Administrator Users with this role add or delete custom attributes available to all user flows in the Azure AD organization. Users with this role can change or add new elements to the end-user schema and impact the behavior of all user flows, and indirectly result in changes to what data may be asked of end users and ultimately sent as claims to applications. This role can't edit user flows. External Identity Provider Administrator This administrator manages federation between Azure AD organizations and external identity providers. With this role, users can add new identity providers and configure all available settings (for example, authentication path, service ID, assigned key containers). This user can enable the Azure AD organization to trust authentications from external identity providers. The resulting impact on end-user experiences depends on the type of organization: -Azure AD organizations for employees and partners: The addition of a federation (for example, with Gmail) immediately impacts all guest invitations not yet redeemed. See Adding Google as an identity provider for B2B guest users. -Azure Active Directory B2C organizations: The addition of a federation (for example, with Facebook, or with another Azure AD organization) doesn't immediately impact end-user flows until the identity provider is added as an option in a user flow (also called a built-in policy). Global Administrator Users with this role have access to all administrative features in Azure Active Directory, and services that use Azure Active Directory identities like the Microsoft 365 Defender portal, the Microsoft Purview compliance portal, Exchange Online, SharePoint Online, and Skype for Business Online. Furthermore, Global Administrators can elevate their access to manage all Azure subscriptions and management groups. This allows Global Administrators to get full access to all Azure resources using the respective Azure AD Tenant. The person who signs up for the Azure AD organization becomes a Global Administrator. There can be more than one Global Administrator at your company. Global Administrators can reset the password for any user and all other administrators. Global Administrator As a best practice, Microsoft recommends that you assign the Global Administrator role to fewer than five people in your organization. Global Reader Users in this role can read settings and administrative information across Microsoft 365 services but can't take management actions. Global Reader is the read-only counterpart to Global Administrator. Assign Global Reader instead of Global Administrator for planning, audits, or investigations. Use Global Reader in combination with other limited admin roles like Exchange Administrator to make it easier to get work done without the assigning the Global Administrator role. Global Reader works with Microsoft 365 admin center, Exchange admin center, SharePoint admin center, Teams admin center, Security center, compliance center, Azure AD admin center, and Device Management admin center. Users with this role can't do the following tasks: -Can't access the Purchase Services area in the Microsoft 365 admin center. Groups Administrator Users in this role can create/manage groups and its settings like naming and expiration policies. It's important to understand that assigning a user to this role gives them the ability to manage all groups in the organization across various workloads like Teams, SharePoint, Yammer in addition to Outlook. Also the user is able to manage the various groups settings across various admin portals like Microsoft admin center, Azure portal, and workload specific ones like Teams and SharePoint admin centers. Guest Inviter Users in this role can manage Azure Active Directory B2B guest user invitations when the Members can invite user setting is set to No. Helpdesk Administrator Users with this role can change passwords, invalidate refresh tokens, create and manage support requests with Microsoft for Azure and Microsoft 365 services, and monitor service health. Invalidating a refresh token forces the user to sign in again. Whether a Helpdesk Administrator can reset a user's password and invalidate refresh tokens depends on the role the user is assigned. Users with this role can't do the following: -Can't change the credentials or reset MFA for members and owners of a role-assignable group. Hybrid Identity Administrator Users in this role can create, manage and deploy provisioning configuration setup from AD to Azure AD using Cloud Provisioning and manage Azure AD Connect, Pass-through Authentication (PTA), Password hash synchronization (PHS), Seamless single sign-on (Seamless SSO), and federation settings. Users can also troubleshoot and monitor logs using this role. Identity Governance Administrator Users with this role can manage Azure AD identity governance configuration, including access packages, access reviews, catalogs and policies, ensuring access is approved and reviewed and guest users who no longer need access are removed. Insights Administrator Users in this role can access the full set of administrative capabilities in the Microsoft Viva Insights app. This role has the ability to read directory information, monitor service health, file support tickets, and access the Insights Administrator settings aspects. Insights Analyst Assign the Insights Analyst role to users who need to do the following tasks: -Analyze data in the Microsoft Viva Insights app, but can't manage any configuration settings -Create, manage, and run queries -View basic settings and reports in the Microsoft 365 admin center -Create and manage service requests in the Microsoft 365 admin center Insights Business Leader Users in this role can access a set of dashboards and insights via the Microsoft Viva Insights app. This includes full access to all dashboards and presented insights and data exploration functionality. Users in this role don't have access to product configuration settings, which is the responsibility of the Insights Administrator role. Intune Administrator Users with this role have global permissions within Microsoft Intune Online, when the service is present. Additionally, this role contains the ability to manage users and devices to associate policy and create and manage groups. This role can create and manage all security groups. However, Intune Administrator doesn't have admin rights over Office groups. That means the admin can't update owners or memberships of all Office groups in the organization. However, you can manage the Office group that's created, which comes as a part of end-user privileges. So, any Office group (not security group) that you create should be counted against your quota of 250. Kaizala Administrator Users with this role have global permissions to manage settings within Microsoft Kaizala, when the service is present and the ability to manage support tickets and monitor service health. Additionally, the user can access reports related to adoption and usage of Kaizala by Organization members and business reports generated using the Kaizala actions. Knowledge Administrator Users in this role have full access to all knowledge, learning and intelligent features settings in the Microsoft 365 admin center. They have a general understanding of the suite of products, licensing details and have responsibility to control access. Knowledge Administrator can create and manage content, like topics, acronyms and learning resources. Additionally, these users can create content centers, monitor service health, and create service requests. Knowledge Manager Users in this role can create and manage content, like topics, acronyms and learning content. These users are primarily responsible for the quality and structure of knowledge. This user has full rights to topic management actions to confirm a topic, approve edits, or delete a topic. This role can also manage taxonomies as part of the term store management tool and create content centers. License Administrator Users in this role can add, remove, and update license assignments on users, groups (using group-based licensing), and manage the usage location on users. The role doesn't grant the ability to purchase or manage subscriptions, create or manage groups, or create or manage users beyond the usage location. This role has no access to view, create, or manage support tickets. Lifecycle Workflows Administrator Assign the Lifecycle Workflows Administrator role to users who need to do the following tasks: -Create and manage all aspects of workflows and tasks associated with Lifecycle Workflows in Azure AD -Check the execution of scheduled workflows -Launch on-demand workflow runs -Inspect workflow execution logs Message Center Privacy Reader Users in this role can monitor all notifications in the Message Center, including data privacy messages. Message Center Privacy Readers get email notifications including those related to data privacy and they can unsubscribe using Message Center Preferences. Only the Global Administrator and the Message Center Privacy Reader can read data privacy messages. Additionally, this role contains the ability to view groups, domains, and subscriptions. This role has no permission to view, create, or manage service requests. Message Center Reader Users in this role can monitor notifications and advisory health updates in Message center for their organization on configured services such as Exchange, Intune, and Microsoft Teams. Message Center Readers receive weekly email digests of posts, updates, and can share message center posts in Microsoft 365. In Azure AD, users assigned to this role will only have read-only access on Azure AD services such as users and groups. This role has no access to view, create, or manage support tickets. Microsoft Hardware Warranty Administrator Assign the Microsoft Hardware Warranty Administrator role to users who need to do the following tasks: -Create new warranty claims for Microsoft manufactured hardware, like Surface and HoloLens -Search and read opened or closed warranty claims -Search and read warranty claims by serial number -Create, read, update, and delete shipping addresses -Read shipping status for open warranty claims -Create and manage service requests in the Microsoft 365 admin center -Read Message center announcements in the Microsoft 365 admin center Microsoft Hardware Warranty Specialist Assign the Microsoft Hardware Warranty Specialist role to users who need to do the following tasks: -Create new warranty claims for Microsoft manufactured hardware, like Surface and HoloLens -Read warranty claims that they created -Read and update existing shipping addresses -Read shipping status for open warranty claims they created -Create and manage service requests in the Microsoft 365 admin center Modern Commerce User Don't use. This role is automatically assigned from Commerce, and isn't intended or supported for any other use. The Modern Commerce User role gives certain users permission to access Microsoft 365 admin center and see the left navigation entries for Home, Billing, and Support. The content available in these areas is controlled by commerce-specific roles assigned to users to manage products that they bought for themselves or your organization. This might include tasks like paying bills, or for access to billing accounts and billing profiles. Users with the Modern Commerce User role typically have administrative permissions in other Microsoft purchasing systems, but don't have Global Administrator or Billing Administrator roles used to access the admin center. Network Administrator Users in this role can review network perimeter architecture recommendations from Microsoft that are based on network telemetry from their user locations. Network performance for Microsoft 365 relies on careful enterprise customer network perimeter architecture, which is generally user location specific. This role allows for editing of discovered user locations and configuration of network parameters for those locations to facilitate improved telemetry measurements and design recommendations Office Apps Administrator Users in this role can manage Microsoft 365 apps' cloud settings. This includes managing cloud policies, self-service download management and the ability to view Office apps related report. This role additionally grants the ability to manage support tickets, and monitor service health within the main admin center. Users assigned to this role can also manage communication of new features in Office apps. Organizational Messages Writer Assign the Organizational Messages Writer role to users who need to do the following tasks: -Write, publish, and delete organizational messages using Microsoft 365 admin center or Microsoft Endpoint Manager -Manage organizational message delivery options using Microsoft 365 admin center or Microsoft Endpoint Manager -Read organizational message delivery results using Microsoft 365 admin center or Microsoft Endpoint Manager -View usage reports and most settings in the Microsoft 365 admin center, but can't make changes Partner Tier1 Support Don't use. This role has been deprecated and will be removed from Azure AD in the future. This role is intended for use by a few Microsoft resale partners, and isn't intended for general use. Partner Tier2 Support Don't use. This role has been deprecated and will be removed from Azure AD in the future. This role is intended for use by a few Microsoft resale partners, and isn't intended for general use. Password Administrator Users with this role have limited ability to manage passwords. This role doesn't grant the ability to manage service requests or monitor service health. Whether a Password Administrator can reset a user's password depends on the role the user is assigned.Users with this role can't do the following tasks: -Can't change the credentials or reset MFA for members and owners of a role-assignable group. Permissions Management Administrator Assign the Permissions Management Administrator role to users who need to do the following tasks: -Manage all aspects of Entra Permissions Management, when the service is present Power Business Intelligence (BI) Administrator Users with this role have global permissions within Microsoft Power BI, when the service is present and the ability to manage support tickets and monitor service health. Power Platform Administrator Users in this role can create and manage all aspects of environments, Power Apps, Flows, Data Loss Prevention policies. Additionally, users with this role have the ability to manage support tickets and monitor service health. Printer Administrator Users in this role can register printers and manage all aspects of all printer configurations in the Microsoft Universal Print solution, including the Universal Print Connector settings. They can consent to all delegated print permission requests. Printer Administrators also have access to print reports. Printer Technician Users with this role can register printers and manage printer status in the Microsoft Universal Print solution. They can also read all connector information. Key task a Printer Technician can't do is set user permissions on printers and sharing printers. Privileged Authentication Administrator Assign the Privileged Authentication Administrator role to users who need to do the following tasks: -Set or reset any authentication method (including passwords) for any user, including Global Administrators. -Delete or restore any users, including Global Administrators. For more information, see Who can perform sensitive actions. -Force users to re-register against existing nonpassword credential (such as MFA or FIDO) and revoke remember MFA on the device, prompting for MFA on the next sign-in of all users. -Update sensitive properties for all users. For more information, see Who can perform sensitive actions. -Create and manage support tickets in Azure and the Microsoft 365 admin center. Users with this role can't do the following tasks: -Can't manage per-user MFA in the legacy MFA management portal. The same functions can be accomplished using the Set-MsolUser commandlet Azure AD PowerShell module. Privileged Role Administrator Users with this role can manage role assignments in Azure Active Directory and within Azure AD Privileged Identity Management. They can create and manage groups that can be assigned to Azure AD roles. In addition, this role allows management of all aspects of Privileged Identity Management and administrative units. Privileged Role Administrator This role grants the ability to manage assignments for all Azure AD roles including the Global Administrator role. This role doesn't include any other privileged abilities in Azure AD like creating or updating users. However, users assigned to this role can grant themselves or others another privilege by assigning extra roles. Reports Reader Users with this role can view usage reporting data and the reports dashboard in Microsoft 365 admin center and the adoption context pack in Power Business Intelligence (Power BI). Additionally, the role provides access to all sign-in logs, audit logs, and activity reports in Azure AD and data returned by the Microsoft Graph reporting API. A user assigned to the Reports Reader role can access only relevant usage and adoption metrics. They don't have any admin permissions to configure settings or access the product-specific admin centers like Exchange. This role has no access to view, create, or manage support tickets. Search Administrator Users in this role have full access to all Microsoft Search management features in the Microsoft 365 admin center. Additionally, these users can view the message center, monitor service health, and create service requests. Search Editor Users in this role can create, manage, and delete content for Microsoft Search in the Microsoft 365 admin center, including bookmarks, questions and answers, and locations. Security Administrator Users with this role have permissions to manage security-related features in the Microsoft 365 Defender portal, Azure Active Directory Identity Protection, Azure Active Directory Authentication, Azure Information Protection, and Office 365 Security and compliance center. Security Operator Users with this role can manage alerts and have global read-only access on security-related features, including all information in Microsoft 365 security center, Azure Active Directory, Identity Protection, Privileged Identity Management and Office 365 Security & compliance center. Security Reader Users with this role have global read-only access on security-related feature, including all information in Microsoft 365 security center, Azure Active Directory, Identity Protection, Privileged Identity Management, and the ability to read Azure Active Directory sign-in reports and audit logs, and in Office 365 Security and compliance center. Service Support Administrator Users with this role can create and manage support requests with Microsoft for Azure and Microsoft 365 services, and view the service dashboard and message center in the Azure portal and Microsoft 365 admin center. SharePoint Administrator Users with this role have global permissions within Microsoft SharePoint Online, when the service is present, and the ability to create and manage all Microsoft 365 groups, manage support tickets, and monitor service health. Skype for Business Administrator Users with this role have global permissions within Microsoft Skype for Business, when the service is present, and manage Skype-specific user attributes in Azure Active Directory. Additionally, this role grants the ability to manage support tickets and monitor service health, and to access the Teams and Skype for Business admin center. The account must also be licensed for Teams or it can't run Teams PowerShell cmdlets. Teams Administrator Users in this role can manage all aspects of the Microsoft Teams workload via the Microsoft Teams and Skype for Business admin center and the respective PowerShell modules. This includes, among other areas, all management tools related to telephony, messaging, meetings, and the teams themselves. This role additionally grants the ability to create and manage all Microsoft 365 groups, manage support tickets, and monitor service health. Teams Communications Administrator Users in this role can manage aspects of the Microsoft Teams workload related to voice and telephony. This includes the management tools for telephone number assignment, voice and meeting policies, and full access to the call analytics toolset. Teams Communications Support Engineer Users in this role can troubleshoot communication issues within Microsoft Teams and Skype for Business using the user call troubleshooting tools in the Microsoft Teams and Skype for Business admin center. Users in this role can view full call record information for all participants involved. This role has no access to view, create, or manage support tickets. Teams Communications Support Specialist Users in this role can troubleshoot communication issues within Microsoft Teams and Skype for Business using the user call troubleshooting tools in the Microsoft Teams and Skype for Business admin center. Users in this role can only view user details in the call for the specific user they've looked up. This role has no access to view, create, or manage support tickets. Teams Devices Administrator Users with this role can manage Teams-certified devices from the Teams admin center. This role allows viewing all devices at single glance, with ability to search and filter devices. The user can check details of each device including logged-in account, make and model of the device. The user can change the settings on the device and update the software versions. This role doesn't grant permissions to check Teams activity and call quality of the device. Tenant Creator Assign the Tenant Creator role to users who need to do the following tasks: -Create both Azure Active Directory and Azure Active Directory B2C tenants even if the tenant creation toggle is turned off in the user settings Usage Summary Reports Reader Users with this role can access tenant level aggregated data and associated insights in Microsoft 365 admin center for Usage and Productivity Score but can't access any user level details or insights. In Microsoft 365 admin center for the two reports, we differentiate between tenant level aggregated data and user level details. This role gives an extra layer of protection on individual user identifiable data, which was requested by both customers and legal teams. User Administrator Assign the User Administrator role to users who need to do the following tasks: -Create users -Update most user properties for all users, including all administrators -Update sensitive properties (including user principal name) for some users -Disable or enable some users -Delete or restore some users -Create and manage user views -Create and manage all groups -Assign licenses for all users, including all administrators -Reset passwords -Invalidate refresh tokens -Update (FIDO) device keys -Update password expiration policies -Create and manage support tickets in Azure and the Microsoft 365 admin center -Monitor service healthUsers with this role can't do the following tasks: -Can't manage MFA. -Can't change the credentials or reset MFA for members and owners of a role-assignable group. -Can't manage shared mailboxes User Administrator Users with this role can change passwords for people who may have access to sensitive or private information or critical configuration inside and outside of Azure Active Directory. Changing the password of a user may mean the ability to assume that user's identity and permissions. For example: -Application Registration and Enterprise Application owners, who can manage credentials of apps they own. Those apps may have privileged permissions in Azure AD and elsewhere not granted to User Administrators. Through this path, a User Administrator may be able to assume the identity of an application owner and then further assume the identity of a privileged application by updating the credentials for the application. -Azure subscription owners, who may have access to sensitive or private information or critical configuration in Azure. -Security Group and Microsoft 365 group owners, who can manage group membership. Those groups may grant access to sensitive or private information or critical configuration in Azure AD and elsewhere. -Administrators in other services outside of Azure AD like Exchange Online, Office Security and compliance center, and human resources systems. -Nonadministrators like executives, legal counsel, and human resources employees who may have access to sensitive or private information. Virtual Visits Administrator Users with this role can do the following tasks: -Manage and configure all aspects of Virtual Visits in Bookings in the Microsoft 365 admin center, and in the Teams Electronic Health Record (EHR) connector -View usage reports for Virtual Visits in the Teams admin center, Microsoft 365 admin center, and Power BI -View features and settings in the Microsoft 365 admin center, but can't edit any settings Windows 365 Administrator Users with this role have global permissions on Windows 365 resources, when the service is present. Additionally, this role contains the ability to manage users and devices in order to associate policy and create and manage groups. This role can create and manage security groups, but doesn't have administrator rights over Microsoft 365 groups. That means administrators can't update owners or memberships of Microsoft 365 groups in the organization. However, they can manage the Microsoft 365 group they create, which is a part of their end-user privileges. So, any Microsoft 365 group (not security group) they create is counted against their quota of 250. Assign the Windows 365 Administrator role to users who need to do the following tasks: -Manage Windows 365 Cloud PCs in Microsoft Endpoint Manager -Enroll and manage devices in Azure AD, including assigning users and policies -Create and manage security groups, but not role-assignable groups -View basic properties in the Microsoft 365 admin center -Read usage reports in the Microsoft 365 admin center -Create and manage support tickets in Azure and the Microsoft 365 admin center Windows Update Deployment Administrator Users in this role can create and manage all aspects of Windows Update deployments through the Windows Update for Business deployment service. The deployment service enables users to define settings for when and how updates are deployed, and specify which updates are offered to groups of devices in their tenant. It also allows users to monitor the update progress. Yammer Administrator Assign the Yammer Administrator role to users who need to do the following tasks: -Manage all aspects of Yammer -Create, manage, and restore Microsoft 365 Groups, but not role-assignable groups -View the hidden members of Security groups and Microsoft 365 groups, including role assignable groups -Read usage reports in the Microsoft 365 admin center -Create and manage service requests in the Microsoft 365 admin center -View announcements in the Message center, but not security announcements -View service health # Deploy Azure AD Domain Services Azure Active Directory Domain Services (Azure AD DS) provides managed domain services such as domain join, group policy, lightweight directory access protocol (LDAP), and Kerberos/New Technology LAN Manager (NTLM) authentication. You use these domain services without the need to deploy, manage, and patch domain controllers (DCs) in the cloud. An Azure AD DS managed domain lets you run legacy applications in the cloud that can't use modern authentication methods or where you don't want directory lookups to always go back to an on-premises AD DS environment. You can lift and shift those legacy applications from your on-premises environment into a managed domain without needing to manage the AD DS environment in the cloud. Azure AD DS integrates with your existing Azure AD tenant. This integration lets users sign in to services and applications connected to the managed domain using their existing credentials. You can also use existing groups and user accounts to secure access to resources. These features provide a smoother lift-and-shift of on-premises resources to Azure. How does Azure AD DS work? # When you create an Azure AD DS managed domain, you define a unique namespace. This namespace is the domain name, such as aaddscontoso.com. Two Windows Server domain controllers (DCs) are then deployed into your selected Azure region. This deployment of DCs is known as a replica set. You don't need to manage, configure, or update these DCs. The Azure platform handles the DCs as part of the managed domain, including backups and encryption at rest using Azure Disk Encryption. A managed domain is configured to perform a one-way synchronization from Azure AD to provide access to a central set of users, groups, and credentials. You can create resources directly in the managed domain, but they aren't synchronized back to Azure AD. Applications, services, and VMs in Azure that connect to the managed domain can then use common AD DS features such as domain join, group policy, LDAP, and Kerberos/NTLM authentication. In a hybrid environment with an on-premises AD DS environment, Azure AD Connect synchronizes identity information with Azure AD, which is then synchronized to the managed domain. Azure AD DS replicates identity information from Azure AD, so it works with Azure AD tenants that are cloud-only or synchronized with an on-premises AD DS environment. The same set of Azure AD DS features exists for both environments. If you have an existing on-premises AD DS environment, you can synchronize user account information to provide a consistent identity for users. For cloud-only environments, you don't need a traditional on-premises AD DS environment to use the centralized identity services of Azure AD DS. You can expand a managed domain to have more than one replica set per Azure AD tenant. Replica sets can be added to any peered virtual network in any Azure region that supports Azure AD DS. Additional replica sets in different Azure regions provide geographical disaster recovery for legacy applications if an Azure region goes offline. Azure AD DS features and benefits # To provide identity services to applications and VMs in the cloud, Azure AD DS is fully compatible with a traditional AD DS environment for operations such as domain-join, secure LDAP (LDAPS), Group Policy, DNS management, and LDAP bind and read support. LDAP write support is available for objects created in the managed domain but not resources synchronized from Azure AD. The following features of Azure AD DS simplify deployment and management operations: Simplified deployment experience: Azure AD DS is enabled for your Azure AD tenant using a single wizard in the Azure portal. Integrated with Azure AD: User accounts, group memberships, and credentials are automatically available from your Azure AD tenant. New users, groups, or changes to attributes from your Azure AD tenant or your on-premises AD DS environment are automatically synchronized to Azure AD DS. Accounts in external directories linked to your Azure AD aren't available in Azure AD DS. Credentials aren't available for those external directories, so they can't be synchronized into a managed domain. Use your corporate credentials/passwords: Passwords for users in Azure AD DS are the same as in your Azure AD tenant. Users can use their corporate credentials to domain-join machines, sign in interactively or over a remote desktop, and authenticate against the managed domain. NTLM and Kerberos authentication: With support for NTLM and Kerberos authentication, you can deploy applications that rely on Windows-integrated authentication. High availability: Azure AD DS includes multiple domain controllers, which provide high availability for your managed domain. This high availability guarantees service uptime and resilience to failures. In regions that support Azure Availability Zones, these domain controllers are distributed across zones for additional resiliency. Replica sets can also be used to provide geographical disaster recovery for legacy applications if an Azure region goes offline. Some key aspects of a managed domain include the following: - The managed domain is a stand-alone domain. It isn't an extension of an on-premises domain. - If needed, you can create one-way outbound forest trusts from Azure AD DS to an on-premises AD DS environment. - Your IT team doesn't need to manage, patch, or monitor domain controllers for this managed domain. For hybrid environments that run AD DS on-premises, you don't need to manage AD replication to the managed domain. User accounts, group memberships, and credentials from your on-premises directory are synchronized to Azure AD via Azure AD Connect. These user accounts, group memberships, and credentials are automatically available within the managed domain. Important Azure AD DS integrates with Azure AD, which can synchronize with an on-premises AD DS environment. This ability extends central identity use cases to traditional web applications that run in Azure as part of a lift-and-shift strategy. Create and manage Azure AD users # Add new users or delete existing users from your Azure Active Directory (Azure AD) tenant. To add or delete users, you must be a User Administrator or Global Administrator. Note For information about viewing or deleting personal data, please review Microsoft's guidance on the Windows data subject requests for the General Data Protection Regulation (GDPR) site. For general information about GDPR, see the GDPR section of the Microsoft Trust Center and the GDPR section of the Service Trust portal. Add a new user # You can create a new user for your organization or invite an external user from the same starting point. Sign in to the Azure portal in the User Administrator role. Navigate to Azure Active Directory > Users. Select either Create new user or Invite external user from the menu. On the New User page, provide the new user's information: Identity: Add a user name and display name for the user. User name and Name are required and can't contain accent characters. You can also add a first and last name. The domain part of the user name must use either the initial default domain name, .onmicrosoft.com , or a custom domain name, such as contoso.com. Groups and roles: Optional. Add the user to one or more existing groups. Group membership can be set at any time. Settings: Optional. Toggle the option to block sign-in for the user or set the user's default location. Job info: Optional. Add the user's job title, department, company name, and manager. These details can be updated at any time. Copy the autogenerated password provided in the Password box. You'll need to give this password to the user to sign in for the first time. Select Create . The user is created and added to your Azure AD organization. Add a new guest user # You can also invite the new guest user to collaborate with your organization by selecting Invite user from the New user page. If your organization's external collaboration settings are configured to allow guests, the user will be emailed an invitation they must accept in order to begin collaborating. Add other users # There might be scenarios where you want to manually create consumer accounts in your Azure Active Directory B2C (Azure AD B2C) directory. If you have an environment with Azure Active Directory (cloud) and Windows Server Active Directory (on-premises), you can add new users by syncing the existing user account data. Delete a user # You can delete an existing user using the Azure Active Directory portal. You must have a Global Administrator, Privileged Authentication Administrator, or User Administrator role assignment to delete users in your organization. Global Admins and Privileged Authentication Admins can delete any users, including other admins. User Administrators can delete any non-admin users, Helpdesk Administrators, and other User Administrators. Sign in to the Azure portal using one of the appropriate roles listed above. Go to Azure Active Directory > Users. Search for and select the user you want to delete from your Azure AD tenant. Select Delete user. The user is deleted and no longer appears on the Users - All users page. The user can be seen on the Deleted users page for the next 30 days and can be restored during that time. When a user is deleted, any licenses consumed by the user are made available for other users. Note To update the identity, contact information, or job information for users whose source of authority is Windows Server Active Directory, you must use Windows Server Active Directory. After you complete the update, you must wait for the next synchronization cycle to complete before you see the changes. Manage users with Azure AD groups # Azure Active Directory (Azure AD) provides several ways to manage access to resources, applications, and tasks. With Azure AD groups, you can grant access and permissions to a group of users instead of each individual user. Limiting access to Azure AD resources to only those users who need access is one of the core security principles of Zero Trust. Azure AD lets you use groups to manage access to applications, data, and resources. Resources can be: Part of the Azure AD organization, such as permissions to manage objects through roles in Azure AD External to the organization, such as for Software as a Service (SaaS) apps Azure services SharePoint sites On-premises resources Some groups can't be managed in the Azure AD portal: Groups synced from on-premises Active Directory can be managed only in on-premises Active Directory. Distribution lists and mail-enabled security groups are managed only in Exchange admin center or Microsoft 365 admin center. You must sign in to the Exchange admin center or Microsoft 365 admin center to manage these groups. What to know before creating a group # There are two group types and three group membership types. Group types: # Security: Used to manage user and computer access to shared resources. For example, you can create a security group so that all group members have the same set of security permissions. Members of a security group can include users, devices, other groups, and service principals, which define access policy and permissions. Owners of a security group can include users and service principals. Microsoft 365: Provides collaboration opportunities by giving group members access to a shared mailbox, calendar, files, SharePoint sites, and more. This option also lets you give people outside of your organization access to the group. Members of a Microsoft 365 group can only include users. Owners of a Microsoft 365 group can include users and service principals. Membership types: # Assigned: Lets you add specific users as members of a group and have unique permissions. Dynamic user: Lets you use dynamic membership rules to automatically add and remove members. If a member's attributes change, the system looks at your dynamic group rules for the directory to see if the member meets the rule requirements (is added), or no longer meets the rules requirements (is removed). Dynamic device: Lets you use dynamic group rules to automatically add and remove devices. If a device's attributes change, the system looks at your dynamic group rules for the directory to see if the device meets the rule requirements (is added), or no longer meets the rules requirements (is removed). Important You can create a dynamic group for either devices or users but not for both. You can't create a device group based on the device owners' attributes. Device membership rules can only reference device attributions. Configure Azure AD administrative units # An administrative unit is an Azure AD resource that can be a container for other Azure AD resources. An administrative unit can contain only users and groups. Administrative units restrict permissions in a role to any portion of your organization that you define. You could, for example, use administrative units to delegate the Helpdesk Administrator role to regional support specialists, so they can manage users only in the region that they support. Note To use administrative units, you need an Azure Active Directory Premium license for each administrative unit admin, and Azure Active Directory Free licenses for administrative unit members. Available roles # Role Description Authentication Administrator Has access to view, set, and reset authentication method information for any non-admin user in the assigned administrative unit only. Groups Administrator Can manage all aspects of groups and groups settings, such as naming and expiration policies, in the assigned administrative unit only. Helpdesk Administrator Can reset passwords for non-administrators and Helpdesk administrators in the assigned administrative unit only. License Administrator Can assign, remove, and update license assignments within the administrative unit only. Password Administrator Can reset passwords for non-administrators and Password Administrators within the assigned administrative unit only. User Administrator Can manage all aspects of users and groups, including resetting passwords for limited admins within the assigned administrative unit only. Implement passwordless authentication # Features like multifactor authentication (MFA) are a great way to secure your organization. Still, users often get frustrated with the additional security layer on top of having to remember their passwords. Passwordless authentication methods are more convenient because the password is removed and replaced with something you have, plus something you are or something you know. Authentication Something you have Something you are or know Passwordless Windows 10 Device, phone, or security key Biometric or PIN Each organization has different needs when it comes to authentication. Microsoft global Azure and Azure Government offer the following three passwordless authentication options that integrate with Azure Active Directory (Azure AD): 1. Windows Hello for Business 2. Microsoft Authenticator 3. Fast Identity Online2 (FIDO2) security keys Windows Hello for Business # Windows Hello for Business is ideal for information workers that have their own designated Windows PC. The biometric and PIN credentials are directly tied to the user's PC, which prevents access from anyone other than the owner. With public key infrastructure (PKI) integration and built-in support for single sign-on (SSO), Windows Hello for Business provides a convenient method for seamlessly accessing corporate resources on-premises and in the cloud. Microsoft Authenticator # You can also allow your employee's phone to become a passwordless authentication method. You may already be using the Authenticator app as a convenient multi-factor authentication option in addition to a password. You can also use the Authenticator App as a passwordless option. Fast Identity Online2 (FIDO2) security keys: # The FIDO (Fast IDentity Online) Alliance helps to promote open authentication standards and reduce the use of passwords as a form of authentication. FIDO2 is the latest standard that incorporates the web authentication (WebAuthn) standard. FIDO2 security keys are an unphishable standards-based passwordless authentication method that can come in any form factor. Fast Identity Online (FIDO) is an open standard for passwordless authentication. FIDO allows users and organizations to leverage the standard to sign in to their resources without a username or password using an external security key or a platform key built into a device. Users can register and then select a FIDO2 security key at the sign-in interface as their main means of authentication. These FIDO2 security keys are typically USB devices but could also use Bluetooth or Near-Field Communication (NFC). With a hardware device that handles the authentication, the security of an account is increased as there's no password that could be exposed or guessed. FIDO2 security keys can be used to sign into their Azure AD or hybrid Azure AD joined Windows 10 devices and get single-sign-on to their cloud and on-premises resources. Users can also sign in to supported browsers. FIDO2 security keys are a great option for enterprises that are very security sensitive or have scenarios or employees who aren't willing or able to use their phone as a second factor. Explore Try-This exercises # Task 1: Review Azure AD # In this task, we'll review Azure Active Directory licensing and tenants. In the Portal , search for and select Azure Active Directory . On the Overview page, locate the license information. Go to the Azure AD pricing page and review the features and pricing for each edition. Task 2: Manage users and groups # Note: This task requires some users and groups to be populated. Dynamic groups requires a Premium P1 license. In this task, we'll create users and groups. Under the Manage blade, click Users . Review the different Sources such as Windows Server AD, Invited User, Microsoft Account, and External Azure Active Directory . Notice the choice for New guest user . Click New user . Review the two ways to create a user: Create user and Invite user . Create a new user. Review Identity, Groups and roles, Settings , and Job Info . Navigate to Azure AD, under Manage click Groups . Review the Group types: Security and Microsoft 365 . Create a new group by clicking \"New Group\" with the Membership type as Assigned . Add a user to the same group. Create another new group with Membership type as Dynamic user . Review the details to construct dynamic group membership rules. Task 3: multifactor authentication in Azure # Note This task requires a user account, AZ500User1. In this demonstration, we will configure and test MFA. Configure MFA # In this task, we'll enable MFA for a user. In the Portal , search for and select Azure Active Directory . Under Manage select Security . Under Manage select MFA . In the center pane, under Configure select Additional cloud-based MFA settings . Select the Users tab. Select AZ500User1 . Make a note of their user name in the form user@domain.com. On the far right click Enable . Read the information about enabling multifactor authentication in Azure. Click enable multi-factor auth . Wait for update. AZ500User1 will now be required to provide two factor authentication. Question # Your organization is considering multifactor authentication in Azure. Your manager asks about secondary verification methods. Which of the following options could serve as secondary verification method? Automated phone call (Ans) . Emailed link to verification website. Microsoft account verification code. Your organization has implemented multifactor authentication in Azure. Your goal is to provide a status report by user account. Which of the following values could be used to provide a valid MFA status? Enrolled Enforced (Ans) Required Which of the following options can be used when configuring multifactor authentication in Azure? Block a user if stolen password is suspected. Configure IP addresses outside the company intranet that should be blocked. Configure a one-time bypass to allow a user to authenticate a single time without performing multi-factor authentication. The bypass is temporary and expires after a specified number of seconds. In situations where the mobile app or phone isn't receiving a notification or phone call, you can allow a one-time bypass so the user can access the desired resource (Ans) . Which of the following roles would allow the user to manage all the groups in a tenant and would be able to assign other admin roles? Global administrator (Ans) Password administrator Security administrator Which of the following methods enable you to automatically add or remove users to security groups or Microsoft 365 groups, so you don't always have to do it manually? Automatic add Dynamic user (Ans) Microsoft 365 user Implement Hybrid identity # Explore how to deploy and configure Azure AD Connect to create a hybrid identity solution for your company. Introduction # Hybrid Identity is the process of connecting your on-premises Active Directory with your Azure Active Directory. You do this to enable a single account to have access to resources on-premises and in the cloud. There are many other security benefits as well. Scenario # A security engineer uses Hybrid Identity to share identity, authentication, and access across on-premises and cloud resources; you will work on such tasks as: Connect your on-premises AD with your Azure AD. Select the best authentication option based on your user's needs and your security goals. Configure authentication options to create your most secure environment. Skills measured # Azure Active Directory is a part of Exam AZ-500: Microsoft Azure Security Engineer . Manage identity and access (30-35%) Manage Azure AD identities configure authentication methods including password hash and Pass-Through Authentication (PTA), OAuth, and passwordless Learning objectives # Configure and deploy Azure AD Connect. Configure password hash synchronization. Implement pass-through authentication. Select and configure the optimal authentication method based on your security posture. Deploy password writeback. Deploy Azure AD connect # Azure AD Connect will integrate your on-premises directories with Azure Active Directory. This allows you to provide a common identity for your users for Microsoft 365, Azure, and SaaS applications integrated with Azure AD. Azure AD Connect provides the following features: Password hash synchronization . A sign-in method that synchronizes a hash of a users on-premises AD password with Azure AD. Pass-through authentication . A sign-in method that allows users to use the same password on-premises and in the cloud, but doesn't require the additional infrastructure of a federated environment. Federation integration . Federation is an optional part of Azure AD Connect and can be used to configure a hybrid environment using an on-premises AD FS infrastructure. It also provides AD FS management capabilities such as certificate renewal and additional AD FS server deployments. Synchronization . Responsible for creating users, groups, and other objects. As well as, making sure identity information for your on-premises users and groups is matching the cloud. This synchronization also includes password hashes. Health Monitoring . Azure AD Connect Health can provide robust monitoring and provide a central location in the Azure portal to view this activity. When you integrate your on-premises directories with Azure AD, your users are more productive because there's a common identity to access both cloud and on-premises resources. However, this integration creates the challenge of ensuring that this environment is healthy so that users can reliably access resources both on premises and in the cloud from any device. Azure Active Directory (Azure AD) Connect Health provides robust monitoring of your on-premises identity infrastructure. It enables you to maintain a reliable connection to Microsoft 365 and Microsoft Online Services. This reliability is achieved by providing monitoring capabilities for your key identity components. Also, it makes the key data points about these components easily accessible. Azure AD Connect Health helps you: Monitor and gain insights into AD FS servers, Azure AD Connect, and AD domain controllers. Monitor and gain insights into the synchronizations that occur between your on-premises AD DS and Azure AD. Monitor and gain insights into your on-premises identity infrastructure that is used to access Microsoft 365 or other Azure AD applications With Azure AD Connect the key data you need is easily accessible. You can view and act on alerts, setup email notifications for critical alerts, and view performance data. Important Using AD Connect Health works by installing an agent on each of your on-premises sync servers. Explore authentication options # Choosing the correct authentication method is the first concern for organizations wanting to move their apps to the cloud. Don't take this decision lightly, for the following reasons: It's the first decision for an organization that wants to move to the cloud. The authentication method is a critical component of an organization\u2019s presence in the cloud. It controls access to all cloud data and resources. It's the foundation of all the other advanced security and user experience features in Azure AD. Identity is the new control plane of IT security, so authentication is an organization\u2019s access guard to the new cloud world. Organizations need an identity control plane that strengthens their security and keeps their cloud apps safe from intruders. Authentication methods # When the Azure AD hybrid identity solution is your new control plane, authentication is the foundation of cloud access. Choosing the correct authentication method is a crucial first decision in setting up an Azure AD hybrid identity solution. Implement the authentication method that is configured by using Azure AD Connect, which also provisions users in the cloud. Azure AD supports the following authentication methods for hybrid identity solutions. Cloud authentication # When you choose this authentication method, Azure AD handles users' sign-in process. Coupled with seamless single sign-on (SSO), users can sign in to cloud apps without having to reenter their credentials. With cloud authentication, you can choose from two options: Option 1: Azure AD password hash synchronization . The simplest way to enable authentication for on-premises directory objects in Azure AD. Users can use the same username and password that they use on-premises without having to deploy any additional infrastructure. Some premium features of Azure AD, like Identity Protection and Azure AD Domain Services, require password hash synchronization, no matter which authentication method you choose. Option 2: Azure AD Pass-through Authentication . Provides a simple password validation for Azure AD authentication services by using a software agent that runs on one or more on-premises servers. The servers validate the users directly with your on-premises Active Directory, which ensures that the password validation doesn't happen in the cloud. Companies with a security requirement to immediately enforce on-premises user account states, password policies, and sign-in hours might use this authentication method. Federated authentication # When you choose the Federated authentication method, Azure AD hands off the authentication process to a separate trusted authentication system, such as on-premises Active Directory Federation Services (AD FS), to validate the user\u2019s password. The authentication system can provide additional advanced authentication requirements. Examples are smartcard-based authentication or third-party multifactor authentication. Summary # This lesson outlines various authentication options that organizations can configure and deploy to support access to cloud apps. To meet various business, security, and technical requirements, organizations can choose between password hash synchronization, Pass-through Authentication, and federation. Consider each authentication method. Does the effort to deploy the solution, and the user's experience of the sign-in process address your business requirements? Evaluate whether your organization needs the advanced scenarios and business continuity features of each authentication method. Finally, evaluate the considerations of each authentication method. Do any of them prevent you from implementing your choice? Configure Password Hash Synchronization (PHS) # The probability that you're blocked from getting your work done due to a forgotten password is related to the number of different passwords you need to remember. The more passwords you need to remember, the higher the probability to forget one. Questions and calls about password resets and other password-related issues demand the most helpdesk resources. Password hash synchronization (PHS) is a feature used to synchronize user passwords from an on-premises Active Directory instance to a cloud-based Azure AD instance. Use this feature to sign in to Azure AD services like Microsoft 365, Microsoft Intune, CRM Online, and Azure Active Directory Domain Services (Azure AD DS). You sign in to the service by using the same password you use to sign in to your on-premises Active Directory instance. Password hash synchronization helps you to: Improve the productivity of your users. Reduce your helpdesk costs. How does this work? # In the background, the password synchronization component takes the user\u2019s password hash from on-premises Active Directory, encrypts it, and passes it as a string to Azure. Azure decrypts the encrypted hash and stores the password hash as a user attribute in Azure AD. When the user signs in to an Azure service, the sign-in challenge dialog box generates a hash of the user\u2019s password and passes that hash back to Azure. Azure then compares the hash with the one in that user\u2019s account. If the two hashes match, then the two passwords must also match and the user receives access to the resource. The dialog box provides the facility to save the credentials so that the next time the user accesses the Azure resource, the user will not be prompted. Important It is important to understand that this is same sign-in, not single sign-on. The user still authenticates against two separate directory services, albeit with the same user name and password. This solution provides a simple alternative to an AD FS implementation. Implement Pass-through Authentication (PTA) # Azure AD Pass-through Authentication (PTA) is an alternative to Azure AD Password Hash Synchronization, and provides the same benefit of cloud authentication to organizations. PTA allows users to sign in to both on-premises and cloud-based applications using the same user account and passwords. When users sign-in using Azure AD, Pass-through authentication validates the users\u2019 passwords directly against an organization's on-premise Active Directory. Feature benefits # Supports user sign-in into all web browser-based applications and into Microsoft Office client applications that use modern authentication. Sign-in usernames can be either the on-premises default username (userPrincipalName) or another attribute configured in Azure AD Connect (known as Alternate ID). Works seamlessly with conditional access features such as Azure Active Directory Multi-Factor Authentication to help secure your users. Integrated with cloud-based self-service password management, including password writeback to on-premises Active Directory and password protection by banning commonly used passwords. Multi-forest environments are supported if there are forest trusts between your AD forests and if name suffix routing is correctly configured. PTA is a free feature, and you don't need any paid editions of Azure AD to use it. PTA can be enabled via Azure AD Connect. PTA uses a lightweight on-premises agent that listens for and responds to password validation requests. Installing multiple agents provides high availability of sign-in requests. PTA protects your on-premises accounts against brute force password attacks in the cloud. Important This feature can be configured without using a federation service so that any organization, regardless of size, can implement a hybrid identity solution. Pass-through authentication is not only for user sign-in but allows an organization to use other Azure AD features, such as password management, role-based access control, published applications, and conditional access policies. Deploy Federation with Azure AD # Federation is a collection of domains that have established trust. The level of trust may vary, but typically includes authentication and almost always includes authorization. A typical federation might include a number of organizations that have established trust for shared access to a set of resources. You can federate your on-premises environment with Azure AD and use this federation for authentication and authorization. This sign-in method ensures that all user authentication occurs on-premises. This method allows administrators to implement more rigorous levels of access control. Explore the authentication decision tree # Choosing the correct authentication method is the first concern for organizations wanting to move their apps to the cloud. Don't take this decision lightly, for the following reasons: - It's the first decision for an organization that wants to move to the cloud. - The authentication method is a critical component of an organization\u2019s presence in the cloud. It controls access to all cloud data and resources. - It's the foundation of all the other advanced security and user experience features in Azure AD. Identity is the new control plane of IT security, so authentication is an organization\u2019s access guard to the new cloud world. Organizations need an identity control plane that strengthens their security and keeps their cloud apps safe from intruders. Authentication methods # Cloud Authentication - When you choose this authentication method, Azure AD handles users' sign-in process. Coupled with seamless single sign-on (SSO), users can sign in to cloud apps without having to reenter their credentials. With cloud authentication, you can choose from two options: - Azure AD password hash Synchronization - Azure AD Pass-through Authentication Federated Authentication - When you choose this authentication method, Azure AD hands off the authentication process to a separate trusted authentication system, such as on-premises Active Directory Federation Services (AD FS), to validate the user\u2019s password. The authentication system can provide additional advanced authentication requirements. Examples are smartcard-based authentication or third-party multifactor authentication. Decision tree # Details on decision questions: Azure AD can handle sign-in for users without relying on on-premises components to verify passwords. Azure AD can hand off user sign-in to a trusted authentication provider such as Microsoft\u2019s AD FS. If you need to apply user-level Active Directory security policies such as account expired, disabled account, password expired, account locked out, and sign-in hours on each user sign-in, Azure AD requires some on-premises components. Sign-in features not natively supported by Azure AD: Sign-in using on-premises MFA Server. Sign-in using third-party authentication solution. Multi-site on-premises authentication solution. Azure AD Identity Protection requires Password Hash Sync, regardless of which sign-in method you choose, to provide the Users with leaked credentials report. Organizations can fail over to Password Hash Sync if their primary sign-in method fails and it was configured before the failure event. Important This decision tree is intended as a starting point to understand your options, but there can be others or even combinations of different options. For example, you can use Azure AD B2C and configure it to allow user sign-in for multi-tenant Azure AD tenants - with or without the traditional support for self-service sign-up and social identity providers. Configure password writeback # Having a cloud-based password reset utility is great but most companies still have an on-premises directory where their users exist. How does Microsoft support keeping traditional on-premises Active Directory Domain Services (AD DS) in sync with password changes in the cloud? Password writeback is a feature enabled with Azure AD Connect that allows password changes in the cloud to be written back to an existing on-premises directory in real time. Password writeback provides: Enforcement of on-premises Active Directory Domain Services password policies . When a user resets their password, it is checked to ensure it meets your on-premises Active Directory Domain Services policy before committing it to that directory. This review includes checking the history, complexity, age, password filters, and any other password restrictions that you have defined in local Active Directory Domain Services. Zero-delay feedback . Password writeback is a synchronous operation. Your users are notified immediately if their password did not meet the policy or could not be reset or changed for any reason. Supports password changes from the access panel and Microsoft 365 . When federated or password hash synchronized users come to change their expired or non-expired passwords, those passwords are written back to your local Active Directory Domain Services environment. Supports password writeback when an admin resets them from the Azure portal . Whenever an admin resets a user\u2019s password in the Azure portal, if that user is federated or password hash synchronized, the password is written back to on-premises. This functionality is currently not supported in the Office admin portal. Doesn\u2019t require any inbound firewall rules . Password writeback uses an Azure Service Bus relay as an underlying communication channel. All communication is outbound over port 443. Important To use self-service password reset (SSPR) you must have already configured Azure AD Connect in your environment. Knowledge check # Choose the best response for each of the questions below. Then select Check your answers . Check your knowledge # The IT helpdesk wants to reduce password reset support tickets. You suggest having users sign-in to both on-premises and cloud-based applications using the same password. Your organization does not plan on using Azure AD Identity Protection, so which feature would be easiest to implement given the requirements? Federation Pass-through authentication Password hash synchronization ( Ans ) Which tool can you use to synchronize Azure AD passwords with on-premises Active Directory? Azure AD Connect ( Ans ) Active Directory Federation Services Password writeback Azure AD supports which of the following security protocols? Kerberos OAuth ( Ans ) OpenID Connect Which of the following is an authentication option that integrates with Azure Active Directory, requiring you to use several differing methods, like your phone, to confirm your identity? FIDO2 security keys Microsoft Authenticator app Azure Active Directory Multi-Factor Authentication ( Ans ) Deploy Azure AD identity protection # Protect identities in Azure AD using Conditional Access, MFA, access reviews, and other capabilities. Deploy and configure Identity Protection Configure MFA for users, groups, and applications Create Conditional Access policies to ensure your security Create and follow an access review process Introduction # Identity Protection is a tool that allows organizations to automate the detection and remediation of identity-based risks, investigate risks using data in the portal, and export risk detection data to third-party utilities for further analysis. Scenario # A security engineer uses Azure AD Identity Protection to configure Azure features that monitor and protect identities in the tenant; you will work on such tasks as: Creating access reviews to check on how each identity is being used and that the correct rights are assigned. Configure policies to identify risky user behaviors and odd sign-in patterns. Control and manage access to resources with conditional access policies. Skills measured # Exam AZ-500: Microsoft Azure Security Engineer Manage identity and access (30-35%) - Configure secure access by using Azure and - Configure Access Reviews - Implement Conditional Access policies, including multi-factor authentication - Configure Azure AD identity protection Learning objectives # Deploy and configure Identity Protection Configure multi-factor authentication for users, groups, and applications Create Conditional Access policies to ensure your security Create and follow an access review process Explore Azure AD identity protection # Identity Protection is a tool that allows organizations to accomplish three key tasks: Automate the detection and remediation of identity-based risks. Investigate risks using data in the portal. Export risk detection data to third-party utilities for further analysis. Identity Protection uses the learnings Microsoft has acquired from their position in organizations with Azure AD, the consumer space with Microsoft Accounts, and in gaming with Xbox to protect your users. Microsoft analyzes 6.5 trillion signals per day to identify and protect customers from threats. Risk detections in Azure AD Identity Protection include any identified suspicious actions related to user accounts in the directory. The signals generated that are fed to Identity Protection, can be further fed into tools like Conditional Access to make access decisions, or fed back to a security information and event management (SIEM) tool for further investigation based on your organization's enforced policies. Identity Protection provides organizations access to powerful resources so they can quickly respond to suspicious activities. Identity Protection policies # Azure Active Directory Identity Protection includes three default policies that administrators can choose to enable. These policies include limited customization but are applicable to most organizations. All the policies allow for excluding users such as your emergency access or break-glass administrator accounts. Azure Multi-Factor Authentication registration policy # Identity Protection can help organizations roll out Azure Multi-Factor Authentication using a Conditional Access policy requiring registration at sign-in. Enabling this policy is a great way to ensure new users in your organization have registered for MFA on their first day. Multi-factor authentication is one of the self-remediation methods for risk events within Identity Protection. Self-remediation allows your users to act on their own to reduce helpdesk call volume. Sign-in risk policy # Identity Protection analyzes signals from each sign-in, both real-time and offline, and calculates a risk score based on the probability that the sign-in wasn't performed by the user. Administrators can decide based on this risk score signal to enforce organizational requirements. Administrators can choose to block access, allow access, or allow access but require multi-factor authentication. If risk is detected, users can perform multi-factor authentication to self-remediate and close the risky sign-in event to prevent unnecessary noise for administrators. Custom Conditional Access policy # Administrators can also choose to create a custom Conditional Access policy, including sign-in risk as an assignment condition. Configure risk event detections # To protect your users, you can configure risk-based policies in Azure Active Directory (Azure AD) that automatically respond to risky behaviors. Azure AD Identity Protection policies can automatically block a sign-in attempt or require additional action, such as requiring a password change or prompt for Azure AD Multi-Factor Authentication. These policies work with existing Azure AD Conditional Access policies as an extra layer of protection for your organization. Users may never trigger a risky behavior in one of these policies, but your organization is protected if an attempt to compromise your security is made. Each day, Microsoft collects and analyses trillions of anonymized signals as part of user sign-in attempts. These signals help build patterns of good user sign-in behavior and identify potential risky sign-in attempts. Azure AD Identity Protection can review user sign-in attempts and take additional action if there's suspicious behavior: Some of the following actions may trigger Azure AD Identity Protection risk detection: - Users with leaked credentials. - Sign-ins from anonymous IP addresses. - Impossible travel to atypical locations. - Sign-ins from infected devices. - Sign-ins from IP addresses with suspicious activity. The following three policies are available in Azure AD Identity Protection to protect users and respond to suspicious activity. You can choose to turn the policy enforcement on or off, select users or groups for the policy to apply to, and decide if you want to block access at sign-in or prompt for additional action. The insight you get for a detected risk detection is tied to your Azure AD subscription. User risk policy - Identifies and responds to user accounts that may have compromised credentials. Can prompt the user to create a new password. Sign-in risk policy - Identifies and responds to suspicious sign-in attempts. Can prompt the user to provide additional forms of verification using Azure AD Multi-Factor Authentication. MFA registration policy - Makes sure users are registered for Azure AD Multi-Factor Authentication. If a sign-in risk policy prompts for MFA, the user must already be registered for Azure AD Multi-Factor Authentication. When you enable a policy user or sign-in risk policy, you can also choose the threshold for risk level - low and above , medium and above, or high. This flexibility lets you decide how aggressive you want to be in enforcing any controls for suspicious sign-in events. Implement user risk policy # Identity Protection can calculate what it believes is normal for a user's behavior and use that to base decisions for their risk. User risk is a calculation of probability that an identity has been compromised. Administrators can decide based on this risk score signal to enforce organizational requirements. Administrators can choose to block access, allow access, or allow access but require a password change using Azure AD self-service password reset. The above image shows the configuration of User Risk Policy applied To user sign-ins Automatically respond based on a specific user\u2019s risk level Provide the condition (risk level) and action (block or allow) Use a high threshold during policy roll out Use a low threshold for greater security Risky users # With the information provided by the risky users report, administrators can find: Which users are at risk, have had risk remediated, or have had risk dismissed? Details about detections History of all risky sign-ins Risk history Administrators can then choose to act on these events. Administrators can choose to: Reset the user password Confirm user compromise Dismiss user risk Block user from signing in Investigate further using Azure ATP Implement sign-in risk policy # Sign-in risk represents the probability that a given authentication request isn't authorized by the identity owner. For users of Azure Identity Protection, sign-in risk can be evaluated as part of a Conditional Access policy. Sign-in Risk Policy supports the following conditions: Location # When configuring location as a condition, organizations can choose to include or exclude locations. These named locations may include the public IPv4 network information, country or region, or even unknown areas that don't map to specific countries or regions. Only IP ranges can be marked as a trusted location. When including any location, this option includes any IP address on the internet not just configured named locations. When selecting any location, administrators can choose to exclude all trusted or selected locations. Client apps # Conditional Access policies by default apply to browser-based applications and applications that utilize modern authentication protocols. In addition to these applications, administrators can choose to include Exchange ActiveSync clients and other clients that utilize legacy protocols. Browser - These include web-based applications that use protocols like SAML, WS-Federation, OpenID Connect, or services registered as an OAuth confidential client. Mobile apps and desktop clients - These access policies are commonly used when requiring a managed device, blocking legacy authentication, and blocking web applications but allowing mobile or desktop app. Risky sign-ins # The risky sign-ins report contains filterable data for up to the past 30 days (1 month). With the information provided by the risky sign-ins report, administrators can find: Which sign-ins are classified as at risk, confirmed compromised, confirmed safe, dismissed, or remediated. Real-time and aggregate risk levels associated with sign-in attempts. Detection types triggered Conditional Access policies applied MFA details Device information Application information Location information Administrators can then choose to take action on these events. Administrators can choose to: Confirm sign-in compromise Confirm sign-in safe Deploy multifactor authentication in Azure # Azure Active Directory Multi-Factor Authentication helps safeguard access to data and applications while maintaining simplicity for users. It provides additional security by requiring a second form of authentication and delivers strong authentication through a range of easy to use authentication methods. For organizations that need to be compliant with industry standards, such as the Payment Card Industry (PCI) Data Security Standard (DSS) version 3.2, MFA is a must have capability to authenticate users. Beyond being compliant with industry standards, enforcing MFA to authenticate users can also help organizations to mitigate credential theft attacks. The security of MFA two-step verification lies in its layered approach. Compromising multiple authentication factors presents a significant challenge for attackers. Even if an attacker manages to learn the user's password, it is useless without also having possession of the additional authentication method. Authentication methods include: Something you know (typically a password) Something you have (a trusted device that is not easily duplicated, like a phone) Something you are (biometrics) Multi-Factor Authentication Features # Get more security with less complexity . Azure MFA helps safeguard access to data and applications and helps to meet customer demand for a simple sign-in process. Get strong authentication with a range of easy verification options\u2014phone call, text message, or mobile app notification\u2014and allow customers to choose the method they prefer. Mitigate threats with real-time monitoring and alerts . MFA helps protect your business with security monitoring and machine-learning-based reports that identify inconsistent sign-in patterns. To help mitigate potential threats, real-time alerts notify your IT department of suspicious account credentials. Use with Microsoft 365, Salesforce, and more . MFA for Microsoft 365 helps secure access to Microsoft 365 applications at no additional cost. Multifactor authentication is also available with Azure Active Directory Premium and thousands of software-as-a-service (SaaS) applications, including Salesforce, Dropbox, and other popular services. Add protection for Azure administrator accounts . MFA adds a layer of security to your Azure administrator account at no additional cost. When it's turned on, you need to confirm your identity to create a virtual machine, manage storage, or use other Azure services. Multi-Factor Authentication Options # Method Description Call to phone Places an automated voice call. The user answers the call and presses # in the phone keypad to authenticate. The phone number is not synchronized to on-premises Active Directory. A voice call to phone is important because it persists through a phone handset upgrade, allowing the user to register the mobile app on the new device. Text message to phone Sends a text message that contains a verification code. The user is prompted to enter the verification code into the sign-in interface. This process is called one-way SMS. Two-way SMS means that the user must text back a particular code. Two-way SMS is deprecated and not supported after November 14, 2018. Users who are configured for two-way SMS are automatically switched to call to phone verification at that time. Notification through mobile app Sends a push notification to your phone or registered device. The user views the notification and selects Approve to complete verification. The Microsoft Authenticator app is available for Windows Phone, Android, and iOS. Push notifications through the mobile app provide the best user experience. Verification code from mobile app The Microsoft Authenticator app generates a new OATH verification code every 30 seconds. The user enters the verification code into the sign-in interface. The Microsoft Authenticator app is available for Windows Phone, Android, and iOS. Verification code from mobile app can be used when the phone has no data connection or cellular signal. Explore multifactor authentication settings # Account lockout # To prevent repeated MFA attempts as part of an attack, the account lockout settings let you specify how many failed attempts to allow before the account becomes locked out for a period of time. The account lockout settings are only applied when a pin code is entered for the MFA prompt. The following settings are available: Number of MFA denials to trigger account lockout Minutes until account lockout counter is reset Minutes until account is automatically unblocked Block and unblock users # If a user's device has been lost or stolen, you can block authentication attempts for the associated account. Notifications # Email notifications can be configured when users report fraud alerts. These notifications are typically sent to identity administrators, as the user's account credentials are likely compromised. OATH tokens # Azure AD supports the use of OATH-TOTP SHA-1 tokens that refresh codes every 30 or 60 seconds. Customers can purchase these tokens from the vendor of their choice. Trusted IPs # Trusted IPs is a feature to allow federated users or IP address ranges to bypass two-step authentication. Notice there are two selections in this screenshot. Which selections you can make depends on whether you have managed or federated tenants. Managed tenants . For managed tenants, you can specify IP ranges that can skip MFA. Federated tenants . For federated tenants, you can specify IP ranges and you can also exempt AD FS claims users. Important The Trusted IPs bypass works only from inside of the company intranet. If you select the All Federated Users option and a user signs in from outside the company intranet, the user must authenticate by using two-step verification. The process is the same even if the user presents an AD FS claim. Enable multifactor authentication # To enable MFA, go to the User Properties in Azure Active Directory, and then the Multi-Factor Authentication option. From there, you can select the users that you want to modify and enable for MFA. You can also bulk enable groups of users with PowerShell. User's states can be Enabled, Enforced, or Disabled. Note On first-time sign-in, after MFA has been enabled, users are prompted to configure their MFA settings. For example, if you enable MFA so that users must use a mobile device, users will be prompted to configure their mobile device for MFA. Users must complete those steps, or they will not be permitted to sign in, which they cannot do until they have validated that their mobile device is MFA-compliant. All users start out Disabled. When you enroll users in per-user Azure AD Multi-Factor Authentication, their state changes to Enabled. When enabled users sign in and complete the registration process, their state changes to Enforced. Administrators may move users between states, including from Enforced to Enabled or Disabled. Enable MFA for Global Admins # Azure AD Multi-Factor Authentication is included free of charge for global administrator security. Enabling MFA for global administrators provides an added level of security when managing and creating Azure resources like virtual machines, managing storage, or using other Azure services. Secondary authentication includes phone call, text message, and the authenticator app. Important Remember, you can only enable MFA for organizational accounts stored in Azure Active Directory. These are also called work or school accounts. Implement Azure AD conditional access # The old world of security behind a corporate firewall, having your secure network perimeter just doesn\u2019t work anymore, not with people wanting to work from anywhere, being able to connect to all sorts of cloud applications. Conditional Access is the tool used by Azure Active Directory to bring signals together, to make decisions, and enforce organizational policies. Conditional Access is at the heart of the new identity driven control plane. Conditional access policy is really a next generation policy that\u2019s built for the cloud. It\u2019s able to consider massive amounts of data, as well as contextual data from a user sign-in flow and make sure that the right controls are enforced. Identity as a Service\u2014the new control plane # What is the basis for saying that identity management is the new control plane? First, what is the control plane? In a switch or router, the control plane is the part that controls where the traffic is to go, but it\u2019s not responsible for the movement of the traffic. The control plane learns the routes, either static or dynamic. The part responsible for moving the traffic is the forwarding plane. The following figure depicts a simple switch diagram. A user\u2019s identity is like a control plane, because it controls which protocols the user will interact with, which organizational programs the user can access, and which devices the user can employ to access those programs. Identity is what helps protect user and corporate data. For example, should that data be encrypted, deleted, or ignored when an issue occurs? Now, everything pivots around that user identity. You know what their activities are, and where they are located. You know what devices they\u2019re using. Then we leverage that information in conditional access policy to be able to enforce things like multifactor authentication or require a compliant device. There are the conditions, which indicate when the policy is going to apply. This can be, again, the location, type of application that you\u2019re on, any detected risk. How is the risk determined? It is determined from all the analysis and intel that we have across organizations using Azure Active Directory, as well as Microsoft consumer identity offerings. Conditional Access is the tool used by Azure Active Directory to bring signals together, to make decisions, and enforce organizational policies. Conditional Access policies at their simplest are if-then statements, if a user wants to access a resource, then they must complete an action. Example: A payroll manager wants to access the payroll application and is required to perform multifactor authentication to access it. Administrators are faced with two primary goals: Empower users to be productive wherever and whenever Protect the organization's assets By using Conditional Access policies, you can apply the right access controls when needed to keep your organization secure and stay out of your user\u2019s way when not needed. Conditional Access policies are enforced after the first-factor authentication has been completed. Conditional Access is not intended as an organization's first line of defense for scenarios like denial-of-service (DoS) attacks but can use signals from these events to determine access. Configure conditional access conditions # Conditional access is a capability of Azure AD (with an Azure AD Premium license) that enables you to enforce controls on the access to apps in your environment based on specific conditions from a central location. With Azure AD conditional access, you can factor how a resource is being accessed into an access control decision. By using conditional access policies, you can apply the correct access controls under the required conditions. Conditional access comes with six conditions: user/group, cloud application, device state, location (IP range), client application, and sign-in risk. You can use combinations of these conditions to get the exact conditional access policy you need. Notice on this image the conditions determine the access control from the previous topic. With access controls, you can either Block Access altogether or Grant Access with more requirements by selecting the desired controls. You can have several options: Require MFA from Azure AD or an on-premises MFA (combined with AD FS). Grant access to only trusted devices. Require a domain-joined device. Require mobile devices to use Intune app protection policies. Requiring more account verification through MFA is a common conditional access scenario. While users may be able to sign in to most of your organization\u2019s cloud apps, you may want more verification for things like your email system, or apps that contain personnel records or sensitive information. In Azure AD, you can accomplish this with a conditional access policy Important The Users and Groups condition is mandatory in a conditional access policy. In your policy, you can either select All users or select specific users and groups. Implement access reviews # Azure Active Directory (Azure AD) access reviews enable organizations to efficiently manage group memberships, access to enterprise applications, and role assignments. User's access can be reviewed on a regular basis to make sure only the right people have continued access. Why are access reviews important? # Azure AD enables you to collaborate internally within your organization and with users from external organizations, such as partners. Users can join groups, invite guests, connect to cloud apps, and work remotely from their work or personal devices. The convenience of leveraging the power of self-service has led to a need for better access management capabilities. As new employees join, how do you ensure they have the right access to be productive? As people move teams or leave the company, how do you ensure their old access is removed, especially when it involves guests? Excessive access rights can lead to audit findings and compromises as they indicate a lack of control over access. You must proactively engage with resource owners to ensure they regularly review who has access to their resources. Use access reviews in the following cases # Too many users in privileged roles : It's a good idea to check how many users have administrative access, how many of them are Global Administrators, and if there are any invited guests or partners that have not been removed after being assigned to do an administrative task. You can recertify the role assignment users in Azure AD roles such as Global Administrators, or Azure resources roles such as User Access Administrator in the Azure AD Privileged Identity Management (PIM) experience. When automation is infeasible : You can create rules for dynamic membership on security groups or Microsoft 365 Groups, but what if the HR data is not in Azure AD or if users still need access after leaving the group to train their replacement? You can then create a review on that group to ensure those who still need access should have continued access. When a group is used for a new purpose : If you have a group that is going to be synced to Azure AD, or if you plan to enable a sales management application for everyone in the Sales team group, it would be useful to ask the group owner to review the group membership prior to the group being used in a different risk content. Business critical data access : for certain resources, it might be required to ask people outside of IT to regularly sign out and give a justification on why they need access for auditing purposes. To maintain a policy's exception list : In an ideal world, all users would follow the access policies to secure access to your organization's resources. However, sometimes there are business cases that require you to make exceptions. As the IT admin, you can manage this task, avoid oversight of policy exceptions, and provide auditors with proof that these exceptions are reviewed regularly. Ask group owners to confirm they still need guests in their groups : Employee access might be automated with some on premises IAM, but not invited guests. If a group gives guests access to business sensitive content, then it's the group owner's responsibility to confirm the guests still have a legitimate business need for access. Have reviews recur periodically : You can set up recurring access reviews of users at set frequencies such as weekly, monthly, quarterly or annually, and the reviewers will be notified at the start of each review. Reviewers can approve or deny access with a friendly interface and with the help of smart recommendations. Depending on what you want to review, you will create your access review in Azure AD access reviews, Azure AD enterprise apps (in preview), or Azure AD PIM. Using this feature requires an Azure AD Premium P2 license. Important Azure AD Premium P2 licenses are not required for users with the Global Administrator or User Administrator roles that set up access reviews, configure settings, or apply the decisions from the reviews. Explore try-this exercises # Task 1 - Configure conditional access (require MFA) # Note This task requires a user account, AZ500User1. If you want to show the MFA verification, the user account must have a phone number. This task will review conditional access policy settings and create a policy that requires MFA when signing in to the Portal. Configure the policy # In the Portal , search for and select Azure Active Directory . Under Manage , select Security . Under Protect , select Conditional access . Click New Policy . Name: AZ500Policy1 Users and groups > Select users and groups > Users and Groups > Select: AZ500User1 Cloud apps or actions > Select apps > Select: Microsoft Azure Management Review the warning that this policy impacts Portal access. Conditions > Sign-in risk > Review the risk levels Device platforms > Review the devices that can be included, such as Android and iOS. Locations > Review the physical location selections. Under Access controls click Grant . Review the Grant options such as MFA. You may require one or more of the controls. Select Require multi-factor authentication . For Enable policy , select On . Click Create. Test the policy # Sign in to the Portal as the AZ500User1 . Before you can sign in, a second authentication is required. If you have a phone number associated with the user, provide and verify the text code. You should be able to sign in to the Portal successfully. If you do not have a phone number associated with the user, this demonstrates that MFA is in effect. You may want to return to the AZ500Policy1 and turn the policy Off . Task 2 - Access review # In this task, we will configure an access review. Configure an access review # In the Portal , search for and select Identity Governance . Under Access Reviews , select Access Reviews . Click New Access Review . We will create an access review to ensure we validate the AZ500Admin group membership. Complete the required information and discuss each setting. Configuration settings are added as you make your selections. For example, if you select a weekly access review, you will be prompted for the duration. Review name: AZ500Review Start date: current date Frequency: One-time Users to review: Members of a group Scope: Everyone Select a group: AZ500Admins Reviewers: Selected user Select reviewers: add yourself as a reviewer Review the Upon completion settings , specifically the action if a reviewer doesn't respond. Review Advanced settings . Start the access review. On the Access review page, ensure the new access review is listed. The Status will change from Not started to Initializing . Conduct an access review # In this task, we will conduct an access review. When the access review is complete, you will receive an email. This is the email associated with your reviewer account. View the email and discuss the review instructions. Note when the review period will end. In the email, click Start review . On the Access reviews page, click the AZ500Review . Notice you are reviewing the AZ500Admin group members. There are two members. Use the Details link to view information about the user. Select Approve for one user and Deny for the other. Be sure to provide a Reason . Submit your reviews. Review the access review results # In this task, we will review the access review results. Return to the Portal . Click the AZ500Review . From the Overview blade, review the results. There should be one member approved and one member denied . Click Results for more detailed information about the reviewer and their reasons. From the Overview blade, click Stop and confirm you want to stop the review. The Review status should now be Complete . Apply the access review # In this task, we will apply the review results. In the Portal , search for and select Azure Active Directory . Under Manage , select Groups . Locate the AZ500Admins group. Review the members of the group.- Confirm there are two members. Return to the AZ500Review . Click Apply . Confirm that you want to remove the denied member. The Review status will change from Applying to Result applied . Verify the AZ500Admins group now only has one member. Knowledge check # Choose the best response for each of the questions below. Then select Check your answers . The compliance auditors want to ensure as employees change jobs or leave the company that their privileges are also changed or revoked. They are especially concerned about the Administrator group. To address their concerns. you implement which of the following? Access reviews ( Ans ) Azure time-based policies JIT virtual machine access Identity Protection has reported that a user\u2019s credentials have been leaked. According to policy, the user\u2019s password must be reset. Which Azure AD role can reset the password? Global Administrator ( Ans ) Security Administrator Security Operator Identity Protection identifies risks in which of the following classifications? Specific IP address Atypical travel ( Ans ) Unregistered device You have implemented Identity Protection and are reviewing the Risky users report. For each reported event you can choose any of the following actions? Confirm user compromise ( Ans ) Delete the risk event Dismiss user account Conditional Access can be used to enable which of the actions listed below? Block or grant access from specific time of day. Designate privileged user accounts. Require multifactor authentication. ( Ans ) Which licensing plan supports Identity Protection? Azure Active Directory Free Azure Active Directory Premium P1 Azure Active Directory Premium P2 ( Ans ) Configure Azure AD privileged identity management # Ensure that your privileged identities have extra protection and are accessed only with the least amount of access needed to do the job. Introduction # Azure AD Privileged Identity Management (PIM) allows you to manage, control, and monitor access to the most important resources in your organization. You can give just-in-time access and just-enough-access to users to allow them to do their tasks. Scenario # A security engineer uses Privileged Identity Management to protect administrator privileges and mitigate the risk of excessive or misused access rights; some common tasks are: Configuring the scope of users and roles based on zero trust. Setting up a PIM workflow to enforce approval for role usage, and monitor the access. Implement just-in-time access. Explore the zero trust model # Cloud-based services and mobile computing have changed the technology landscape for the modern enterprise. Today\u2019s workforce often requires access to applications and resources outside traditional corporate network boundaries, rendering security architectures that rely on firewalls and virtual private networks (VPNs) insufficient. Changes brought about by cloud migration and a more mobile workforce has led to the development of an access architecture called Zero Trust. The Zero Trust model # Based on the principle of \u201cnever trust, always verify,\u201d Zero Trust helps secure corporate resources by eliminating unknown and unmanaged devices and limiting lateral movement. Implementing a true Zero Trust model requires that all components\u2014user identity, device, network, and applications\u2014be validated and proven trustworthy. Zero Trust verifies identity and device health prior to granting access to corporate resources. When access is granted, applying the principle of least privilege limits user access to only those resources that are explicitly authorized for each user, thus reducing the risk of lateral movement within the environment. In an ideal Zero Trust environment, the following four elements are necessary: Strong identity authentication everywhere (user verification via authentication) Devices are enrolled in device management, and their health is validated Least-privilege user rights (access is limited to only what is needed) The health of services is verified (future goal) For Microsoft, Zero Trust establishes a strict boundary around corporate and customer data. For end users, Zero Trust delivers a simplified user experience that allows them to easily manage and find their content. And for customers, Zero Trust creates a unified access platform that they can use to enhance the overall security of their entire ecosystem. Zero Trust architecture # A Zero Trust approach extends throughout the entire digital estate and serves as an integrated security philosophy and end-to-end strategy. The illustration below provides a representation of the primary elements that contribute to Zero Trust. In the illustration above: Security policy enforcement is at the center of a Zero Trust architecture. This includes Multi-Factor authentication with conditional access that takes into account user account risk, device status, and other criteria and policies that you set. Identities, devices (also called endpoints), data, applications, network, and other infrastructure components are all configured with appropriate security. Policies that are configured for each of these components are coordinated with your overall Zero Trust strategy. For example, device policies determine the criteria for healthy devices and conditional access policies require healthy devices for access to specific apps and data. Threat protection and intelligence monitors the environment, surfaces current risks, and takes automated action to remediate attacks. Guiding principles of Zero Trust # Today, organizations need a new security model that effectively adapts to the complexity of the modern environment, embraces the mobile workforce, and protects people, devices, applications, and data wherever they are located. To address this new world of computing, Microsoft highly recommends the Zero Trust security model, which is based on these guiding principles: Verify explicitly - Always authenticate and authorize based on all available data points. Use least privilege access - Limit user access with Just-In-Time and Just-Enough-Access (JIT/JEA), risk-based adaptive policies, and data protection. Assume breach - Minimize blast radius and segment access. Verify end-to-end encryption and use analytics to get visibility, drive threat detection, and improve defenses. Microsoft's Zero Trust architecture # Below is a simplified reference architecture for our approach to implementing Zero Trust. The primary components of this process are Intune for device management and device security policy configuration, Azure AD conditional access for device health validation, and Azure AD for user and device inventory. The system works with Intune, pushing device configuration requirements to the managed devices. The device then generates a statement of health, which is stored in Azure AD. When the device user requests access to a resource, the device health state is verified as part of the authentication exchange with Azure AD. Important The National Institute of Standards and Technology has a Zero Trust Architecture, NIST 800-207, publication. Review the evolution of identity management # Microsoft Identity Manager or MIM helps organizations manage the users, credentials, policies, and access within their organizations and hybrid environments. With MIM, organizations can simplify identity lifecycle management with automated workflows, business rules, and easy integration with heterogenous platforms across the datacenter. MIM enables Active Directory Domain Services to have the right users and access rights for on-premises apps. Azure AD Connect can then make those users and permissions available in Azure AD for Microsoft 365 and cloud-hosted apps. On-premises Active Directory Domain Services, Azure Active Directory (Azure AD), or a hybrid combination of the two all offer services for user and device authentication, identity and role management, and provisioning. Identity has become the common factor among many services, like Microsoft 365 and Xbox Live, where the person is the center of the services. Identity is now the security boundary, the new firewall, the control plane\u2014whichever comparison you prefer. Your digital identity is the combination of who you are and what you\u2019re allowed to do. That is: Credentials + privileges = digital identity First step, you need to help protect your privileged accounts. These identities have more than the normal user rights and, if compromised, allow a malicious hacker to access sensitive corporate assets. Helping secure these privileged identities is a critical step to establishing security assurances for business assets in a modern organization. Cybercriminals target these accounts and other privileged services in their kill chain to carry out their objectives. Evolution of identities # Identity management approaches have evolved from traditional, to advanced, to optimal. Traditional identity approaches On-premises identity providers. No single sign-on is present between on-premises and cloud apps. Visibility into identity risk is very limited. Advanced identity approaches Conditional access policies gate access and provide remediation actions. Analytics improve visibility into identity risk. Optimal identity approaches Passwordless authentication is enabled. User, location, devices, and behavior are analyzed in real time. Continuous protection to identity risk. Steps for a passwordless world # Enforce MFA \u2014 Conform to the fast identity online (FIDO) 2.0 standard, so you can require a PIN and a biometric for authentication rather than a password. Windows Hello is one good example, but choose the MFA method that works for your organization. Reduce legacy authentication workflows \u2014 Place apps that require passwords into a separate user access portal and migrate users to modern authentication flows most of the time. At Microsoft only 10 percent of our users enter a password on a given day. Remove passwords \u2014 Create consistency across Active Directory Domain Services and Azure Active Directory (Azure AD) to enable administrators to remove passwords from identity directory. Important We recommend Azure AD Privileged Identity Management as the service to help protect your privileged accounts. Deploy Azure AD privileged identity management # With the Azure AD Privileged Identity Management (PIM) service, you can manage, control, and monitor access to important resources in your organization. This includes access to resources in Azure AD; Azure; and other Microsoft Online Services, like Microsoft 365 and Microsoft Intune. This control does not eliminate the need for users to carry out privileged operations in Azure AD, Azure, Microsoft 365, and Software as a Service (SaaS) apps. Organizations can give users just-in-time (JIT) privileged access to Azure resources and Azure AD. Oversight is needed for what those users do with their administrator privileges. PIM helps mitigate the risk of excessive, unnecessary, or misused access rights. Key PIM features # Providing just-in-time privileged access to Azure AD and Azure resources. IT administrators can pick an activation period between 0.5 and a role's maximum duration (max is 24 hours). They will only receive the privilege for that period of time. After the activation period admins will have to go through the activation process again. Assigning time-bound access to resources by using start and end dates. PIM allows you to set an end time for the role. This is particularly useful in a guest scenario. If your organization has guests that are working for a specific time the role privilege will expire automatically. Requiring approval to activate privileged roles. You can designate one or more approvers. These approvers will receive an email once a request is made. Approval is required to activate the privilege. Enforcing Azure Multi-Factor Authentication (MFA) to activate any role. If your organization already has MFA enabled, PIM will not ask the user to sign in again. Using justification to understand why users activate. This benefits both internal and external auditors understanding why the role was activated. You can also require a service ticket number from whatever service product you are using. Getting notifications when a user is assigned a privilege and when that privilege is activated. Conducting access reviews to know which users have privileged roles in the organization and if they still need them. Downloading an audit history for an internal or external audit. This keeps tracks of all PIM events. Ways to use PIM We use Azure AD PIM in the following ways: View which users are assigned privileged roles to manage Azure resources, as well as which users are assigned administrative roles in Azure AD. Enable on-demand, \u201cjust in time\u201d administrative access to Microsoft Online Services like Microsoft 365 and Intune, and to Azure resources of subscriptions, resource groups, and individual resources such as Virtual Machines. Review a history of administrator activation, including what changes administrators made to Azure resources. Get alerts about changes in administrator assignments. Require approval to activate Azure AD privileged admin roles. Review membership of administrative roles and require users to provide a justification for continued membership. Configure privileged identity management scope # Azure AD roles . These roles are all in Azure Active Directory (such as Global Administrator, Exchange Administrator, and Security Administrator). You can read more about the roles and their functionality in Administrator role permissions in Azure Active Directory. Azure resource roles . These roles are linked to an Azure resource, resource group, subscription, or management group. Privileged Identity Management provides just-in-time access to both built-in roles like Owner, User Access Administrator, and Contributor, as well as custom roles. Azure AD roles # Users can be assigned to different administrative roles in Azure AD. These role assignments control which tasks, such as adding or removing users or changing service settings, the users are able to perform on Azure AD, Microsoft 365 and other Microsoft Online Services and connected applications. A global administrator can update which users are permanently assigned to roles in Azure AD, using PowerShell cmdlets such as Add-MsolRoleMember and Remove-MsolRoleMember, or through the Azure portal. Azure AD Privileged Identity Management (PIM) manages policies for privileged access for users in Azure AD. PIM assigns users to one or more roles in Azure AD, and you can assign someone to be permanently in the role, or eligible for the role. When a user is permanently assigned to a role, or activates an eligible role assignment, then they can manage Azure Active Directory, Microsoft 365, and other applications with the permissions assigned to their roles. There's no difference in the access given to someone with a permanent versus an eligible role assignment. The only difference is that some people don't need that access all the time. They are made eligible for the role, and can turn it on and off whenever they need to. Roles managed in PIM # Privileged Identity Management lets you assign users to common administrator roles, including: Global administrator (also known as a Company administrator) has access to all administrative features. You can have more than one global admin in your organization. The person who signs up to purchase Microsoft 365 automatically becomes a global admin. Privileged role administrator manages Azure AD PIM and updates role assignments for other users. Billing administrator makes purchases, manages subscriptions, manages support tickets, and monitors service health. Password administrator users with this role have limited ability to manage passwords. This role does not grant the ability to manage service requests or monitor service health. Whether a Password Administrator can reset a user's password depends on the user's role. Service administrator manages service requests and monitors service health. User management administrator resets passwords, monitors service health, and manages user accounts, user groups, and service requests. The user management admin can\u2019t delete a global admin, create other admin roles, or reset passwords for billing, global, and service admins. Exchange administrator has administrative access to Exchange Online through the Exchange admin center (EAC), and can perform almost any task in Exchange Online. SharePoint administrator has administrative access to SharePoint Online through the SharePoint Online admin center, and can perform almost any task in SharePoint Online. Skype for Business administrator has administrative access to Skype for Business through the Skype for Business admin center, and can perform almost any task in Skype for Business Online. Roles not managed in PIM # Roles within Exchange Online or SharePoint Online, except for those mentioned above, are not represented in Azure AD and so are not visible in PIM. Azure subscriptions and resource groups are also not represented in Azure AD. Azure resources # When you first set up Privileged Identity Management for Azure resources, you need to discover and select the resources to protect with Privileged Identity Management. There's no limit to the number of resources that you can manage with Privileged Identity Management. However, we recommend starting with your most critical (production) resources. Implement privileged identity management onboarding # To use PIM, you need one of the following paid or trial licenses: Azure AD Premium P2, Enterprise Mobility + Security (EMS) E5, or Microsoft 365 M5. PIM Access The first Global Administrator to use PIM in your instance of Azure AD is automatically assigned the Security Administrator and Privileged Role Administrator roles in the directory. This person must be an eligible Azure AD user. Only privileged role administrators can manage the Azure AD directory role assignments of users. In addition, you can choose to run the security wizard that walks you through the initial discovery and assignment experience. Users or members of a group assigned to the Owner or User Access Administrator roles, and Global Administrators that enable subscription management in Azure AD, are Resource Administrators. These administrators can assign roles, configure role settings, and review access by using PIM for Azure resources. No one else in your Azure Active Directory (Azure AD) organization gets write access by default, though, including other Global administrators. Other Global administrators, Security administrators, and Security readers have read-only access to Privileged Identity Management. To grant access to Privileged Identity Management, the first user can assign others to the Privileged Role Administrator role. Important Make sure there are always at least two users in a Privileged Role Administrator role, in case one user is locked out or their account is deleted. Explore privileged identity management configuration settings # Activation settings # Activation duration . Set the maximum time, in hours, that a role stays active before it expires. This value can be from one to 24 hours. Require multifactor authentication on activation . You can require users who are eligible for a role to prove who they are using Azure Active Directory Multi-Factor Authentication (MFA) before they can activate. Multifactor authentication ensures that the user is who they say they are with reasonable certainty. Enforcing this option protects critical resources in situations when the user account might have been compromised. Require justification . You can require that users enter a business justification when they activate. Require approval to activate . If setting multiple approvers, approval completes as soon as one of them approves or denies. You can't require approval from at least two users. Assignment settings # Allow permanent eligible assignment . Global admins and Privileged role admins can assign permanent eligible assignment. They can also require that all eligible assignments have a specified start and end date. Allow permanent active assignment . Global admins and Privileged role admins can assign active eligible assignment. They can also require that all active assignments have a specified start and end date. Note In some cases, you might want to assign a user to a role for a short duration (one day, for example). In this case, the assigned users don't need to request activation. In this scenario, Privileged Identity Management can't enforce multifactor authentication when the user uses their role assignment because they are already active in the role from the time that it is assigned. Notification settings # Notifications can be sent when members are assigned as eligible in a role, assigned as active in a role, and when the role is activated. Notifications can be sent to Admins, Requestors, and Approvers. Implement a privileged identity management workflow # By configuring Azure AD PIM to manage our elevated access roles in Azure AD, we now have JIT access for more than 28 configurable privileged roles. We can also monitor access, audit account elevations, and receive additional alerts through a management dashboard in the Azure portal. Elevated access includes job roles that need greater access, including support, resource administrators, resource owners, service administrators, and global administrators. We manage role-based access at the resource level. Because elevated access accounts could be misused if they\u2019re compromised, we rationalize new requests for elevated access and perform regular re-attestation for elevated roles. The following diagram of the elevated access workflow. JIT administrator access # Historically, we could assign an employee to an administrative role through the Azure portal or through Windows PowerShell and that employee would be a permanent administrator; their elevated access would remain active in the assigned role. Azure AD PIM introduced the concept of permanent and eligible administrators in Azure AD and Azure. Permanent administrators have persistent elevated role connections; whereas eligible administrators have privileged access only when they need it. The eligible administrator role is inactive until the employee needs access, then they complete an activation process and become an active administrator for a set amount of time. We\u2019ve stopped using permanent administrators for named individual accounts, although we do have some automated service accounts that still use the role. Role activation in Azure Active Directory # Azure AD PIM uses administrative roles, such as tenant admin and global admin, to manage temporary access to various roles. With Azure AD PIM, you can manage the administrators by adding or removing permanent or eligible administrators to each role. Azure AD PIM includes several built-in Azure AD roles as well as Azure that we manage. To activate a role, an eligible admin will initialize Azure AD PIM in the Azure portal and request a time-limited role activation. The activation is requested using the Activate my role option in Azure AD PIM. Users requesting activation must satisfy conditional access policies to ensure that they are coming from authorized devices and locations, and their identities must be verified through multifactor authentication. To help secure transactions while enabling mobility, we use Azure AD PIM to customize role activation variables in Azure, including the number of sign-in attempts, the length of time the role is activated after sign-in, and the type of credentials required (such as single sign-in or multifactor authentication). At Microsoft, when an individual joins a team or changes teams, they might need administrative rights for their new business role. For example, someone might join a team in which their user account will require Exchange Online Administrator privileged access rights in the future. That user makes a request, then their manager validates that user\u2019s request, as does a service owner. With those approvals, Core Services Engineering and Operations (CSEO, formerly Microsoft IT) administrators in the Privileged Role Administrator role are notified. A CSEO administrator uses Azure AD PIM via the Azure portal to make that user eligible for that role. The user can then use Azure AD PIM to activate that role. Tracking the use of privileged roles using the dashboard # A dashboard through the Azure portal gives a centralized view of: Alerts that point out opportunities to improve security. The number of users who are assigned to each privileged role. The number of eligible and permanent admins. Ongoing access reviews. We can track how employees and admins are using their privileged roles by viewing the audit history or by setting up a regular access review. Both options are available through the PIM dashboard in the Azure portal. The PIM audit log tracks changes in privileged role assignments and role activation history. We use the audit log to view all user assignments and activations within a specified period. The audit history helps us determine, in real time, which accounts haven\u2019t signed in recently, or if employees have changed roles. Access reviews can be performed by an assigned reviewer, or employees can review themselves. This is an effective way to monitor who still needs access, and who can be removed. We\u2019re looking at the data that\u2019s collected, and the monitoring team is assessing the best way to configure monitoring alerts to notify us about out-of-band changes\u2014for example, if too many administrator roles are being created for an Azure resource. The information also helps us determine whether our current elevation time settings are appropriate for the various privileged admin roles. Like all organizations, we want to minimize the number of people who have access to our secure information or resources, because that reduces the chance of a malicious user getting access or an authorized user inadvertently impacting a sensitive resource. However, our people still need to carry out privileged operations in Azure AD, Azure, Microsoft 365, and SaaS apps. We can give users privileged access to Azure resources like Subscriptions, and Azure AD. Oversight is needed for what our users are doing with their admin privileges. We use Azure AD PIM to mitigate the risk of excessive, unnecessary, and misused access rights. In Azure AD, we use Azure AD PIM to manage the users we assign to built-in Azure AD organizational roles, such as Global Administrator. In Azure, we use Azure AD PIM to manage our users and groups that we assign via Azure RBAC roles, including Owner and Contributor. Explore Try-This exercises # Task 1: Azure AD PIM for roles # Configure PIM settings # Note This task requires a AZ500User1 account with no assigned roles. In this task, we will review and configure the basic PIM settings. In the Portal , search for and select Azure AD Privileged Identity Management . Under Manage select Azure AD Roles . Under Manage select Settings . Select the Billing Administrator role. Click Edit . Notice the Activation, Assignment , and Notification tabs. By default, MFA is required on activation. For this demonstration, change the requirement to None . Check the box to Require approval to activate . Discuss the other possible settings including Activation maximum duration and Require approval to activate . Switch to the Assignment tab and require the settings. Notice the ability to expire eligible and active assignments. Switch to the Notifications tab and discuss the settings. Notice you can send notifications when members are assigned and activated. Click Update . Configure PIM for Roles # In this task, we will add the Billing Administrator role to PIM. In the Portal , search for and select Azure AD Privileged Identity Management . Under Manage select Azure AD Roles . Under Manage select Roles . Review the list of roles. Select the Billing Administrator role. Review Eligible roles and Active roles . Click Add member . Click Select member and Select the AZ500User1 user. You are now a Billing Administrator. Select Set membership settings . Notice the settings can be permanent or limited in time. - Assignment type: Eligible - Permanently eligible: check the box . Save your changes and Add the assignment. Verify the Billing Administrator is listed as an eligible role. Activate a role # In this task, we will activate the Billing Administrator role. In the Portal , search for and select Azure Active Directory . Under Manage click Users . Select AZ500User1 . Under Manage click Assigned roles . Verify the user is not assigned to any roles. Sign in the Portal as AZ500User1 . Search for and select Azure AD Privileged Identity Management . Under Tasks select My roles . Under Activate select Azure AD Roles . Select the Active roles and verify there are no roles listed. On the Eligible roles tab notice the Billing Administrator role. Under the Action column, select Activate . Assignment details are shown in the Portal. This includes start and end times, and the ability to add a reason. Add a reason and then click Activate . The Activation status should show all the activation stages have been completed. Use the link to Sign out . You must sign out and log back in to start using your newly activated role. Test the role access # In this task, test the Billing Administrator role. Sign in to the Portal as AZ500User1 . Search for and select Azure AD Privileged Identity Management . Under Activate select Azure AD Roles . Select the Active roles tab and verify the Billing Administrator role has been activated. The role should show Activated . Notice the ability to Deactivate the role. Task 2: Azure AD PIM for resources # In this task, we will configure PIM for Azure resources, activate the Virtual Machine Contributor role, and test the role access. Configure PIM for Azure resources # In this task, we will add the subscription to PIM, then add the Virtual Machine Contributor role as an Active role. In the Portal , search for and select Azure AD Privileged Identity Management . Under Manage select Azure Resources . Click Discover resources . Notice the Resource state is Unmanaged . Select the subscription you want to manage. Click Manage resource . Click Yes to confirm that PIM will manage all child objects for the selected resource. Return to the Azure resources blade. Select your subscription. Under Manage click Roles . Search for and select the Virtual machine contributor role. Click Add assignments , then click Select member(s) and add the AZ500User1 to the group. On the Membership settings page set the Assignment type is Active . Add the role and Save your changes. Sign out of the Portal. Activate the role # In this task, we will sign-in as a user and activate the role. Sign in to the Portal and AZ500User1. Search for and select Azure AD Privileged Identity Management. Under Tasks select My roles. Under Activate select Azure resources. On the Active roles tab notice you have no assigned roles. On the Eligible roles tab scroll to the right and Activate the role. Notice the Start time and Duration. Provide a reason for the activation. For example, 'Need to add a NIC'. Click Activate. The Activation status should show all the activation stages have been completed. Use the link to Sign out. You must sign out and log back in to start using your newly activated role. Test the role access # In this task, we will check to ensure the role has been assigned. Sign in to the Portal as AZ500User1. Search for and select Azure AD Privileged Identity Management. Under Activate select Azure resources. Select the Active roles tab and verify the Virtual Machine Contributor role has been activated. Sign out of the Portal. Sign in to the Portal using a Global Admin account. Search for and select Azure Active Directory. Under Manage click Users. Select AZ500User1. Under Manage click Assigned roles. Verify there are no roles listed. Under Manage select Azure role assignments. Verify the Virtual machine contributor role is listed. Knowledge check # Check your knowledge 1. To enable Azure AD PIM for your directory, what Azure AD Role do you need to enable PIM? Office 365 Admin Co-Administrator Global Admin ( Ans ) A company has implemented Azure AD PIM. There's a need to ensure a new hire's request elevation before they make any changes in Azure, what should you do? Activate the new hire. Assign the new hire the Eligible role membership type. ( Ans ) Include the new hire in an access review. Azure AD PIM is used to manage which of the following roles? Azure privileged users Azure resource groups Azure AD roles ( Ans ) An organization has enabled Azure AD PIM. The senior IT manager wants the role set up so no action is required, what should you do? Give the manager JIT access to the role. Make the manager Permanent Active in the role. ( Ans ) Make the manager Assigned to a role. Design an enterprise governance strategy # Introduction # The first part of your organization's secure Azure tenant is set up, but you need to monitor and maintain it. Enterprise Governance is the process of setting strategic tools, systems, and process into motion to keep your systems secure and running well. Scenario # A security engineer uses enterprise governance tools and policies to manage and maintain a secure Azure solution; some common tasks are: Designing an Azure secure access hierarchy. Using RBAC and Azure Policy to control and manage access. Creating blueprints of secure deployments that can be reused. Review the shared responsibility model # Organizations face many challenges with securing their data centers, including recruiting and keeping security experts, using many security tools, and keeping pace with the volume and complexity of threats. As computing environments move from customer-controlled data centers to the cloud, the security responsibility also shifts. Security of the operational environment is now a concern shared by both cloud providers and customers. By shifting these responsibilities to a cloud service like Azure, organizations can reduce focus on activities that aren't core business competencies. Depending on the specific technology choices, some security protections will be built into the service, while others will remain the customer\u2019s responsibility. To ensure that the proper security controls are provided, a careful evaluation of the services and technology choices becomes necessary. The first thing to understand about cloud security is that different scopes of responsibility exist depending on the kinds of services you use. For example, if you use virtual machines (VMs) in Azure, which provide Infrastructure as a Service (IaaS), Microsoft will be responsible for helping secure the physical network, physical storage, and virtualization platform, which includes updating the virtualization hosts. But you\u2019ll need to take care of helping secure your virtual network and public endpoints and updating the guest operating system (OS) of your VMs. The following figure depicts the various responsibility zones. For all cloud deployment types, you own your data and identities. You're responsible for helping secure your data and identities, your on-premises resources, and the cloud components you control (which vary by service type). Regardless of the deployment type, you always retain responsibility for the following: Data Endpoints Accounts Access management Important It\u2019s important to understand the division of responsibility between you and Microsoft in a Software as a Service (SaaS), Platform as a Service (PaaS), or Infrastructure as a Service (IaaS) deployment. Explore the Azure cloud security advantages # The cloud offers significant advantages for solving long standing information security challenges. In an on-premises environment, organizations likely have unmet responsibilities and limited resources available to invest in security, which creates an environment where attackers can exploit vulnerabilities at all layers. The following diagram shows a traditional approach where many security responsibilities are unmet due to limited resources. In the cloud-enabled approach, you can shift day-to-day security responsibilities to your cloud provider and reallocate your resources. In the cloud-enabled approach, you are also able to leverage cloud-based security capabilities for more effectiveness and use cloud intelligence to improve your threat detection and response time. By shifting responsibilities to the cloud provider, organizations can get more security coverage, which enables them to reallocate security resources and budget to other business priorities. Important What security advantages are you expecting from leveraging the cloud? Review Azure hierarchy of systems # Azure Resource Manager is the deployment and management service for Azure. It provides a consistent management layer that allows you to create, update, and delete resources in your Azure subscription. You can use its access control, auditing, and tagging features to help secure and organize your resources after deployment. When you take actions through the portal, Azure PowerShell, the Azure CLI, REST APIs, or client software development kits (SDKs), the Resource Manager API handles your request. Because the same API handles all requests, you get consistent results and capabilities from all the different tools. Functionality initially released through APIs should be represented in the portal within 180 days of the initial release. Understand Scope # Azure provides four levels of scope: management groups, subscriptions, resource groups, and resources. The following image shows an example of these layers. Though not labeled as such, the blue cubes are resources. You apply management settings at any of these levels of scope. The level you select determines how widely the setting is applied. Lower levels inherit settings from higher levels. For example, when you apply a policy to the subscription, the policy is applied to all resource groups and resources in your subscription. When you apply a policy on the resource group, that policy is applied to the resource group and all its resources. However, another resource group doesn't have that policy assignment. You can deploy templates to management groups, subscriptions, or resource groups. Resource Groups # There are some important factors to consider when defining your resource group: All the resources in your group should share the same lifecycle. You deploy, update, and delete them together. If one resource, such as a database server, needs to exist on a different deployment cycle it should be in another resource group. Each resource can only exist in one resource group. You can add or remove a resource to a resource group at any time. You can move a resource from one resource group to another group. A resource group can contain resources that are located in different regions. A resource group can be used to scope access control for administrative actions. A resource can interact with resources in other resource groups. This interaction is common when the two resources are related but don't share the same lifecycle (for example, web apps connecting to a database). When creating a resource group, you need to provide a location for that resource group. You may be wondering, \"Why does a resource group need a location? And, if the resources can have different locations than the resource group, why does the resource group location matter at all?\" The resource group stores metadata about the resources. Therefore, when you specify a location for the resource group, you're specifying where that metadata is stored. For compliance reasons, you may need to ensure that your data is stored in a particular region. If the resource group's region is temporarily unavailable, you can't update resources in the resource group because the metadata is unavailable. The resources in other regions will still function as expected, but you can't update them. Management Groups # Management groups are an Azure resource to create flexible and very maintainable hierarchies within the structure of your environment. Management groups exist above the subscription level thus allowing subscriptions to be grouped together. This grouping facilitates applying policies and RBAC permissions to those management groups. Policies and RBAC permissions are inherited to all resources in the management group. Management groups give you enterprise-grade management at a large scale no matter what type of subscriptions you might have. All subscriptions within a single management group must trust the same Azure Active Directory tenant. Management group hierarchies can be up to six levels deep. This provides you with the flexibility to create a hierarchy that combines several of these strategies to meet your organizational needs. For example, the diagram below shows an organizational hierarchy that combines a business unit strategy with a geographic strategy. The value of management groups # Group your subscriptions. Provide user access to multiple subscriptions Allows for new organizational models and logically grouping of resources. Allows for single assignment of controls that applies to all subscriptions. Provides aggregated views above the subscription level. Mirror your organization's structure. Create a flexible hierarchy that can be updated quickly. The hierarchy does not need to model the organization's billing hierarchy. The structure can easily scale up or down depending on your needs. Apply policies or access controls to any service. Create one RBAC assignment on the management group, which will inherit that access to all the subscriptions. Use Azure Resource Manager integrations that allow integrations with other Azure services: Azure Cost Management, Privileged Identity Management, and Microsoft Defender for Cloud. Important By using management groups, you can reduce your workload and reduce the risk of error by avoiding duplicate assignments. Instead of applying multiple assignments across numerous resources and subscriptions, you can apply the one assignment on the one management group that contains the target resources. This will save time in the application of assignments, creates one point for maintenance, and allows for better controls on who can control the assignment. Configure Azure policies # Azure Policy is a service you use to create, assign, and manage policies. These policies enforce different rules and effects over your resources so that those resources stay compliant with your corporate standards and service level agreements. Azure Policy meets this need by evaluating your resources for noncompliance with assigned policies. For example, you might have a policy that allows virtual machines of only a certain size in your environment. After this policy is implemented, new and existing resources are evaluated for compliance. With the right type of policy, existing resources can be brought into compliance. There are three main pillars in the functionalities of Azure policy. The first pillar is around real-time enforcement and compliance assessment. For example, a policy would block the creation of resources that are located outside of US regions. Each policy also provides compliance assessment on all your existing resources to bring a state of compliance for each resource. The data then powers the compliance view which aggregates results across all of the applied policies. Policies can be used to ensure that resource groups are getting tagged properly and automatically inheriting those tags from the resource group down to the resources. The second pillar of policy is applying policies at scale by leveraging Management Groups. By assigning policy to a management group one can impact hundreds of subscriptions and all its reach resources through a single policy assignment. There also is the concept called policy initiative that allows you to group policies together so that you can view the aggregated compliance result. At the initiative level there's also a concept called exclusion where one can exclude either the child management group subscription resource group or resources from the policy assignment. The third pillar of your policy is remediation by leveraging a remediation policy that will automatically remediate the non-compliant resource so that your environment always stays compliant. For existing resources, they will be flagged as non-compliant but they won't automatically be changed because there can be impact to the environment. For these cases you can create a remediation task to bring these resources to compliance. Azure policy is a free service to use. Policy permissions and custom policies # Azure Policy has several permissions, known as operations, in two resource providers: Microsoft.Authorization Microsoft.PolicyInsights Many built-in roles grant permissions to Azure Policy resources. The Resource Policy Contributor role includes most Azure Policy operations. The Owner role has full rights. Both Contributor and Reader can use all Azure Policy read operations, but Contributor can also trigger remediation. If none of the built-in roles have the required permissions, create a custom role. Azure has by default, security policies that work across subscriptions or on management groups. If these policies need to be augmented with your own organizational policies, new policies can be created. Whatever the business driver for creating a custom policy, the steps are the same for defining the new custom policy. Before creating a custom policy, check the policy samples to determine if a policy that matches your needs already exists. The approach to creating a custom policy follows these steps: Identify your business requirements Map each requirement to an Azure resource property Map the property to an alias Determine which effect to use Compose the policy definition Composing an Azure Policy # The steps for composing and implementing a policy in Azure Policy begins with creating: Policy definition - Every policy definition has conditions under which it's enforced. And, it has a defined effect that takes place if the conditions are met. Policy assignment - A policy definition that has been assigned to take place within a specific scope. This scope could range from a management group to an individual resource. The term scope refers to all the resources, resource groups, subscriptions, or management groups that the policy definition is assigned to. Policy parameters - They help simplify your policy management by reducing the number of policy definitions you must create. You can define parameters when creating a policy definition to make it more generic. Create and assign an Initiative definition # In order to easily track compliance for multiple resources, create and assign an Initiative definition. With an initiative definition, you can group several policy definitions to achieve one overarching goal. An initiative evaluates resources within scope of the assignment for compliance to the included policies. To implement these policy definitions (both built-in and custom definitions), you'll need to assign them. You can assign any of these policies through the Azure portal, PowerShell, or Azure CLI. Enable Azure role-based access control (RBAC) # When it comes to identity and access, most organizations that are considering using the public cloud are concerned about two things: Ensuring that when people leave the organization, they lose access to resources in the cloud. Striking the right balance between autonomy and central governance\u2014for example, giving project teams the ability to create and manage virtual machines in the cloud while centrally controlling the networks to which those virtual machines connect. RBAC is an authorization system built on Azure Resource Manager that provides fine-grained access management of Azure resources. Azure AD and Role Based Access Control (RBAC) make it simple for you to carry out these goals. After you extend your on-premises Active Directory to the cloud by using Azure AD Connect, your employees can use and manage their Azure subscriptions by using their existing work identities. These Azure subscriptions automatically connect to Azure AD for SSO and access management. When you disable an on-premises Active Directory account, it automatically loses access to all Azure subscriptions connected with Azure AD. Additionally, synchronizing passwords to the cloud to support these checks also add resiliency during some attacks. Customers affected by (Not)Petya attacks were able to continue business operations when password hashes were synced to Azure AD (vs. near zero communications and IT services for customers affected organizations that had not synchronized passwords). RBAC enables fine-grained access management for Azure. Using RBAC, you can grant just the amount of access that users need to perform their jobs. For example, you can use RBAC to let one employee manage virtual machines in a subscription while another manages SQL databases within the same subscription. Each Azure subscription is associated with one Azure AD directory . Users, groups, and applications in that directory can manage resources in the Azure subscription. Grant access by assigning the appropriate RBAC role to users, groups, and applications at a certain scope. The scope of a role assignment can be a subscription, a resource group, or a single resource. A role assigned at a parent scope also grants access to the child scopes contained within it. For example, a user with access to a resource group can manage all the resources it contains, like websites, virtual machines, and subnets. The RBAC role that you assign dictates what resources the user, group, or application can manage within that scope. The following diagram depicts how the classic subscription administrator roles, RBAC roles, and Azure AD administrator roles are related at a high level. Roles assigned at a higher scope, like a subscription, are inherited by child scopes, like service instances. Important Note that a subscription is associated with only one Azure AD tenant. Also note that a resource group can have multiple resources but is associated with only one subscription. Lastly, a resource can be bound to only one resource group. Compare and contrast Azure RBAC vs Azure policies # There are a few key differences between Azure Policy and Azure role-based access control (Azure RBAC). Azure Policy evaluates the state by examining properties on resources that are represented in Resource Manager and properties of some Resource Providers. Azure Policy ensures that the resource state is compliant with your business rules without concern for who made the change or who has permission to make a change. Azure Policy, through the DenyAction effect, can also block specific actions on resources. Some Azure Policy resources, such as policy definitions, initiative definitions, and assignments, are visible to all users. This design enables transparency to all users and services regarding what policy rules are set in their environment. Azure RBAC focuses on managing user actions at different scopes. If control of an action is required based on user information, then Azure RBAC is the correct tool to use. Even if an individual has access to perform an action, if the result is a non-compliant resource, Azure Policy still blocks the create or update task. The combination of Azure role-based access control (Azure RBAC) and Azure Policy provides full-scope control in Azure. Azure Policy has several permissions, known as operations, in two Resource Providers: Microsoft.Authorization Microsoft.PolicyInsights Many built-in roles grant permission to Azure Policy resources. The Resource Policy Contributor role includes most Azure Policy operations. The owner has full rights. Both Contributor and Reader have access to all read Azure Policy operations. A contributor may trigger resource remediation but can't create or update definitions and assignments. User Access Administrator is necessary to grant the managed identity on deployIfNotExists or modify the assignment's necessary permissions. Note All Policy objects, including definitions, initiatives, and assignments, will be readable to all roles over its scope. For example, a Policy assignment scoped to an Azure subscription will be readable by all role holders at the subscription scope and below. If none of the built-in roles have the permissions required, create a custom role. Azure Policy operations can have a significant impact on your Azure environment. Only the minimum set of permissions necessary to perform a task should be assigned and these permissions should not be granted to users who do not need them. Note The managed identity of a deployIfNotExists or modify policy assignment needs enough permissions to create or update targeted resources. Example: Role-Based Access Control (RBAC) vs. Azure Policy # Important RBAC and Polices in Azure play a vital role in a governance strategy. While different, they both work together to ensure organizational business rules are followed by ensuring proper access and resource creation guidelines are met. Configure built-in roles # Azure role-based access control (RBAC) has several Azure built-in roles that you can assign to users, groups, service principals, and managed identities. Role assignments are the way you control access to Azure resources. If the built-in roles don't meet the specific needs of your organization, you can create your own Azure custom roles. The four general built-in roles are: Built-in Role Description Contributor Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries. Owner Grants full access to manage all resources, including the ability to assign roles in Azure RBAC. Reader View all resources, but does not allow you to make any changes. User Access Administrator Lets you manage user access to Azure resources. Custom roles for Azure resources # If the built-in roles for Azure resources don't meet the specific needs of your organization, you can create your own custom roles. Just like built-in roles, you can assign custom roles to users, groups, and service principals at management group, subscription, and resource group scopes. Custom roles can be shared between subscriptions that trust the same Azure AD directory. There is a limit of 5,000 custom roles per directory. (For Azure Germany and Azure China 21Vianet, the limit is 2,000 custom roles.) Custom roles can be created using the Azure portal, Azure PowerShell, Azure CLI, or the REST API. Custom role limits # The following list describes the limits for custom roles. Each directory can have up to 5000 custom roles. Azure Germany and Azure China 21Vianet can have up to 2000 custom roles for each directory. You cannot set AssignableScopes to the root scope (\"/\"). You can only define one management group in AssignableScopes of a custom role. Adding a management group to AssignableScopes is currently in preview. Custom roles with DataActions cannot be assigned at the management group scope. Azure Resource Manager doesn't validate the management group's existence in the role definition's assignable scope. Enable resource locks # As an administrator, you may need to lock a subscription, resource group, or resource to prevent other users in your organization from accidentally deleting or modifying critical resources. You can set the lock level to CanNotDelete or ReadOnly. In the portal, the locks are called Delete and Read-only respectively. CanNotDelete means authorized users can still read and modify a resource, but they can't delete the resource. ReadOnly means authorized users can read a resource, but they can't delete or update the resource. Applying this lock is similar to restricting all authorized users to the permissions granted by the Reader role. Who can create or delete locks # To create or delete management locks, you must have access to **Microsoft.Authorization/***or Microsoft.Authorization/locks/* actions. Of the built-in roles, only Owner and User Access Administrator are granted those actions. Deploy Azure blueprints # Just as a blueprint allows an engineer or an architect to sketch a project's design parameters, Azure Blueprints enables cloud architects and central information technology groups to define a repeatable set of Azure resources that implements and adheres to an organization's standards, patterns, and requirements. Azure Blueprints allows development teams to rapidly build and stand up new environments with the trust they're building within organizational compliance with a set of built-in components, such as networking, to speed up development and delivery. Blueprints are a declarative way to orchestrate the deployment of various resource templates and other artifacts, such as: Role Assignments Policy Assignments Azure Resource Manager templates Resource Groups The Azure Blueprints service is supported by the globally distributed Azure Cosmos Data Base. Blueprint objects are replicated in multiple Azure regions. This replication provides low latency, high availability, and consistent access to your blueprint objects, regardless of which region Blueprints deploys your resources to. How is it different from Azure Resource Manager templates? # The service design helps with environment setup. This setup often includes resource groups, policies, role assignments, and Resource Manager template deployments assigned to a subscription in a single audited and tracked operation. A blueprint is a package to bring each artifact type together and allows you to compose and version that package into a continuous integration and pipeline. Nearly everything that you want to include for deployment in Blueprints is also with a Resource Manager template. However, a Resource Manager template is a document that doesn't exist natively in Azure \u2013 it's stored either locally or in source control. The template gets used for deployments of one or more Azure resources, but once those resources deploy, there's no active connection or relationship to the template. Blueprints save the relationship between the blueprint definition and the blueprint assignment. This connection supports improved tracking and auditing of deployments. Blueprints can upgrade several subscriptions governed by the exact blueprint. There's no need to choose between a Resource Manager template and a blueprint. Each blueprint can consist of zero or more Resource Manager template artifacts. This support means that previous efforts to develop and maintain a library of Resource Manager templates are reusable in Blueprints. How it's different from Azure Policy # A blueprint is a package or container for composing focus-specific standards, patterns, and requirements for implementing Azure cloud services, security, and design reused to maintain consistency and compliance. An Azure policy is a default allow and explicit deny system focused on resource properties during deployment and for existing resources. It supports cloud governance by validating that help within a subscription adhere to requirements and standards. Including an Azure policy in a blueprint enables the creation of the correct pattern or design during the assignment of the blueprint. The policy inclusion ensures that only approved or expected changes can be made to the environment to protect ongoing compliance with the intent of the blueprint. An Azure policy is available as one of many artifacts in a blueprint definition. Blueprints also support using parameters with policies and initiatives. Blueprint definition # A blueprint is composed of artifacts. Azure Blueprints currently supports the following resources as artifacts: Resource Hierarchy options Description Resource Groups Subscription Create a new resource group for use by other artifacts within the blueprint. These placeholder resource groups enable you to organize resources exactly how you want them structured and provide a scope limiter for included policy and role assignment artifacts and ARM templates. ARM template Subscription, Resource Group Templates, including nested and linked templates, are used to compose complex environments. Example environments: a SharePoint farm, Azure Automation State Configuration, or a Log Analytics workspace. Policy Assignment Subscription, Resource Group Allows assignment of a policy or initiative to the subscription the blueprint is assigned to. The policy or initiative must be within the scope of the blueprint definition location. If the policy or initiative has parameters, these parameters are assigned at the creation of the blueprint or during blueprint assignment. Role Assignment Subscription, Resource Group Add an existing user or group to a built-in role to make sure the right people always have the right access to your resources. Role assignments can be defined for the entire subscription or nested to a specific resource group included in the blueprint. Blueprint definition locations # When creating a blueprint definition, you'll define where the blueprint is saved. Blueprints can be saved to a management group or subscription that you have Contributor access to. If the location is a management group, the blueprint is available to assign to any child subscription of that management group. Blueprint parameters # Blueprints can pass parameters to either a policy/initiative or an ARM template. When adding either artifact to a blueprint, the author decides to provide a defined value for each blueprint assignment or to allow each blueprint assignment to provide a value at assignment time. This flexibility provides the option to define a pre-determined value for all uses of the blueprint or to enable that decision to be made at the time of assignment. Note Assigning a blueprint definition to a management group means the assignment object exists in the management group. The deployment of artifacts still targets a subscription. To perform a management group assignment, the Create Or Update REST API must be used, and the request body must include a value for properties.scope to define the target subscription. Design an Azure subscription management plan # An Azure Active Directory (AD) tenant is created for you when you sign up for Azure. The tenant represents your account. You use the tenant to manage access to your subscriptions and resources. When you create a new subscription, it's hosted in your account's Azure AD tenant. If you want to give others access to your subscription or its resources, you need to invite them to join your tenant. Doing so helps you control access to your subscriptions and resources. You can create additional subscriptions for your account in Azure. You might want an additional subscription to avoid reaching subscription limits, to create separate environments for billing and security, or to isolate data for compliance reasons. If you want to create Azure subscriptions under your organization's Enterprise Agreement (EA), you need to have the Account Owner role for your organization. If you need to transfer billing ownership of your Azure subscription if you're leaving your organization, or you want your subscription to be billed to another account. Transferring billing ownership to another account provides the administrators in the new account permission for billing tasks. They can change the payment method, view charges, and cancel the subscription. Manage API access to Azure subscriptions and resources # When you publish APIs through API Management, it's easy and common to gain access to those APIs by using subscription keys. Client applications that consume the published APIs need to include a valid subscription key in HTTP requests when they make calls to those APIs. To get a subscription key for accessing APIs, a subscription is required. A subscription is essentially a named container for a pair of subscription keys. Developers who need to consume the published APIs can get subscriptions, and they don't need approval from API publishers. API publishers can also directly create subscriptions for API consumers. API Management supports additional mechanisms for gaining access to APIs, including: OAuth 2.0 Client certificates IP allowlists Azure policies encapsulate common API management functions, like those for access control, protection, transformation, and caching. You can chain these policies together into a pipeline that mutates a request\u2019s context or changes the API behavior. You can apply these policies to a variety of scopes, trigger them on an error, and set them in the inbound and outbound directions. Who can transfer a subscription? # A billing administrator or the account administrator is a person who has permission to manage billing for an account. They're authorized to access billing on the Azure portal and do various billing tasks like create subscriptions, view and pay invoices, or update payment methods. If you're an Enterprise Agreement (EA) customer, your enterprise administrators can transfer billing ownership of your subscriptions between accounts. To identify accounts for which you're a billing administrator, use the following steps: Visit the Cost Management + Billing page in Azure portal. Select All billing scopes from the left-hand pane. The subscriptions page lists all subscriptions where you're a billing administrator. Explore Try-This exercises # Use your own Azure subscription to perform these Try-This exercises - Enterprise Governance. Task 1 - Navigating Azure # In this task, you'll learn how to access and use the Azure portal. Locate the Azure portal In this task, you'll access the lab environment and the Azure portal. Ask your instructor how to access the lab environment. After accessing the lab environment, navigate to the Azure portal. Bookmark this page. You'll use the Portal throughout the course labs and demonstrations. In the top right corner of the Portal, select your user account. Notice you can View account and Switch directory. Switch directory lets you view My permissions and View my bill. Select the Settings icon (top right menu bar - cog icon). Review the Portal settings including the General and Language & region settings. Use the Search resources, services, and docs textbox to search for Virtual machines. You can search for not only general Azure resources but specifically but named resources. Select Use the Portal menu (left corner three bars icon). Notice you can Create a resource, view All services, and view All resources. Take some time to browse around the interface, search and explore different areas. Launch the Cloud Shell (first icon top menu bar). Notice the drop-down for PowerShell or Bash. Task 2 - Azure RBAC Role Assignments # In this task, we'll learn about role assignments. Locate Access Control blade Access the Azure portal, and select a resource group. Make a note of what resource group you use. Select the Access Control (IAM) blade. This blade is available for many different resources so you can control access. Review role permissions Click the Roles tab (top). Select a desired role from the list by clicking the associated box next to the Role Name. Click the View link under the Details column on the far right of the page. The Permissions view is displayed including three columns from left to right that is (i.e., Other, Read, Write, and Delete) Permissions, and Description. Return to the Access Control (IAM) blade. Task 3 - Manage resource locks # Note: This task requires a resource group. In this task, we'll create resource locks. In the Portal navigate to a resource group. In the Settings section, click Locks, and then click + Add. Discuss the different types of locks and applying the locks at different levels. Create a new lock with a Lock type of Delete. From the Overview blade, click Delete resource group. Type the name of the resource group and click OK. You should receive an error message stating the resource group is locked and can't be deleted. Add a Storage Account to the resource group. After the storage account is created, try to delete the storage account. You receive an error message stating the resource or its parent has a delete lock. Review how the storage account inherits the lock from the parent and can't be deleted. Return to the resource group blade and, in the Settings section, click Locks. Scroll all the way to the right, then click the Delete link to the right of the lock. Return to the storage account and confirm you can now delete the resource. Knowledge check # Choose the best response for each of the questions below. Then select Check your answers. Check your knowledge # The company hires a new administrator and needs to create a new Azure AD user account for them. The new hire must be able to: - Read/write resource deployments they're responsible for. - Read Azure AD access permissions They shouldn't be able to view Azure subscription information. What should be configured to make this work? Assign the user the Contributor role at the resource group level. ( Ans ) Assign the user the Owner role at the resource level. Assign the user the Global Administrator role. Which of the following would be good example of when to use a resource lock? An ExpressRoute circuit with connectivity back to your on-premises network. ( Ans ) A virtual machine used to test occasional application builds. A storage account used to store images processed in a development environment. A company has three virtual machines (VM1, VM2, and VM3) in a resource group. The Helpdesk hires a new employee. The new employee must be able to modify the settings on VM3, but not on VM1 and VM2. Your solution must minimize administrative overhead. What should be set up? Assign the user to the Contributor role on the resource group. Assign the user to the Contributor role on VM3. ( Ans ) Move VM3 to a new resource group and assign the user to the Contributor role on VM3. You need to target policies and review spend budgets across several subscriptions you manage. What should be created for the subscriptions? A billing group A management group ( Ans ) A nested resource group A manager asks for an explanation of how Azure uses resource groups. Which of the following capabilities is a feature of how Azure uses resource groups? Resources can be in multiple resource groups. Resources can be moved from one resource group to another resource group. ( Ans ) Resource groups can be nested.","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#az-500-microsoft-certified-azure-security-engineer-associate","text":"","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#manage-identity-and-access","text":"Secure Azure solutions with Azure Active Directory Introduction Explore Azure Active Directory features Self-managed Active Directory Domain Services, Azure Active Directory, and managed Azure Active Directory Domain Services Azure AD DS and self-managed AD DS Azure AD DS and Azure AD Investigate roles in Azure AD Azure AD built-in roles Deploy Azure AD Domain Services Create and manage Azure AD users Manage users with Azure AD groups Configure Azure AD administrative units Implement passwordless authentication Exercises Implement Hybrid identity Introduction Deploy Azure AD connect Explore authentication options Configure Password Hash Synchronization (PHS) Implement Pass-through Authentication (PTA) Deploy Federation with Azure AD Explore the authentication decision tree Configure password writeback Deploy Azure AD identity protection Introduction Explore Azure AD identity protection Configure risk event detections Implement user risk policy Implement sign-in risk policy Deploy multifactor authentication in Azure Explore multifactor authentication settings Enable multifactor authentication Implement Azure AD conditional access Configure conditional access conditions Implement access reviews Exercises Configure Azure AD privileged identity management Introduction Explore the zero trust model Review the evolution of identity management Deploy Azure AD privileged identity management Configure privileged identity management scope Implement privileged identity management onboarding Explore privileged identity management configuration settings Implement a privileged identity management workflow Exercises Design an enterprise governance strategy Introduction Review the shared responsibility model Review Azure hierarchy of systems Configure Azure policies Enable Azure role-based access control (RBAC) Compare and contrast Azure RBAC vs Azure policies Configure built-in roles Enable resource locks Deploy Azure blueprints Design an Azure subscription management plan Exercises The Azure security engineer implements, manages, and monitors security for resources in Azure, multi-cloud, and hybrid environments as part of an end-to-end infrastructure. They recommend security components and configurations to protect identity & access, data, applications, and networks. Responsibilities for an Azure security engineer include managing the security posture, identifying and remediating vulnerabilities, performing threat modelling, and implementing threat protection. They may also participate in responding to security incidents. Azure security engineers work with architects, administrators, and developers to plan and implement solutions that meet security and compliance requirements. The Azure security engineer should have practical experience in administration of Microsoft Azure and hybrid environments. The Azure security engineer should have a strong familiarity with compute, network, and storage in Azure, as well as Azure Active Directory, part of Microsoft Entra.","title":"Manage Identity and Access"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#az-500-manage-identity-and-access","text":"Explore how identity is the starting point for all security within your company. Learn to authenticate and authorize users and apps with Azure Active Directory.","title":"AZ-500: Manage Identity and Access"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#introduction","text":"Azure Active Directory (Azure AD) is Microsoft\u2019s cloud-based identity and access management service, which helps your employee's sign in and access resources in: External resources, such as Microsoft 365, the Azure portal, and thousands of other SaaS applications. Internal resources, such as apps on your corporate network and intranet, along with any cloud apps developed by your own organization.","title":"Introduction"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#scenario","text":"A security engineer uses Azure Active Directory's identity and access management services to execute and facilitate the following tasks: Create users, groups, and administrative units to securely access resources and services. Configure access to systems to be passwordless. Define a strategy for using Azure AD and Azure AD Domain Services to lock down access to your solutions.","title":"Scenario"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#skills-measured","text":"Azure Active Directory is a part of Exam AZ-500: Microsoft Azure Security Engineer. Manage identity and access (30-35%) Manage Azure AD identities Manage Azure AD directory groups Manage Azure AD users Manage administrative unit","title":"Skills measured"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#learning-objectives","text":"Compare and contrast Azure AD versus on-premises directory services with Azure AD Domain Services. Configure and deploy users, groups, and administrative units to securely access resources in your tenant. Deploy a passwordless login solution for your Azure users and resources.","title":"Learning objectives"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-azure-active-directory-features","text":"Azure Active Directory (Azure AD) is a cloud-based identity and access management service. This service helps employees access external resources, such as Microsoft 365, the Azure portal, and thousands of other software-as-a-service (SaaS) applications. Azure Active Directory also helps them access internal resources like apps on your corporate intranet network, along with any cloud apps developed for your organization.","title":"Explore Azure Active Directory features"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#who-uses-azure-ad","text":"Azure AD is intended for: IT admins: As an IT admin, use Azure AD to control access to your apps and your app resources based on your business requirements. For example, you can use Azure AD to require multi-factor authentication when accessing important organizational resources. You can also use Azure AD to automate user provisioning between your existing Windows Server AD and your cloud apps, including Microsoft 365. Finally, Azure AD gives you powerful tools to automatically help protect user identities and credentials and to meet your access governance requirements. App developers: As an app developer, you can use Azure AD as a standards-based approach for adding single sign-on (SSO) to your app, allowing it to work with a user's pre-existing credentials. Azure AD also provides APIs that can help you build personalized app experiences using existing organizational data. Microsoft 365, Office 365, Azure, or Dynamics Customer relationship management (CRM) Online subscribers: As a subscriber, you're already using Azure AD. Each Microsoft 365, Office 365, Azure, and Dynamics CRM Online tenant is automatically an Azure AD tenant. You can immediately start to manage access to your integrated cloud apps.","title":"Who uses Azure AD?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#what-are-the-azure-ad-licenses","text":"Microsoft Online business services, such as Microsoft 365 or Microsoft Azure, require Azure AD for sign-in activities and to help with identity protection. If you subscribe to any Microsoft Online business service, you automatically get Azure AD with access to all the free features. To enhance your Azure AD implementation, you can also add paid capabilities by upgrading to Azure Active Directory Premium P1 or Premium P2 licenses. Azure AD paid licenses are built on top of your existing free directory. The licenses provide self-service, enhanced monitoring, security reporting, and secure access for your mobile users. Azure Active Directory Free: Provides user and group management, on-premises directory synchronization, basic reports, self-service password change for cloud users, and single sign-on across Azure, Microsoft 365, and many popular SaaS apps. Azure Active Directory Premium P1: In addition to the Free features, P1 lets your hybrid users access both on-premises and cloud resources. It also supports advanced administration, such as dynamic groups, self-service group management, Microsoft Identity Manager, and cloud write-back capabilities, which allow self-service password reset for your on-premises users. Azure Active Directory Premium P2: In addition to the Free and P1 features, P2 also offers Azure Active Directory Identity Protection to help provide risk-based Conditional Access to your apps and critical company data and Privileged Identity Management to help discover, restrict, and monitor administrators and their access to resources and to provide just-in-time access when needed. \"Pay as you go\" feature licenses: You also get additional feature licenses, such as Azure Active Directory Business-to-Customer (B2C). B2C can help you provide identity and access management solutions for your customer-facing apps.","title":"What are the Azure AD licenses?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#which-features-work-in-azure-ad","text":"After you choose your Azure AD license, you'll get access to some or all of the following features: Category Description Application management Manage your cloud and on-premises apps using Application Proxy, single sign-on, the My Apps portal, and Software as a Service (SaaS) apps. Authentication Manage Azure Active Directory self-service password reset, Multi-Factor Authentication, custom banned password list, and smart lockout. Azure Active Directory for developers Build apps that sign in all Microsoft identities, and get tokens to call Microsoft Graph, other Microsoft APIs, or custom APIs. Business-to-Business (B2B) Manages your guest users and external partners while maintaining control over your own corporate data. Business-to-Customer (B2C) Manages your guest users and external partners while maintaining control over your own corporate data. Conditional Access Manage access to your cloud apps. Device Management Manage how your cloud or on-premises devices access your corporate data. Domain services Join Azure virtual machines to a domain without using domain controllers. Enterprise users Manage license assignments, access to apps, and set up delegates using groups and administrator roles. Hybrid identity Use Azure Active Directory Connect and Connect Health to provide a single user identity for authentication and authorization to all resources, regardless of location (cloud or on-premises). Identity governance Manage your organization's identity through employee, business partner, vendor, service, and app access controls. You can also perform access reviews. Identity protection Detect potential vulnerabilities affecting your organization's identities, configure policies to respond to suspicious actions, and then take appropriate action to resolve them. Managed identities for Azure resources Provide your Azure services with an automatically managed identity in Azure AD that can authenticate any Azure AD-supported authentication service, including Key Vault. Privileged identity management (PIM) Manages, controls, and monitors access within your organization. This feature includes access to resources in Azure AD and Azure, and other Microsoft Online Services, like Microsoft 365 or Intune. Reports and monitoring Gain insights into the security and usage patterns in your environment.","title":"Which features work in Azure AD?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#self-managed-active-directory-domain-services-azure-active-directory-and-managed-azure-active-directory-domain-services","text":"To provide applications, services, or devices access to a central identity, there are three common ways to use Active Directory-based services in Azure. This choice in identity solutions gives you the flexibility to use the most appropriate directory for your organization's needs. For example, if you mostly manage cloud-only users that run mobile devices, it may not make sense to build and run your own Active Directory Domain Services (AD DS) identity solution. Instead, you could use Azure Active Directory. Although the three Active Directory-based identity solutions share a common name and technology, they're designed to provide services that meet different customer demands. At a high level, these identity solutions and feature sets are: Azure Active Directory (Azure AD) - Cloud-based identity and mobile device management that provides user account and authentication services for resources such as Microsoft 365, the Azure portal, or SaaS applications. Azure AD can be synchronized with an on-premises AD DS environment to provide a single identity to users that works natively in the cloud. Active Directory Domain Services (AD DS) - Enterprise-ready lightweight directory access protocol (LDAP) server that provides key features such as identity and authentication, computer object management, group policy, and trusts. AD DS is a central component in many organizations with an on-premises IT environment and provides core user account authentication and computer management features. Azure Active Directory Domain Services (Azure AD DS) - Provides managed domain services with a subset of fully compatible traditional AD DS features such as domain join, group policy, LDAP, and Kerberos / New Technology LAN Manager (NTLM) authentication. Azure AD DS integrates with Azure AD, which can synchronize with an on-premises AD DS environment. This ability extends central identity use cases to traditional web applications that run in Azure as part of a lift-and-shift strategy.","title":"Self-managed Active Directory Domain Services, Azure Active Directory, and managed Azure Active Directory Domain Services"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-ad-ds-and-self-managed-ad-ds","text":"If you have applications and services that need access to traditional authentication mechanisms such as Kerberos or NTLM, there are two ways to provide Active Directory Domain Services in the cloud: A managed domain that you create using Azure Active Directory Domain Services (Azure AD DS). Microsoft creates and manages the required resources. A self-managed domain that you create and configure using traditional resources such as virtual machines (VMs), Windows Server guest OS, and Active Directory Domain Services (AD DS). You then continue to administer these resources. With Azure AD DS, the core service components are deployed and maintained for you by Microsoft as a managed domain experience. You don't deploy, manage, patch, and secure the AD DS infrastructure for components like the VMs, Windows Server OS, or domain controllers (DCs). Azure AD DS provides a smaller subset of features to traditional self-managed AD DS environment, which reduces some of the design and management complexity. For example, there are no AD forests, domains, sites, and replication links to design and maintain. For applications and services that run in the cloud and need access to traditional authentication mechanisms such as Kerberos or NTLM, Azure AD DS provides a managed domain experience with a minimal amount of administrative overhead. When you deploy and run a self-managed AD DS environment, you must maintain all of the associated infrastructure and directory components. There's additional maintenance overhead with a self-managed AD DS environment, but you're then able to do additional tasks, such as extending the schema or create forest trusts. Common deployment models for a self-managed AD DS environment that provides identity to applications and services in the cloud include the following: Standalone cloud-only AD DS - Azure VMs are configured as domain controllers, and a separate, cloud-only AD DS environment is created. This AD DS environment doesn't integrate with an on-premises AD DS environment. A different set of credentials is used to sign in and administer VMs in the cloud. Resource forest deployment - Azure VMs are configured as domain controllers, and an AD DS domain that's part of an existing forest is created. A trust relationship is then configured to an on-premises AD DS environment. Other Azure VMs can domain-join this resource forest in the cloud. User authentication runs over a VPN / ExpressRoute connection to the on-premises AD DS environment. Extend on-premises domain to Azure - An Azure virtual network connects to an on-premises network using a VPN / ExpressRoute connection. Azure VMs connect to this Azure virtual network, which lets them domain-join to the on-premises AD DS environment. An alternative is to create Azure VMs and promote them as replica domain controllers from the on-premises AD DS domain. These domain controllers replicate over a VPN / ExpressRoute connection to the on-premises AD DS environment. The on-premises AD DS domain is effectively extended into Azure. The following table outlines some of the features you may need for your organization and the differences between a managed Azure AD DS domain or a self-managed AD DS domain: Feature Azure Active Directory Services (Azure AD DS) Self-managed AD DS Managed service \u2713 \u2715 Secure deployments \u2713 The administrator secures the deployment Domain Name System (DNS) server \u2713 (managed service) \u2713 Domain or Enterprise administrator privileges \u2715 \u2713 Domain join \u2713 \u2713 Domain authentication using New Technology LAN Manager (NTLM) and Kerberos \u2713 \u2713 Kerberos constrained delegation Resource-based Resource-based & account-based Custom organizational unit (OU) structure \u2713 \u2713 Group Policy \u2713 \u2713 Schema extensions \u2715 \u2713 Active Directory domain/forest trusts \u2713 (one-way outbound forest trusts only) \u2713 Secure Lightweight Directory Access Protocols (LDAPs) \u2713 \u2713 Lightweight Directory Access Protocol (LDAP) read \u2713 \u2713 Lightweight Directory Access Protocol (LDAP) write \u2713 (within the managed domain) \u2713 Geographical-distributed (Geo-distributed) deployments \u2713 \u2713","title":"Azure AD DS and self-managed AD DS"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-ad-ds-and-azure-ad","text":"Azure AD lets you manage the identity of devices used by the organization and control access to corporate resources from those devices. Users can also register their personal device (a bring-your-own (BYO) model) with Azure AD, which provides the device with an identity. Azure AD then authenticates the device when a user signs in to Azure AD and uses the device to access secured resources. The device can be managed using Mobile Device Management (MDM) software like Microsoft Intune. This management ability lets you restrict access to sensitive resources to managed and policy-compliant devices. Traditional computers and laptops can also join Azure AD. This mechanism offers the same benefits of registering a personal device with Azure AD, such as allowing users to sign in to the device using their corporate credentials. Azure AD joined devices give you the following benefits: Single sign-on (SSO) to applications secured by Azure AD. Enterprise policy-compliant roaming of user settings across devices. Access to the Windows Store for Business using corporate credentials. Windows Hello for Business. Restricted access to apps and resources from devices compliant with corporate policy. Devices can be joined to Azure AD with or without a hybrid deployment that includes an on-premises AD DS environment. The following table outlines common device ownership models and how they would typically be joined to a domain: Type of device Device platforms Mechanism Personal devices Windows 10, iOS, Android, macOS Azure AD registered Organization-owned device not joined to on-premises AD DS Windows 10 Azure AD joined Organization-owned device joined to an on-premises AD DS Windows 10 Hybrid Azure AD joined On an Azure AD-joined or registered device, user authentication happens using modern OAuth / OpenID Connect-based protocols. These protocols are designed to work over the internet, so are great for mobile scenarios where users access corporate resources from anywhere. With Azure AD DS-joined devices, applications can use the Kerberos and New Technology LAN Manager (NTLM) protocols for authentication, so can support legacy applications migrated to run on Azure VMs as part of a lift-and-shift strategy. The following table outlines differences in how the devices are represented and can authenticate themselves against the directory: Aspect Azure AD-joined Azure AD DS-joined Device controlled by Azure AD Azure AD Domain Services managed domain Representation in the directory Device objects in the Azure AD directory Computer objects in the Azure AD DS managed domain Authentication Open Authorization OAuth / OpenID Connect-based protocols Kerberos and NTLM protocols Management Mobile Device Management (MDM) software like Intune Group Policy Networking Works over the internet Must be connected to, or peered with, the virtual network where the managed domain is deployed Great for... End-user mobile or desktop devices Server VMs deployed in Azure If on-premises AD DS and Azure AD are configured for federated authentication using Active Directory Federation Services (ADFS), then there's no (current/valid) password hash available in Azure DS. Azure AD user accounts created before fed auth was implemented might have an old password hash that doesn't match a hash of their on-premises password. Hence Azure AD DS won't validate the user's credentials.","title":"Azure AD DS and Azure AD"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#investigate-roles-in-azure-ad","text":"","title":"Investigate roles in Azure AD"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#categories-of-azure-ad-roles","text":"Azure AD built-in roles differ in where they can be used, which fall into the following three broad categories. Azure AD-specific roles: These roles grant permissions to manage resources within Azure AD only. For example, User Administrator, Application Administrator, and Groups Administrator all grant permissions to manage resources that live in Azure AD. Service-specific roles: For major Microsoft 365 services (non-Azure AD), we have built service-specific roles that grant permissions to manage all features within the service. For example, Exchange Administrator, Intune Administrator, SharePoint Administrator, and Teams Administrator roles can manage features with their respective services. Exchange Administrator can manage mailboxes, Intune Administrator can manage device policies, SharePoint Administrator can manage site collections, Teams Administrator can manage call qualities, and so on. Cross-service roles: There are some roles that span services. We have two global roles - Global Administrator and Global Reader. All Microsoft 365 services honor these two roles. Also, there are some security-related roles like Security Administrator and Security Reader that grant access across multiple security services within Microsoft 365. For example, using Security Administrator roles in Azure AD, you can manage Microsoft 365 Defender portal, Microsoft Defender Advanced Threat Protection, and Microsoft Defender for Cloud Apps. Similarly, in the Compliance Administrator role, you can manage Compliance-related settings in the Compliance portal, Exchange, and so on. The following table is offered as an aid to understanding these role categories. The categories are named arbitrarily and aren't intended to imply any other capabilities beyond the documented Azure AD role permissions. Category Role Azure AD-specific roles Application Administrator Application Developer Authentication Administrator Business to consumer (B2C) Identity Experience Framework (IEF) Keyset Administrator Business to consumer (B2C) Identity Experience Framework (IEF) Policy Administrator Cloud Application Administrator Cloud Device Administrator Conditional Access Administrator Device Administrators Directory Readers Directory Synchronization Accounts Directory Writers External ID User Flow Administrator External ID User Flow Attribute Administrator External Identity Provider Administrator Groups Administrator Guest Inviter Helpdesk Administrator Hybrid Identity Administrator License Administrator Partner Tier1 Support Partner Tier2 Support Password Administrator Privileged Authentication Administrator Privileged Role Administrator Reports Reader User Administrator Cross-service roles Global Administrator Compliance Administrator Compliance Data Administrator Global Reader Security Administrator Security Operator Security Reader Service Support Administrator Service-specific roles Azure DevOps Administrator Azure Information Protection Administrator Billing Administrator Customer relationship management (CRM) Service Administrator Customer Lockbox Access Approver Desktop Analytics Administrator Exchange Service Administrator Insights Administrator Insights Business Leader Intune Service Administrator Kaizala Administrator Lync Service Administrator Message Center Privacy Reader Message Center Reader Modern Commerce User Network Administrator Office Apps Administrator Power BI Service Administrator Power Platform Administrator Printer Administrator Printer Technician Search Administrator Search Editor SharePoint Service Administrator Teams Communications Administrator Teams Communications Support Engineer Teams Communications Support Specialist Teams Devices Administrator Teams Administrator","title":"Categories of Azure AD roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-ad-built-in-roles","text":"In Azure Active Directory (Azure AD), if another administrator or nonadministrator needs to manage Azure AD resources, you assign them an Azure AD role that provides the permissions they need. For example, you can assign roles to allow adding or changing users, resetting user passwords, managing user licenses, or managing domain names.","title":"Azure AD built-in roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#all-roles","text":"Role Description Application Administrator Users in this role can create and manage all aspects of enterprise applications, application registrations, and application proxy settings. Users assigned to this role aren't added as owners when creating new application registrations or enterprise applications. This role also grants the ability to consent for delegated permissions and application permissions, except for application permissions for Microsoft Graph. Application Developer Can create application registrations independent of the Users can register applications setting. Attack Payload Author Users in this role can create attack payloads but not actually launch or schedule them. Attack payloads are then available to all administrators in the tenant, who can use them to create a simulation. Attack Simulation Administrator Users in this role can create and manage all aspects of attack simulation creation, launch/scheduling of a simulation, and the review of simulation results. Members of this role have this access for all simulations in the tenant. Attribute Assignment Administrator Users with this role can assign and remove custom security attribute keys and values for supported Azure AD objects such as users, service principals, and devices. By default, Global Administrator and other administrator roles don't have permissions to read, define, or assign custom security attributes. To work with custom security attributes, you must be assigned one of the custom security attribute roles. Attribute Assignment Reader Users with this role can read custom security attribute keys and values for supported Azure AD objects. By default, Global Administrator and other administrator roles don't have permissions to read, define, or assign custom security attributes. You must be assigned one of the custom security attribute roles to work with custom security attributes. Attribute Definition Administrator Users with this role can define a valid set of custom security attributes that can be assigned to supported Azure AD objects. This role can also activate and deactivate custom security attributes. By default, Global Administrator and other administrator roles don't have permissions to read, define, or assign custom security attributes. To work with custom security attributes, you must be assigned one of the custom security attribute roles. Authentication Administrator Assign the Authentication Administrator role to users who need to do the following: -Set or reset any authentication method (including passwords) for nonadministrators and some roles. -Require users who are nonadministrators or assigned to some roles to re-register against existing nonpassword credentials (for example, Multifactor authentication (MFA) or Fast ID Online (FIDO), and can also revoke remember MFA on the device, which prompts for MFA on the next sign-in. -Perform sensitive actions for some users. -Create and manage support tickets in Azure and the Microsoft 365 admin center. Users with this role can't do the following tasks: -Can't change the credentials or reset MFA for members and owners of a role-assignable group. -Can't manage MFA settings in the legacy MFA management portal or Hardware OATH tokens. The same functions can be accomplished using the Set-MsolUser commandlet Azure AD PowerShell module. Authentication Policy Administrator Assign the Authentication Policy Administrator role to users who need to do the following: -Configure the authentication methods policy, tenant-wide MFA settings, and password protection policy that determine which methods each user can register and use. -Manage Password Protection settings: smart lockout configurations and updating the custom banned passwords list. -Create and manage verifiable credentials. -Create and manage Azure support tickets. Users with this role can't do the following tasks: -Can't update sensitive properties. -Can't delete or restore users. -Can't manage MFA settings in the legacy MFA management portal or Hardware OATH tokens. Azure AD Joined Device Local Administrator This role is available for assignment only as another local administrator in Device settings. Users with this role become local machine administrators on all Windows 10 devices that are joined to Azure Active Directory. They don't have the ability to manage device objects in Azure Active Directory. Azure DevOps Administrator Users with this role can manage all enterprise Azure DevOps policies applicable to all Azure DevOps organizations backed by the Azure AD. Users in this role can manage these policies by navigating to any Azure DevOps organization that is backed by the company's Azure AD. Users in this role can claim ownership of orphaned Azure DevOps organizations. This role grants no other Azure DevOps-specific permissions (for example, Project Collection Administrators) inside any of the Azure DevOps organizations backed by the company's Azure AD organization. Azure Information Protection Administrator Users with this role have all permissions in the Azure Information Protection service. This role allows configuring labels for the Azure Information Protection policy, managing protection templates, and activating protection. This role doesn't grant any permissions in Identity Protection Center, Privileged Identity Management, Monitor Microsoft 365 Service Health, or Office 365 Security and compliance center. Business-to-Consumer (B2C) Identity Experience Framework (IEF) Keyset Administrator Users can create and manage policy keys and secrets for token encryption, token signatures, and claim encryption/decryption. By adding new keys to existing key containers, this limited administrator can roll over secrets as needed without impacting existing applications. This user can see the full content of these secrets and their expiration dates even after their creation. Business-to-Consumer (B2C) Identity Experience Framework (IEF) Policy Administrator Users in this role have the ability to create, read, update, and delete all custom policies in Azure AD B2C and therefore have full control over the Identity Experience Framework in the relevant Azure AD B2C organization. By editing policies, this user can establish direct federation with external identity providers, change the directory schema, change all user-facing content HyperText Markup Language (HTML), Cascading Style Sheets (CSS), JavaScript), change the requirements to complete authentication, create new users, send user data to external systems including full migrations, and edit all user information including sensitive fields like passwords and phone numbers. Conversely, this role can't change the encryption keys or edit the secrets used for federation in the organization. Billing Administrator Makes purchases, manages subscriptions, manages support tickets, and monitors service health. Cloud App Security Administrator Users with this role have full permissions in Defender for Cloud Apps. They can add administrators, add Microsoft Defender for Cloud Apps policies and settings, upload logs, and perform governance actions. Cloud Application Administrator Users in this role have the same permissions as the Application Administrator role, excluding the ability to manage application proxy. This role grants the ability to create and manage all aspects of enterprise applications and application registrations. Users assigned to this role aren't added as owners when creating new application registrations or enterprise applications. This role also grants the ability to consent for delegated permissions and application permissions, except for application permissions for Microsoft Graph. Cloud Device Administrator Users in this role can enable, disable, and delete devices in Azure AD and read Windows 10 BitLocker keys (if present) in the Azure portal. The role doesn't grant permissions to manage any other properties on the device. Compliance Administrator Users with this role have permissions to manage compliance-related features in the Microsoft Purview compliance portal, Microsoft 365 admin center, Azure, and Office 365 Security and compliance center. Assignees can also manage all features within the Exchange admin center and create support tickets for Azure and Microsoft 365. Compliance Data Administrator Users with this role have permissions to track data in the Microsoft Purview compliance portal, Microsoft 365 admin center, and Azure. Users can also track compliance data within the Exchange admin center, Compliance Manager, and Teams and Skype for Business admin center and create support tickets for Azure and Microsoft 365. Conditional Access Administrator Users with this role have the ability to manage Azure Active Directory Conditional Access settings. Customer Lockbox Access Approver Manages Microsoft Purview Customer Lockbox requests in your organization. They receive email notifications for Customer Lockbox requests and can approve and deny requests from the Microsoft 365 admin center. They can also turn the Customer Lockbox feature on or off. Only Global Administrators can reset the passwords of people assigned to this role. Desktop Analytics Administrator Users in this role can manage the Desktop Analytics service, including viewing asset inventory, creating deployment plans, and viewing deployment and health status. Directory Readers Users in this role can read basic directory information. This role should be used for: -Granting a specific set of guest users read access instead of granting it to all guest users. -Granting a specific set of nonadmin users access to the Azure portal when \"Restrict access to Azure AD portal to admins only\" is set to \"Yes\". -Granting service principals access to the directory where Directory.Read.All isn't an option. Directory Synchronization Accounts Don't use. This role is automatically assigned to the Azure AD Connect service and isn't intended or supported for any other use. Directory Writers Users in this role can read and update basic information of users, groups, and service principals. Domain Name Administrator Users with this role can manage (read, add, verify, update, and delete) domain names. They can also read directory information about users, groups, and applications, as these objects possess domain dependencies. For on-premises environments, users with this role can configure domain names for federation so that associated users are always authenticated on-premises. These users can then sign into Azure AD-based services with their on-premises passwords via single sign-on. Federation settings need to be synced via Azure AD Connect so users also have permissions to manage Azure AD Connect. Dynamics 365 Administrator Users with this role have global permissions within Microsoft Dynamics 365 Online when the service is present, and the ability to manage support tickets and monitor service health. Edge Administrator Users in this role can create and manage the enterprise site list required for Internet Explorer mode on Microsoft Edge. This role grants permissions to create, edit, and publish the site list and additionally allows access to manage support tickets. Exchange Administrator Users with this role have global permissions within Microsoft Exchange Online, when the service is present. Also has the ability to create and manage all Microsoft 365 groups, manage support tickets, and monitor service health. Exchange Recipient Administrator Users with this role have read access to recipients and write access to the attributes of those recipients in Exchange Online. External ID User Flow Administrator Users with this role can create and manage user flows (also called \"built-in\" policies) in the Azure portal. These users can customize HTML/CSS/JavaScript content, change MFA requirements, select claims in the token, manage API connectors and their credentials, and configure session settings for all user flows in the Azure AD organization. On the other hand, this role doesn't include the ability to review user data or make changes to the attributes that are included in the organization schema. Changes to Identity Experience Framework policies (also known as custom policies) are also outside the scope of this role. External ID User Flow Attribute Administrator Users with this role add or delete custom attributes available to all user flows in the Azure AD organization. Users with this role can change or add new elements to the end-user schema and impact the behavior of all user flows, and indirectly result in changes to what data may be asked of end users and ultimately sent as claims to applications. This role can't edit user flows. External Identity Provider Administrator This administrator manages federation between Azure AD organizations and external identity providers. With this role, users can add new identity providers and configure all available settings (for example, authentication path, service ID, assigned key containers). This user can enable the Azure AD organization to trust authentications from external identity providers. The resulting impact on end-user experiences depends on the type of organization: -Azure AD organizations for employees and partners: The addition of a federation (for example, with Gmail) immediately impacts all guest invitations not yet redeemed. See Adding Google as an identity provider for B2B guest users. -Azure Active Directory B2C organizations: The addition of a federation (for example, with Facebook, or with another Azure AD organization) doesn't immediately impact end-user flows until the identity provider is added as an option in a user flow (also called a built-in policy). Global Administrator Users with this role have access to all administrative features in Azure Active Directory, and services that use Azure Active Directory identities like the Microsoft 365 Defender portal, the Microsoft Purview compliance portal, Exchange Online, SharePoint Online, and Skype for Business Online. Furthermore, Global Administrators can elevate their access to manage all Azure subscriptions and management groups. This allows Global Administrators to get full access to all Azure resources using the respective Azure AD Tenant. The person who signs up for the Azure AD organization becomes a Global Administrator. There can be more than one Global Administrator at your company. Global Administrators can reset the password for any user and all other administrators. Global Administrator As a best practice, Microsoft recommends that you assign the Global Administrator role to fewer than five people in your organization. Global Reader Users in this role can read settings and administrative information across Microsoft 365 services but can't take management actions. Global Reader is the read-only counterpart to Global Administrator. Assign Global Reader instead of Global Administrator for planning, audits, or investigations. Use Global Reader in combination with other limited admin roles like Exchange Administrator to make it easier to get work done without the assigning the Global Administrator role. Global Reader works with Microsoft 365 admin center, Exchange admin center, SharePoint admin center, Teams admin center, Security center, compliance center, Azure AD admin center, and Device Management admin center. Users with this role can't do the following tasks: -Can't access the Purchase Services area in the Microsoft 365 admin center. Groups Administrator Users in this role can create/manage groups and its settings like naming and expiration policies. It's important to understand that assigning a user to this role gives them the ability to manage all groups in the organization across various workloads like Teams, SharePoint, Yammer in addition to Outlook. Also the user is able to manage the various groups settings across various admin portals like Microsoft admin center, Azure portal, and workload specific ones like Teams and SharePoint admin centers. Guest Inviter Users in this role can manage Azure Active Directory B2B guest user invitations when the Members can invite user setting is set to No. Helpdesk Administrator Users with this role can change passwords, invalidate refresh tokens, create and manage support requests with Microsoft for Azure and Microsoft 365 services, and monitor service health. Invalidating a refresh token forces the user to sign in again. Whether a Helpdesk Administrator can reset a user's password and invalidate refresh tokens depends on the role the user is assigned. Users with this role can't do the following: -Can't change the credentials or reset MFA for members and owners of a role-assignable group. Hybrid Identity Administrator Users in this role can create, manage and deploy provisioning configuration setup from AD to Azure AD using Cloud Provisioning and manage Azure AD Connect, Pass-through Authentication (PTA), Password hash synchronization (PHS), Seamless single sign-on (Seamless SSO), and federation settings. Users can also troubleshoot and monitor logs using this role. Identity Governance Administrator Users with this role can manage Azure AD identity governance configuration, including access packages, access reviews, catalogs and policies, ensuring access is approved and reviewed and guest users who no longer need access are removed. Insights Administrator Users in this role can access the full set of administrative capabilities in the Microsoft Viva Insights app. This role has the ability to read directory information, monitor service health, file support tickets, and access the Insights Administrator settings aspects. Insights Analyst Assign the Insights Analyst role to users who need to do the following tasks: -Analyze data in the Microsoft Viva Insights app, but can't manage any configuration settings -Create, manage, and run queries -View basic settings and reports in the Microsoft 365 admin center -Create and manage service requests in the Microsoft 365 admin center Insights Business Leader Users in this role can access a set of dashboards and insights via the Microsoft Viva Insights app. This includes full access to all dashboards and presented insights and data exploration functionality. Users in this role don't have access to product configuration settings, which is the responsibility of the Insights Administrator role. Intune Administrator Users with this role have global permissions within Microsoft Intune Online, when the service is present. Additionally, this role contains the ability to manage users and devices to associate policy and create and manage groups. This role can create and manage all security groups. However, Intune Administrator doesn't have admin rights over Office groups. That means the admin can't update owners or memberships of all Office groups in the organization. However, you can manage the Office group that's created, which comes as a part of end-user privileges. So, any Office group (not security group) that you create should be counted against your quota of 250. Kaizala Administrator Users with this role have global permissions to manage settings within Microsoft Kaizala, when the service is present and the ability to manage support tickets and monitor service health. Additionally, the user can access reports related to adoption and usage of Kaizala by Organization members and business reports generated using the Kaizala actions. Knowledge Administrator Users in this role have full access to all knowledge, learning and intelligent features settings in the Microsoft 365 admin center. They have a general understanding of the suite of products, licensing details and have responsibility to control access. Knowledge Administrator can create and manage content, like topics, acronyms and learning resources. Additionally, these users can create content centers, monitor service health, and create service requests. Knowledge Manager Users in this role can create and manage content, like topics, acronyms and learning content. These users are primarily responsible for the quality and structure of knowledge. This user has full rights to topic management actions to confirm a topic, approve edits, or delete a topic. This role can also manage taxonomies as part of the term store management tool and create content centers. License Administrator Users in this role can add, remove, and update license assignments on users, groups (using group-based licensing), and manage the usage location on users. The role doesn't grant the ability to purchase or manage subscriptions, create or manage groups, or create or manage users beyond the usage location. This role has no access to view, create, or manage support tickets. Lifecycle Workflows Administrator Assign the Lifecycle Workflows Administrator role to users who need to do the following tasks: -Create and manage all aspects of workflows and tasks associated with Lifecycle Workflows in Azure AD -Check the execution of scheduled workflows -Launch on-demand workflow runs -Inspect workflow execution logs Message Center Privacy Reader Users in this role can monitor all notifications in the Message Center, including data privacy messages. Message Center Privacy Readers get email notifications including those related to data privacy and they can unsubscribe using Message Center Preferences. Only the Global Administrator and the Message Center Privacy Reader can read data privacy messages. Additionally, this role contains the ability to view groups, domains, and subscriptions. This role has no permission to view, create, or manage service requests. Message Center Reader Users in this role can monitor notifications and advisory health updates in Message center for their organization on configured services such as Exchange, Intune, and Microsoft Teams. Message Center Readers receive weekly email digests of posts, updates, and can share message center posts in Microsoft 365. In Azure AD, users assigned to this role will only have read-only access on Azure AD services such as users and groups. This role has no access to view, create, or manage support tickets. Microsoft Hardware Warranty Administrator Assign the Microsoft Hardware Warranty Administrator role to users who need to do the following tasks: -Create new warranty claims for Microsoft manufactured hardware, like Surface and HoloLens -Search and read opened or closed warranty claims -Search and read warranty claims by serial number -Create, read, update, and delete shipping addresses -Read shipping status for open warranty claims -Create and manage service requests in the Microsoft 365 admin center -Read Message center announcements in the Microsoft 365 admin center Microsoft Hardware Warranty Specialist Assign the Microsoft Hardware Warranty Specialist role to users who need to do the following tasks: -Create new warranty claims for Microsoft manufactured hardware, like Surface and HoloLens -Read warranty claims that they created -Read and update existing shipping addresses -Read shipping status for open warranty claims they created -Create and manage service requests in the Microsoft 365 admin center Modern Commerce User Don't use. This role is automatically assigned from Commerce, and isn't intended or supported for any other use. The Modern Commerce User role gives certain users permission to access Microsoft 365 admin center and see the left navigation entries for Home, Billing, and Support. The content available in these areas is controlled by commerce-specific roles assigned to users to manage products that they bought for themselves or your organization. This might include tasks like paying bills, or for access to billing accounts and billing profiles. Users with the Modern Commerce User role typically have administrative permissions in other Microsoft purchasing systems, but don't have Global Administrator or Billing Administrator roles used to access the admin center. Network Administrator Users in this role can review network perimeter architecture recommendations from Microsoft that are based on network telemetry from their user locations. Network performance for Microsoft 365 relies on careful enterprise customer network perimeter architecture, which is generally user location specific. This role allows for editing of discovered user locations and configuration of network parameters for those locations to facilitate improved telemetry measurements and design recommendations Office Apps Administrator Users in this role can manage Microsoft 365 apps' cloud settings. This includes managing cloud policies, self-service download management and the ability to view Office apps related report. This role additionally grants the ability to manage support tickets, and monitor service health within the main admin center. Users assigned to this role can also manage communication of new features in Office apps. Organizational Messages Writer Assign the Organizational Messages Writer role to users who need to do the following tasks: -Write, publish, and delete organizational messages using Microsoft 365 admin center or Microsoft Endpoint Manager -Manage organizational message delivery options using Microsoft 365 admin center or Microsoft Endpoint Manager -Read organizational message delivery results using Microsoft 365 admin center or Microsoft Endpoint Manager -View usage reports and most settings in the Microsoft 365 admin center, but can't make changes Partner Tier1 Support Don't use. This role has been deprecated and will be removed from Azure AD in the future. This role is intended for use by a few Microsoft resale partners, and isn't intended for general use. Partner Tier2 Support Don't use. This role has been deprecated and will be removed from Azure AD in the future. This role is intended for use by a few Microsoft resale partners, and isn't intended for general use. Password Administrator Users with this role have limited ability to manage passwords. This role doesn't grant the ability to manage service requests or monitor service health. Whether a Password Administrator can reset a user's password depends on the role the user is assigned.Users with this role can't do the following tasks: -Can't change the credentials or reset MFA for members and owners of a role-assignable group. Permissions Management Administrator Assign the Permissions Management Administrator role to users who need to do the following tasks: -Manage all aspects of Entra Permissions Management, when the service is present Power Business Intelligence (BI) Administrator Users with this role have global permissions within Microsoft Power BI, when the service is present and the ability to manage support tickets and monitor service health. Power Platform Administrator Users in this role can create and manage all aspects of environments, Power Apps, Flows, Data Loss Prevention policies. Additionally, users with this role have the ability to manage support tickets and monitor service health. Printer Administrator Users in this role can register printers and manage all aspects of all printer configurations in the Microsoft Universal Print solution, including the Universal Print Connector settings. They can consent to all delegated print permission requests. Printer Administrators also have access to print reports. Printer Technician Users with this role can register printers and manage printer status in the Microsoft Universal Print solution. They can also read all connector information. Key task a Printer Technician can't do is set user permissions on printers and sharing printers. Privileged Authentication Administrator Assign the Privileged Authentication Administrator role to users who need to do the following tasks: -Set or reset any authentication method (including passwords) for any user, including Global Administrators. -Delete or restore any users, including Global Administrators. For more information, see Who can perform sensitive actions. -Force users to re-register against existing nonpassword credential (such as MFA or FIDO) and revoke remember MFA on the device, prompting for MFA on the next sign-in of all users. -Update sensitive properties for all users. For more information, see Who can perform sensitive actions. -Create and manage support tickets in Azure and the Microsoft 365 admin center. Users with this role can't do the following tasks: -Can't manage per-user MFA in the legacy MFA management portal. The same functions can be accomplished using the Set-MsolUser commandlet Azure AD PowerShell module. Privileged Role Administrator Users with this role can manage role assignments in Azure Active Directory and within Azure AD Privileged Identity Management. They can create and manage groups that can be assigned to Azure AD roles. In addition, this role allows management of all aspects of Privileged Identity Management and administrative units. Privileged Role Administrator This role grants the ability to manage assignments for all Azure AD roles including the Global Administrator role. This role doesn't include any other privileged abilities in Azure AD like creating or updating users. However, users assigned to this role can grant themselves or others another privilege by assigning extra roles. Reports Reader Users with this role can view usage reporting data and the reports dashboard in Microsoft 365 admin center and the adoption context pack in Power Business Intelligence (Power BI). Additionally, the role provides access to all sign-in logs, audit logs, and activity reports in Azure AD and data returned by the Microsoft Graph reporting API. A user assigned to the Reports Reader role can access only relevant usage and adoption metrics. They don't have any admin permissions to configure settings or access the product-specific admin centers like Exchange. This role has no access to view, create, or manage support tickets. Search Administrator Users in this role have full access to all Microsoft Search management features in the Microsoft 365 admin center. Additionally, these users can view the message center, monitor service health, and create service requests. Search Editor Users in this role can create, manage, and delete content for Microsoft Search in the Microsoft 365 admin center, including bookmarks, questions and answers, and locations. Security Administrator Users with this role have permissions to manage security-related features in the Microsoft 365 Defender portal, Azure Active Directory Identity Protection, Azure Active Directory Authentication, Azure Information Protection, and Office 365 Security and compliance center. Security Operator Users with this role can manage alerts and have global read-only access on security-related features, including all information in Microsoft 365 security center, Azure Active Directory, Identity Protection, Privileged Identity Management and Office 365 Security & compliance center. Security Reader Users with this role have global read-only access on security-related feature, including all information in Microsoft 365 security center, Azure Active Directory, Identity Protection, Privileged Identity Management, and the ability to read Azure Active Directory sign-in reports and audit logs, and in Office 365 Security and compliance center. Service Support Administrator Users with this role can create and manage support requests with Microsoft for Azure and Microsoft 365 services, and view the service dashboard and message center in the Azure portal and Microsoft 365 admin center. SharePoint Administrator Users with this role have global permissions within Microsoft SharePoint Online, when the service is present, and the ability to create and manage all Microsoft 365 groups, manage support tickets, and monitor service health. Skype for Business Administrator Users with this role have global permissions within Microsoft Skype for Business, when the service is present, and manage Skype-specific user attributes in Azure Active Directory. Additionally, this role grants the ability to manage support tickets and monitor service health, and to access the Teams and Skype for Business admin center. The account must also be licensed for Teams or it can't run Teams PowerShell cmdlets. Teams Administrator Users in this role can manage all aspects of the Microsoft Teams workload via the Microsoft Teams and Skype for Business admin center and the respective PowerShell modules. This includes, among other areas, all management tools related to telephony, messaging, meetings, and the teams themselves. This role additionally grants the ability to create and manage all Microsoft 365 groups, manage support tickets, and monitor service health. Teams Communications Administrator Users in this role can manage aspects of the Microsoft Teams workload related to voice and telephony. This includes the management tools for telephone number assignment, voice and meeting policies, and full access to the call analytics toolset. Teams Communications Support Engineer Users in this role can troubleshoot communication issues within Microsoft Teams and Skype for Business using the user call troubleshooting tools in the Microsoft Teams and Skype for Business admin center. Users in this role can view full call record information for all participants involved. This role has no access to view, create, or manage support tickets. Teams Communications Support Specialist Users in this role can troubleshoot communication issues within Microsoft Teams and Skype for Business using the user call troubleshooting tools in the Microsoft Teams and Skype for Business admin center. Users in this role can only view user details in the call for the specific user they've looked up. This role has no access to view, create, or manage support tickets. Teams Devices Administrator Users with this role can manage Teams-certified devices from the Teams admin center. This role allows viewing all devices at single glance, with ability to search and filter devices. The user can check details of each device including logged-in account, make and model of the device. The user can change the settings on the device and update the software versions. This role doesn't grant permissions to check Teams activity and call quality of the device. Tenant Creator Assign the Tenant Creator role to users who need to do the following tasks: -Create both Azure Active Directory and Azure Active Directory B2C tenants even if the tenant creation toggle is turned off in the user settings Usage Summary Reports Reader Users with this role can access tenant level aggregated data and associated insights in Microsoft 365 admin center for Usage and Productivity Score but can't access any user level details or insights. In Microsoft 365 admin center for the two reports, we differentiate between tenant level aggregated data and user level details. This role gives an extra layer of protection on individual user identifiable data, which was requested by both customers and legal teams. User Administrator Assign the User Administrator role to users who need to do the following tasks: -Create users -Update most user properties for all users, including all administrators -Update sensitive properties (including user principal name) for some users -Disable or enable some users -Delete or restore some users -Create and manage user views -Create and manage all groups -Assign licenses for all users, including all administrators -Reset passwords -Invalidate refresh tokens -Update (FIDO) device keys -Update password expiration policies -Create and manage support tickets in Azure and the Microsoft 365 admin center -Monitor service healthUsers with this role can't do the following tasks: -Can't manage MFA. -Can't change the credentials or reset MFA for members and owners of a role-assignable group. -Can't manage shared mailboxes User Administrator Users with this role can change passwords for people who may have access to sensitive or private information or critical configuration inside and outside of Azure Active Directory. Changing the password of a user may mean the ability to assume that user's identity and permissions. For example: -Application Registration and Enterprise Application owners, who can manage credentials of apps they own. Those apps may have privileged permissions in Azure AD and elsewhere not granted to User Administrators. Through this path, a User Administrator may be able to assume the identity of an application owner and then further assume the identity of a privileged application by updating the credentials for the application. -Azure subscription owners, who may have access to sensitive or private information or critical configuration in Azure. -Security Group and Microsoft 365 group owners, who can manage group membership. Those groups may grant access to sensitive or private information or critical configuration in Azure AD and elsewhere. -Administrators in other services outside of Azure AD like Exchange Online, Office Security and compliance center, and human resources systems. -Nonadministrators like executives, legal counsel, and human resources employees who may have access to sensitive or private information. Virtual Visits Administrator Users with this role can do the following tasks: -Manage and configure all aspects of Virtual Visits in Bookings in the Microsoft 365 admin center, and in the Teams Electronic Health Record (EHR) connector -View usage reports for Virtual Visits in the Teams admin center, Microsoft 365 admin center, and Power BI -View features and settings in the Microsoft 365 admin center, but can't edit any settings Windows 365 Administrator Users with this role have global permissions on Windows 365 resources, when the service is present. Additionally, this role contains the ability to manage users and devices in order to associate policy and create and manage groups. This role can create and manage security groups, but doesn't have administrator rights over Microsoft 365 groups. That means administrators can't update owners or memberships of Microsoft 365 groups in the organization. However, they can manage the Microsoft 365 group they create, which is a part of their end-user privileges. So, any Microsoft 365 group (not security group) they create is counted against their quota of 250. Assign the Windows 365 Administrator role to users who need to do the following tasks: -Manage Windows 365 Cloud PCs in Microsoft Endpoint Manager -Enroll and manage devices in Azure AD, including assigning users and policies -Create and manage security groups, but not role-assignable groups -View basic properties in the Microsoft 365 admin center -Read usage reports in the Microsoft 365 admin center -Create and manage support tickets in Azure and the Microsoft 365 admin center Windows Update Deployment Administrator Users in this role can create and manage all aspects of Windows Update deployments through the Windows Update for Business deployment service. The deployment service enables users to define settings for when and how updates are deployed, and specify which updates are offered to groups of devices in their tenant. It also allows users to monitor the update progress. Yammer Administrator Assign the Yammer Administrator role to users who need to do the following tasks: -Manage all aspects of Yammer -Create, manage, and restore Microsoft 365 Groups, but not role-assignable groups -View the hidden members of Security groups and Microsoft 365 groups, including role assignable groups -Read usage reports in the Microsoft 365 admin center -Create and manage service requests in the Microsoft 365 admin center -View announcements in the Message center, but not security announcements -View service health # Deploy Azure AD Domain Services Azure Active Directory Domain Services (Azure AD DS) provides managed domain services such as domain join, group policy, lightweight directory access protocol (LDAP), and Kerberos/New Technology LAN Manager (NTLM) authentication. You use these domain services without the need to deploy, manage, and patch domain controllers (DCs) in the cloud. An Azure AD DS managed domain lets you run legacy applications in the cloud that can't use modern authentication methods or where you don't want directory lookups to always go back to an on-premises AD DS environment. You can lift and shift those legacy applications from your on-premises environment into a managed domain without needing to manage the AD DS environment in the cloud. Azure AD DS integrates with your existing Azure AD tenant. This integration lets users sign in to services and applications connected to the managed domain using their existing credentials. You can also use existing groups and user accounts to secure access to resources. These features provide a smoother lift-and-shift of on-premises resources to Azure.","title":"All Roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#how-does-azure-ad-ds-work","text":"When you create an Azure AD DS managed domain, you define a unique namespace. This namespace is the domain name, such as aaddscontoso.com. Two Windows Server domain controllers (DCs) are then deployed into your selected Azure region. This deployment of DCs is known as a replica set. You don't need to manage, configure, or update these DCs. The Azure platform handles the DCs as part of the managed domain, including backups and encryption at rest using Azure Disk Encryption. A managed domain is configured to perform a one-way synchronization from Azure AD to provide access to a central set of users, groups, and credentials. You can create resources directly in the managed domain, but they aren't synchronized back to Azure AD. Applications, services, and VMs in Azure that connect to the managed domain can then use common AD DS features such as domain join, group policy, LDAP, and Kerberos/NTLM authentication. In a hybrid environment with an on-premises AD DS environment, Azure AD Connect synchronizes identity information with Azure AD, which is then synchronized to the managed domain. Azure AD DS replicates identity information from Azure AD, so it works with Azure AD tenants that are cloud-only or synchronized with an on-premises AD DS environment. The same set of Azure AD DS features exists for both environments. If you have an existing on-premises AD DS environment, you can synchronize user account information to provide a consistent identity for users. For cloud-only environments, you don't need a traditional on-premises AD DS environment to use the centralized identity services of Azure AD DS. You can expand a managed domain to have more than one replica set per Azure AD tenant. Replica sets can be added to any peered virtual network in any Azure region that supports Azure AD DS. Additional replica sets in different Azure regions provide geographical disaster recovery for legacy applications if an Azure region goes offline.","title":"How does Azure AD DS work?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-ad-ds-features-and-benefits","text":"To provide identity services to applications and VMs in the cloud, Azure AD DS is fully compatible with a traditional AD DS environment for operations such as domain-join, secure LDAP (LDAPS), Group Policy, DNS management, and LDAP bind and read support. LDAP write support is available for objects created in the managed domain but not resources synchronized from Azure AD. The following features of Azure AD DS simplify deployment and management operations: Simplified deployment experience: Azure AD DS is enabled for your Azure AD tenant using a single wizard in the Azure portal. Integrated with Azure AD: User accounts, group memberships, and credentials are automatically available from your Azure AD tenant. New users, groups, or changes to attributes from your Azure AD tenant or your on-premises AD DS environment are automatically synchronized to Azure AD DS. Accounts in external directories linked to your Azure AD aren't available in Azure AD DS. Credentials aren't available for those external directories, so they can't be synchronized into a managed domain. Use your corporate credentials/passwords: Passwords for users in Azure AD DS are the same as in your Azure AD tenant. Users can use their corporate credentials to domain-join machines, sign in interactively or over a remote desktop, and authenticate against the managed domain. NTLM and Kerberos authentication: With support for NTLM and Kerberos authentication, you can deploy applications that rely on Windows-integrated authentication. High availability: Azure AD DS includes multiple domain controllers, which provide high availability for your managed domain. This high availability guarantees service uptime and resilience to failures. In regions that support Azure Availability Zones, these domain controllers are distributed across zones for additional resiliency. Replica sets can also be used to provide geographical disaster recovery for legacy applications if an Azure region goes offline. Some key aspects of a managed domain include the following: - The managed domain is a stand-alone domain. It isn't an extension of an on-premises domain. - If needed, you can create one-way outbound forest trusts from Azure AD DS to an on-premises AD DS environment. - Your IT team doesn't need to manage, patch, or monitor domain controllers for this managed domain. For hybrid environments that run AD DS on-premises, you don't need to manage AD replication to the managed domain. User accounts, group memberships, and credentials from your on-premises directory are synchronized to Azure AD via Azure AD Connect. These user accounts, group memberships, and credentials are automatically available within the managed domain. Important Azure AD DS integrates with Azure AD, which can synchronize with an on-premises AD DS environment. This ability extends central identity use cases to traditional web applications that run in Azure as part of a lift-and-shift strategy.","title":"Azure AD DS features and benefits"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#create-and-manage-azure-ad-users","text":"Add new users or delete existing users from your Azure Active Directory (Azure AD) tenant. To add or delete users, you must be a User Administrator or Global Administrator. Note For information about viewing or deleting personal data, please review Microsoft's guidance on the Windows data subject requests for the General Data Protection Regulation (GDPR) site. For general information about GDPR, see the GDPR section of the Microsoft Trust Center and the GDPR section of the Service Trust portal.","title":"Create and manage Azure AD users"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#add-a-new-user","text":"You can create a new user for your organization or invite an external user from the same starting point. Sign in to the Azure portal in the User Administrator role. Navigate to Azure Active Directory > Users. Select either Create new user or Invite external user from the menu. On the New User page, provide the new user's information: Identity: Add a user name and display name for the user. User name and Name are required and can't contain accent characters. You can also add a first and last name. The domain part of the user name must use either the initial default domain name, .onmicrosoft.com , or a custom domain name, such as contoso.com. Groups and roles: Optional. Add the user to one or more existing groups. Group membership can be set at any time. Settings: Optional. Toggle the option to block sign-in for the user or set the user's default location. Job info: Optional. Add the user's job title, department, company name, and manager. These details can be updated at any time. Copy the autogenerated password provided in the Password box. You'll need to give this password to the user to sign in for the first time. Select Create . The user is created and added to your Azure AD organization.","title":"Add a new user"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#add-a-new-guest-user","text":"You can also invite the new guest user to collaborate with your organization by selecting Invite user from the New user page. If your organization's external collaboration settings are configured to allow guests, the user will be emailed an invitation they must accept in order to begin collaborating.","title":"Add a new guest user"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#add-other-users","text":"There might be scenarios where you want to manually create consumer accounts in your Azure Active Directory B2C (Azure AD B2C) directory. If you have an environment with Azure Active Directory (cloud) and Windows Server Active Directory (on-premises), you can add new users by syncing the existing user account data.","title":"Add other users"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#delete-a-user","text":"You can delete an existing user using the Azure Active Directory portal. You must have a Global Administrator, Privileged Authentication Administrator, or User Administrator role assignment to delete users in your organization. Global Admins and Privileged Authentication Admins can delete any users, including other admins. User Administrators can delete any non-admin users, Helpdesk Administrators, and other User Administrators. Sign in to the Azure portal using one of the appropriate roles listed above. Go to Azure Active Directory > Users. Search for and select the user you want to delete from your Azure AD tenant. Select Delete user. The user is deleted and no longer appears on the Users - All users page. The user can be seen on the Deleted users page for the next 30 days and can be restored during that time. When a user is deleted, any licenses consumed by the user are made available for other users. Note To update the identity, contact information, or job information for users whose source of authority is Windows Server Active Directory, you must use Windows Server Active Directory. After you complete the update, you must wait for the next synchronization cycle to complete before you see the changes.","title":"Delete a user"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#manage-users-with-azure-ad-groups","text":"Azure Active Directory (Azure AD) provides several ways to manage access to resources, applications, and tasks. With Azure AD groups, you can grant access and permissions to a group of users instead of each individual user. Limiting access to Azure AD resources to only those users who need access is one of the core security principles of Zero Trust. Azure AD lets you use groups to manage access to applications, data, and resources. Resources can be: Part of the Azure AD organization, such as permissions to manage objects through roles in Azure AD External to the organization, such as for Software as a Service (SaaS) apps Azure services SharePoint sites On-premises resources Some groups can't be managed in the Azure AD portal: Groups synced from on-premises Active Directory can be managed only in on-premises Active Directory. Distribution lists and mail-enabled security groups are managed only in Exchange admin center or Microsoft 365 admin center. You must sign in to the Exchange admin center or Microsoft 365 admin center to manage these groups.","title":"Manage users with Azure AD groups"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#what-to-know-before-creating-a-group","text":"There are two group types and three group membership types.","title":"What to know before creating a group"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#group-types","text":"Security: Used to manage user and computer access to shared resources. For example, you can create a security group so that all group members have the same set of security permissions. Members of a security group can include users, devices, other groups, and service principals, which define access policy and permissions. Owners of a security group can include users and service principals. Microsoft 365: Provides collaboration opportunities by giving group members access to a shared mailbox, calendar, files, SharePoint sites, and more. This option also lets you give people outside of your organization access to the group. Members of a Microsoft 365 group can only include users. Owners of a Microsoft 365 group can include users and service principals.","title":"Group types:"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#membership-types","text":"Assigned: Lets you add specific users as members of a group and have unique permissions. Dynamic user: Lets you use dynamic membership rules to automatically add and remove members. If a member's attributes change, the system looks at your dynamic group rules for the directory to see if the member meets the rule requirements (is added), or no longer meets the rules requirements (is removed). Dynamic device: Lets you use dynamic group rules to automatically add and remove devices. If a device's attributes change, the system looks at your dynamic group rules for the directory to see if the device meets the rule requirements (is added), or no longer meets the rules requirements (is removed). Important You can create a dynamic group for either devices or users but not for both. You can't create a device group based on the device owners' attributes. Device membership rules can only reference device attributions.","title":"Membership types:"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-azure-ad-administrative-units","text":"An administrative unit is an Azure AD resource that can be a container for other Azure AD resources. An administrative unit can contain only users and groups. Administrative units restrict permissions in a role to any portion of your organization that you define. You could, for example, use administrative units to delegate the Helpdesk Administrator role to regional support specialists, so they can manage users only in the region that they support. Note To use administrative units, you need an Azure Active Directory Premium license for each administrative unit admin, and Azure Active Directory Free licenses for administrative unit members.","title":"Configure Azure AD administrative units"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#available-roles","text":"Role Description Authentication Administrator Has access to view, set, and reset authentication method information for any non-admin user in the assigned administrative unit only. Groups Administrator Can manage all aspects of groups and groups settings, such as naming and expiration policies, in the assigned administrative unit only. Helpdesk Administrator Can reset passwords for non-administrators and Helpdesk administrators in the assigned administrative unit only. License Administrator Can assign, remove, and update license assignments within the administrative unit only. Password Administrator Can reset passwords for non-administrators and Password Administrators within the assigned administrative unit only. User Administrator Can manage all aspects of users and groups, including resetting passwords for limited admins within the assigned administrative unit only.","title":"Available roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-passwordless-authentication","text":"Features like multifactor authentication (MFA) are a great way to secure your organization. Still, users often get frustrated with the additional security layer on top of having to remember their passwords. Passwordless authentication methods are more convenient because the password is removed and replaced with something you have, plus something you are or something you know. Authentication Something you have Something you are or know Passwordless Windows 10 Device, phone, or security key Biometric or PIN Each organization has different needs when it comes to authentication. Microsoft global Azure and Azure Government offer the following three passwordless authentication options that integrate with Azure Active Directory (Azure AD): 1. Windows Hello for Business 2. Microsoft Authenticator 3. Fast Identity Online2 (FIDO2) security keys","title":"Implement passwordless authentication"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#windows-hello-for-business","text":"Windows Hello for Business is ideal for information workers that have their own designated Windows PC. The biometric and PIN credentials are directly tied to the user's PC, which prevents access from anyone other than the owner. With public key infrastructure (PKI) integration and built-in support for single sign-on (SSO), Windows Hello for Business provides a convenient method for seamlessly accessing corporate resources on-premises and in the cloud.","title":"Windows Hello for Business"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#microsoft-authenticator","text":"You can also allow your employee's phone to become a passwordless authentication method. You may already be using the Authenticator app as a convenient multi-factor authentication option in addition to a password. You can also use the Authenticator App as a passwordless option.","title":"Microsoft Authenticator"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#fast-identity-online2-fido2-security-keys","text":"The FIDO (Fast IDentity Online) Alliance helps to promote open authentication standards and reduce the use of passwords as a form of authentication. FIDO2 is the latest standard that incorporates the web authentication (WebAuthn) standard. FIDO2 security keys are an unphishable standards-based passwordless authentication method that can come in any form factor. Fast Identity Online (FIDO) is an open standard for passwordless authentication. FIDO allows users and organizations to leverage the standard to sign in to their resources without a username or password using an external security key or a platform key built into a device. Users can register and then select a FIDO2 security key at the sign-in interface as their main means of authentication. These FIDO2 security keys are typically USB devices but could also use Bluetooth or Near-Field Communication (NFC). With a hardware device that handles the authentication, the security of an account is increased as there's no password that could be exposed or guessed. FIDO2 security keys can be used to sign into their Azure AD or hybrid Azure AD joined Windows 10 devices and get single-sign-on to their cloud and on-premises resources. Users can also sign in to supported browsers. FIDO2 security keys are a great option for enterprises that are very security sensitive or have scenarios or employees who aren't willing or able to use their phone as a second factor.","title":"Fast Identity Online2 (FIDO2) security keys:"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-try-this-exercises","text":"","title":"Explore Try-This exercises"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-1-review-azure-ad","text":"In this task, we'll review Azure Active Directory licensing and tenants. In the Portal , search for and select Azure Active Directory . On the Overview page, locate the license information. Go to the Azure AD pricing page and review the features and pricing for each edition.","title":"Task 1: Review Azure AD"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-2-manage-users-and-groups","text":"Note: This task requires some users and groups to be populated. Dynamic groups requires a Premium P1 license. In this task, we'll create users and groups. Under the Manage blade, click Users . Review the different Sources such as Windows Server AD, Invited User, Microsoft Account, and External Azure Active Directory . Notice the choice for New guest user . Click New user . Review the two ways to create a user: Create user and Invite user . Create a new user. Review Identity, Groups and roles, Settings , and Job Info . Navigate to Azure AD, under Manage click Groups . Review the Group types: Security and Microsoft 365 . Create a new group by clicking \"New Group\" with the Membership type as Assigned . Add a user to the same group. Create another new group with Membership type as Dynamic user . Review the details to construct dynamic group membership rules.","title":"Task 2: Manage users and groups"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-3-multifactor-authentication-in-azure","text":"Note This task requires a user account, AZ500User1. In this demonstration, we will configure and test MFA.","title":"Task 3: multifactor authentication in Azure"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-mfa","text":"In this task, we'll enable MFA for a user. In the Portal , search for and select Azure Active Directory . Under Manage select Security . Under Manage select MFA . In the center pane, under Configure select Additional cloud-based MFA settings . Select the Users tab. Select AZ500User1 . Make a note of their user name in the form user@domain.com. On the far right click Enable . Read the information about enabling multifactor authentication in Azure. Click enable multi-factor auth . Wait for update. AZ500User1 will now be required to provide two factor authentication.","title":"Configure MFA"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#question","text":"Your organization is considering multifactor authentication in Azure. Your manager asks about secondary verification methods. Which of the following options could serve as secondary verification method? Automated phone call (Ans) . Emailed link to verification website. Microsoft account verification code. Your organization has implemented multifactor authentication in Azure. Your goal is to provide a status report by user account. Which of the following values could be used to provide a valid MFA status? Enrolled Enforced (Ans) Required Which of the following options can be used when configuring multifactor authentication in Azure? Block a user if stolen password is suspected. Configure IP addresses outside the company intranet that should be blocked. Configure a one-time bypass to allow a user to authenticate a single time without performing multi-factor authentication. The bypass is temporary and expires after a specified number of seconds. In situations where the mobile app or phone isn't receiving a notification or phone call, you can allow a one-time bypass so the user can access the desired resource (Ans) . Which of the following roles would allow the user to manage all the groups in a tenant and would be able to assign other admin roles? Global administrator (Ans) Password administrator Security administrator Which of the following methods enable you to automatically add or remove users to security groups or Microsoft 365 groups, so you don't always have to do it manually? Automatic add Dynamic user (Ans) Microsoft 365 user","title":"Question"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-hybrid-identity","text":"Explore how to deploy and configure Azure AD Connect to create a hybrid identity solution for your company.","title":"Implement Hybrid identity"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#introduction_1","text":"Hybrid Identity is the process of connecting your on-premises Active Directory with your Azure Active Directory. You do this to enable a single account to have access to resources on-premises and in the cloud. There are many other security benefits as well.","title":"Introduction"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#scenario_1","text":"A security engineer uses Hybrid Identity to share identity, authentication, and access across on-premises and cloud resources; you will work on such tasks as: Connect your on-premises AD with your Azure AD. Select the best authentication option based on your user's needs and your security goals. Configure authentication options to create your most secure environment.","title":"Scenario"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#skills-measured_1","text":"Azure Active Directory is a part of Exam AZ-500: Microsoft Azure Security Engineer . Manage identity and access (30-35%) Manage Azure AD identities configure authentication methods including password hash and Pass-Through Authentication (PTA), OAuth, and passwordless","title":"Skills measured"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#learning-objectives_1","text":"Configure and deploy Azure AD Connect. Configure password hash synchronization. Implement pass-through authentication. Select and configure the optimal authentication method based on your security posture. Deploy password writeback.","title":"Learning objectives"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#deploy-azure-ad-connect","text":"Azure AD Connect will integrate your on-premises directories with Azure Active Directory. This allows you to provide a common identity for your users for Microsoft 365, Azure, and SaaS applications integrated with Azure AD. Azure AD Connect provides the following features: Password hash synchronization . A sign-in method that synchronizes a hash of a users on-premises AD password with Azure AD. Pass-through authentication . A sign-in method that allows users to use the same password on-premises and in the cloud, but doesn't require the additional infrastructure of a federated environment. Federation integration . Federation is an optional part of Azure AD Connect and can be used to configure a hybrid environment using an on-premises AD FS infrastructure. It also provides AD FS management capabilities such as certificate renewal and additional AD FS server deployments. Synchronization . Responsible for creating users, groups, and other objects. As well as, making sure identity information for your on-premises users and groups is matching the cloud. This synchronization also includes password hashes. Health Monitoring . Azure AD Connect Health can provide robust monitoring and provide a central location in the Azure portal to view this activity. When you integrate your on-premises directories with Azure AD, your users are more productive because there's a common identity to access both cloud and on-premises resources. However, this integration creates the challenge of ensuring that this environment is healthy so that users can reliably access resources both on premises and in the cloud from any device. Azure Active Directory (Azure AD) Connect Health provides robust monitoring of your on-premises identity infrastructure. It enables you to maintain a reliable connection to Microsoft 365 and Microsoft Online Services. This reliability is achieved by providing monitoring capabilities for your key identity components. Also, it makes the key data points about these components easily accessible. Azure AD Connect Health helps you: Monitor and gain insights into AD FS servers, Azure AD Connect, and AD domain controllers. Monitor and gain insights into the synchronizations that occur between your on-premises AD DS and Azure AD. Monitor and gain insights into your on-premises identity infrastructure that is used to access Microsoft 365 or other Azure AD applications With Azure AD Connect the key data you need is easily accessible. You can view and act on alerts, setup email notifications for critical alerts, and view performance data. Important Using AD Connect Health works by installing an agent on each of your on-premises sync servers.","title":"Deploy Azure AD connect"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-authentication-options","text":"Choosing the correct authentication method is the first concern for organizations wanting to move their apps to the cloud. Don't take this decision lightly, for the following reasons: It's the first decision for an organization that wants to move to the cloud. The authentication method is a critical component of an organization\u2019s presence in the cloud. It controls access to all cloud data and resources. It's the foundation of all the other advanced security and user experience features in Azure AD. Identity is the new control plane of IT security, so authentication is an organization\u2019s access guard to the new cloud world. Organizations need an identity control plane that strengthens their security and keeps their cloud apps safe from intruders.","title":"Explore authentication options"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#authentication-methods","text":"When the Azure AD hybrid identity solution is your new control plane, authentication is the foundation of cloud access. Choosing the correct authentication method is a crucial first decision in setting up an Azure AD hybrid identity solution. Implement the authentication method that is configured by using Azure AD Connect, which also provisions users in the cloud. Azure AD supports the following authentication methods for hybrid identity solutions.","title":"Authentication methods"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#cloud-authentication","text":"When you choose this authentication method, Azure AD handles users' sign-in process. Coupled with seamless single sign-on (SSO), users can sign in to cloud apps without having to reenter their credentials. With cloud authentication, you can choose from two options: Option 1: Azure AD password hash synchronization . The simplest way to enable authentication for on-premises directory objects in Azure AD. Users can use the same username and password that they use on-premises without having to deploy any additional infrastructure. Some premium features of Azure AD, like Identity Protection and Azure AD Domain Services, require password hash synchronization, no matter which authentication method you choose. Option 2: Azure AD Pass-through Authentication . Provides a simple password validation for Azure AD authentication services by using a software agent that runs on one or more on-premises servers. The servers validate the users directly with your on-premises Active Directory, which ensures that the password validation doesn't happen in the cloud. Companies with a security requirement to immediately enforce on-premises user account states, password policies, and sign-in hours might use this authentication method.","title":"Cloud authentication"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#federated-authentication","text":"When you choose the Federated authentication method, Azure AD hands off the authentication process to a separate trusted authentication system, such as on-premises Active Directory Federation Services (AD FS), to validate the user\u2019s password. The authentication system can provide additional advanced authentication requirements. Examples are smartcard-based authentication or third-party multifactor authentication.","title":"Federated authentication"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#summary","text":"This lesson outlines various authentication options that organizations can configure and deploy to support access to cloud apps. To meet various business, security, and technical requirements, organizations can choose between password hash synchronization, Pass-through Authentication, and federation. Consider each authentication method. Does the effort to deploy the solution, and the user's experience of the sign-in process address your business requirements? Evaluate whether your organization needs the advanced scenarios and business continuity features of each authentication method. Finally, evaluate the considerations of each authentication method. Do any of them prevent you from implementing your choice?","title":"Summary"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-password-hash-synchronization-phs","text":"The probability that you're blocked from getting your work done due to a forgotten password is related to the number of different passwords you need to remember. The more passwords you need to remember, the higher the probability to forget one. Questions and calls about password resets and other password-related issues demand the most helpdesk resources. Password hash synchronization (PHS) is a feature used to synchronize user passwords from an on-premises Active Directory instance to a cloud-based Azure AD instance. Use this feature to sign in to Azure AD services like Microsoft 365, Microsoft Intune, CRM Online, and Azure Active Directory Domain Services (Azure AD DS). You sign in to the service by using the same password you use to sign in to your on-premises Active Directory instance. Password hash synchronization helps you to: Improve the productivity of your users. Reduce your helpdesk costs.","title":"Configure Password Hash Synchronization (PHS)"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#how-does-this-work","text":"In the background, the password synchronization component takes the user\u2019s password hash from on-premises Active Directory, encrypts it, and passes it as a string to Azure. Azure decrypts the encrypted hash and stores the password hash as a user attribute in Azure AD. When the user signs in to an Azure service, the sign-in challenge dialog box generates a hash of the user\u2019s password and passes that hash back to Azure. Azure then compares the hash with the one in that user\u2019s account. If the two hashes match, then the two passwords must also match and the user receives access to the resource. The dialog box provides the facility to save the credentials so that the next time the user accesses the Azure resource, the user will not be prompted. Important It is important to understand that this is same sign-in, not single sign-on. The user still authenticates against two separate directory services, albeit with the same user name and password. This solution provides a simple alternative to an AD FS implementation.","title":"How does this work?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-pass-through-authentication-pta","text":"Azure AD Pass-through Authentication (PTA) is an alternative to Azure AD Password Hash Synchronization, and provides the same benefit of cloud authentication to organizations. PTA allows users to sign in to both on-premises and cloud-based applications using the same user account and passwords. When users sign-in using Azure AD, Pass-through authentication validates the users\u2019 passwords directly against an organization's on-premise Active Directory.","title":"Implement Pass-through Authentication (PTA)"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#feature-benefits","text":"Supports user sign-in into all web browser-based applications and into Microsoft Office client applications that use modern authentication. Sign-in usernames can be either the on-premises default username (userPrincipalName) or another attribute configured in Azure AD Connect (known as Alternate ID). Works seamlessly with conditional access features such as Azure Active Directory Multi-Factor Authentication to help secure your users. Integrated with cloud-based self-service password management, including password writeback to on-premises Active Directory and password protection by banning commonly used passwords. Multi-forest environments are supported if there are forest trusts between your AD forests and if name suffix routing is correctly configured. PTA is a free feature, and you don't need any paid editions of Azure AD to use it. PTA can be enabled via Azure AD Connect. PTA uses a lightweight on-premises agent that listens for and responds to password validation requests. Installing multiple agents provides high availability of sign-in requests. PTA protects your on-premises accounts against brute force password attacks in the cloud. Important This feature can be configured without using a federation service so that any organization, regardless of size, can implement a hybrid identity solution. Pass-through authentication is not only for user sign-in but allows an organization to use other Azure AD features, such as password management, role-based access control, published applications, and conditional access policies.","title":"Feature benefits"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#deploy-federation-with-azure-ad","text":"Federation is a collection of domains that have established trust. The level of trust may vary, but typically includes authentication and almost always includes authorization. A typical federation might include a number of organizations that have established trust for shared access to a set of resources. You can federate your on-premises environment with Azure AD and use this federation for authentication and authorization. This sign-in method ensures that all user authentication occurs on-premises. This method allows administrators to implement more rigorous levels of access control.","title":"Deploy Federation with Azure AD"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-the-authentication-decision-tree","text":"Choosing the correct authentication method is the first concern for organizations wanting to move their apps to the cloud. Don't take this decision lightly, for the following reasons: - It's the first decision for an organization that wants to move to the cloud. - The authentication method is a critical component of an organization\u2019s presence in the cloud. It controls access to all cloud data and resources. - It's the foundation of all the other advanced security and user experience features in Azure AD. Identity is the new control plane of IT security, so authentication is an organization\u2019s access guard to the new cloud world. Organizations need an identity control plane that strengthens their security and keeps their cloud apps safe from intruders.","title":"Explore the authentication decision tree"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#authentication-methods_1","text":"Cloud Authentication - When you choose this authentication method, Azure AD handles users' sign-in process. Coupled with seamless single sign-on (SSO), users can sign in to cloud apps without having to reenter their credentials. With cloud authentication, you can choose from two options: - Azure AD password hash Synchronization - Azure AD Pass-through Authentication Federated Authentication - When you choose this authentication method, Azure AD hands off the authentication process to a separate trusted authentication system, such as on-premises Active Directory Federation Services (AD FS), to validate the user\u2019s password. The authentication system can provide additional advanced authentication requirements. Examples are smartcard-based authentication or third-party multifactor authentication.","title":"Authentication methods"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#decision-tree","text":"Details on decision questions: Azure AD can handle sign-in for users without relying on on-premises components to verify passwords. Azure AD can hand off user sign-in to a trusted authentication provider such as Microsoft\u2019s AD FS. If you need to apply user-level Active Directory security policies such as account expired, disabled account, password expired, account locked out, and sign-in hours on each user sign-in, Azure AD requires some on-premises components. Sign-in features not natively supported by Azure AD: Sign-in using on-premises MFA Server. Sign-in using third-party authentication solution. Multi-site on-premises authentication solution. Azure AD Identity Protection requires Password Hash Sync, regardless of which sign-in method you choose, to provide the Users with leaked credentials report. Organizations can fail over to Password Hash Sync if their primary sign-in method fails and it was configured before the failure event. Important This decision tree is intended as a starting point to understand your options, but there can be others or even combinations of different options. For example, you can use Azure AD B2C and configure it to allow user sign-in for multi-tenant Azure AD tenants - with or without the traditional support for self-service sign-up and social identity providers.","title":"Decision tree"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-password-writeback","text":"Having a cloud-based password reset utility is great but most companies still have an on-premises directory where their users exist. How does Microsoft support keeping traditional on-premises Active Directory Domain Services (AD DS) in sync with password changes in the cloud? Password writeback is a feature enabled with Azure AD Connect that allows password changes in the cloud to be written back to an existing on-premises directory in real time. Password writeback provides: Enforcement of on-premises Active Directory Domain Services password policies . When a user resets their password, it is checked to ensure it meets your on-premises Active Directory Domain Services policy before committing it to that directory. This review includes checking the history, complexity, age, password filters, and any other password restrictions that you have defined in local Active Directory Domain Services. Zero-delay feedback . Password writeback is a synchronous operation. Your users are notified immediately if their password did not meet the policy or could not be reset or changed for any reason. Supports password changes from the access panel and Microsoft 365 . When federated or password hash synchronized users come to change their expired or non-expired passwords, those passwords are written back to your local Active Directory Domain Services environment. Supports password writeback when an admin resets them from the Azure portal . Whenever an admin resets a user\u2019s password in the Azure portal, if that user is federated or password hash synchronized, the password is written back to on-premises. This functionality is currently not supported in the Office admin portal. Doesn\u2019t require any inbound firewall rules . Password writeback uses an Azure Service Bus relay as an underlying communication channel. All communication is outbound over port 443. Important To use self-service password reset (SSPR) you must have already configured Azure AD Connect in your environment.","title":"Configure password writeback"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#knowledge-check","text":"Choose the best response for each of the questions below. Then select Check your answers .","title":"Knowledge check"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#check-your-knowledge","text":"The IT helpdesk wants to reduce password reset support tickets. You suggest having users sign-in to both on-premises and cloud-based applications using the same password. Your organization does not plan on using Azure AD Identity Protection, so which feature would be easiest to implement given the requirements? Federation Pass-through authentication Password hash synchronization ( Ans ) Which tool can you use to synchronize Azure AD passwords with on-premises Active Directory? Azure AD Connect ( Ans ) Active Directory Federation Services Password writeback Azure AD supports which of the following security protocols? Kerberos OAuth ( Ans ) OpenID Connect Which of the following is an authentication option that integrates with Azure Active Directory, requiring you to use several differing methods, like your phone, to confirm your identity? FIDO2 security keys Microsoft Authenticator app Azure Active Directory Multi-Factor Authentication ( Ans )","title":"Check your knowledge"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#deploy-azure-ad-identity-protection","text":"Protect identities in Azure AD using Conditional Access, MFA, access reviews, and other capabilities. Deploy and configure Identity Protection Configure MFA for users, groups, and applications Create Conditional Access policies to ensure your security Create and follow an access review process","title":"Deploy Azure AD identity protection"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#introduction_2","text":"Identity Protection is a tool that allows organizations to automate the detection and remediation of identity-based risks, investigate risks using data in the portal, and export risk detection data to third-party utilities for further analysis.","title":"Introduction"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#scenario_2","text":"A security engineer uses Azure AD Identity Protection to configure Azure features that monitor and protect identities in the tenant; you will work on such tasks as: Creating access reviews to check on how each identity is being used and that the correct rights are assigned. Configure policies to identify risky user behaviors and odd sign-in patterns. Control and manage access to resources with conditional access policies.","title":"Scenario"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#skills-measured_2","text":"Exam AZ-500: Microsoft Azure Security Engineer Manage identity and access (30-35%) - Configure secure access by using Azure and - Configure Access Reviews - Implement Conditional Access policies, including multi-factor authentication - Configure Azure AD identity protection","title":"Skills measured"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#learning-objectives_2","text":"Deploy and configure Identity Protection Configure multi-factor authentication for users, groups, and applications Create Conditional Access policies to ensure your security Create and follow an access review process","title":"Learning objectives"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-azure-ad-identity-protection","text":"Identity Protection is a tool that allows organizations to accomplish three key tasks: Automate the detection and remediation of identity-based risks. Investigate risks using data in the portal. Export risk detection data to third-party utilities for further analysis. Identity Protection uses the learnings Microsoft has acquired from their position in organizations with Azure AD, the consumer space with Microsoft Accounts, and in gaming with Xbox to protect your users. Microsoft analyzes 6.5 trillion signals per day to identify and protect customers from threats. Risk detections in Azure AD Identity Protection include any identified suspicious actions related to user accounts in the directory. The signals generated that are fed to Identity Protection, can be further fed into tools like Conditional Access to make access decisions, or fed back to a security information and event management (SIEM) tool for further investigation based on your organization's enforced policies. Identity Protection provides organizations access to powerful resources so they can quickly respond to suspicious activities.","title":"Explore Azure AD identity protection"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#identity-protection-policies","text":"Azure Active Directory Identity Protection includes three default policies that administrators can choose to enable. These policies include limited customization but are applicable to most organizations. All the policies allow for excluding users such as your emergency access or break-glass administrator accounts.","title":"Identity Protection policies"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-multi-factor-authentication-registration-policy","text":"Identity Protection can help organizations roll out Azure Multi-Factor Authentication using a Conditional Access policy requiring registration at sign-in. Enabling this policy is a great way to ensure new users in your organization have registered for MFA on their first day. Multi-factor authentication is one of the self-remediation methods for risk events within Identity Protection. Self-remediation allows your users to act on their own to reduce helpdesk call volume.","title":"Azure Multi-Factor Authentication registration policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#sign-in-risk-policy","text":"Identity Protection analyzes signals from each sign-in, both real-time and offline, and calculates a risk score based on the probability that the sign-in wasn't performed by the user. Administrators can decide based on this risk score signal to enforce organizational requirements. Administrators can choose to block access, allow access, or allow access but require multi-factor authentication. If risk is detected, users can perform multi-factor authentication to self-remediate and close the risky sign-in event to prevent unnecessary noise for administrators.","title":"Sign-in risk policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#custom-conditional-access-policy","text":"Administrators can also choose to create a custom Conditional Access policy, including sign-in risk as an assignment condition.","title":"Custom Conditional Access policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-risk-event-detections","text":"To protect your users, you can configure risk-based policies in Azure Active Directory (Azure AD) that automatically respond to risky behaviors. Azure AD Identity Protection policies can automatically block a sign-in attempt or require additional action, such as requiring a password change or prompt for Azure AD Multi-Factor Authentication. These policies work with existing Azure AD Conditional Access policies as an extra layer of protection for your organization. Users may never trigger a risky behavior in one of these policies, but your organization is protected if an attempt to compromise your security is made. Each day, Microsoft collects and analyses trillions of anonymized signals as part of user sign-in attempts. These signals help build patterns of good user sign-in behavior and identify potential risky sign-in attempts. Azure AD Identity Protection can review user sign-in attempts and take additional action if there's suspicious behavior: Some of the following actions may trigger Azure AD Identity Protection risk detection: - Users with leaked credentials. - Sign-ins from anonymous IP addresses. - Impossible travel to atypical locations. - Sign-ins from infected devices. - Sign-ins from IP addresses with suspicious activity. The following three policies are available in Azure AD Identity Protection to protect users and respond to suspicious activity. You can choose to turn the policy enforcement on or off, select users or groups for the policy to apply to, and decide if you want to block access at sign-in or prompt for additional action. The insight you get for a detected risk detection is tied to your Azure AD subscription. User risk policy - Identifies and responds to user accounts that may have compromised credentials. Can prompt the user to create a new password. Sign-in risk policy - Identifies and responds to suspicious sign-in attempts. Can prompt the user to provide additional forms of verification using Azure AD Multi-Factor Authentication. MFA registration policy - Makes sure users are registered for Azure AD Multi-Factor Authentication. If a sign-in risk policy prompts for MFA, the user must already be registered for Azure AD Multi-Factor Authentication. When you enable a policy user or sign-in risk policy, you can also choose the threshold for risk level - low and above , medium and above, or high. This flexibility lets you decide how aggressive you want to be in enforcing any controls for suspicious sign-in events.","title":"Configure risk event detections"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-user-risk-policy","text":"Identity Protection can calculate what it believes is normal for a user's behavior and use that to base decisions for their risk. User risk is a calculation of probability that an identity has been compromised. Administrators can decide based on this risk score signal to enforce organizational requirements. Administrators can choose to block access, allow access, or allow access but require a password change using Azure AD self-service password reset. The above image shows the configuration of User Risk Policy applied To user sign-ins Automatically respond based on a specific user\u2019s risk level Provide the condition (risk level) and action (block or allow) Use a high threshold during policy roll out Use a low threshold for greater security","title":"Implement user risk policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#risky-users","text":"With the information provided by the risky users report, administrators can find: Which users are at risk, have had risk remediated, or have had risk dismissed? Details about detections History of all risky sign-ins Risk history Administrators can then choose to act on these events. Administrators can choose to: Reset the user password Confirm user compromise Dismiss user risk Block user from signing in Investigate further using Azure ATP","title":"Risky users"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-sign-in-risk-policy","text":"Sign-in risk represents the probability that a given authentication request isn't authorized by the identity owner. For users of Azure Identity Protection, sign-in risk can be evaluated as part of a Conditional Access policy. Sign-in Risk Policy supports the following conditions:","title":"Implement sign-in risk policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#location","text":"When configuring location as a condition, organizations can choose to include or exclude locations. These named locations may include the public IPv4 network information, country or region, or even unknown areas that don't map to specific countries or regions. Only IP ranges can be marked as a trusted location. When including any location, this option includes any IP address on the internet not just configured named locations. When selecting any location, administrators can choose to exclude all trusted or selected locations.","title":"Location"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#client-apps","text":"Conditional Access policies by default apply to browser-based applications and applications that utilize modern authentication protocols. In addition to these applications, administrators can choose to include Exchange ActiveSync clients and other clients that utilize legacy protocols. Browser - These include web-based applications that use protocols like SAML, WS-Federation, OpenID Connect, or services registered as an OAuth confidential client. Mobile apps and desktop clients - These access policies are commonly used when requiring a managed device, blocking legacy authentication, and blocking web applications but allowing mobile or desktop app.","title":"Client apps"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#risky-sign-ins","text":"The risky sign-ins report contains filterable data for up to the past 30 days (1 month). With the information provided by the risky sign-ins report, administrators can find: Which sign-ins are classified as at risk, confirmed compromised, confirmed safe, dismissed, or remediated. Real-time and aggregate risk levels associated with sign-in attempts. Detection types triggered Conditional Access policies applied MFA details Device information Application information Location information Administrators can then choose to take action on these events. Administrators can choose to: Confirm sign-in compromise Confirm sign-in safe","title":"Risky sign-ins"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#deploy-multifactor-authentication-in-azure","text":"Azure Active Directory Multi-Factor Authentication helps safeguard access to data and applications while maintaining simplicity for users. It provides additional security by requiring a second form of authentication and delivers strong authentication through a range of easy to use authentication methods. For organizations that need to be compliant with industry standards, such as the Payment Card Industry (PCI) Data Security Standard (DSS) version 3.2, MFA is a must have capability to authenticate users. Beyond being compliant with industry standards, enforcing MFA to authenticate users can also help organizations to mitigate credential theft attacks. The security of MFA two-step verification lies in its layered approach. Compromising multiple authentication factors presents a significant challenge for attackers. Even if an attacker manages to learn the user's password, it is useless without also having possession of the additional authentication method. Authentication methods include: Something you know (typically a password) Something you have (a trusted device that is not easily duplicated, like a phone) Something you are (biometrics)","title":"Deploy multifactor authentication in Azure"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#multi-factor-authentication-features","text":"Get more security with less complexity . Azure MFA helps safeguard access to data and applications and helps to meet customer demand for a simple sign-in process. Get strong authentication with a range of easy verification options\u2014phone call, text message, or mobile app notification\u2014and allow customers to choose the method they prefer. Mitigate threats with real-time monitoring and alerts . MFA helps protect your business with security monitoring and machine-learning-based reports that identify inconsistent sign-in patterns. To help mitigate potential threats, real-time alerts notify your IT department of suspicious account credentials. Use with Microsoft 365, Salesforce, and more . MFA for Microsoft 365 helps secure access to Microsoft 365 applications at no additional cost. Multifactor authentication is also available with Azure Active Directory Premium and thousands of software-as-a-service (SaaS) applications, including Salesforce, Dropbox, and other popular services. Add protection for Azure administrator accounts . MFA adds a layer of security to your Azure administrator account at no additional cost. When it's turned on, you need to confirm your identity to create a virtual machine, manage storage, or use other Azure services.","title":"Multi-Factor Authentication Features"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#multi-factor-authentication-options","text":"Method Description Call to phone Places an automated voice call. The user answers the call and presses # in the phone keypad to authenticate. The phone number is not synchronized to on-premises Active Directory. A voice call to phone is important because it persists through a phone handset upgrade, allowing the user to register the mobile app on the new device. Text message to phone Sends a text message that contains a verification code. The user is prompted to enter the verification code into the sign-in interface. This process is called one-way SMS. Two-way SMS means that the user must text back a particular code. Two-way SMS is deprecated and not supported after November 14, 2018. Users who are configured for two-way SMS are automatically switched to call to phone verification at that time. Notification through mobile app Sends a push notification to your phone or registered device. The user views the notification and selects Approve to complete verification. The Microsoft Authenticator app is available for Windows Phone, Android, and iOS. Push notifications through the mobile app provide the best user experience. Verification code from mobile app The Microsoft Authenticator app generates a new OATH verification code every 30 seconds. The user enters the verification code into the sign-in interface. The Microsoft Authenticator app is available for Windows Phone, Android, and iOS. Verification code from mobile app can be used when the phone has no data connection or cellular signal.","title":"Multi-Factor Authentication Options"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-multifactor-authentication-settings","text":"","title":"Explore multifactor authentication settings"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#account-lockout","text":"To prevent repeated MFA attempts as part of an attack, the account lockout settings let you specify how many failed attempts to allow before the account becomes locked out for a period of time. The account lockout settings are only applied when a pin code is entered for the MFA prompt. The following settings are available: Number of MFA denials to trigger account lockout Minutes until account lockout counter is reset Minutes until account is automatically unblocked","title":"Account lockout"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#block-and-unblock-users","text":"If a user's device has been lost or stolen, you can block authentication attempts for the associated account.","title":"Block and unblock users"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#notifications","text":"Email notifications can be configured when users report fraud alerts. These notifications are typically sent to identity administrators, as the user's account credentials are likely compromised.","title":"Notifications"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#oath-tokens","text":"Azure AD supports the use of OATH-TOTP SHA-1 tokens that refresh codes every 30 or 60 seconds. Customers can purchase these tokens from the vendor of their choice.","title":"OATH tokens"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#trusted-ips","text":"Trusted IPs is a feature to allow federated users or IP address ranges to bypass two-step authentication. Notice there are two selections in this screenshot. Which selections you can make depends on whether you have managed or federated tenants. Managed tenants . For managed tenants, you can specify IP ranges that can skip MFA. Federated tenants . For federated tenants, you can specify IP ranges and you can also exempt AD FS claims users. Important The Trusted IPs bypass works only from inside of the company intranet. If you select the All Federated Users option and a user signs in from outside the company intranet, the user must authenticate by using two-step verification. The process is the same even if the user presents an AD FS claim.","title":"Trusted IPs"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#enable-multifactor-authentication","text":"To enable MFA, go to the User Properties in Azure Active Directory, and then the Multi-Factor Authentication option. From there, you can select the users that you want to modify and enable for MFA. You can also bulk enable groups of users with PowerShell. User's states can be Enabled, Enforced, or Disabled. Note On first-time sign-in, after MFA has been enabled, users are prompted to configure their MFA settings. For example, if you enable MFA so that users must use a mobile device, users will be prompted to configure their mobile device for MFA. Users must complete those steps, or they will not be permitted to sign in, which they cannot do until they have validated that their mobile device is MFA-compliant. All users start out Disabled. When you enroll users in per-user Azure AD Multi-Factor Authentication, their state changes to Enabled. When enabled users sign in and complete the registration process, their state changes to Enforced. Administrators may move users between states, including from Enforced to Enabled or Disabled.","title":"Enable multifactor authentication"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#enable-mfa-for-global-admins","text":"Azure AD Multi-Factor Authentication is included free of charge for global administrator security. Enabling MFA for global administrators provides an added level of security when managing and creating Azure resources like virtual machines, managing storage, or using other Azure services. Secondary authentication includes phone call, text message, and the authenticator app. Important Remember, you can only enable MFA for organizational accounts stored in Azure Active Directory. These are also called work or school accounts.","title":"Enable MFA for Global Admins"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-azure-ad-conditional-access","text":"The old world of security behind a corporate firewall, having your secure network perimeter just doesn\u2019t work anymore, not with people wanting to work from anywhere, being able to connect to all sorts of cloud applications. Conditional Access is the tool used by Azure Active Directory to bring signals together, to make decisions, and enforce organizational policies. Conditional Access is at the heart of the new identity driven control plane. Conditional access policy is really a next generation policy that\u2019s built for the cloud. It\u2019s able to consider massive amounts of data, as well as contextual data from a user sign-in flow and make sure that the right controls are enforced.","title":"Implement Azure AD conditional access"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#identity-as-a-servicethe-new-control-plane","text":"What is the basis for saying that identity management is the new control plane? First, what is the control plane? In a switch or router, the control plane is the part that controls where the traffic is to go, but it\u2019s not responsible for the movement of the traffic. The control plane learns the routes, either static or dynamic. The part responsible for moving the traffic is the forwarding plane. The following figure depicts a simple switch diagram. A user\u2019s identity is like a control plane, because it controls which protocols the user will interact with, which organizational programs the user can access, and which devices the user can employ to access those programs. Identity is what helps protect user and corporate data. For example, should that data be encrypted, deleted, or ignored when an issue occurs? Now, everything pivots around that user identity. You know what their activities are, and where they are located. You know what devices they\u2019re using. Then we leverage that information in conditional access policy to be able to enforce things like multifactor authentication or require a compliant device. There are the conditions, which indicate when the policy is going to apply. This can be, again, the location, type of application that you\u2019re on, any detected risk. How is the risk determined? It is determined from all the analysis and intel that we have across organizations using Azure Active Directory, as well as Microsoft consumer identity offerings. Conditional Access is the tool used by Azure Active Directory to bring signals together, to make decisions, and enforce organizational policies. Conditional Access policies at their simplest are if-then statements, if a user wants to access a resource, then they must complete an action. Example: A payroll manager wants to access the payroll application and is required to perform multifactor authentication to access it. Administrators are faced with two primary goals: Empower users to be productive wherever and whenever Protect the organization's assets By using Conditional Access policies, you can apply the right access controls when needed to keep your organization secure and stay out of your user\u2019s way when not needed. Conditional Access policies are enforced after the first-factor authentication has been completed. Conditional Access is not intended as an organization's first line of defense for scenarios like denial-of-service (DoS) attacks but can use signals from these events to determine access.","title":"Identity as a Service\u2014the new control plane"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-conditional-access-conditions","text":"Conditional access is a capability of Azure AD (with an Azure AD Premium license) that enables you to enforce controls on the access to apps in your environment based on specific conditions from a central location. With Azure AD conditional access, you can factor how a resource is being accessed into an access control decision. By using conditional access policies, you can apply the correct access controls under the required conditions. Conditional access comes with six conditions: user/group, cloud application, device state, location (IP range), client application, and sign-in risk. You can use combinations of these conditions to get the exact conditional access policy you need. Notice on this image the conditions determine the access control from the previous topic. With access controls, you can either Block Access altogether or Grant Access with more requirements by selecting the desired controls. You can have several options: Require MFA from Azure AD or an on-premises MFA (combined with AD FS). Grant access to only trusted devices. Require a domain-joined device. Require mobile devices to use Intune app protection policies. Requiring more account verification through MFA is a common conditional access scenario. While users may be able to sign in to most of your organization\u2019s cloud apps, you may want more verification for things like your email system, or apps that contain personnel records or sensitive information. In Azure AD, you can accomplish this with a conditional access policy Important The Users and Groups condition is mandatory in a conditional access policy. In your policy, you can either select All users or select specific users and groups.","title":"Configure conditional access conditions"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-access-reviews","text":"Azure Active Directory (Azure AD) access reviews enable organizations to efficiently manage group memberships, access to enterprise applications, and role assignments. User's access can be reviewed on a regular basis to make sure only the right people have continued access.","title":"Implement access reviews"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#why-are-access-reviews-important","text":"Azure AD enables you to collaborate internally within your organization and with users from external organizations, such as partners. Users can join groups, invite guests, connect to cloud apps, and work remotely from their work or personal devices. The convenience of leveraging the power of self-service has led to a need for better access management capabilities. As new employees join, how do you ensure they have the right access to be productive? As people move teams or leave the company, how do you ensure their old access is removed, especially when it involves guests? Excessive access rights can lead to audit findings and compromises as they indicate a lack of control over access. You must proactively engage with resource owners to ensure they regularly review who has access to their resources.","title":"Why are access reviews important?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#use-access-reviews-in-the-following-cases","text":"Too many users in privileged roles : It's a good idea to check how many users have administrative access, how many of them are Global Administrators, and if there are any invited guests or partners that have not been removed after being assigned to do an administrative task. You can recertify the role assignment users in Azure AD roles such as Global Administrators, or Azure resources roles such as User Access Administrator in the Azure AD Privileged Identity Management (PIM) experience. When automation is infeasible : You can create rules for dynamic membership on security groups or Microsoft 365 Groups, but what if the HR data is not in Azure AD or if users still need access after leaving the group to train their replacement? You can then create a review on that group to ensure those who still need access should have continued access. When a group is used for a new purpose : If you have a group that is going to be synced to Azure AD, or if you plan to enable a sales management application for everyone in the Sales team group, it would be useful to ask the group owner to review the group membership prior to the group being used in a different risk content. Business critical data access : for certain resources, it might be required to ask people outside of IT to regularly sign out and give a justification on why they need access for auditing purposes. To maintain a policy's exception list : In an ideal world, all users would follow the access policies to secure access to your organization's resources. However, sometimes there are business cases that require you to make exceptions. As the IT admin, you can manage this task, avoid oversight of policy exceptions, and provide auditors with proof that these exceptions are reviewed regularly. Ask group owners to confirm they still need guests in their groups : Employee access might be automated with some on premises IAM, but not invited guests. If a group gives guests access to business sensitive content, then it's the group owner's responsibility to confirm the guests still have a legitimate business need for access. Have reviews recur periodically : You can set up recurring access reviews of users at set frequencies such as weekly, monthly, quarterly or annually, and the reviewers will be notified at the start of each review. Reviewers can approve or deny access with a friendly interface and with the help of smart recommendations. Depending on what you want to review, you will create your access review in Azure AD access reviews, Azure AD enterprise apps (in preview), or Azure AD PIM. Using this feature requires an Azure AD Premium P2 license. Important Azure AD Premium P2 licenses are not required for users with the Global Administrator or User Administrator roles that set up access reviews, configure settings, or apply the decisions from the reviews.","title":"Use access reviews in the following cases"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-try-this-exercises_1","text":"","title":"Explore try-this exercises"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-1-configure-conditional-access-require-mfa","text":"Note This task requires a user account, AZ500User1. If you want to show the MFA verification, the user account must have a phone number. This task will review conditional access policy settings and create a policy that requires MFA when signing in to the Portal.","title":"Task 1 - Configure conditional access (require MFA)"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-the-policy","text":"In the Portal , search for and select Azure Active Directory . Under Manage , select Security . Under Protect , select Conditional access . Click New Policy . Name: AZ500Policy1 Users and groups > Select users and groups > Users and Groups > Select: AZ500User1 Cloud apps or actions > Select apps > Select: Microsoft Azure Management Review the warning that this policy impacts Portal access. Conditions > Sign-in risk > Review the risk levels Device platforms > Review the devices that can be included, such as Android and iOS. Locations > Review the physical location selections. Under Access controls click Grant . Review the Grant options such as MFA. You may require one or more of the controls. Select Require multi-factor authentication . For Enable policy , select On . Click Create.","title":"Configure the policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#test-the-policy","text":"Sign in to the Portal as the AZ500User1 . Before you can sign in, a second authentication is required. If you have a phone number associated with the user, provide and verify the text code. You should be able to sign in to the Portal successfully. If you do not have a phone number associated with the user, this demonstrates that MFA is in effect. You may want to return to the AZ500Policy1 and turn the policy Off .","title":"Test the policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-2-access-review","text":"In this task, we will configure an access review.","title":"Task 2 - Access review"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-an-access-review","text":"In the Portal , search for and select Identity Governance . Under Access Reviews , select Access Reviews . Click New Access Review . We will create an access review to ensure we validate the AZ500Admin group membership. Complete the required information and discuss each setting. Configuration settings are added as you make your selections. For example, if you select a weekly access review, you will be prompted for the duration. Review name: AZ500Review Start date: current date Frequency: One-time Users to review: Members of a group Scope: Everyone Select a group: AZ500Admins Reviewers: Selected user Select reviewers: add yourself as a reviewer Review the Upon completion settings , specifically the action if a reviewer doesn't respond. Review Advanced settings . Start the access review. On the Access review page, ensure the new access review is listed. The Status will change from Not started to Initializing .","title":"Configure an access review"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#conduct-an-access-review","text":"In this task, we will conduct an access review. When the access review is complete, you will receive an email. This is the email associated with your reviewer account. View the email and discuss the review instructions. Note when the review period will end. In the email, click Start review . On the Access reviews page, click the AZ500Review . Notice you are reviewing the AZ500Admin group members. There are two members. Use the Details link to view information about the user. Select Approve for one user and Deny for the other. Be sure to provide a Reason . Submit your reviews.","title":"Conduct an access review"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#review-the-access-review-results","text":"In this task, we will review the access review results. Return to the Portal . Click the AZ500Review . From the Overview blade, review the results. There should be one member approved and one member denied . Click Results for more detailed information about the reviewer and their reasons. From the Overview blade, click Stop and confirm you want to stop the review. The Review status should now be Complete .","title":"Review the access review results"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#apply-the-access-review","text":"In this task, we will apply the review results. In the Portal , search for and select Azure Active Directory . Under Manage , select Groups . Locate the AZ500Admins group. Review the members of the group.- Confirm there are two members. Return to the AZ500Review . Click Apply . Confirm that you want to remove the denied member. The Review status will change from Applying to Result applied . Verify the AZ500Admins group now only has one member.","title":"Apply the access review"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#knowledge-check_1","text":"Choose the best response for each of the questions below. Then select Check your answers . The compliance auditors want to ensure as employees change jobs or leave the company that their privileges are also changed or revoked. They are especially concerned about the Administrator group. To address their concerns. you implement which of the following? Access reviews ( Ans ) Azure time-based policies JIT virtual machine access Identity Protection has reported that a user\u2019s credentials have been leaked. According to policy, the user\u2019s password must be reset. Which Azure AD role can reset the password? Global Administrator ( Ans ) Security Administrator Security Operator Identity Protection identifies risks in which of the following classifications? Specific IP address Atypical travel ( Ans ) Unregistered device You have implemented Identity Protection and are reviewing the Risky users report. For each reported event you can choose any of the following actions? Confirm user compromise ( Ans ) Delete the risk event Dismiss user account Conditional Access can be used to enable which of the actions listed below? Block or grant access from specific time of day. Designate privileged user accounts. Require multifactor authentication. ( Ans ) Which licensing plan supports Identity Protection? Azure Active Directory Free Azure Active Directory Premium P1 Azure Active Directory Premium P2 ( Ans )","title":"Knowledge check"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-azure-ad-privileged-identity-management","text":"Ensure that your privileged identities have extra protection and are accessed only with the least amount of access needed to do the job.","title":"Configure Azure AD privileged identity management"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#introduction_3","text":"Azure AD Privileged Identity Management (PIM) allows you to manage, control, and monitor access to the most important resources in your organization. You can give just-in-time access and just-enough-access to users to allow them to do their tasks.","title":"Introduction"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#scenario_3","text":"A security engineer uses Privileged Identity Management to protect administrator privileges and mitigate the risk of excessive or misused access rights; some common tasks are: Configuring the scope of users and roles based on zero trust. Setting up a PIM workflow to enforce approval for role usage, and monitor the access. Implement just-in-time access.","title":"Scenario"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-the-zero-trust-model","text":"Cloud-based services and mobile computing have changed the technology landscape for the modern enterprise. Today\u2019s workforce often requires access to applications and resources outside traditional corporate network boundaries, rendering security architectures that rely on firewalls and virtual private networks (VPNs) insufficient. Changes brought about by cloud migration and a more mobile workforce has led to the development of an access architecture called Zero Trust.","title":"Explore the zero trust model"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#the-zero-trust-model","text":"Based on the principle of \u201cnever trust, always verify,\u201d Zero Trust helps secure corporate resources by eliminating unknown and unmanaged devices and limiting lateral movement. Implementing a true Zero Trust model requires that all components\u2014user identity, device, network, and applications\u2014be validated and proven trustworthy. Zero Trust verifies identity and device health prior to granting access to corporate resources. When access is granted, applying the principle of least privilege limits user access to only those resources that are explicitly authorized for each user, thus reducing the risk of lateral movement within the environment. In an ideal Zero Trust environment, the following four elements are necessary: Strong identity authentication everywhere (user verification via authentication) Devices are enrolled in device management, and their health is validated Least-privilege user rights (access is limited to only what is needed) The health of services is verified (future goal) For Microsoft, Zero Trust establishes a strict boundary around corporate and customer data. For end users, Zero Trust delivers a simplified user experience that allows them to easily manage and find their content. And for customers, Zero Trust creates a unified access platform that they can use to enhance the overall security of their entire ecosystem.","title":"The Zero Trust model"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#zero-trust-architecture","text":"A Zero Trust approach extends throughout the entire digital estate and serves as an integrated security philosophy and end-to-end strategy. The illustration below provides a representation of the primary elements that contribute to Zero Trust. In the illustration above: Security policy enforcement is at the center of a Zero Trust architecture. This includes Multi-Factor authentication with conditional access that takes into account user account risk, device status, and other criteria and policies that you set. Identities, devices (also called endpoints), data, applications, network, and other infrastructure components are all configured with appropriate security. Policies that are configured for each of these components are coordinated with your overall Zero Trust strategy. For example, device policies determine the criteria for healthy devices and conditional access policies require healthy devices for access to specific apps and data. Threat protection and intelligence monitors the environment, surfaces current risks, and takes automated action to remediate attacks.","title":"Zero Trust architecture"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#guiding-principles-of-zero-trust","text":"Today, organizations need a new security model that effectively adapts to the complexity of the modern environment, embraces the mobile workforce, and protects people, devices, applications, and data wherever they are located. To address this new world of computing, Microsoft highly recommends the Zero Trust security model, which is based on these guiding principles: Verify explicitly - Always authenticate and authorize based on all available data points. Use least privilege access - Limit user access with Just-In-Time and Just-Enough-Access (JIT/JEA), risk-based adaptive policies, and data protection. Assume breach - Minimize blast radius and segment access. Verify end-to-end encryption and use analytics to get visibility, drive threat detection, and improve defenses.","title":"Guiding principles of Zero Trust"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#microsofts-zero-trust-architecture","text":"Below is a simplified reference architecture for our approach to implementing Zero Trust. The primary components of this process are Intune for device management and device security policy configuration, Azure AD conditional access for device health validation, and Azure AD for user and device inventory. The system works with Intune, pushing device configuration requirements to the managed devices. The device then generates a statement of health, which is stored in Azure AD. When the device user requests access to a resource, the device health state is verified as part of the authentication exchange with Azure AD. Important The National Institute of Standards and Technology has a Zero Trust Architecture, NIST 800-207, publication.","title":"Microsoft's Zero Trust architecture"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#review-the-evolution-of-identity-management","text":"Microsoft Identity Manager or MIM helps organizations manage the users, credentials, policies, and access within their organizations and hybrid environments. With MIM, organizations can simplify identity lifecycle management with automated workflows, business rules, and easy integration with heterogenous platforms across the datacenter. MIM enables Active Directory Domain Services to have the right users and access rights for on-premises apps. Azure AD Connect can then make those users and permissions available in Azure AD for Microsoft 365 and cloud-hosted apps. On-premises Active Directory Domain Services, Azure Active Directory (Azure AD), or a hybrid combination of the two all offer services for user and device authentication, identity and role management, and provisioning. Identity has become the common factor among many services, like Microsoft 365 and Xbox Live, where the person is the center of the services. Identity is now the security boundary, the new firewall, the control plane\u2014whichever comparison you prefer. Your digital identity is the combination of who you are and what you\u2019re allowed to do. That is: Credentials + privileges = digital identity First step, you need to help protect your privileged accounts. These identities have more than the normal user rights and, if compromised, allow a malicious hacker to access sensitive corporate assets. Helping secure these privileged identities is a critical step to establishing security assurances for business assets in a modern organization. Cybercriminals target these accounts and other privileged services in their kill chain to carry out their objectives.","title":"Review the evolution of identity management"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#evolution-of-identities","text":"Identity management approaches have evolved from traditional, to advanced, to optimal. Traditional identity approaches On-premises identity providers. No single sign-on is present between on-premises and cloud apps. Visibility into identity risk is very limited. Advanced identity approaches Conditional access policies gate access and provide remediation actions. Analytics improve visibility into identity risk. Optimal identity approaches Passwordless authentication is enabled. User, location, devices, and behavior are analyzed in real time. Continuous protection to identity risk.","title":"Evolution of identities"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#steps-for-a-passwordless-world","text":"Enforce MFA \u2014 Conform to the fast identity online (FIDO) 2.0 standard, so you can require a PIN and a biometric for authentication rather than a password. Windows Hello is one good example, but choose the MFA method that works for your organization. Reduce legacy authentication workflows \u2014 Place apps that require passwords into a separate user access portal and migrate users to modern authentication flows most of the time. At Microsoft only 10 percent of our users enter a password on a given day. Remove passwords \u2014 Create consistency across Active Directory Domain Services and Azure Active Directory (Azure AD) to enable administrators to remove passwords from identity directory. Important We recommend Azure AD Privileged Identity Management as the service to help protect your privileged accounts.","title":"Steps for a passwordless world"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#deploy-azure-ad-privileged-identity-management","text":"With the Azure AD Privileged Identity Management (PIM) service, you can manage, control, and monitor access to important resources in your organization. This includes access to resources in Azure AD; Azure; and other Microsoft Online Services, like Microsoft 365 and Microsoft Intune. This control does not eliminate the need for users to carry out privileged operations in Azure AD, Azure, Microsoft 365, and Software as a Service (SaaS) apps. Organizations can give users just-in-time (JIT) privileged access to Azure resources and Azure AD. Oversight is needed for what those users do with their administrator privileges. PIM helps mitigate the risk of excessive, unnecessary, or misused access rights.","title":"Deploy Azure AD privileged identity management"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#key-pim-features","text":"Providing just-in-time privileged access to Azure AD and Azure resources. IT administrators can pick an activation period between 0.5 and a role's maximum duration (max is 24 hours). They will only receive the privilege for that period of time. After the activation period admins will have to go through the activation process again. Assigning time-bound access to resources by using start and end dates. PIM allows you to set an end time for the role. This is particularly useful in a guest scenario. If your organization has guests that are working for a specific time the role privilege will expire automatically. Requiring approval to activate privileged roles. You can designate one or more approvers. These approvers will receive an email once a request is made. Approval is required to activate the privilege. Enforcing Azure Multi-Factor Authentication (MFA) to activate any role. If your organization already has MFA enabled, PIM will not ask the user to sign in again. Using justification to understand why users activate. This benefits both internal and external auditors understanding why the role was activated. You can also require a service ticket number from whatever service product you are using. Getting notifications when a user is assigned a privilege and when that privilege is activated. Conducting access reviews to know which users have privileged roles in the organization and if they still need them. Downloading an audit history for an internal or external audit. This keeps tracks of all PIM events. Ways to use PIM We use Azure AD PIM in the following ways: View which users are assigned privileged roles to manage Azure resources, as well as which users are assigned administrative roles in Azure AD. Enable on-demand, \u201cjust in time\u201d administrative access to Microsoft Online Services like Microsoft 365 and Intune, and to Azure resources of subscriptions, resource groups, and individual resources such as Virtual Machines. Review a history of administrator activation, including what changes administrators made to Azure resources. Get alerts about changes in administrator assignments. Require approval to activate Azure AD privileged admin roles. Review membership of administrative roles and require users to provide a justification for continued membership.","title":"Key PIM features"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-privileged-identity-management-scope","text":"Azure AD roles . These roles are all in Azure Active Directory (such as Global Administrator, Exchange Administrator, and Security Administrator). You can read more about the roles and their functionality in Administrator role permissions in Azure Active Directory. Azure resource roles . These roles are linked to an Azure resource, resource group, subscription, or management group. Privileged Identity Management provides just-in-time access to both built-in roles like Owner, User Access Administrator, and Contributor, as well as custom roles.","title":"Configure privileged identity management scope"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-ad-roles","text":"Users can be assigned to different administrative roles in Azure AD. These role assignments control which tasks, such as adding or removing users or changing service settings, the users are able to perform on Azure AD, Microsoft 365 and other Microsoft Online Services and connected applications. A global administrator can update which users are permanently assigned to roles in Azure AD, using PowerShell cmdlets such as Add-MsolRoleMember and Remove-MsolRoleMember, or through the Azure portal. Azure AD Privileged Identity Management (PIM) manages policies for privileged access for users in Azure AD. PIM assigns users to one or more roles in Azure AD, and you can assign someone to be permanently in the role, or eligible for the role. When a user is permanently assigned to a role, or activates an eligible role assignment, then they can manage Azure Active Directory, Microsoft 365, and other applications with the permissions assigned to their roles. There's no difference in the access given to someone with a permanent versus an eligible role assignment. The only difference is that some people don't need that access all the time. They are made eligible for the role, and can turn it on and off whenever they need to.","title":"Azure AD roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#roles-managed-in-pim","text":"Privileged Identity Management lets you assign users to common administrator roles, including: Global administrator (also known as a Company administrator) has access to all administrative features. You can have more than one global admin in your organization. The person who signs up to purchase Microsoft 365 automatically becomes a global admin. Privileged role administrator manages Azure AD PIM and updates role assignments for other users. Billing administrator makes purchases, manages subscriptions, manages support tickets, and monitors service health. Password administrator users with this role have limited ability to manage passwords. This role does not grant the ability to manage service requests or monitor service health. Whether a Password Administrator can reset a user's password depends on the user's role. Service administrator manages service requests and monitors service health. User management administrator resets passwords, monitors service health, and manages user accounts, user groups, and service requests. The user management admin can\u2019t delete a global admin, create other admin roles, or reset passwords for billing, global, and service admins. Exchange administrator has administrative access to Exchange Online through the Exchange admin center (EAC), and can perform almost any task in Exchange Online. SharePoint administrator has administrative access to SharePoint Online through the SharePoint Online admin center, and can perform almost any task in SharePoint Online. Skype for Business administrator has administrative access to Skype for Business through the Skype for Business admin center, and can perform almost any task in Skype for Business Online.","title":"Roles managed in PIM"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#roles-not-managed-in-pim","text":"Roles within Exchange Online or SharePoint Online, except for those mentioned above, are not represented in Azure AD and so are not visible in PIM. Azure subscriptions and resource groups are also not represented in Azure AD.","title":"Roles not managed in PIM"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#azure-resources","text":"When you first set up Privileged Identity Management for Azure resources, you need to discover and select the resources to protect with Privileged Identity Management. There's no limit to the number of resources that you can manage with Privileged Identity Management. However, we recommend starting with your most critical (production) resources.","title":"Azure resources"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-privileged-identity-management-onboarding","text":"To use PIM, you need one of the following paid or trial licenses: Azure AD Premium P2, Enterprise Mobility + Security (EMS) E5, or Microsoft 365 M5. PIM Access The first Global Administrator to use PIM in your instance of Azure AD is automatically assigned the Security Administrator and Privileged Role Administrator roles in the directory. This person must be an eligible Azure AD user. Only privileged role administrators can manage the Azure AD directory role assignments of users. In addition, you can choose to run the security wizard that walks you through the initial discovery and assignment experience. Users or members of a group assigned to the Owner or User Access Administrator roles, and Global Administrators that enable subscription management in Azure AD, are Resource Administrators. These administrators can assign roles, configure role settings, and review access by using PIM for Azure resources. No one else in your Azure Active Directory (Azure AD) organization gets write access by default, though, including other Global administrators. Other Global administrators, Security administrators, and Security readers have read-only access to Privileged Identity Management. To grant access to Privileged Identity Management, the first user can assign others to the Privileged Role Administrator role. Important Make sure there are always at least two users in a Privileged Role Administrator role, in case one user is locked out or their account is deleted.","title":"Implement privileged identity management onboarding"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-privileged-identity-management-configuration-settings","text":"","title":"Explore privileged identity management configuration settings"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#activation-settings","text":"Activation duration . Set the maximum time, in hours, that a role stays active before it expires. This value can be from one to 24 hours. Require multifactor authentication on activation . You can require users who are eligible for a role to prove who they are using Azure Active Directory Multi-Factor Authentication (MFA) before they can activate. Multifactor authentication ensures that the user is who they say they are with reasonable certainty. Enforcing this option protects critical resources in situations when the user account might have been compromised. Require justification . You can require that users enter a business justification when they activate. Require approval to activate . If setting multiple approvers, approval completes as soon as one of them approves or denies. You can't require approval from at least two users.","title":"Activation settings"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#assignment-settings","text":"Allow permanent eligible assignment . Global admins and Privileged role admins can assign permanent eligible assignment. They can also require that all eligible assignments have a specified start and end date. Allow permanent active assignment . Global admins and Privileged role admins can assign active eligible assignment. They can also require that all active assignments have a specified start and end date. Note In some cases, you might want to assign a user to a role for a short duration (one day, for example). In this case, the assigned users don't need to request activation. In this scenario, Privileged Identity Management can't enforce multifactor authentication when the user uses their role assignment because they are already active in the role from the time that it is assigned.","title":"Assignment settings"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#notification-settings","text":"Notifications can be sent when members are assigned as eligible in a role, assigned as active in a role, and when the role is activated. Notifications can be sent to Admins, Requestors, and Approvers.","title":"Notification settings"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#implement-a-privileged-identity-management-workflow","text":"By configuring Azure AD PIM to manage our elevated access roles in Azure AD, we now have JIT access for more than 28 configurable privileged roles. We can also monitor access, audit account elevations, and receive additional alerts through a management dashboard in the Azure portal. Elevated access includes job roles that need greater access, including support, resource administrators, resource owners, service administrators, and global administrators. We manage role-based access at the resource level. Because elevated access accounts could be misused if they\u2019re compromised, we rationalize new requests for elevated access and perform regular re-attestation for elevated roles. The following diagram of the elevated access workflow.","title":"Implement a privileged identity management workflow"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#jit-administrator-access","text":"Historically, we could assign an employee to an administrative role through the Azure portal or through Windows PowerShell and that employee would be a permanent administrator; their elevated access would remain active in the assigned role. Azure AD PIM introduced the concept of permanent and eligible administrators in Azure AD and Azure. Permanent administrators have persistent elevated role connections; whereas eligible administrators have privileged access only when they need it. The eligible administrator role is inactive until the employee needs access, then they complete an activation process and become an active administrator for a set amount of time. We\u2019ve stopped using permanent administrators for named individual accounts, although we do have some automated service accounts that still use the role.","title":"JIT administrator access"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#role-activation-in-azure-active-directory","text":"Azure AD PIM uses administrative roles, such as tenant admin and global admin, to manage temporary access to various roles. With Azure AD PIM, you can manage the administrators by adding or removing permanent or eligible administrators to each role. Azure AD PIM includes several built-in Azure AD roles as well as Azure that we manage. To activate a role, an eligible admin will initialize Azure AD PIM in the Azure portal and request a time-limited role activation. The activation is requested using the Activate my role option in Azure AD PIM. Users requesting activation must satisfy conditional access policies to ensure that they are coming from authorized devices and locations, and their identities must be verified through multifactor authentication. To help secure transactions while enabling mobility, we use Azure AD PIM to customize role activation variables in Azure, including the number of sign-in attempts, the length of time the role is activated after sign-in, and the type of credentials required (such as single sign-in or multifactor authentication). At Microsoft, when an individual joins a team or changes teams, they might need administrative rights for their new business role. For example, someone might join a team in which their user account will require Exchange Online Administrator privileged access rights in the future. That user makes a request, then their manager validates that user\u2019s request, as does a service owner. With those approvals, Core Services Engineering and Operations (CSEO, formerly Microsoft IT) administrators in the Privileged Role Administrator role are notified. A CSEO administrator uses Azure AD PIM via the Azure portal to make that user eligible for that role. The user can then use Azure AD PIM to activate that role.","title":"Role activation in Azure Active Directory"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#tracking-the-use-of-privileged-roles-using-the-dashboard","text":"A dashboard through the Azure portal gives a centralized view of: Alerts that point out opportunities to improve security. The number of users who are assigned to each privileged role. The number of eligible and permanent admins. Ongoing access reviews. We can track how employees and admins are using their privileged roles by viewing the audit history or by setting up a regular access review. Both options are available through the PIM dashboard in the Azure portal. The PIM audit log tracks changes in privileged role assignments and role activation history. We use the audit log to view all user assignments and activations within a specified period. The audit history helps us determine, in real time, which accounts haven\u2019t signed in recently, or if employees have changed roles. Access reviews can be performed by an assigned reviewer, or employees can review themselves. This is an effective way to monitor who still needs access, and who can be removed. We\u2019re looking at the data that\u2019s collected, and the monitoring team is assessing the best way to configure monitoring alerts to notify us about out-of-band changes\u2014for example, if too many administrator roles are being created for an Azure resource. The information also helps us determine whether our current elevation time settings are appropriate for the various privileged admin roles. Like all organizations, we want to minimize the number of people who have access to our secure information or resources, because that reduces the chance of a malicious user getting access or an authorized user inadvertently impacting a sensitive resource. However, our people still need to carry out privileged operations in Azure AD, Azure, Microsoft 365, and SaaS apps. We can give users privileged access to Azure resources like Subscriptions, and Azure AD. Oversight is needed for what our users are doing with their admin privileges. We use Azure AD PIM to mitigate the risk of excessive, unnecessary, and misused access rights. In Azure AD, we use Azure AD PIM to manage the users we assign to built-in Azure AD organizational roles, such as Global Administrator. In Azure, we use Azure AD PIM to manage our users and groups that we assign via Azure RBAC roles, including Owner and Contributor.","title":"Tracking the use of privileged roles using the dashboard"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-try-this-exercises_2","text":"","title":"Explore Try-This exercises"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-1-azure-ad-pim-for-roles","text":"","title":"Task 1: Azure AD PIM for roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-pim-settings","text":"Note This task requires a AZ500User1 account with no assigned roles. In this task, we will review and configure the basic PIM settings. In the Portal , search for and select Azure AD Privileged Identity Management . Under Manage select Azure AD Roles . Under Manage select Settings . Select the Billing Administrator role. Click Edit . Notice the Activation, Assignment , and Notification tabs. By default, MFA is required on activation. For this demonstration, change the requirement to None . Check the box to Require approval to activate . Discuss the other possible settings including Activation maximum duration and Require approval to activate . Switch to the Assignment tab and require the settings. Notice the ability to expire eligible and active assignments. Switch to the Notifications tab and discuss the settings. Notice you can send notifications when members are assigned and activated. Click Update .","title":"Configure PIM settings"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-pim-for-roles","text":"In this task, we will add the Billing Administrator role to PIM. In the Portal , search for and select Azure AD Privileged Identity Management . Under Manage select Azure AD Roles . Under Manage select Roles . Review the list of roles. Select the Billing Administrator role. Review Eligible roles and Active roles . Click Add member . Click Select member and Select the AZ500User1 user. You are now a Billing Administrator. Select Set membership settings . Notice the settings can be permanent or limited in time. - Assignment type: Eligible - Permanently eligible: check the box . Save your changes and Add the assignment. Verify the Billing Administrator is listed as an eligible role.","title":"Configure PIM for Roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#activate-a-role","text":"In this task, we will activate the Billing Administrator role. In the Portal , search for and select Azure Active Directory . Under Manage click Users . Select AZ500User1 . Under Manage click Assigned roles . Verify the user is not assigned to any roles. Sign in the Portal as AZ500User1 . Search for and select Azure AD Privileged Identity Management . Under Tasks select My roles . Under Activate select Azure AD Roles . Select the Active roles and verify there are no roles listed. On the Eligible roles tab notice the Billing Administrator role. Under the Action column, select Activate . Assignment details are shown in the Portal. This includes start and end times, and the ability to add a reason. Add a reason and then click Activate . The Activation status should show all the activation stages have been completed. Use the link to Sign out . You must sign out and log back in to start using your newly activated role.","title":"Activate a role"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#test-the-role-access","text":"In this task, test the Billing Administrator role. Sign in to the Portal as AZ500User1 . Search for and select Azure AD Privileged Identity Management . Under Activate select Azure AD Roles . Select the Active roles tab and verify the Billing Administrator role has been activated. The role should show Activated . Notice the ability to Deactivate the role.","title":"Test the role access"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-2-azure-ad-pim-for-resources","text":"In this task, we will configure PIM for Azure resources, activate the Virtual Machine Contributor role, and test the role access.","title":"Task 2: Azure AD PIM for resources"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-pim-for-azure-resources","text":"In this task, we will add the subscription to PIM, then add the Virtual Machine Contributor role as an Active role. In the Portal , search for and select Azure AD Privileged Identity Management . Under Manage select Azure Resources . Click Discover resources . Notice the Resource state is Unmanaged . Select the subscription you want to manage. Click Manage resource . Click Yes to confirm that PIM will manage all child objects for the selected resource. Return to the Azure resources blade. Select your subscription. Under Manage click Roles . Search for and select the Virtual machine contributor role. Click Add assignments , then click Select member(s) and add the AZ500User1 to the group. On the Membership settings page set the Assignment type is Active . Add the role and Save your changes. Sign out of the Portal.","title":"Configure PIM for Azure resources"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#activate-the-role","text":"In this task, we will sign-in as a user and activate the role. Sign in to the Portal and AZ500User1. Search for and select Azure AD Privileged Identity Management. Under Tasks select My roles. Under Activate select Azure resources. On the Active roles tab notice you have no assigned roles. On the Eligible roles tab scroll to the right and Activate the role. Notice the Start time and Duration. Provide a reason for the activation. For example, 'Need to add a NIC'. Click Activate. The Activation status should show all the activation stages have been completed. Use the link to Sign out. You must sign out and log back in to start using your newly activated role.","title":"Activate the role"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#test-the-role-access_1","text":"In this task, we will check to ensure the role has been assigned. Sign in to the Portal as AZ500User1. Search for and select Azure AD Privileged Identity Management. Under Activate select Azure resources. Select the Active roles tab and verify the Virtual Machine Contributor role has been activated. Sign out of the Portal. Sign in to the Portal using a Global Admin account. Search for and select Azure Active Directory. Under Manage click Users. Select AZ500User1. Under Manage click Assigned roles. Verify there are no roles listed. Under Manage select Azure role assignments. Verify the Virtual machine contributor role is listed.","title":"Test the role access"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#knowledge-check_2","text":"Check your knowledge 1. To enable Azure AD PIM for your directory, what Azure AD Role do you need to enable PIM? Office 365 Admin Co-Administrator Global Admin ( Ans ) A company has implemented Azure AD PIM. There's a need to ensure a new hire's request elevation before they make any changes in Azure, what should you do? Activate the new hire. Assign the new hire the Eligible role membership type. ( Ans ) Include the new hire in an access review. Azure AD PIM is used to manage which of the following roles? Azure privileged users Azure resource groups Azure AD roles ( Ans ) An organization has enabled Azure AD PIM. The senior IT manager wants the role set up so no action is required, what should you do? Give the manager JIT access to the role. Make the manager Permanent Active in the role. ( Ans ) Make the manager Assigned to a role.","title":"Knowledge check"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#design-an-enterprise-governance-strategy","text":"","title":"Design an enterprise governance strategy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#introduction_4","text":"The first part of your organization's secure Azure tenant is set up, but you need to monitor and maintain it. Enterprise Governance is the process of setting strategic tools, systems, and process into motion to keep your systems secure and running well.","title":"Introduction"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#scenario_4","text":"A security engineer uses enterprise governance tools and policies to manage and maintain a secure Azure solution; some common tasks are: Designing an Azure secure access hierarchy. Using RBAC and Azure Policy to control and manage access. Creating blueprints of secure deployments that can be reused.","title":"Scenario"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#review-the-shared-responsibility-model","text":"Organizations face many challenges with securing their data centers, including recruiting and keeping security experts, using many security tools, and keeping pace with the volume and complexity of threats. As computing environments move from customer-controlled data centers to the cloud, the security responsibility also shifts. Security of the operational environment is now a concern shared by both cloud providers and customers. By shifting these responsibilities to a cloud service like Azure, organizations can reduce focus on activities that aren't core business competencies. Depending on the specific technology choices, some security protections will be built into the service, while others will remain the customer\u2019s responsibility. To ensure that the proper security controls are provided, a careful evaluation of the services and technology choices becomes necessary. The first thing to understand about cloud security is that different scopes of responsibility exist depending on the kinds of services you use. For example, if you use virtual machines (VMs) in Azure, which provide Infrastructure as a Service (IaaS), Microsoft will be responsible for helping secure the physical network, physical storage, and virtualization platform, which includes updating the virtualization hosts. But you\u2019ll need to take care of helping secure your virtual network and public endpoints and updating the guest operating system (OS) of your VMs. The following figure depicts the various responsibility zones. For all cloud deployment types, you own your data and identities. You're responsible for helping secure your data and identities, your on-premises resources, and the cloud components you control (which vary by service type). Regardless of the deployment type, you always retain responsibility for the following: Data Endpoints Accounts Access management Important It\u2019s important to understand the division of responsibility between you and Microsoft in a Software as a Service (SaaS), Platform as a Service (PaaS), or Infrastructure as a Service (IaaS) deployment.","title":"Review the shared responsibility model"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-the-azure-cloud-security-advantages","text":"The cloud offers significant advantages for solving long standing information security challenges. In an on-premises environment, organizations likely have unmet responsibilities and limited resources available to invest in security, which creates an environment where attackers can exploit vulnerabilities at all layers. The following diagram shows a traditional approach where many security responsibilities are unmet due to limited resources. In the cloud-enabled approach, you can shift day-to-day security responsibilities to your cloud provider and reallocate your resources. In the cloud-enabled approach, you are also able to leverage cloud-based security capabilities for more effectiveness and use cloud intelligence to improve your threat detection and response time. By shifting responsibilities to the cloud provider, organizations can get more security coverage, which enables them to reallocate security resources and budget to other business priorities. Important What security advantages are you expecting from leveraging the cloud?","title":"Explore the Azure cloud security advantages"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#review-azure-hierarchy-of-systems","text":"Azure Resource Manager is the deployment and management service for Azure. It provides a consistent management layer that allows you to create, update, and delete resources in your Azure subscription. You can use its access control, auditing, and tagging features to help secure and organize your resources after deployment. When you take actions through the portal, Azure PowerShell, the Azure CLI, REST APIs, or client software development kits (SDKs), the Resource Manager API handles your request. Because the same API handles all requests, you get consistent results and capabilities from all the different tools. Functionality initially released through APIs should be represented in the portal within 180 days of the initial release.","title":"Review Azure hierarchy of systems"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#understand-scope","text":"Azure provides four levels of scope: management groups, subscriptions, resource groups, and resources. The following image shows an example of these layers. Though not labeled as such, the blue cubes are resources. You apply management settings at any of these levels of scope. The level you select determines how widely the setting is applied. Lower levels inherit settings from higher levels. For example, when you apply a policy to the subscription, the policy is applied to all resource groups and resources in your subscription. When you apply a policy on the resource group, that policy is applied to the resource group and all its resources. However, another resource group doesn't have that policy assignment. You can deploy templates to management groups, subscriptions, or resource groups.","title":"Understand Scope"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#resource-groups","text":"There are some important factors to consider when defining your resource group: All the resources in your group should share the same lifecycle. You deploy, update, and delete them together. If one resource, such as a database server, needs to exist on a different deployment cycle it should be in another resource group. Each resource can only exist in one resource group. You can add or remove a resource to a resource group at any time. You can move a resource from one resource group to another group. A resource group can contain resources that are located in different regions. A resource group can be used to scope access control for administrative actions. A resource can interact with resources in other resource groups. This interaction is common when the two resources are related but don't share the same lifecycle (for example, web apps connecting to a database). When creating a resource group, you need to provide a location for that resource group. You may be wondering, \"Why does a resource group need a location? And, if the resources can have different locations than the resource group, why does the resource group location matter at all?\" The resource group stores metadata about the resources. Therefore, when you specify a location for the resource group, you're specifying where that metadata is stored. For compliance reasons, you may need to ensure that your data is stored in a particular region. If the resource group's region is temporarily unavailable, you can't update resources in the resource group because the metadata is unavailable. The resources in other regions will still function as expected, but you can't update them.","title":"Resource Groups"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#management-groups","text":"Management groups are an Azure resource to create flexible and very maintainable hierarchies within the structure of your environment. Management groups exist above the subscription level thus allowing subscriptions to be grouped together. This grouping facilitates applying policies and RBAC permissions to those management groups. Policies and RBAC permissions are inherited to all resources in the management group. Management groups give you enterprise-grade management at a large scale no matter what type of subscriptions you might have. All subscriptions within a single management group must trust the same Azure Active Directory tenant. Management group hierarchies can be up to six levels deep. This provides you with the flexibility to create a hierarchy that combines several of these strategies to meet your organizational needs. For example, the diagram below shows an organizational hierarchy that combines a business unit strategy with a geographic strategy.","title":"Management Groups"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#the-value-of-management-groups","text":"Group your subscriptions. Provide user access to multiple subscriptions Allows for new organizational models and logically grouping of resources. Allows for single assignment of controls that applies to all subscriptions. Provides aggregated views above the subscription level. Mirror your organization's structure. Create a flexible hierarchy that can be updated quickly. The hierarchy does not need to model the organization's billing hierarchy. The structure can easily scale up or down depending on your needs. Apply policies or access controls to any service. Create one RBAC assignment on the management group, which will inherit that access to all the subscriptions. Use Azure Resource Manager integrations that allow integrations with other Azure services: Azure Cost Management, Privileged Identity Management, and Microsoft Defender for Cloud. Important By using management groups, you can reduce your workload and reduce the risk of error by avoiding duplicate assignments. Instead of applying multiple assignments across numerous resources and subscriptions, you can apply the one assignment on the one management group that contains the target resources. This will save time in the application of assignments, creates one point for maintenance, and allows for better controls on who can control the assignment.","title":"The value of management groups"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-azure-policies","text":"Azure Policy is a service you use to create, assign, and manage policies. These policies enforce different rules and effects over your resources so that those resources stay compliant with your corporate standards and service level agreements. Azure Policy meets this need by evaluating your resources for noncompliance with assigned policies. For example, you might have a policy that allows virtual machines of only a certain size in your environment. After this policy is implemented, new and existing resources are evaluated for compliance. With the right type of policy, existing resources can be brought into compliance. There are three main pillars in the functionalities of Azure policy. The first pillar is around real-time enforcement and compliance assessment. For example, a policy would block the creation of resources that are located outside of US regions. Each policy also provides compliance assessment on all your existing resources to bring a state of compliance for each resource. The data then powers the compliance view which aggregates results across all of the applied policies. Policies can be used to ensure that resource groups are getting tagged properly and automatically inheriting those tags from the resource group down to the resources. The second pillar of policy is applying policies at scale by leveraging Management Groups. By assigning policy to a management group one can impact hundreds of subscriptions and all its reach resources through a single policy assignment. There also is the concept called policy initiative that allows you to group policies together so that you can view the aggregated compliance result. At the initiative level there's also a concept called exclusion where one can exclude either the child management group subscription resource group or resources from the policy assignment. The third pillar of your policy is remediation by leveraging a remediation policy that will automatically remediate the non-compliant resource so that your environment always stays compliant. For existing resources, they will be flagged as non-compliant but they won't automatically be changed because there can be impact to the environment. For these cases you can create a remediation task to bring these resources to compliance. Azure policy is a free service to use.","title":"Configure Azure policies"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#policy-permissions-and-custom-policies","text":"Azure Policy has several permissions, known as operations, in two resource providers: Microsoft.Authorization Microsoft.PolicyInsights Many built-in roles grant permissions to Azure Policy resources. The Resource Policy Contributor role includes most Azure Policy operations. The Owner role has full rights. Both Contributor and Reader can use all Azure Policy read operations, but Contributor can also trigger remediation. If none of the built-in roles have the required permissions, create a custom role. Azure has by default, security policies that work across subscriptions or on management groups. If these policies need to be augmented with your own organizational policies, new policies can be created. Whatever the business driver for creating a custom policy, the steps are the same for defining the new custom policy. Before creating a custom policy, check the policy samples to determine if a policy that matches your needs already exists. The approach to creating a custom policy follows these steps: Identify your business requirements Map each requirement to an Azure resource property Map the property to an alias Determine which effect to use Compose the policy definition","title":"Policy permissions and custom policies"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#composing-an-azure-policy","text":"The steps for composing and implementing a policy in Azure Policy begins with creating: Policy definition - Every policy definition has conditions under which it's enforced. And, it has a defined effect that takes place if the conditions are met. Policy assignment - A policy definition that has been assigned to take place within a specific scope. This scope could range from a management group to an individual resource. The term scope refers to all the resources, resource groups, subscriptions, or management groups that the policy definition is assigned to. Policy parameters - They help simplify your policy management by reducing the number of policy definitions you must create. You can define parameters when creating a policy definition to make it more generic.","title":"Composing an Azure Policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#create-and-assign-an-initiative-definition","text":"In order to easily track compliance for multiple resources, create and assign an Initiative definition. With an initiative definition, you can group several policy definitions to achieve one overarching goal. An initiative evaluates resources within scope of the assignment for compliance to the included policies. To implement these policy definitions (both built-in and custom definitions), you'll need to assign them. You can assign any of these policies through the Azure portal, PowerShell, or Azure CLI.","title":"Create and assign an Initiative definition"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#enable-azure-role-based-access-control-rbac","text":"When it comes to identity and access, most organizations that are considering using the public cloud are concerned about two things: Ensuring that when people leave the organization, they lose access to resources in the cloud. Striking the right balance between autonomy and central governance\u2014for example, giving project teams the ability to create and manage virtual machines in the cloud while centrally controlling the networks to which those virtual machines connect. RBAC is an authorization system built on Azure Resource Manager that provides fine-grained access management of Azure resources. Azure AD and Role Based Access Control (RBAC) make it simple for you to carry out these goals. After you extend your on-premises Active Directory to the cloud by using Azure AD Connect, your employees can use and manage their Azure subscriptions by using their existing work identities. These Azure subscriptions automatically connect to Azure AD for SSO and access management. When you disable an on-premises Active Directory account, it automatically loses access to all Azure subscriptions connected with Azure AD. Additionally, synchronizing passwords to the cloud to support these checks also add resiliency during some attacks. Customers affected by (Not)Petya attacks were able to continue business operations when password hashes were synced to Azure AD (vs. near zero communications and IT services for customers affected organizations that had not synchronized passwords). RBAC enables fine-grained access management for Azure. Using RBAC, you can grant just the amount of access that users need to perform their jobs. For example, you can use RBAC to let one employee manage virtual machines in a subscription while another manages SQL databases within the same subscription. Each Azure subscription is associated with one Azure AD directory . Users, groups, and applications in that directory can manage resources in the Azure subscription. Grant access by assigning the appropriate RBAC role to users, groups, and applications at a certain scope. The scope of a role assignment can be a subscription, a resource group, or a single resource. A role assigned at a parent scope also grants access to the child scopes contained within it. For example, a user with access to a resource group can manage all the resources it contains, like websites, virtual machines, and subnets. The RBAC role that you assign dictates what resources the user, group, or application can manage within that scope. The following diagram depicts how the classic subscription administrator roles, RBAC roles, and Azure AD administrator roles are related at a high level. Roles assigned at a higher scope, like a subscription, are inherited by child scopes, like service instances. Important Note that a subscription is associated with only one Azure AD tenant. Also note that a resource group can have multiple resources but is associated with only one subscription. Lastly, a resource can be bound to only one resource group.","title":"Enable Azure role-based access control (RBAC)"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#compare-and-contrast-azure-rbac-vs-azure-policies","text":"There are a few key differences between Azure Policy and Azure role-based access control (Azure RBAC). Azure Policy evaluates the state by examining properties on resources that are represented in Resource Manager and properties of some Resource Providers. Azure Policy ensures that the resource state is compliant with your business rules without concern for who made the change or who has permission to make a change. Azure Policy, through the DenyAction effect, can also block specific actions on resources. Some Azure Policy resources, such as policy definitions, initiative definitions, and assignments, are visible to all users. This design enables transparency to all users and services regarding what policy rules are set in their environment. Azure RBAC focuses on managing user actions at different scopes. If control of an action is required based on user information, then Azure RBAC is the correct tool to use. Even if an individual has access to perform an action, if the result is a non-compliant resource, Azure Policy still blocks the create or update task. The combination of Azure role-based access control (Azure RBAC) and Azure Policy provides full-scope control in Azure. Azure Policy has several permissions, known as operations, in two Resource Providers: Microsoft.Authorization Microsoft.PolicyInsights Many built-in roles grant permission to Azure Policy resources. The Resource Policy Contributor role includes most Azure Policy operations. The owner has full rights. Both Contributor and Reader have access to all read Azure Policy operations. A contributor may trigger resource remediation but can't create or update definitions and assignments. User Access Administrator is necessary to grant the managed identity on deployIfNotExists or modify the assignment's necessary permissions. Note All Policy objects, including definitions, initiatives, and assignments, will be readable to all roles over its scope. For example, a Policy assignment scoped to an Azure subscription will be readable by all role holders at the subscription scope and below. If none of the built-in roles have the permissions required, create a custom role. Azure Policy operations can have a significant impact on your Azure environment. Only the minimum set of permissions necessary to perform a task should be assigned and these permissions should not be granted to users who do not need them. Note The managed identity of a deployIfNotExists or modify policy assignment needs enough permissions to create or update targeted resources.","title":"Compare and contrast Azure RBAC vs Azure policies"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#example-role-based-access-control-rbac-vs-azure-policy","text":"Important RBAC and Polices in Azure play a vital role in a governance strategy. While different, they both work together to ensure organizational business rules are followed by ensuring proper access and resource creation guidelines are met.","title":"Example: Role-Based Access Control (RBAC) vs. Azure Policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#configure-built-in-roles","text":"Azure role-based access control (RBAC) has several Azure built-in roles that you can assign to users, groups, service principals, and managed identities. Role assignments are the way you control access to Azure resources. If the built-in roles don't meet the specific needs of your organization, you can create your own Azure custom roles. The four general built-in roles are: Built-in Role Description Contributor Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries. Owner Grants full access to manage all resources, including the ability to assign roles in Azure RBAC. Reader View all resources, but does not allow you to make any changes. User Access Administrator Lets you manage user access to Azure resources.","title":"Configure built-in roles"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#custom-roles-for-azure-resources","text":"If the built-in roles for Azure resources don't meet the specific needs of your organization, you can create your own custom roles. Just like built-in roles, you can assign custom roles to users, groups, and service principals at management group, subscription, and resource group scopes. Custom roles can be shared between subscriptions that trust the same Azure AD directory. There is a limit of 5,000 custom roles per directory. (For Azure Germany and Azure China 21Vianet, the limit is 2,000 custom roles.) Custom roles can be created using the Azure portal, Azure PowerShell, Azure CLI, or the REST API.","title":"Custom roles for Azure resources"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#custom-role-limits","text":"The following list describes the limits for custom roles. Each directory can have up to 5000 custom roles. Azure Germany and Azure China 21Vianet can have up to 2000 custom roles for each directory. You cannot set AssignableScopes to the root scope (\"/\"). You can only define one management group in AssignableScopes of a custom role. Adding a management group to AssignableScopes is currently in preview. Custom roles with DataActions cannot be assigned at the management group scope. Azure Resource Manager doesn't validate the management group's existence in the role definition's assignable scope.","title":"Custom role limits"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#enable-resource-locks","text":"As an administrator, you may need to lock a subscription, resource group, or resource to prevent other users in your organization from accidentally deleting or modifying critical resources. You can set the lock level to CanNotDelete or ReadOnly. In the portal, the locks are called Delete and Read-only respectively. CanNotDelete means authorized users can still read and modify a resource, but they can't delete the resource. ReadOnly means authorized users can read a resource, but they can't delete or update the resource. Applying this lock is similar to restricting all authorized users to the permissions granted by the Reader role.","title":"Enable resource locks"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#who-can-create-or-delete-locks","text":"To create or delete management locks, you must have access to **Microsoft.Authorization/***or Microsoft.Authorization/locks/* actions. Of the built-in roles, only Owner and User Access Administrator are granted those actions.","title":"Who can create or delete locks"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#deploy-azure-blueprints","text":"Just as a blueprint allows an engineer or an architect to sketch a project's design parameters, Azure Blueprints enables cloud architects and central information technology groups to define a repeatable set of Azure resources that implements and adheres to an organization's standards, patterns, and requirements. Azure Blueprints allows development teams to rapidly build and stand up new environments with the trust they're building within organizational compliance with a set of built-in components, such as networking, to speed up development and delivery. Blueprints are a declarative way to orchestrate the deployment of various resource templates and other artifacts, such as: Role Assignments Policy Assignments Azure Resource Manager templates Resource Groups The Azure Blueprints service is supported by the globally distributed Azure Cosmos Data Base. Blueprint objects are replicated in multiple Azure regions. This replication provides low latency, high availability, and consistent access to your blueprint objects, regardless of which region Blueprints deploys your resources to.","title":"Deploy Azure blueprints"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#how-is-it-different-from-azure-resource-manager-templates","text":"The service design helps with environment setup. This setup often includes resource groups, policies, role assignments, and Resource Manager template deployments assigned to a subscription in a single audited and tracked operation. A blueprint is a package to bring each artifact type together and allows you to compose and version that package into a continuous integration and pipeline. Nearly everything that you want to include for deployment in Blueprints is also with a Resource Manager template. However, a Resource Manager template is a document that doesn't exist natively in Azure \u2013 it's stored either locally or in source control. The template gets used for deployments of one or more Azure resources, but once those resources deploy, there's no active connection or relationship to the template. Blueprints save the relationship between the blueprint definition and the blueprint assignment. This connection supports improved tracking and auditing of deployments. Blueprints can upgrade several subscriptions governed by the exact blueprint. There's no need to choose between a Resource Manager template and a blueprint. Each blueprint can consist of zero or more Resource Manager template artifacts. This support means that previous efforts to develop and maintain a library of Resource Manager templates are reusable in Blueprints.","title":"How is it different from Azure Resource Manager templates?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#how-its-different-from-azure-policy","text":"A blueprint is a package or container for composing focus-specific standards, patterns, and requirements for implementing Azure cloud services, security, and design reused to maintain consistency and compliance. An Azure policy is a default allow and explicit deny system focused on resource properties during deployment and for existing resources. It supports cloud governance by validating that help within a subscription adhere to requirements and standards. Including an Azure policy in a blueprint enables the creation of the correct pattern or design during the assignment of the blueprint. The policy inclusion ensures that only approved or expected changes can be made to the environment to protect ongoing compliance with the intent of the blueprint. An Azure policy is available as one of many artifacts in a blueprint definition. Blueprints also support using parameters with policies and initiatives.","title":"How it's different from Azure Policy"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#blueprint-definition","text":"A blueprint is composed of artifacts. Azure Blueprints currently supports the following resources as artifacts: Resource Hierarchy options Description Resource Groups Subscription Create a new resource group for use by other artifacts within the blueprint. These placeholder resource groups enable you to organize resources exactly how you want them structured and provide a scope limiter for included policy and role assignment artifacts and ARM templates. ARM template Subscription, Resource Group Templates, including nested and linked templates, are used to compose complex environments. Example environments: a SharePoint farm, Azure Automation State Configuration, or a Log Analytics workspace. Policy Assignment Subscription, Resource Group Allows assignment of a policy or initiative to the subscription the blueprint is assigned to. The policy or initiative must be within the scope of the blueprint definition location. If the policy or initiative has parameters, these parameters are assigned at the creation of the blueprint or during blueprint assignment. Role Assignment Subscription, Resource Group Add an existing user or group to a built-in role to make sure the right people always have the right access to your resources. Role assignments can be defined for the entire subscription or nested to a specific resource group included in the blueprint.","title":"Blueprint definition"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#blueprint-definition-locations","text":"When creating a blueprint definition, you'll define where the blueprint is saved. Blueprints can be saved to a management group or subscription that you have Contributor access to. If the location is a management group, the blueprint is available to assign to any child subscription of that management group.","title":"Blueprint definition locations"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#blueprint-parameters","text":"Blueprints can pass parameters to either a policy/initiative or an ARM template. When adding either artifact to a blueprint, the author decides to provide a defined value for each blueprint assignment or to allow each blueprint assignment to provide a value at assignment time. This flexibility provides the option to define a pre-determined value for all uses of the blueprint or to enable that decision to be made at the time of assignment. Note Assigning a blueprint definition to a management group means the assignment object exists in the management group. The deployment of artifacts still targets a subscription. To perform a management group assignment, the Create Or Update REST API must be used, and the request body must include a value for properties.scope to define the target subscription.","title":"Blueprint parameters"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#design-an-azure-subscription-management-plan","text":"An Azure Active Directory (AD) tenant is created for you when you sign up for Azure. The tenant represents your account. You use the tenant to manage access to your subscriptions and resources. When you create a new subscription, it's hosted in your account's Azure AD tenant. If you want to give others access to your subscription or its resources, you need to invite them to join your tenant. Doing so helps you control access to your subscriptions and resources. You can create additional subscriptions for your account in Azure. You might want an additional subscription to avoid reaching subscription limits, to create separate environments for billing and security, or to isolate data for compliance reasons. If you want to create Azure subscriptions under your organization's Enterprise Agreement (EA), you need to have the Account Owner role for your organization. If you need to transfer billing ownership of your Azure subscription if you're leaving your organization, or you want your subscription to be billed to another account. Transferring billing ownership to another account provides the administrators in the new account permission for billing tasks. They can change the payment method, view charges, and cancel the subscription.","title":"Design an Azure subscription management plan"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#manage-api-access-to-azure-subscriptions-and-resources","text":"When you publish APIs through API Management, it's easy and common to gain access to those APIs by using subscription keys. Client applications that consume the published APIs need to include a valid subscription key in HTTP requests when they make calls to those APIs. To get a subscription key for accessing APIs, a subscription is required. A subscription is essentially a named container for a pair of subscription keys. Developers who need to consume the published APIs can get subscriptions, and they don't need approval from API publishers. API publishers can also directly create subscriptions for API consumers. API Management supports additional mechanisms for gaining access to APIs, including: OAuth 2.0 Client certificates IP allowlists Azure policies encapsulate common API management functions, like those for access control, protection, transformation, and caching. You can chain these policies together into a pipeline that mutates a request\u2019s context or changes the API behavior. You can apply these policies to a variety of scopes, trigger them on an error, and set them in the inbound and outbound directions.","title":"Manage API access to Azure subscriptions and resources"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#who-can-transfer-a-subscription","text":"A billing administrator or the account administrator is a person who has permission to manage billing for an account. They're authorized to access billing on the Azure portal and do various billing tasks like create subscriptions, view and pay invoices, or update payment methods. If you're an Enterprise Agreement (EA) customer, your enterprise administrators can transfer billing ownership of your subscriptions between accounts. To identify accounts for which you're a billing administrator, use the following steps: Visit the Cost Management + Billing page in Azure portal. Select All billing scopes from the left-hand pane. The subscriptions page lists all subscriptions where you're a billing administrator.","title":"Who can transfer a subscription?"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#explore-try-this-exercises_3","text":"Use your own Azure subscription to perform these Try-This exercises - Enterprise Governance.","title":"Explore Try-This exercises"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-1-navigating-azure","text":"In this task, you'll learn how to access and use the Azure portal. Locate the Azure portal In this task, you'll access the lab environment and the Azure portal. Ask your instructor how to access the lab environment. After accessing the lab environment, navigate to the Azure portal. Bookmark this page. You'll use the Portal throughout the course labs and demonstrations. In the top right corner of the Portal, select your user account. Notice you can View account and Switch directory. Switch directory lets you view My permissions and View my bill. Select the Settings icon (top right menu bar - cog icon). Review the Portal settings including the General and Language & region settings. Use the Search resources, services, and docs textbox to search for Virtual machines. You can search for not only general Azure resources but specifically but named resources. Select Use the Portal menu (left corner three bars icon). Notice you can Create a resource, view All services, and view All resources. Take some time to browse around the interface, search and explore different areas. Launch the Cloud Shell (first icon top menu bar). Notice the drop-down for PowerShell or Bash.","title":"Task 1 - Navigating Azure"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-2-azure-rbac-role-assignments","text":"In this task, we'll learn about role assignments. Locate Access Control blade Access the Azure portal, and select a resource group. Make a note of what resource group you use. Select the Access Control (IAM) blade. This blade is available for many different resources so you can control access. Review role permissions Click the Roles tab (top). Select a desired role from the list by clicking the associated box next to the Role Name. Click the View link under the Details column on the far right of the page. The Permissions view is displayed including three columns from left to right that is (i.e., Other, Read, Write, and Delete) Permissions, and Description. Return to the Access Control (IAM) blade.","title":"Task 2 - Azure RBAC Role Assignments"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#task-3-manage-resource-locks","text":"Note: This task requires a resource group. In this task, we'll create resource locks. In the Portal navigate to a resource group. In the Settings section, click Locks, and then click + Add. Discuss the different types of locks and applying the locks at different levels. Create a new lock with a Lock type of Delete. From the Overview blade, click Delete resource group. Type the name of the resource group and click OK. You should receive an error message stating the resource group is locked and can't be deleted. Add a Storage Account to the resource group. After the storage account is created, try to delete the storage account. You receive an error message stating the resource or its parent has a delete lock. Review how the storage account inherits the lock from the parent and can't be deleted. Return to the resource group blade and, in the Settings section, click Locks. Scroll all the way to the right, then click the Delete link to the right of the lock. Return to the storage account and confirm you can now delete the resource.","title":"Task 3 - Manage resource locks"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#knowledge-check_3","text":"Choose the best response for each of the questions below. Then select Check your answers.","title":"Knowledge check"},{"location":"Cloud/Azure/AZ-500/Manage-Identity-and-Access.html#check-your-knowledge_1","text":"The company hires a new administrator and needs to create a new Azure AD user account for them. The new hire must be able to: - Read/write resource deployments they're responsible for. - Read Azure AD access permissions They shouldn't be able to view Azure subscription information. What should be configured to make this work? Assign the user the Contributor role at the resource group level. ( Ans ) Assign the user the Owner role at the resource level. Assign the user the Global Administrator role. Which of the following would be good example of when to use a resource lock? An ExpressRoute circuit with connectivity back to your on-premises network. ( Ans ) A virtual machine used to test occasional application builds. A storage account used to store images processed in a development environment. A company has three virtual machines (VM1, VM2, and VM3) in a resource group. The Helpdesk hires a new employee. The new employee must be able to modify the settings on VM3, but not on VM1 and VM2. Your solution must minimize administrative overhead. What should be set up? Assign the user to the Contributor role on the resource group. Assign the user to the Contributor role on VM3. ( Ans ) Move VM3 to a new resource group and assign the user to the Contributor role on VM3. You need to target policies and review spend budgets across several subscriptions you manage. What should be created for the subscriptions? A billing group A management group ( Ans ) A nested resource group A manager asks for an explanation of how Azure uses resource groups. Which of the following capabilities is a feature of how Azure uses resource groups? Resources can be in multiple resource groups. Resources can be moved from one resource group to another resource group. ( Ans ) Resource groups can be nested.","title":"Check your knowledge"},{"location":"Cloud/Azure/AZ-500/Manage-security-operation.html","text":"AZ-500: Microsoft Certified: Azure Security Engineer Associate # Manage security operation # Once you have deployed and secured your Azure environment, learn to monitor, operate, and continuously improve the security of your solutions. This learning path helps prepare you for Exam AZ-500: Microsoft Azure Security Technologies . Configure and manage Azure Monitor Introduction Explore Azure Monitor Configure and monitor metrics and logs Enable Log Analytics Manage connected sources for log analytics Enable Azure monitor Alerts Configure properties for diagnostic logging Perform try-this exercises Knowledge check Summary Enable and manage Microsoft Defender for Cloud Introduction MITRE Attack matrix Implement Microsoft Defender for Cloud Security posture Workload protections Deploy Microsoft Defender for Cloud Azure Arc Azure Arc capabilities Microsoft cloud security benchmark Configure Microsoft Defender for Cloud policies View and edit security policies Manage and implement Microsoft Defender for Cloud recommendations Explore secure score Define brute force attacks Understand just-in-time VM access Implement just-in-time VM access Perform try-this exercises Knowledge check Summary Configure and monitor Microsoft Sentinel Introduction Enable Azure Sentinel Configure data connections to Sentinel Create workbooks for explore Sentinel data Enable rules to create incidents Configure playbooks Hunt and investigate potential breaches Knowledge check Summary Configure and manage Azure Monitor # Introduction Azure Monitor is a key resource to keep watch on how all your Azure resources are performing, and to trigger alerts if there is any sort of problem. Monitoring your systems and updating them as needed is as important as your initial secure setup. Explore Azure Monitor Earlier, this course discussed Microsoft Azure Monitor. The following high-level diagram depicts the two fundamental data types that Azure Monitor uses, Metrics and Logs. On the left side of the figure are the sources of monitoring data that populate these data stores. On the right side are the different functions that Azure Monitor performs with this collected data, such as analysis, alerting, and streaming to external systems. For many Azure resources, you\u2019ll find the data that Azure Monitor collects right in the resource\u2019s Overview page in the Azure portal. Check out any virtual machine (VM). for example, and you'll notice several charts displaying performance metrics. Select any of the graphs to open the data in Metrics Explorer, which allows you to chart the values of multiple metrics over time. You can view the charts interactively or pin them to a dashboard to view them with other visualizations. Exporting data to a SIEM Processed events that Microsoft Defender for Cloud produces are published to the Azure activity log, one of the log types available through Azure Monitor. Azure Monitor offers a consolidated pipeline for routing any of your monitoring data into a SIEM tool. This is done by streaming that data to an event hub, where it can then be pulled into a partner tool. This pipe uses the Azure Monitor single pipeline for getting access to the monitoring data from your Azure environment. This allows you to easily set up SIEMs and monitoring tools to consume the data. Currently, the exposed security data from Microsoft Defender for Cloud to a SIEM consists of security alerts. Microsoft Defender for Cloud security alerts Microsoft Defender for Cloud automatically collects, analyzes, and integrates log data from your Azure resources; the network; and connected partner solutions, like firewall and endpoint protection solutions, to detect real threats and reduce false positives. Microsoft Defender for Cloud displays a list of prioritized security alerts along with the information you need to quickly investigate the problem and recommendations for how to remediate an attack. The following sections describe how you can configure data to be streamed to an event hub. The steps assume that you already have Microsoft Defender for Cloud configured in your Azure subscription. Azure Event Hubs Azure Event Hubs is a streaming platform and event ingestion service that can transform and store data by using any real-time analytics provider or batching/storage adapters. Use Event Hubs to stream log data from Azure Monitor to a Microsoft Sentinel or a partner SIEM and monitoring tools. What data can be sent into an event hub? Within your Azure environment, there are several 'tiers' of monitoring data, and the method of accessing data from each tier varies slightly. Typically, these tiers can be described as: Application monitoring data - Data about the performance and functionality of the code you have written and are running on Azure. Examples of application monitoring data include performance traces, application logs, and user telemetry. Application monitoring data is usually collected in one of the following ways: By instrumenting your code with an SDK such as the Application Insights SDK. By running a monitoring agent that listens for new application logs on the machine running your application, such as the Windows Azure Diagnostic Agent or Linux Azure Diagnostic Agent. Guest OS monitoring data - Data about the operating system on which your application is running. Examples of guest OS monitoring data would be Linux syslog or Windows system events. To collect this type of data, you need to install an agent such as the Windows Azure Diagnostic Agent or Linux Azure Diagnostic Agent. Azure resource monitoring data - Data about the operation of an Azure resource. For some Azure resource types, such as virtual machines, there is a guest OS and application(s) to monitor inside of that Azure service. For other Azure resources, such as Network Security Groups, the resource monitoring data is the highest tier of data available (since there is no guest OS or application running in those resources). This data can be collected using resource diagnostic settings. Azure subscription monitoring data - Data about the operation and management of an Azure subscription, as well as data about the health and operation of Azure itself. The activity log contains most subscription monitoring data, such as service health incidents and Azure Resource Manager audits. You can collect this data using a Log Profile. Azure tenant monitoring data - Data about the operation of tenant-level Azure services, such as Azure Active Directory. The Azure Active Directory audits and sign-ins are examples of tenant monitoring data. This data can be collected using a tenant diagnostic setting. Data from any tier can be sent into an event hub, where it can be pulled into a tool. Some sources can be configured to send data directly to an event hub while another process such as a Logic App may be required to retrieve the required data. Connecting to Microsoft Sentinel Microsoft Sentinel is now generally available. With Microsoft Sentinel, enterprises worldwide can now keep pace with the exponential growth in security data, improve security outcomes without adding analyst resources, and reduce hardware and operational costs. Microsoft Sentinel brings together the power of Azure and AI to enable Security Operations Centers to achieve more. Some of the features of Microsoft Sentinel are: More than 100 built-in alert rules Sentinel's alert rule wizard to create your own. Alerts can be triggered by a single event or based on a threshold, or by correlating different datasets or by using built-in machine learning algorithms. Jupyter Notebooks that use a growing collection of hunting queries, exploratory queries, and python libraries. Investigation graph for visualizing and traversing the connections between entities like users, assets, applications, or URLs and related activities like logins, data transfers, or application usage to rapidly understand the scope and impact of an incident. The Microsoft Sentinel GitHub repository has grown to over 400 detection, exploratory, and hunting queries, plus Azure Notebooks samples and related Python libraries, playbooks samples, and parsers. The bulk of these were developed by Microsoft's security researchers based on their vast global security experience and threat intelligence. To on-board Microsoft Sentinel, you first need to enable Microsoft Sentinel, and then connect your data sources. Microsoft Sentinel comes with a number of connectors for Microsoft solutions, available out of the box and providing real-time integration, including Microsoft Threat Protection solutions, Microsoft 365 sources, including Microsoft 365, Azure AD, Azure ATP, and Microsoft Cloud App Security, and more. In addition, there are built-in connectors to the broader security ecosystem for non-Microsoft solutions. You can also use common event format, Syslog or REST-API to connect your data sources with Azure Sentinel. After you connect your data sources, choose from a gallery of expertly created dashboards that surface insights based on your data. These dashboards can be easily customized to your needs. Configure and monitor metrics and logs All data that Azure Monitor collects fits into one of two fundamental types: metrics or logs. Azure Monitor Metrics Azure Monitor Metrics is a feature of Azure Monitor that collects numeric data from monitored resources into a time series database. Metrics are numerical values that are collected at regular intervals and describe some aspect of a system at a particular time. Azure Monitor Metrics - Navigation Example Use Metrics Explorer to interactively analyze the data in your metric database and chart the values of multiple metrics over time. You can pin the charts to a dashboard to view them with other visualizations. You can also retrieve metrics by using the Azure monitoring REST API. Behind the scene, log-based metrics translate into log queries. Their retention matches the retention of events in underlying logs. For Application Insights resources, logs are stored for 90 days. Types of metrics There are multiple types of metrics supported by Azure Monitor Metrics: Native metrics use tools in Azure Monitor for analysis and alerting. Platform metrics are collected from Azure resources. They require no configuration and have no cost. Custom metrics are collected from different sources that you configure, including applications and agents running on virtual machines. Prometheus metrics (preview) are collected from Kubernetes clusters, including Azure Kubernetes Service (AKS), and use industry-standard tools for analyzing and alerting, such as PromQL and Grafana. Additional Background and Information What is Prometheus? Prometheus is an open-source toolkit that collects data for monitoring and alerting. Prometheus Features: A multi-dimensional data model with time series data identified by metric name and key/value pairs PromQL (PromQL component called Prom Kubernetes - an extension to support Prometheus) provides a flexible query language to use this dimensionality. Time series collection happens via a pull model over Hypertext Transfer Protocol (HTTP) Pushing time series is supported via an intermediary gateway Targets are discovered via service discovery or static configuration What is Azure Managed Grafana? Azure Managed Grafana is a data visualization platform built on top of the Grafana software by Grafana Labs. It's built as a fully managed Azure service operated and supported by Microsoft. Grafana helps you combine metrics, logs, and traces into a single user interface. With its extensive support for data sources and graphing capabilities, you can view and analyze your application and infrastructure telemetry data in real-time. Azure Managed Grafana is optimized for the Azure environment. It works seamlessly with many Azure services. Specifically, for the current preview, it provides with the following integration features: Built-in support for Azure Monitor and Azure Data Explorer User authentication and access control using Azure Active Directory identities Direct import of existing charts from the Azure portal Why use Azure Managed Grafana? Managed Grafana lets you bring together all your telemetry data into one place. It can access various data sources supported, including your data stores in Azure and elsewhere. By combining charts, logs, and alerts into one view, you can get a holistic view of your application and infrastructure and correlate information across multiple datasets. As a fully managed service, Azure Managed Grafana lets you deploy Grafana without having to deal with setup. The service provides high availability, service level agreement (SLA) guarantees, and automatic software updates. You can share Grafana dashboards with people inside and outside your organization and allow others to join in for monitoring or troubleshooting. Managed Grafana uses Azure Active Directory (Azure AD)\u2019s centralized identity management, which allows you to control which users can use a Grafana instance, and you can use managed identities to access Azure data stores, such as Azure Monitor. You can create dashboards instantaneously by importing existing charts directly from the Azure portal or by using prebuilt dashboards. The differences between each of the metrics are summarized in the following table. Category Native platform metrics Native custom metrics Prometheus metrics (preview) Sources Azure resources Azure Monitor agent Application Insights Representational State Transfer (REST) Application Programming Interface (API) Azure Kubernetes Service (AKS) cluster Any Kubernetes cluster through remote-write Configuration None Varies by source Enable Azure Monitor managed service for Prometheus Stored Subscription Subscription Azure Monitor workspace Cost No Yes Yes (free during preview) Aggregation pre-aggregated pre-aggregated raw data Analyze Metrics Explorer Metrics Explorer Prometheus Querying (PromQL) LanguageGrafana dashboards Alert metrics alert rule metrics alert rule Prometheus alert rule Visualize WorkbooksAzure dashboardGrafana WorkbooksAzure dashboardGrafana Grafana Retrieve Azure Command-Line Interface (CLI) Azure PowerShell cmdletsRepresentational State Transfer (REST) Application Programming Interface (API) or client library.NETGoJavaJavaScriptPython Azure Command-Line Interface (CLI) Azure PowerShell cmdletsRepresentational State Transfer (REST) Application Programming Interface (API) or client library.NETGoJavaJavaScriptPython Grafana Data collection Azure Monitor collects metrics from the following sources. After these metrics are collected in the Azure Monitor metric database, they can be evaluated together regardless of their source: Azure resources: Platform metrics are created by Azure resources and give you visibility into their health and performance. Each type of resource creates a distinct set of metrics without any configuration required. Platform metrics are collected from Azure resources at a one-minute frequency unless specified otherwise in the metric's definition. Applications: Application Insights creates metrics for your monitored applications to help you detect performance issues and track trends in how your application is used. Values include Server response time and Browser exceptions. Virtual machine agents: Metrics are collected from the guest operating system of a virtual machine. You can enable guest operating system (OS) metrics for Windows virtual machines using the Windows diagnostic extension and Linux virtual machines by using the InfluxData Telegraf agent. Custom metrics: You can define metrics in addition to the standard metrics that are automatically available. You can define custom metrics in your application that are monitored by Application Insights. You can also create custom metrics for an Azure service by using the custom metrics Application Programming Interface (API). Kubernetes clusters: Kubernetes clusters typically send metric data to a local Prometheus server that you must maintain. Azure Monitor managed service for Prometheus provides a managed service that collects metrics from Kubernetes clusters and stores them in Azure Monitor Metrics. A common type of log entry is an event, which is collected sporadically. Events are created by an application or service and typically include enough information to provide complete context on their own. For example, an event can indicate that a particular resource was created or modified, a new host started in response to increased traffic, or an error was detected in an application. Because the format of the data can vary, applications can create custom logs by using the structure that they need. Metric data can even be stored in Logs to combine them with other monitoring data for trending and other data analysis. The following is a list of the different ways that you can use Logs in Azure Monitor. Analyze - Use Log Analytics in the Azure portal to write log queries and interactively analyze log data using the powerful Data Explorer analysis engine. Use the Application Insights analytics console in the Azure portal to write log queries and interactively analyze log data from Application Insights. Visualize - Pin query results rendered as tables or charts to an Azure dashboard. Create a workbook to combine with multiple sets of data in an interactive report. Export the results of a query to Power BI to use different visualizations and share with users outside of Azure. Export the results of a query to Grafana to use its dashboarding and combine with other data sources. Alert - Configure a log alert rule that sends a notification or takes automated action when the results of the query match a particular result. Configure a metric alert rule on certain log data logs extracted as metrics. Retrieve - Access log query results from a command line using Azure command-line interface (CLI). Access log query results from a command line using PowerShell cmdlets. Access log query results from a custom application using Representational State Transfer (REST) Application Programming Interface (API). Export - Build a workflow to retrieve log data and copy it to an external location using Logic Apps. Log queries Data in Azure Monitor Logs is retrieved using a log query written with the Kusto query language, which allows you to quickly retrieve, consolidate, and analyze collected data. Use Log Analytics to write and test log queries in the Azure portal. It allows you to work with results interactively or pin them to a dashboard to view them with other visualizations. Security tools use of Monitor logs Microsoft Defender for Cloud stores data that it collects in a Log Analytics workspace where it can be analyzed with other log data. Azure Sentinel stores data from data sources into a Log Analytics workspace. Enable Log Analytics Log Analytics is part of Microsoft Azure's overall monitoring solution. Log Analytics helps you monitors cloud and on-premises environments to maintain availability and performance. Log Analytics is the primary tool in the Azure portal for writing log queries and interactively analyzing their results. Even if a log query is used elsewhere in Azure Monitor, you'll typically write and test the query first using Log Analytics. You can start Log Analytics from several places in the Azure portal. The scope of the data available to Log Analytics is determined by how you start it. Select Logs from the Azure Monitor menu or Log Analytics workspaces menu. Select Analytics from the Overview page of an Application Insights application. Select Logs from the menu of an Azure resource. In addition to interactively working with log queries and their results in Log Analytics, areas in Azure Monitor where you will use queries include the following: Alert rules. Alert rules proactively identify issues from data in your workspace. Each alert rule is based on a log search that is automatically run at regular intervals. The results are inspected to determine if an alert should be created. Dashboards. You can pin the results of any query into an Azure dashboard which allow you to visualize log and metric data together and optionally share with other Azure users. Views. You can create visualizations of data to be included in user dashboards with View Designer. Log queries provide the data used by tiles and visualization parts in each view. Export. When you import log data from Azure Monitor into Excel or Power BI, you create a log query to define the data to export. PowerShell. Use the results of a log query in a PowerShell script from a command line or an Azure Automation runbook that uses Invoke-AzOperationalInsightsQuery. Azure Monitor Logs API. The Azure Monitor Logs API allows any REST API client to retrieve log data from the workspace. The API request includes a query that is run against Azure Monitor to determine the data to retrieve. At the center of Log Analytics is the Log Analytics workspace, which is hosted in Azure. Log Analytics collects data in the workspace from connected sources by configuring data sources and adding solutions to your subscription. Data sources and solutions each create different record types, each with its own set of properties. But you can still analyze sources and solutions together in queries to the workspace. This capability allows you to use the same tools and methods to work with a variety of data collected by a variety of sources. Use the Log Analytics workspaces menu to create a Log Analytics workspace using the Azure portal. A Log Analytics workspace is a unique environment for Azure Monitor log data. Each workspace has its own data repository and configuration, and data sources and solutions are configured to store their data in a particular workspace. You require a Log Analytics workspace if you intend on collecting data from the following sources: Azure resources in your subscription On-premises computers monitored by System Center Operations Manager Device collections from Configuration Manager Diagnostics or log data from Azure storage Manage connected sources for log analytics The Azure Log Analytics agent was developed for comprehensive management across virtual machines in any cloud, on-premises machines, and those monitored by System Center Operations Manager. The Windows and Linux agents send collected data from different sources to your Log Analytics workspace in Azure Monitor, as well as any unique logs or metrics as defined in a monitoring solution. The Log Analytics agent also supports insights and other services in Azure Monitor such as Azure Monitor for VMs, Microsoft Defender for Cloud, and Azure Automation. Comparison to Azure diagnostics extension The Azure diagnostics extension in Azure Monitor can also be used to collect monitoring data from the guest operating system of Azure virtual machines. You may choose to use either or both depending on your requirements. The key differences to consider are: Azure Diagnostics Extension can be used only with Azure virtual machines. The Log Analytics agent can be used with virtual machines in Azure, other clouds, and on-premises. Azure Diagnostics extension sends data to Azure Storage, Azure Monitor Metrics (Windows only) and Event Hubs. The Log Analytics agent collects data to Azure Monitor Logs. The Log Analytics agent is required for solutions, Azure Monitor for VMs, and other services such as Microsoft Defender for Cloud. Data destinations The Log Analytics agent sends data to a Log Analytics workspace in Azure Monitor. The Windows agent can be multihomed to send data to multiple workspaces and System Center Operations Manager management groups. The Linux agent can send to only a single destination. Other services The agent for Linux and Windows isn't only for connecting to Azure Monitor, it also supports Azure Automation to host the Hybrid Runbook worker role and other services such as Change Tracking, Update Management, and Microsoft Defender for Cloud. Enable Azure monitor Alerts As discussed already, Azure monitor has metrics, logging, and analytics features. Another feature is Monitor Alerts. Responding to critical situations In addition to allowing you to interactively analyze monitoring data, an effective monitoring solution must be able to proactively respond to critical conditions identified in the data that it collects. This could be sending a text or mail to an administrator responsible for investigating an issue. Or you could launch an automated process that attempts to correct an error condition. Alerts Alerts in Azure Monitor proactively notify you of critical conditions and potentially attempt to take corrective action. Alert rules based on metrics provide near real time alerting based on numeric values, while rules based on logs allow for complex logic across data from multiple sources. Alert rules in Azure Monitor use action groups, which contain unique sets of recipients and actions that can be shared across multiple rules. Based on your requirements, action groups can perform such actions as using webhooks to have alerts start external actions or to integrate with your ITSM tools. The unified alert experience in Azure Monitor includes alerts that were previously managed by Log Analytics and Application Insights. In the past, Azure Monitor, Application Insights, Log Analytics, and Service Health had separate alerting capabilities. Over time, Azure improved and combined both the user interface and different methods of alerting. The consolidation is still in process. Overview of Alerts in Azure The diagram below represents the flow of alerts. Alert rules are separated from alerts and the actions taken when an alert fires. The alert rule captures the target and criteria for alerting. The alert rule can be in an enabled or a disabled state. Alerts only fire when enabled. The following are key attributes of an alert rule as shown: Target Resource: Defines the scope and signals available for alerting. A target can be any Azure resource. Example targets: a virtual machine, a storage account, a virtual machine scale set, a Log Analytics workspace, or an Application Insights resource. For certain resources (like virtual machines), you can specify multiple resources as the target of the alert rule. Signal: Emitted by the target resource. Signals can be of the following types: metric, activity log, Application Insights, and log. Criteria: A combination of signal and logic applied on a target resource. Examples: Percentage CPU > 70% Server Response Time > 4 ms Result count of a log query > 100 Alert Name: A specific name for the alert rule configured by the user. Alert Description: A description for the alert rule configured by the user. Severity: The severity of the alert after the criteria specified in the alert rule is met. Severity can range from 0 to 4. Sev 0 = Critical Sev 1 = Error Sev 2 = Warning Sev 3 = Informational Sev 4 = Verbose Action: A specific action taken when the alert is fired. What You Can Alert On You can alert on metrics and logs. These include but are not limited to: - Metric values - Log search queries - Activity log events - Health of the underlying Azure platform - Tests for website availability With the consolidation of alerting services still in process, there are some alerting capabilities that are not yet in the new alerts system. Monitor source Signal type Description Service health Activity log Not supported. View Create activity log alerts on service notifications. Application Insights Web availability tests Not supported. View Web test alerts. Available to any website that's instrumented to send data to Application Insights. Receive a notification when availability or responsiveness of a website is below expectations. Configure properties for diagnostic logging Azure Monitor diagnostic logs are logs produced by an Azure service that provide rich, frequently collected data about the operation of that service. Azure Monitor makes two types of diagnostic logs available: Tenant logs. These logs come from tenant-level services that exist outside an Azure subscription, such as Azure Active Directory (Azure AD). Resource logs. These logs come from Azure services that deploy resources within an Azure subscription, such as Network Security Groups (NSGs) or storage accounts. The content of these logs varies by Azure service and resource type. For example, NSG rule counters and Azure Key Vault audits are two types of diagnostic logs. These logs differ from the activity log. The activity log provides insight into the operations, such as creating a VM or deleting a logic app, that Azure Resource Manager performed on resources in your subscription using. The activity log is a subscription-level log. Resource-level diagnostic logs provide insight into operations that were performed within that resource itself, such as getting a secret from a key vault. These logs also differ from guest operating system (OS)\u2013level diagnostic logs. Guest OS diagnostic logs are those collected by an agent running inside a VM or other supported resource type. Resource-level diagnostic logs require no agent and capture resource-specific data from the Azure platform itself, whereas guest OS\u2013level diagnostic logs capture data from the OS and applications running on a VM. Create diagnostic settings in Azure portal You can configure diagnostic settings in the Azure portal either from the Azure Monitor menu or from the menu for the resource. Uses for diagnostic logs Here are some of the things you can do with diagnostic logs: Save them to a storage account for auditing or manual inspection. You can specify the retention time (in days) by using resource diagnostic settings. Stream them to event hubs for ingestion by a third-party service or custom analytics solution, such as Power BI. Analyze them with Azure Monitor, such that the data is immediately written to Azure Monitor with no need to first write the data to storage. Streaming of diagnostic logs can be enabled programmatically, via the portal, or using the Azure Monitor REST APIs. Either way, you create a diagnostic setting in which you specify an Event Hubs namespace and the log categories and metrics you want to send in to the namespace. An event hub is created in the namespace for each log category you enable. A diagnostic log category is a type of log that a resource may collect. Perform try-this exercises Task 1 - Activity Logs and Alerts In this task, we will configure an alert. Sign into the Portal. Search for and launch Monitor. Review the capabilities of Monitor: Monitor & Visualize Metrics, Query & Analyze Logs, and Setup Alerts & Actions. Select Activity log. Under the filters, click Timespan and review the drop-down choices. Open an event and discuss. Back in the Monitor main page, click Alerts then click + New alert rule. Under Resource click Select. Discuss how alerts can be scoped by subscription, resource type, and location. Select a resource for the alert and then click Done. Under Condition click Add. Select a signal, such as All Administrative operations, and then click Done. Under Action group, click Create. Review how action groups are used. Under Select an action type review the various ways the action group can be alerted. Select Email/SMS/Push/Voice. Review the configuration choices and finish creating your action group. Complete the Alert details and click Create alert rule. On the Alerts page, review how you can search your alerts by resource and time range. Task 2 - Log Analytics This lab requires a virtual machine in a running state. In this task, we will configure Log Analytics and run a query. Sign into the Portal. Search for and launch Log Analytics workspaces. Click Add or Create. On the Basics tab, review and complete the required information. Under the Essentials view, review the Pricing tier detail (example: Pricing tier: Pay-as-you-go) Finish creating the workspace and wait for it to deploy. Go to resource and discuss how Log Analytics is used and configured. Under Workspace Data Sources select Virtual machines. Select a virtual machine and click Connect. While you wait for the connection, under Settings click Advanced settings. Click Connected sources. Discuss the possible sources like virtual machines and storage accounts. Click Data. Review the different data sources. Show how Windows event logs can be collected. Save any changes you make. Back at the Log Analytics workspace, Under General select Logs. Review how log data is stored in tables and can be queried. Select the Event table and then click Run. Review the results. Knowledge check Data collected by Azure Monitor collects fits into which two fundamental types. What are those types of data? Events and Alerts Logs and Metrics ( Ans ) Records and Triggers When running a query of the Log Analytics workspace, which query language is used? Contextual Query Language Embedded SQL Kusto Query Language ( Ans ) To be notified when any virtual machine in the production resource group is deleted, what should be configured? Activity log alert ( Ans ) Application alert Log alert The IT managers would like to use a visualization tool for the Azure Monitor results. Each of the following is available, but there is a need to pick the one that will allow for insights and investigation of the data; which should be used? Dashboard Monitor Metrics ( Ans ) Power BI Enable and manage Microsoft Defender for Cloud Introduction Microsoft Defender for Cloud is your central location for setting and monitoring your organization's security posture. You can see which solutions adhere to security measures and which systems need to be secured. Scenario A security engineer uses Microsoft Defender for Cloud to track, maintain and improve an organization's security posture, you'll work on such tasks as: Implement security recommendations from Microsoft Defender for Cloud. Review your secure score and respond to it. Prevent attacks with Microsoft Defender for Cloud. MITRE Attack matrix The MITRE ATT&CK matrix is a publicly accessible knowledge base for understanding the various tactics and techniques used by attackers during a cyberattack. The knowledge base is organized into several categories: pre-attack, initial access, execution, persistence, privilege escalation, defense evasion, credential access, discovery, lateral movement, collection, exfiltration, and command and control. Tactics (T) represent the \"why\" of an ATT&CK technique or sub-technique. It is the adversary's tactical goal: the reason for performing an action. For example, an adversary may want to achieve credential access. Techniques (T) represent \"how'\" an adversary achieves a tactical goal by performing an action. For example, an adversary may dump credentials to achieve credential access. Common Knowledge (CK) in ATT&CK stands for common knowledge, essentially the documented modus operandi of tactics and techniques executed by adversaries. Defender for Cloud uses the MITRE Attack matrix to associate alerts with their perceived intent, helping formalize security domain knowledge. Example: Pre-Attack MITRE Attack Tactic Description Pre-Attack Pre-Attack could be either an attempt to access a certain resource regardless of a malicious intent, or a failed attempt to gain access to a target system to gather information prior to exploitation. This step is detected as an attempt, originating from outside the network, to scan the target system and identify an entry point. Example: Initial Access MITRE Attack Tactic Description Initial Access Initial Access is the stage where an attacker manages to get a foothold on the attacked resource. This stage is relevant for compute hosts and resources such as user accounts, certificates etc. Threat actors will often be able to control the resource after this stage. Implement Microsoft Defender for Cloud Microsoft Defender for Cloud is a solution for cloud security posture management (CSPM) and cloud workload protection (CWP) that finds weak spots across your cloud configuration, helps strengthen the overall security posture of your environment, and can protect workloads across multicloud and hybrid environments from evolving threats. For an interactive overview of how to Manage your cloud security posture with Microsoft Defender for Cloud, click on the image below. Defender for Cloud fills three vital needs as you manage the security of your resources and workloads in the cloud and on-premises: Defender for Cloud secure score continually assesses your security posture so you can track new security opportunities and precisely report on the progress of your security efforts. Defender for Cloud recommendations secures your workloads with step-by-step actions that protect your workloads from known security risks. Defender for Cloud alerts defends your workloads in real-time so you can react immediately and prevent security events from developing. Strengthen the security posture of your cloud resources Get a continuous assessment of the security of your cloud resources running in Azure, AWS, and Google Cloud. Use built-in policies and prioritized recommendations that are aligned to key industry and regulatory standards or build custom requirements that meet your organization's needs. Gather actionable insights by discovering your complete digital footprint and external attack surface signals and use them to automate recommendations and help ensure that resources are configured securely and meet your compliance needs. Protect cloud and hybrid workloads against threats Microsoft Defender for Cloud enables you to protect against evolving threats across multicloud and hybrid environments. You will be able to understand vulnerabilities with insights from industry-leading security research and secure your critical workloads across VMs, containers, databases, storage, app services, and more. Use many options to automate and streamline your security administration from a single place. Protect your resources and track your security progress Microsoft Defender for Cloud's features covers the two broad pillars of cloud security: Cloud Security Posture Management (CSPM) - Remediate security issues and watch your security posture improve Cloud Workload Protection (CWP) - Identify unique workload security requirements Protect all of your resources under one roof Because Defender for Cloud is an Azure-native service, many Azure services are monitored and protected without needing any deployment, but you can also add resources that are on-premises or in other public clouds. When necessary, Defender for Cloud can automatically deploy a Log Analytics agent to gather security-related data. For Azure machines, deployment is handled directly. For hybrid and multicloud environments, Microsoft Defender plans are extended to non-Azure machines with the help of Azure Arc. Cloud Security Posture Management (CSPM) features are extended to multicloud machines without the need for any agents. Defend your Azure-native resources Defender for Cloud helps you detect threats across: Azure Platform as a Service (PaaS) services - Detect threats targeting Azure services, including Azure App Service, Azure SQL, Azure Storage Account, and more data services. You can also perform anomaly detection on your Azure activity logs using the native integration with Microsoft Defender for Cloud Apps (formerly known as Microsoft Cloud App Security). Azure data services - Defender for Cloud includes capabilities that help you automatically classify your data in Azure SQL. You can also get assessments for potential vulnerabilities across Azure SQL and Storage services and recommendations for how to mitigate them. Networks - Defender for Cloud helps you limit exposure to brute force attacks. By reducing access to virtual machine ports and using just-in-time VM access, you can harden your network by preventing unnecessary access. You can set secure access policies on selected ports for only authorized users, allowed source IP address ranges or IP addresses, and for a limited amount of time. Defend your on-premises resources In addition to defending your Azure environment, you can add Defender for Cloud capabilities to your hybrid cloud environment to protect your non-Azure servers. To help you focus on what matters the most, you'll get customized threat intelligence and prioritized alerts according to your specific environment. To extend protection to on-premises machines, deploy Azure Arc and enable Defender for Cloud's enhanced security features. Defend resources running on other clouds Defender for Cloud can protect resources in other clouds (such as Amazon Web Services AWS and Google Cloud Platform GCP). For example, if you've connected an Amazon Web Services (AWS) account to an Azure subscription, you can enable any of these protections: Defender for Cloud's CSPM features extend to your AWS resources. This agentless plan assesses your AWS resources according to AWS-specific security recommendations, and these are included in your secure score. The resources will also be assessed for compliance with built-in standards specific to AWS (AWS Center for Internet Security (CIS), AWS Payment Card Industry (PCI) Data Security Standards (DSS), and AWS Foundational Security Best Practices). Defender for Cloud's asset inventory page is a multicloud enabled feature helping you manage your AWS resources alongside your Azure resources. Microsoft Defender for Kubernetes extends its container threat detection and advanced defenses to your Amazon Elastic Kubernetes Service (EKS) Linux clusters. Microsoft Defender for Servers brings threat detection and advanced defenses to your Windows and Linux Elastic Compute Cloud 2 (EC2) instances. This plan includes the integrated license for Microsoft Defender for Endpoint, security baselines, and OS level assessments, vulnerability assessment scanning, adaptive application controls (AAC), file integrity monitoring (FIM), and more. Close vulnerabilities before they get exploited Defender for Cloud includes vulnerability assessment solutions for virtual machines, container registries, and SQL servers as part of the enhanced security features. Some of the scanners are powered by Qualys. But you don't need a Qualys license or even a Qualys account - everything's handled seamlessly inside Defender for Cloud. Microsoft Defender for Servers includes automatic, native integration with Microsoft Defender for Endpoint. With this integration enabled, you'll have access to the vulnerability findings from Microsoft Defender Vulnerability Management. Review the findings from these vulnerability scanners and respond to them all from within Defender for Cloud. This broad approach brings Defender for Cloud closer to being the single pane of glass for all of your cloud security efforts. Enforce your security policy from the top down It's a security basic to know and make sure your workloads are secure, and it starts with having tailored security policies in place. Because policies in Defender for Cloud are built on top of Azure Policy controls, you're getting the full range and flexibility of a world-class policy solution. In Defender for Cloud, you can set your policies to run on management groups, across subscriptions, and even for a whole tenant. Defender for Cloud continuously discovers new resources that are being deployed across your workloads and assesses whether they're configured according to security best practices. If not, they're flagged, and you get a prioritized list of recommendations for what you need to fix. Recommendations help you reduce the attack surface across each of your resources. The list of recommendations is enabled and supported by the Microsoft cloud security benchmark. This Microsoft-authored benchmark, based on common compliance frameworks, began with Azure and now provides a set of guidelines for security and compliance best practices for multiple cloud environments. In this way, Defender for Cloud enables you not just to set security policies but to apply secure configuration standards across your resources. Extend Defender for Cloud with Defender plans and external monitoring You can extend the Defender for Cloud protection with the following: Advanced threat protection features for virtual machines, Structured Query Language SQL databases, containers, web applications, your network, and more - Protections include securing the management ports of your VMs with just-in-time access and adaptive application controls to create allowlists for what apps should and shouldn't run on your machines. The Defender plans of Microsoft Defender for Cloud offer comprehensive defenses for the compute, data, and service layers of your environment: Microsoft Defender for Servers Microsoft Defender for Storage Microsoft Defender for Structured Query Language (SQL) Microsoft Defender for Containers Microsoft Defender for App Service Microsoft Defender for Key Vault Microsoft Defender for Resource Manager Microsoft Defender for Domain Name System (DNS) Microsoft Defender for open-source relational databases Microsoft Defender for Azure Cosmos Database (DB) Defender Cloud Security Posture Management (CSPM) Security governance and regulatory compliance Cloud security explorer Attack path analysis Agentless scanning for machines Defender for DevOps Use the advanced protection tiles in the workload protections Azure dashboard to monitor and configure each of these protections. Microsoft Defender for the Internet of Things (IoT) is a separate product. Security alerts - When Defender for Cloud detects a threat in any area of your environment, it generates a security alert. These alerts describe details of the affected resources, suggested remediation steps, and in some cases, an option to trigger a logic app in response. Whether an alert is generated by Defender for Cloud or received by Defender for Cloud from an integrated security product, you can export it. To export your alerts to Microsoft Sentinel, any third-party Security information and event management (SIEM), or any other external tool, follow the instructions in Stream alerts to a SIEM, Security orchestration, automation and response (SOAR), or IT Service Management solution. Defender for Cloud's threat protection includes fusion kill-chain analysis, which automatically correlates alerts in your environment based on cyber kill-chain analysis, to help you better understand the full story of an attack campaign, where it started, and what kind of impact it had on your resources. Defender for Cloud's supported kill chain intents are based on version 9 of the MITRE ATT&CK matrix. Security posture Cloud Security Posture Management (CSPM) - Remediate security issues and watch your security posture improve In Defender for Cloud, the posture management features provide the following: Hardening guidance - to help you efficiently and effectively improve your security Visibility - to help you understand your current security situation Defender for Cloud continually assesses your resources, subscriptions, and organization for security issues and shows your security posture in the secure score, an aggregated score of the security findings that tells you, at a glance, your current security situation: the higher the score, the lower the identified risk level. As soon as you open Defender for Cloud for the first time, Defender for Cloud: Generates a secure score for your subscriptions based on an assessment of your connected resources compared with the guidance in the Microsoft cloud security benchmark. Use the score to understand your security posture and the compliance dashboard to review your compliance with the built-in benchmark. When you've enabled the enhanced security features, you can customize the standards used to assess your compliance and add other regulations, such as the National Institute of Standards and Technology (NIST) and Azure Center for Internet Security (CIS) or organization-specific security requirements. You can also apply recommendations and score based on the AWS Foundational Security Best practices standards. Provides hardening recommendations based on any identified security misconfigurations and weaknesses. Use these security recommendations to strengthen the security posture of your organization's Azure, hybrid, and multicloud resources. Analyzes and secure's your attack paths through the cloud security graph, which is a graph-based context engine that exists within Defender for Cloud. The cloud security graph collects data from your multicloud environment and other data sources. For example, the cloud assets inventory, connections and lateral movement possibilities between resources, exposure to the internet, permissions, network connections, vulnerabilities, and more. The data collected is then used to build a graph representing your multicloud environment. Attack path analysis is a graph-based algorithm that scans the cloud security graph. The scans expose exploitable paths attackers may use to breach your environment to reach your high-impact assets. Attack path analysis exposes those attack paths and suggests recommendations as to how best to remediate the issues that will break the attack path and prevent a successful breach. By taking your environment's contextual information into account, such as internet exposure, permissions, lateral movement, and more. Attack path analysis identifies issues that may lead to a breach in your environment and helps you to remediate the highest risk ones first. Additional background and details National Institute of Standards and Technology (NIST) The National Institute of Standards and Technology (NIST) promotes and maintains measurement standards and guidance to help organizations assess risk. In response to Executive Order 13636 on strengthening the cybersecurity of federal networks and critical infrastructure, NIST released the Framework for Improving Critical Infrastructure Cybersecurity (FICIC) in February 2014. The main priorities of the FICIC were to establish a set of standards and practices to help organizations manage cybersecurity risk, while enabling business efficiency. The NIST Framework addresses cybersecurity risk without imposing additional regulatory requirements for both government and private sector organizations. The FICIC references globally recognized standards including NIST Special Publication 800-53 found in Appendix A of the NIST's Framework for Improving Critical Infrastructure Cybersecurity. Each control within the FICIC framework is mapped to corresponding NIST 800-53 controls within the FedRAMP Moderate Baseline. Center for Internet Security (CIS) The Center for Internet Security is a nonprofit entity whose mission is to identify, develop, validate, promote, and sustain best practice solutions for cyberdefense. It draws on the expertise of cybersecurity and IT professionals from government, business, and academia from around the world. To develop standards and best practices, including CIS benchmarks, controls, and hardened images, they follow a consensus decision-making model. CIS benchmarks are configuration baselines and best practices for securely configuring a system. Each of the guidance recommendations references one or more CIS controls that were developed to help organizations improve their cyberdefense capabilities. CIS controls map to many established standards and regulatory frameworks, including the NIST Cybersecurity Framework (CSF) and NIST SP 800-53, the International Organization for Standardization (ISO) 27000 series of standards, Payment Card Industry (PCI) Data Security Standards (DSS), Health Insurance Portability and Accountability Act of 1996 (HIPAA), and others. Each benchmark undergoes two phases of consensus review. The first occurs during initial development when experts convene to discuss, create, and test working drafts until they reach consensus on the benchmark. During the second phase, after the benchmark has been published, the consensus team reviews the feedback from the internet community for incorporation into the benchmark. The Center for Internet Security (CIS) benchmarks provide two levels of security settings: Level 1 recommends essential basic security requirements that can be configured on any system and should cause little or no interruption of service or reduced functionality. Level 2 recommends security settings for environments requiring greater security that could result in some reduced functionality. CIS Hardened Images are securely configured virtual machine images based on CIS Benchmarks hardened to either a Level 1 or Level 2 CIS benchmark profile. Hardening is a process that helps protect against unauthorized access, denial of service, and other cyberthreats by limiting potential weaknesses that make systems vulnerable to cyberattacks. Workload protections Defender for Cloud offers security alerts that are powered by Microsoft Threat Intelligence. It also includes a range of advanced, intelligent protections for your workloads. The workload protections are provided through Microsoft Defender plans specific to the types of resources in your subscriptions. For example, you can enable Microsoft Defender for Storage to get alerted about suspicious activities related to your storage resources. The Cloud workload dashboard includes the following sections: Microsoft Defender for Cloud coverage - Here you can see the resources types in your subscription that are eligible for protection by Defender for Cloud. Wherever relevant, you can upgrade here as well. If you want to upgrade all possible eligible resources, select Upgrade all. Security alerts - When Defender for Cloud detects a threat in any area of your environment, it generates an alert. These alerts describe details of the affected resources, suggested remediation steps, and in some cases an option to trigger a logic app in response. Selecting anywhere in this graph opens the Security alerts page. Advanced protection - Defender for Cloud includes many advanced threat protection capabilities for virtual machines, Structured Query Language (SQL) databases, containers, web applications, your network, and more. In this advanced protection section, you can see the status of the resources in your selected subscriptions for each of these protections. Select any of them to go directly to the configuration area for that protection type. Insights - This rolling pane of news, suggested reading, and high priority alerts gives Defender for Cloud's insights into pressing security matters that are relevant to you and your subscription. Whether it's a list of high severity Common Vulnerabilities and Exposures (CVEs) discovered on your VMs by a vulnerability analysis tool, or a new blog post by a member of the Defender for Cloud team, you'll find it here in the Insights panel. Deploy Microsoft Defender for Cloud Basic and enhanced security features Basic features When you open Defender for Cloud in the Azure portal for the first time or if you enable it through the Application Programming Interface (API), Defender for Cloud is enabled for free on all your Azure subscriptions. Defender for Cloud provides foundational cloud security and posture management (CSPM) features by default. The foundational CSPM includes a secure score, security policy and basic recommendations, and network security assessment to help you protect your Azure resources. Defender Cloud Security Posture Management (CSPM) plan options Defender for cloud offers foundational multicloud CSPM capabilities for free. These capabilities are automatically enabled by default on any subscription or account that has been onboarded to Defender for Cloud. The foundational CSPM includes asset discovery, continuous assessment and security recommendations for posture hardening, compliance with Microsoft Cloud Security Benchmark (MCSB), and a Secure score which measure the current status of your organization\u2019s posture. The optional Defender CSPM plan provides advanced posture management capabilities such as Attack path analysis, Cloud security explorer, advanced threat hunting, security governance capabilities, and also tools to assess your security compliance with a wide range of benchmarks, regulatory standards, and any custom security policies required in your organization, industry, or region. The following table summarizes each plan and its cloud availability. Enhanced features If you want to try out the enhanced security features, enable enhanced security features for free for the first 30 days. At the end of 30 days, if you decide to continue using the service, we'll automatically start charging for usage. Prerequisites To get started with Defender for Cloud, you'll need a Microsoft Azure subscription with Defender for Cloud enabled. If you don't have an Azure subscription, you can sign up for a free subscription. You can enable Microsoft Defender for Storage accounts at either the subscription level or resource level. You can enable Microsoft Defender for SQL (Structured Query Language) at either the subscription level or resource level. You can enable Microsoft Defender for open-source relational databases at the resource level only. The Microsoft Defender plans available at the workspace level are Microsoft Defender for Servers and Microsoft Defender for SQL servers on machines. When you enabled Defender plans on an entire Azure subscription, the protections are inherited by all resources in the subscription. Microsoft Defender for Cloud uses monitoring components to collect data from your resources. These extensions are automatically deployed when you turn on a Defender plan. Each Defender plan has its own requirements for monitoring components, so it's important that the required extensions are deployed to your resources to get all of the benefits of each plan. The Defender plans show you the monitoring coverage for each Defender plan. If the monitoring coverage is Full, all of the necessary extensions are installed. If the monitoring coverage is Partial, the information tooltip tells you what extensions are missing. For some plans, you can configure specific monitoring settings. Enhanced features When you enable the enhanced security features (paid), Defender for Cloud can provide unified security management and threat protection across your hybrid cloud workloads, including: Microsoft Defender for Endpoint - Microsoft Defender for Servers includes Microsoft Defender for Endpoint for comprehensive endpoint detection and response (EDR). Vulnerability assessment for virtual machines, container registries, and SQL resources - Easily enable vulnerability assessment solutions to discover, manage, and resolve vulnerabilities. View, investigate, and remediate the findings directly from within Defender for Cloud. Multicloud security - Connect your accounts from Amazon Web Services (AWS) and Google Cloud Platform (GCP) to protect resources and workloads on those platforms with a range of Microsoft Defender for Cloud security features. Hybrid security \u2013 Get a unified view of security across all of your on-premises and cloud workloads. Apply security policies and continuously assess the security of your hybrid cloud workloads to ensure compliance with security standards. Collect, search, and analyze security data from multiple sources, including firewalls and other partner solutions. Threat protection alerts - Advanced behavioral analytics and the Microsoft Intelligent Security Graph provide an edge over evolving cyber-attacks. Built-in behavioral analytics and machine learning can identify attacks and zero-day exploits. Monitor networks, machines, data stores (SQL servers hosted inside and outside Azure, Azure SQL databases, Azure SQL Managed Instance, and Azure Storage), and cloud services for incoming attacks and post-breach activity. Streamline investigation with interactive tools and contextual threat intelligence. Track compliance with a range of standards - Defender for Cloud continuously assesses your hybrid cloud environment to analyze the risk factors according to the controls and best practices in the Microsoft cloud security benchmark. When you enable enhanced security features, you can apply a range of other industry standards, regulatory standards, and benchmarks according to your organization's needs. Add standards and track your compliance with them from the regulatory compliance dashboard. Access and application controls - Block malware and other unwanted applications by applying machine learning-powered recommendations adapted to your specific workloads to create allowlists and blocklists. Reduce the network attack surface with just-in-time, controlled access to management ports on Azure VMs. Access and application control drastically reduce exposure to brute force and other network attacks. Container security features - Benefit from vulnerability management and real-time threat protection in your containerized environments. Charges are based on the number of unique container images pushed to your connected registry. After an image has been scanned once, you won't be charged for it again unless it's modified and pushed once more. Breadth threat protection for resources connected to Azure - Cloud-native threat protection for the Azure services common to all of your resources: Azure Resource Manager, Azure Domain Name System (DNS), Azure network layer, and Azure Key Vault. Defender for Cloud has unique visibility into the Azure management layer and the Azure DNS layer and can therefore protect cloud resources that are connected to those layers. Manage your Cloud Security Posture Management (CSPM) - CSPM offers you the ability to remediate security issues and review your security posture through the tools provided. These tools include: - Security governance and regulatory compliance - What is Security governance and regulatory compliance? Security governance and regulatory compliance refer to the policies and processes which organizations have in place to ensure that they comply with laws, rules, and regulations put in place by external bodies (government) that control activity in a given jurisdiction. Defender for Cloud allows you to view your regulatory compliance through the regulatory compliance dashboard. Defender for Cloud continuously assesses your hybrid cloud environment to analyze the risk factors according to the controls and best practices in the standards you've applied to your subscriptions. The dashboard reflects the status of your compliance with these standards. - Cloud security graph - What is a cloud security graph? The cloud security graph is a graph-based context engine that exists within Defender for Cloud. The cloud security graph collects data from your multicloud environment and other data sources. For example, the cloud assets inventory, connections and lateral movement possibilities between resources, exposure to the internet, permissions, network connections, vulnerabilities, and more. The data collected is then used to build a graph representing your multicloud environment. Defender for Cloud then uses the generated graph to perform an attack path analysis and find the issues with the highest risk that exist within your environment. You can also query the graph using the cloud security explorer. - Attack path analysis - What is Attack path analysis? Attack path analysis helps you to address the security issues that pose immediate threats with the greatest potential of being exploited in your environment. Defender for Cloud analyzes which security issues are part of potential attack paths that attackers could use to breach your environment. It also highlights the security recommendations that need to be resolved in order to mitigate the issue. - Agentless scanning for machines - What is agentless scanning for machines? Microsoft Defender for Cloud maximizes coverage on OS posture issues and extends beyond the reach of agent-based assessments. With agentless scanning for VMs, you can get frictionless, wide, and instant visibility on actionable posture issues without installed agents, network connectivity requirements, or machine performance impact. Agentless scanning for VMs provides vulnerability assessment and software inventory powered by Defender vulnerability management in Azure and Amazon AWS environments. Agentless scanning is available in Defender Cloud Security Posture Management (CSPM) and Defender for Servers. Azure Arc Today, companies struggle to control and govern increasingly complex environments that extend across data centers, multiple clouds, and edge. Each environment and cloud possesses its own set of management tools, and new DevOps and IT operations (ITOps) operational models can be hard to implement across resources. Azure Arc simplifies governance and management by delivering a consistent multicloud and on-premises management platform. Azure Arc provides a centralized, unified way to: Manage your entire environment together by projecting your existing non-Azure and/or on-premises resources into Azure Resource Manager. Manage virtual machines, Kubernetes clusters, and databases as if they are running in Azure. Use familiar Azure services and management capabilities, regardless of where they live. Continue using traditional IT operations (ITOps) while introducing DevOps practices to support new cloud-native patterns in your environment. Configure custom locations as an abstraction layer on top of Azure Arc-enabled Kubernetes clusters and cluster extensions. Azure Arc capabilities Currently, Azure Arc allows you to manage the following resource types hosted outside of Azure: Servers: Manage Windows and Linux physical servers and virtual machines hosted outside of Azure. Kubernetes clusters: Attach and configure Kubernetes clusters running anywhere with multiple supported distributions. Azure data services: Run Azure data services on-premises, at the edge, and in public clouds using Kubernetes and the infrastructure of your choice. SQL Managed Instance and PostgreSQL server (preview) services are currently available. SQL Server: Extend Azure services to SQL Server instances hosted outside of Azure. Virtual machines (preview): Provision, resize, delete, and manage virtual machines based on VMware vSphere or Azure Stack hyper-converged infrastructure (HCI) and enable VM self-service through role-based access. Key features and benefits Some of the key scenarios that Azure Arc supports are: Implement consistent inventory, management, governance, and security for servers across your environment. Configure Azure VM extensions to use Azure management services to monitor, secure, and update your servers. Manage and govern Kubernetes clusters at scale. Use GitOps to deploy configuration across one or more clusters from Git repositories. Zero-touch compliance and configuration for Kubernetes clusters using Azure Policy. Run Azure data services on any Kubernetes environment as if it runs in Azure (specifically Azure SQL Managed Instance and Azure Database for PostgreSQL server, with benefits such as upgrades, updates, security, and monitoring). Use elastic scale and apply updates without any application downtime, even without continuous connection to Azure. Create custom locations on top of your Azure Arc-enabled Kubernetes clusters, using them as target locations for deploying Azure services instances. Deploy your Azure service cluster extensions for Azure Arc-enabled data services, App services on Azure Arc (including web, function, and logic apps), and Event Grid on Kubernetes. Perform virtual machine lifecycle and management operations for VMware vSphere and Azure Stack hyper-converged infrastructure (HCI) environments. A unified experience viewing your Azure Arc-enabled resources, whether you are using the Azure portal, the Azure CLI, Azure PowerShell, or Azure REST API. Microsoft cloud security benchmark The Microsoft cloud security benchmark (MCSB) provides prescriptive best practices and recommendations to help improve the security of workloads, data, and services on Azure and your multi-cloud environment, focusing on cloud-centric control areas with input from a set of holistic Microsoft and industry security guidance that includes: Cloud Adoption Framework: Guidance on security, including strategy, roles and responsibilities, Azure Top 10 Security Best Practices, and reference implementation. Azure Well-Architected Framework: Guidance on securing your workloads on Azure. The Chief Information Security Officer (CISO) Workshop: Program guidance and reference strategies to accelerate security modernization using Zero Trust principles. Other industry and cloud service provider's security best practice standards and framework: Examples include the Amazon Web Services (AWS) Well-Architected Framework, Center for Internet Security (CIS) Controls, National Institute of Standards and Technology (NIST), and Payment Card Industry Data Security Standard (PCI-DSS). Microsoft cloud security benchmark features Comprehensive multi-cloud security framework: Organizations often have to build an internal security standard to reconcile security controls across multiple cloud platforms to meet security and compliance requirements on each of them. This often requires security teams to repeat the same implementation, monitoring, and assessment across the different cloud environments (often for different compliance standards). This creates unnecessary overhead, cost, and effort. To address this concern, we enhanced the Azure Security Benchmark (ASB) to the Microsoft cloud security benchmark (MCSB) to help you quickly work with different clouds by: Providing a single control framework to easily meet the security controls across clouds Providing consistent user experience for monitoring and enforcing the multi-cloud security benchmark in Defender for Cloud Staying aligned with Industry Standards (e.g., Center for Internet Security, National Institute of Standards and Technology, Payment Card Industry) Automated control monitoring for AWS in Microsoft Defender for Cloud: You can use Microsoft Defender for Cloud Regulatory Compliance Dashboard to monitor your AWS environment against Microsoft cloud security benchmark (MCSB), just like how you monitor your Azure environment. We developed approximately 180 AWS checks for the new AWS security guidance in MCSB, allowing you to monitor your AWS environment and resources in Microsoft Defender for Cloud. Example: Microsoft Defender for Cloud - Regulatory compliance dashboard Azure guidance and security principles: Azure security guidance, security principles, features, and capabilities. Controls Control Domains Description Network security (NS) Network Security covers controls to secure and protect networks, including securing virtual networks, establishing private connections, preventing and mitigating external attacks, and securing Domain Name System (DNS). Identity Management (IM) Identity Management covers controls to establish a secure identity and access controls using identity and access management systems, including the use of single sign-on, strong authentications, managed identities (and service principals) for applications, conditional access, and account anomalies monitoring. Privileged Access (PA) Privileged Access covers controls to protect privileged access to your tenant and resources, including a range of controls to protect your administrative model, administrative accounts, and privileged access workstations against deliberate and inadvertent risk. Data Protection (DP) Data Protection covers control of data protection at rest, in transit, and via authorized access mechanisms, including discover, classify, protect, and monitoring sensitive data assets using access control, encryption, key management, and certificate management. Asset Management (AM) Asset Management covers controls to ensure security visibility and governance over your resources, including recommendations on permissions for security personnel, security access to asset inventory and managing approvals for services and resources (inventory, track, and correct). Logging and Threat Detection (LT) Logging and Threat Detection covers controls for detecting threats on the cloud and enabling, collecting, and storing audit logs for cloud services, including enabling detection, investigation, and remediation processes with controls to generate high-quality alerts with native threat detection in cloud services; it also includes collecting logs with a cloud monitoring service, centralizing security analysis with a security event management (SEM), time synchronization, and log retention. Incident Response (IR) Incident Response covers controls in the incident response life cycle - preparation, detection and analysis, containment, and post-incident activities, including using Azure services (such as Microsoft Defender for Cloud and Sentinel) and/or other cloud services to automate the incident response process. Posture and Vulnerability Management (PV) Posture and Vulnerability Management focuses on controls for assessing and improving the cloud security posture, including vulnerability scanning, penetration testing, and remediation, as well as security configuration tracking, reporting, and correction in cloud resources. Endpoint Security (ES) Endpoint Security covers controls in endpoint detection and response, including the use of endpoint detection and response (EDR) and anti-malware service for endpoints in cloud environments. Backup and Recovery (BR) Backup and Recovery covers controls to ensure that data and configuration backups at the different service tiers are performed, validated, and protected. DevOps Security (DS) DevOps Security covers the controls related to the security engineering and operations in the DevOps processes, including deployment of critical security checks (such as static application security testing and vulnerability management) prior to the deployment phase to ensure the security throughout the DevOps process; it also includes common topics such as threat modeling and software supply security. Governance and Strategy (GS) Governance and Strategy provides guidance for ensuring a coherent security strategy and documented governance approach to guide and sustain security assurance, including establishing roles and responsibilities for the different cloud security functions, unified technical strategy, and supporting policies and standards. Configure Microsoft Defender for Cloud policies What are security policies, initiatives Microsoft Defender for Cloud applies security initiatives to your subscriptions. These initiatives contain one or more security policies. Each of those policies results in a security recommendation for improving your security posture. What is a security initiative? A security initiative is a collection of Azure Policy definitions or rules that are grouped together towards a specific goal or purpose. Security initiatives simplify the management of your policies by grouping a set of policies together, logically, as a single item. A security initiative defines the desired configuration of your workloads and helps ensure you comply with the security requirements of your company or regulators. Like security policies, Defender for Cloud initiatives are also created in Azure Policy. You can use Azure Policy to manage your policies, build initiatives, and assign initiatives to multiple subscriptions or entire management groups. The default initiative automatically assigned to every subscription in Microsoft Defender for Cloud is the Microsoft cloud security benchmark. This benchmark is the Microsoft-authored set of guidelines for security and compliance best practices based on common compliance frameworks. This widely respected benchmark builds on the controls from the Center for Internet Security (CIS) and the National Institute of Standards and Technology (NIST) with a focus on cloud-centric security. Defender for Cloud offers the following options for working with security initiatives and policies: View and edit the built-in default initiative - When you enable Defender for Cloud, the initiative named 'Microsoft cloud security benchmark' is automatically assigned to all Defender for Cloud registered subscriptions. To customize this initiative, you can enable or disable individual policies within it by editing a policy's parameters. Add your own custom initiatives - If you want to customize the security initiatives applied to your subscription, you can do so within Defender for Cloud. You'll then receive recommendations if your machines don't follow the policies you create. Add regulatory compliance standards as initiatives - Defender for Cloud's regulatory compliance dashboard shows the status of all the assessments within your environment in the context of a particular standard or regulation (such as Azure Center for Internet Security (CIS), National Institute of Standards and Technology (NIST) Special Publications (SP) SP 800-53 Rev.4, Swift\u2019s Customer Security Program (CSP) Call Session Control Function (CSCF) v2020). Example: Builtin security initiative What is a security policy? An Azure Policy definition, created in Azure Policy, is a rule about specific security conditions you want to be controlled. Built-in definitions include things like controlling what type of resources can be deployed or enforcing the use of tags on all resources. You can also create your own custom policy definitions. To implement these policy definitions (whether built-in or custom), you'll need to assign them. You can assign any of these policies through the Azure portal, PowerShell, or Azure CLI. Policies can be disabled or enabled from Azure Policy. There are different types of policies in Azure Policy. Defender for Cloud mainly uses 'Audit' policies that check specific conditions and configurations and then report on compliance. There are also \"Enforce' policies that can be used to apply security settings. Example: Built-in security policy View and edit security policies Defender for Cloud uses Azure role-based access control (Azure RBAC), which provides built-in roles you can assign to Azure users, groups, and services. When users open Defender for Cloud, they see only information related to the resources they can access. Users are assigned the owner, contributor, or reader role to the resource's subscription. There are two specific roles for Defender for Cloud: Security Administrator: Has the same view rights as security reader. Can also update the security policy and dismiss alerts. Security reader: Has rights to view Defender for Cloud items such as recommendations, alerts, policy, and health. Can't make changes. You can edit security policies through the Azure Policy portal via Representational State Transfer Application Programming Interface (REST API) or using Windows PowerShell. The Security Policy screen reflects the action taken by the policies assigned to the subscription or management group you selected. Use the links at the top to open a policy assignment that applies to the subscription or management group. These links let you access the assignment and edit or disable the policy. For example, if you see that a particular policy assignment is effectively denying endpoint protection, use the link to edit or disable the policy. In the list of policies, you can see the effective application of the policy on your subscription or management group. The settings of each policy that apply to the scope are taken into consideration, and the cumulative outcome of actions taken by the policy is shown. For example, if one assignment of the policy is disabled, but in another, it's set to AuditIfNotExist, then the cumulative effect applies AuditIfNotExist. The more active effect always takes precedence. The policies' effect can be: Append, Audit, AuditIfNotExists, Deny, DeployIfNotExists, or Disabled. Manage and implement Microsoft Defender for Cloud recommendations What is a security recommendation? Using the policies, Defender for Cloud periodically analyzes the compliance status of your resources to identify potential security misconfigurations and weaknesses. It then provides you with recommendations on how to remediate those issues. Recommendations result from assessing your resources against the relevant policies and identifying resources that aren't meeting your defined requirements. Defender for Cloud makes its security recommendations based on your chosen initiatives. When a policy from your initiative is compared against your resources and finds one or more that isn't compliant, it's presented as a recommendation in Defender for Cloud. Example: Microsoft Defender for Cloud - All recommendations Recommendations are actions for you to take to secure and harden your resources. Each recommendation provides you with the following information: A short description of the issue The remediation steps to carry out in order to implement the recommendation The affected resources In practice, it works like this: Microsoft Cloud security benchmark is an initiative that contains requirements. For example, Azure Storage accounts must restrict network access to reduce their attack surface. The initiative includes multiple policies, each requiring a specific resource type. These policies enforce the requirements in the initiative. To continue the example, the storage requirement is enforced with the policy \"Storage accounts should restrict network access using virtual network rules.\" Microsoft Defender for Cloud continually assesses your connected subscriptions. If it finds a resource that doesn't satisfy a policy, it displays a recommendation to fix that situation and harden the security of resources that aren't meeting your security requirements. For example, if an Azure Storage account on your protected subscriptions isn't protected with virtual network rules, you see the recommendation to harden those resources. So, (1) an initiative includes (2) policies that generate (3) environment-specific recommendations. Security recommendation details Security recommendations contain details that help you understand its significance and how to handle it. The recommendation details shown are: For supported recommendations, the top toolbar shows any or all of the following buttons: Enforce and Deny View the policy definition to go directly to the Azure Policy entry for the underlying policy. Open query - You can view the detailed information about the affected resources using Azure Resource Graph Explorer. Severity indicator Freshness interval Count of exempted resources if exemptions exist for a recommendation; this shows the number of resources that have been exempted with a link to view the specific resources. Mapping to MITRE ATT&CK tactics and techniques if a recommendation has defined tactics and techniques, select the icon for links to the relevant pages on MITRE's site. This applies only to Azure-scored recommendations. 6. Description - A short description of the security issue. 7. When relevant, the details page also includes a table of related recommendations: The relationship types are: - Prerequisite - A recommendation that must be completed before the selected recommendation - Alternative - A different recommendation that provides another way of achieving the goals of the selected recommendation - Dependent - A recommendation for which the selected recommendation is a prerequisite For each related recommendation, the number of unhealthy resources is shown in the \"Affected resources\" column. 8. Remediation steps - A description of the manual steps required to remediate the security issue on the affected resources. For recommendations with the Fix option, you can select View remediation logic before applying the suggested fix to your resources. 9. Affected resources - Your resources are grouped into tabs: - Healthy resources \u2013 Relevant resources that either aren't impacted or on which you have already remediated the issue. - Unhealthy resources \u2013 Resources that are still impacted by the identified issue. - Not applicable resources \u2013 Resources for which the recommendation can't give a definitive answer. The not applicable tab also includes reasons for each resource. 10. Action buttons to remediate the recommendation or trigger a logic app. Viewing the relationship between a recommendation and a policy As mentioned above, Defender for Cloud's built-in recommendations are based on the Microsoft cloud security benchmark. Almost every recommendation has an underlying policy that is derived from a requirement in the benchmark. When you're reviewing the details of a recommendation, it's often helpful to be able to see the underlying policy. For every recommendation supported by a policy, use the View policy definition link from the recommendation details page to go directly to the Azure Policy entry for the relevant policy: Use this link to view the policy definition and review the evaluation logic. If you're reviewing the list of recommendations on our Security recommendations reference guide, you'll also see links to the policy definition pages: Explore secure score Overview of secure score Microsoft Defender for Cloud has two main goals: to help you understand your current security situation to help you efficiently and effectively improve your security The central feature in Defender for Cloud that enables you to achieve those goals is the secure score. Defender for Cloud continually assesses your cross-cloud resources for security issues. It then aggregates all the findings into a single score so that you can tell, at a glance, your current security situation: the higher the score, the lower the identified risk level. In the Azure portal pages, the secure score is shown as a percentage value, and the underlying values are also clearly presented: In the Azure mobile app, the secure score is shown as a percentage value, and you can tap the secure score to see the details that explain the score: To increase your security, review Defender for Cloud's recommendations page and remediate the recommendation by implementing the remediation instructions for each issue. Recommendations are grouped into security controls. Each control is a logical group of related security recommendations and reflects your vulnerable attack surfaces. Your score only improves when you remediate all of the recommendations for a single resource within a control. To see how well your organization is securing each individual attack surface, review the scores for each security control. How your secure score is calculated To get all the possible points for security control, all of your resources must comply with all of the security recommendations within the security control. For example, Defender for Cloud has multiple recommendations regarding how to secure your management ports. You'll need to remediate them all to make a difference to your secure score. Example scores for a control In this example: Remediate vulnerabilities security control - This control group has multiple recommendations related to discovering and resolving known vulnerabilities. Max score - The maximum number of points you can gain by completing all recommendations within a control. The maximum score for a control indicates the relative significance of that control and is fixed for every environment. Use the max score values to triage the issues to work on first. Current score - The current score for this control. Current score = [Score per resource] * [Number of healthy resources] Each control contributes towards the total score. In this example, the control is contributing 2.00 points to the current total secure score. Potential score increase - The remaining points available to you are within your control. If you remediate all the recommendations in this control, your score will increase by 9%. Potential score increase = [Score per resource] * [Number of unhealthy resources] Insights - Gives you extra details for each recommendation, such as: Preview recommendation - This recommendation won't affect your secure score until it's GA. Fix - From within the recommendation details page, you can use 'Fix' to resolve this issue. Enforce - From within the recommendation details page, you can automatically deploy a policy to fix this issue whenever someone creates a non-compliant resource. Deny - From within the recommendation details page, you can prevent new resources from being created with this issue. Which recommendations are included in the secure score calculations? Only built-in recommendations have an impact on the secure score. Recommendations flagged as Preview aren't included in the calculations of your secure score. They should still be remediated wherever possible so that when the preview period ends, they'll contribute towards your score. Preview recommendations are marked with: Improve your secure score To improve your secure score, remediate security recommendations from your recommendations list. You can remediate each recommendation manually for each resource or use the Fix option (when available) to resolve an issue on multiple resources quickly. You can also configure the Enforce and Deny options on the relevant recommendations to improve your score and make sure your users don't create resources that negatively impact your score. Frequently asked questions (FAQ) Secure score If I address only three out of four recommendations in security control, will my secure score change? No. It won't change until you remediate all of the recommendations for a single resource. To get the maximum score for a control, you must remediate all recommendations for all resources. If a recommendation isn't applicable to me, and I disable it in the policy, will my security control be fulfilled and my secure score updated? Yes. We recommend disabling recommendations when they're inapplicable in your environment. If a security control offers me zero points toward my secure score, should I ignore it? In some cases, you'll see a control max score greater than zero, but the impact is zero. When the incremental score for fixing resources is negligible, it's rounded to zero. Don't ignore these recommendations because they still bring security improvements. The only exception is the \"Additional Best Practice\" control. Remediating these recommendations won't increase your score, but it will enhance your overall security. Define brute force attacks Brute force attacks A brute force attack is a type of hacking technique in which an attacker tries to gain access to a network or system by guessing the username and password combination through an automated process. The attacker typically uses a program that generates a large number of login attempts in a short period of time to try every possible combination of characters until the correct one is discovered. This type of attack can be very effective against weak passwords and security systems with no protection against brute force attacks, but it is time-consuming and can be detected by security measures such as account lockouts after a certain number of failed login attempts. Management services, ports, and protocols Adversaries without prior knowledge of legitimate credentials within the system or environment may guess passwords to attempt access to accounts. Without knowledge of the password for an account, an adversary may opt to systematically guess the password using a repetitive or iterative mechanism. An adversary may guess login credentials without prior knowledge of system or environment passwords during an operation by using a list of common passwords. Password guessing may or may not take into account the target's policies on password complexity or use policies that may lock accounts out after a number of failed attempts. Typically, management services over commonly used ports are used when guessing passwords. Commonly targeted services include the following: Brute force attack programs and use cases These programs can be used individually or in combination to launch a successful brute force attack on a target network or system. There are several types of brute force attack programs used by attackers, including: Indications of an attack Extreme counts of failed sign-ins from many unknown usernames Never previously successfully authenticated from multiple remote desktop protocol (RDP) connections or from new source IP addresses Example: Potential SQL Brute Force attempt alert Practices to blunt a Brute Force Attacks To counteract brute-force attacks, you can take multiple measures such as: Disable the public IP address and use one of these connection methods: Use a point-to-site virtual private network (VPN) Create a site-to-site VPN Use Azure ExpressRoute to create secure links from your on-premises network to Azure. Require two-factor authentication Increase password length and complexity Limit login attempts Implement Captcha About CAPTCHAs - Any time you let people register on your site or even enter a name and URL (like for a blog comment), you might get a flood of fake names. These are often left by automated programs (bots) that try to leave URLs on every website they can find. (A common motivation is to post the URLs of products for sale.) You can help make sure that a user is a real person and not a computer program by using a CAPTCHA to validate users when they register or otherwise enter their name and site. CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. A CAPTCHA is a challenge-response test in which the user is asked to do something that is easy for a person to do but hard for an automated program to do. The most common type of CAPTCHA is one where you see distorted letters and are asked to type them. (The distortion is supposed to make it hard for bots to decipher the letters.) Limit the amount of time that the ports are open. Understand just-in-time VM access The risk of open management ports on a virtual machine Threat actors actively hunt accessible machines with open management ports, like remote desktop protocol (RDP) or secure shell protocol (SSH). All of your virtual machines are potential targets for an attack. When a VM is successfully compromised, it's used as the entry point to attack further resources within your environment. Why JIT VM access is the solution As with all cybersecurity prevention techniques, your goal should be to reduce the attack surface. In this case, that means having fewer open ports, especially management ports. Your legitimate users also use these ports, so it's not practical to keep them closed. To solve this dilemma, Microsoft Defender for Cloud offers JIT. With JIT, you can lock down the inbound traffic to your VMs, reducing exposure to attacks while providing easy access to connect to VMs when needed. How JIT operates with network resources in Azure and AWS In Azure, you can block inbound traffic on specific ports, by enabling just-in-time VM access. Defender for Cloud ensures \"deny all inbound traffic\" rules exist for your selected ports in the network security group (NSG) and Azure Firewall rules. These rules restrict access to your Azure VMs\u2019 management ports and defend them from attack. If other rules already exist for the selected ports, then those existing rules take priority over the new \"deny all inbound traffic\" rules. If there are no existing rules on the selected ports, then the new rules take top priority in the NSG and Azure Firewall. In AWS, by enabling JIT-access the relevant rules in the attached Amazon Elastic Compute Cloud (Amazon EC2) security groups, for the selected ports, are revoked which blocks inbound traffic on those specific ports. When a user requests access to a VM, Defender for Cloud checks that the user has Azure role-based access control (Azure RBAC) permissions for that VM. If the request is approved, Defender for Cloud configures the NSGs and Azure Firewall to allow inbound traffic to the selected ports from the relevant IP address (or range), for the amount of time that was specified. In AWS, Defender for Cloud creates a new EC2 security group that allows inbound traffic to the specified ports. After the time has expired, Defender for Cloud restores the NSGs to their previous states. Connections that are already established are not interrupted. Just-in-time VM enabled on an Azure Virtual Machine. Example: Azure virtual machine The diagram shows the logic that Defender for Cloud applies when deciding how to categorize your supported VMs (i.e., Azure Virtual Machine) Just-in-time VM enabled on an AWS EC2 Instance. The diagram shows the logic that Defender for Cloud applies when deciding how to categorize your supported VMs (i.e., AWS EC2 instance) Example: AWS EC2 Instance Added to the recommendation\u2019s Unhealthy resources tab The diagram shows the logic Defender for Cloud applies when deciding how to categorize your supported VM. When Defender for Cloud finds a machine that can benefit from JIT, it adds that machine to the recommendation's Unhealthy resources tab. Example: Affected resources Implement just-in-time VM access Just-in-time (JIT) virtual machine (VM) access is used to lock down inbound traffic to your Azure VMs, reducing exposure to attacks while providing easy access to connect to VMs when needed. When you enable JIT VM Access for your VMs, you next create a policy that determines the ports to help protect, how long ports should remain open, and the approved IP addresses that can access these ports. The policy helps you stay in control of what users can do when they request access. Requests are logged in the Azure activity log, so you can easily monitor and audit access. The policy will also help you quickly identify the existing VMs that have JIT VM Access enabled and the VMs where JIT VM Access is recommended. How JIT VM Access works To use Just-in-Time VM access, you must enable Microsoft Defender for Cloud. After you enable Defender, you can view which virtual machines have JIT configured. Enable JIT on any virtual machine that is not Healthy. For each virtual machine, you are recommended specific ports and access. You can accept the recommendations or Add other ports of your choosing. Once everything is in place, users must request access to the virtual machine. You can also monitor the usage of each virtual machine. Perform try-this exercises Task 1: Microsoft Defender for Cloud overview and recommendations In this task, you will review Microsoft Defender for Cloud. In the Portal, navigate to Microsoft Defender for Cloud. Under General, select Overview. Discuss the Overview page. Under Management, select Environment settings. Select your subscription, and then review the Microsoft Defender for Cloud features. Return to the main Microsoft Defender for Cloud blade. Under General, select Recommendations. Review Secure Score, Recommendations status, and Resource Health. From the main Microsoft Defender for Cloud blade, under Cloud Security, select Regulatory compliance. Review the compliance assessment and available compliance controls. Scroll down and, under Microsoft cloud security benchmark, expand each compliance control to review several recommendations. For example, NS. Network Security, DP. Data Protection, and ES. Endpoint Security. From the main Microsoft Defender for Cloud blade, under Cloud Security, select Workload protections. Review the Defender for Cloud coverage, Security alerts, and Advanced protection features. Knowledge check Which tasks are not included in the Microsoft Defender for Cloud free tier? Monitor IoT hubs and resources Monitor network access and endpoint security Monitor non-Azure resources ( Ans ) An organization compliance group requires client authentication using Azure AD and Key Vault diagnostic logs. What is the easiest way to implement the requirement for client authentication? Configure management groups Implement Microsoft Defender for Cloud policies ( Ans ) Create Desired Configuration State scripts An organization is working with an outside agency that needs to access a virtual machine. There's a real concern about brute-force login attacks targeted at virtual machine management ports. Which of the following components would open the management ports for a defined time range? Select one. Azure Firewall Bastion service Just-in-Time virtual machine access ( Ans ) When using Microsoft Defender for Cloud to provide visibility into virtual machine security settings, the monitoring system will notify administrators as issues arise. Which incident below would require a different monitoring tool to discover it? A newer operating system version is available. ( Ans ) System security updates and critical updates that are missing. Disk encryption is applied on virtual machines. Summary Microsoft Defender for Cloud and Secure Score is the keys to keeping your solutions and data secure. You should be able to: - Define the most common types of cyber-attacks - Configure Microsoft Defender for Cloud based on your security posture - Review Secure Score and raise it - Lock down your solutions using Microsoft Defender for Cloud recommendations Configure and monitor Microsoft Sentinel Introduction In the world we live in today, there is always the chance that your system may be hacked. What do you do at this point to find and evaluate a breach? Microsoft Sentinel is a tool to help you locate a breach and track what the intruder did while on your systems. Scenario A security engineer uses Microsoft Sentinel to discover and track a breach, you will work on such tasks as: Configure Microsoft Sentinel. Build queries to find breaches. Track incidents with investigation and hunting. Enable Azure Sentinel Microsoft Sentinel is a scalable, cloud-native, security information event management (SIEM) and security orchestration automated response (SOAR) solution. Microsoft Sentinel delivers intelligent security analytics and threat intelligence across the enterprise, providing a single solution for alert detection, threat visibility, proactive hunting, and threat response. Value of a security information and event management tool Security Operations (SecOps) teams are inundated with a high volume of alerts and spend far too much time in tasks like infrastructure setup and maintenance. As a result, many legitimate threats go unnoticed. According to the \u201cCybersecurity Jobs Report, 2018-2021\u201d by Cybersecurity Ventures, An expected shortfall of 3.5M security professionals by 2021 will further increase the challenges for security operations teams. Alert fatigue is real. Security analysts face a huge burden of triage as they not only have to sift through a sea of alerts but also correlate alerts from different products manually or using a traditional correlation engine. Microsoft Azure Sentinel Microsoft Sentinel offers nearly limitless cloud scale and speed to address your security needs. Think of Microsoft Sentinel as the first SIEM-as-a-service that brings the power of the cloud and artificial intelligence to help security operations teams efficiently identify and stop cyber-attacks before they cause harm. Microsoft Sentinel enriches your investigation and detection by providing both Microsoft's threat intelligence stream and external threat intelligence streams. Microsoft Sentinel integrates with Microsoft 365 solution and correlates millions of signals from different products such as Azure Identity Protection, Microsoft Cloud App Security, and soon Azure Advanced Threat Protection, Windows Advanced Threat Protection, M365 Advanced Threat Protection, Intune, and Azure Information Protection. It enables the following services: - Collect data at cloud scale across all users, devices, applications, and infrastructure, both on-premises and in multiple clouds. - Detect previously undetected threats, and minimize false positives using Microsoft's analytics and unparalleled threat intelligence. - Investigate threats with artificial intelligence, and hunt for suspicious activities at scale, tapping into years of cyber security work at Microsoft. - Respond to incidents rapidly with built-in orchestration and automation of common tasks Building on the full range of existing Azure services, Microsoft Sentinel natively incorporates proven foundations, like Log Analytics, and Logic Apps. Microsoft Sentinel enriches your investigation and detection with AI, and provides Microsoft's threat intelligence stream, and enables you to bring your own threat intelligence. Configure data connections to Sentinel To onboard Microsoft Sentinel, you first need to connect to your security sources. Microsoft Sentinel comes with a number of connectors for Microsoft solutions, available out of the box and providing real-time integration, including Microsoft Threat Protection solutions, and Microsoft 365 sources, including Microsoft 365, Azure AD, Azure ATP, and Microsoft Cloud App Security, and more. In addition, there are built-in connectors to the broader security ecosystem for non-Microsoft solutions. You can also use common event format, Syslog or REST-API to connect your data sources with Microsoft Sentinel as well. Data connection methods The following data connection methods are supported by Microsoft Sentinel: Service to service integration: Some services are connected natively, such as AWS and Microsoft services, these services leverage the Azure foundation for out-of-the-box integration, the following solutions can be connected in a few clicks: Amazon Web Services - CloudTrail Azure Activity Azure AD audit logs and sign-ins Azure AD Identity Protection Azure Advanced Threat Protection Azure Information Protection Microsoft Defender for Cloud Cloud App SecurityCloud App Security Domain name server Microsoft 365 Microsoft Defender ATP Microsoft web application firewall Windows firewall Windows security events External solutions via API Some data sources are connected using APIs that are provided by the connected data source. Typically, most security technologies provide a set of APIs through which event logs can be retrieved. The APIs connect to Microsoft Sentinel and gather specific data types and send them to Azure Log Analytics External solutions via an agen Microsoft Sentinel can be connected to all other data sources that can perform real-time log streaming using the Syslog protocol, via an agent. The Microsoft Sentinel agent, which is based on the Log Analytics agent, converts CEF formatted logs into a format that can be ingested by Log Analytics. Depending on the appliance type, the agent is installed either directly on the appliance, or on a dedicated Linux server. Agent connection options To connect your external appliance to Microsoft Sentinel, the agent must be deployed on a dedicated machine (VM or on-premises) to support the communication between the appliance and Microsoft Sentinel. You can deploy the agent automatically or manually. Automatic deployment is only available if your dedicated machine is a new VM you are creating in Azure. Alternatively, you can deploy the agent manually on an existing Azure VM, on a VM in another cloud, or on an on-premises machine. Global prerequisites Active Azure Subscription Log Analytics workspace. To enable Microsoft Sentinel, you need contributor permissions to the subscription in which the Microsoft Sentinel workspace resides. To use Microsoft Sentinel, you need either contributor or reader permissions on the resource group that the workspace belongs to. Additional permissions may be needed to connect specific data sources. Microsoft Sentinel is a paid service. Create workbooks for explore Sentinel data After you connect your data sources to Microsoft Sentinel, you can monitor the data using the Microsoft Sentinel integration with Azure Monitor Workbooks, which provides versatility in creating custom workbooks. While Workbooks are displayed differently in Microsoft Sentinel, it may be helpful for you to determine how to create interactive reports with Azure Monitor Workbooks. Microsoft Sentinel allows you to create custom workbooks across your data and comes with built-in workbook templates to quickly gain insights across your data as soon as you connect a data source. Workbooks combine text, Analytics queries, Azure Metrics, and parameters into rich interactive reports. Workbooks are editable by other team members who have access to the same Azure resources. Workbooks are helpful for scenarios like: Exploring the usage of your app when you don't know the metrics of interest in advance: numbers of users, retention rates, conversion rates, etc. Unlike other usage analytics tools, workbooks let you combine multiple kinds of visualizations and analyses, making them great for this kind of free-form exploration. Explaining to your team how a newly released feature is performing by showing user counts for key interactions and other metrics. Sharing the results of an A/B experiment in your app with other members of your team. You can explain the goals for the experiment with text, then show each usage metric and Analytics query used to evaluate the experiment, along with clear call-outs for whether each metric was above- or below-target. Reporting the impact of an outage on the usage of your app, combining data, text explanation, and a discussion of next steps to prevent outages in the future. Saving and sharing workbooks with your team Workbooks are saved within an Application Insights resource, either in the My Reports section that's private to you or in the Shared Reports section accessible to everyone with access to the Application Insights resource. A workbook can be shared with a link or via email. Keep in mind that recipients of the link need access to this resource in the Azure portal to view the workbook. To make edits, recipients need at least Contributor permissions for the resource. Analytics To help you reduce noise and minimize the number of alerts you have to review and investigate, Microsoft Sentinel uses analytics to correlate alerts into incidents. Incidents are groups of related alerts that together create a possible actionable threat that you can investigate and resolve. Use the built-in correlation rules as-is, or use them as a starting point to build your own. Microsoft Sentinel also provides machine learning rules to map your network behavior and then look for anomalies across your resources. These analytics connect the dots by combining low-fidelity alerts about different entities into potential high-fidelity security incidents. Enable rules to create incidents Alerts triggered in Microsoft security solutions that are connected to Microsoft Sentinel, such as Microsoft Defender for Cloud Apps and Azure Advanced Threat Protection, do not automatically create incidents in Microsoft Sentinel. By default, when you connect a Microsoft solution to Microsoft Sentinel, any alert generated in that service will be stored as raw data in Microsoft Sentinel, in the Security Alert table in your Microsoft Sentinel workspace. You can then use that data like any other raw data you connect to Sentinel. Prerequisites You must connect Microsoft security solutions to enable incident creation from security service alerts. Using Microsoft Security incident creation analytic rules Use the built-in rules available in Microsoft Sentinel to choose which connected Microsoft security solutions should create Microsoft Sentinel incidents automatically in real-time. You can also edit the rules to define more specific options for filtering which of the alerts generated by the Microsoft security solution should create incidents in Microsoft Sentinel. For example, you can choose to create Microsoft Sentinel incidents automatically only from high-severity Microsoft Defender for Cloud alerts. You can create more than one Microsoft Security analytic rule per Microsoft security service type. This action does not create duplicate incidents since each rule is used as a filter. Even if an alert matches more than one Microsoft Security analytic rule, it creates just one Microsoft Sentinel incident. When you connect a Microsoft security solution, you can select whether you want the alerts from the security solution to automatically generate incidents in Microsoft Sentinel automatically. Configure playbooks Security automation and orchestration lets you automate your common tasks and simplify security orchestration with playbooks that integrate with Azure services as well as your existing tools. Built on the foundation of Azure Logic Apps, Azure Sentinel's automation and orchestration solution provides a highly-extensible architecture that enables scalable automation as new technologies and threats emerge. To build playbooks with Azure Logic Apps, you can choose from a growing gallery of built-in playbooks. These include 200+ connectors for services such as Azure functions. The connectors allow you to apply any custom logic in code, ServiceNow, Jira, Zendesk, HTTP requests, Microsoft Teams, Slack, Windows Defender ATP, and Cloud App Security. For example, if you use the ServiceNow ticketing system, you can use the tools provided to use Azure Logic Apps to automate your workflows and open a ticket in ServiceNow each time a particular event is detected. Hunt and investigate potential breaches Currently in preview, Microsoft Sentinel deep investigation tools help you to understand the scope and find the root cause, of a potential security threat. You can choose an entity on the interactive graph to ask interesting questions for a specific entity, and drill down into that entity and its connections to get to the root cause of the threat. An incident can include multiple alerts. It's an aggregation of all the relevant evidence for a specific investigation. An incident is created based on analytic rules that you created in the Analytics page. The properties related to the alerts, such as severity, and status, are set at the incident level. After you let Microsoft Sentinel know what kinds of threats you're looking for and how to find them, you can monitor detected threats by investigating incidents. Use the investigation graph to deep dive The investigation graph enables analysts to ask the right questions for each investigation. The investigation graph helps you understand the scope, and identify the root cause, of a potential security threat by correlating relevant data with any involved entity. You can dive deeper and investigate any entity presented in the graph by selecting it and choosing between different expansion options. The investigation graph provides you with: - Visual context from raw data: The live, visual graph displays entity relationships extracted automatically from the raw data. This enables you to easily view connections across different data sources. - Full investigation scope discovery: Expand your investigation scope using built-in exploration queries to surface the full scope of a breach. - Built-in investigation steps: Use predefined exploration options to make sure you are asking the right questions in the face of a threat. To use the investigation graph: Select an incident, then select Investigate. This takes you to the investigation graph. The graph provides an illustrative map of the entities directly connected to the alert and each resource connected further. You'll only be able to investigate the incident if you used the entity mapping fields when you set up your analytic rule. The investigation graph requires that your original incident includes entities. Hunting Use Microsoft's Sentinel's powerful hunting search-and-query tools, based on the MITRE framework, which enable you to proactively hunt for security threats across your organization\u2019s data sources, before an alert is triggered. After you discover which hunting query provides high-value insights into possible attacks, you can also create custom detection rules based on your query, and surface those insights as alerts to your security incident responders. While hunting, you can create bookmarks for interesting events, enabling you to return to them later, share them with others, and group them with other correlating events to create a compelling incident for investigation. For example, one built-in query provides data about the most uncommon processes running on your infrastructure. You may not want an alert each time they are run. With Microsoft Sentinel hunting, you can take advantage of the following capabilities: Built-in queries: To get you started, a starting page provides preloaded query examples designed to get you started and get you familiar with the tables and the query language. These built-in hunting queries are developed by Microsoft security researchers on a continuous basis, adding new queries, and fine-tuning existing queries to provide you with an entry point to look for new detections and figure out where to start hunting for the beginnings of new attacks. Powerful query language with IntelliSense: Built on top of a query language that gives you the flexibility you need to take hunting to the next level. Create your own bookmarks: During the hunting process, you may come across matches or findings, dashboards, or activities that look unusual or suspicious. In order to mark those items so you can come back to them in the future, use the bookmark functionality. Bookmarks let you save items for later, to be used to create an incident for investigation. Use notebooks to automate investigation: Notebooks are like step-by-step playbooks that you can build to walk through the steps of an investigation and hunt. Notebooks encapsulate all the hunting steps in a reusable playbook that can be shared with others in your organization. Query the stored data: The data is accessible in tables for you to query. For example, you can query process creation, DNS events, and many other event types. Links to the community: Leverage the power of the greater community to find additional queries and data sources. Community The Microsoft Sentinel community is a powerful resource for threat detection and automation. Our Microsoft security analysts constantly create and add new workbooks, playbooks, hunting queries, and more, posting them to the community for you to use in your environment. You can download sample content from the private community GitHub repository to create custom workbooks, hunting queries, notebooks, and playbooks for Microsoft Sentinel. Knowledge check Where can custom security alerts be created and managed? Azure Security Center Azure Sentinel ( Ans ) Azure Storage Which of the items below would exceed the capabilities of an Azure Sentinel playbook? A Sentinel playbook can help automate and orchestrate an incident response. A Sentinel playbook be run manually or set to run automatically when specific alerts are triggered. A Sentinel playbook be created to handle several subscriptions at once. ( Ans ) Sentinel is being used to investigate an incident. When viewing the incident detailed information, which value has to be assigned, instead of being included in the data? Incident ID Incident owner ( Ans ) Number of entities involved When creating roles within a security operations team to grant appropriate access to Azure Sentinel. Which role below would have to be created versus being built-in? Azure Sentinel reader Azure Sentinel responder Azure Sentinel owner ( Ans ) An investigator wants to be proactive about looking for security threats. The security officer has read about Sentinel\u2019s hunting capabilities and notebooks. What is an Azure Sentinel notebook? A step-by-step playbook that provides the ability to walk through the steps of an investigation and hunt. ( Ans ) A table to query and locate actions like DNS events. A saved item for the creation of an incident for investigation.","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Manage-security-operation.html#az-500-microsoft-certified-azure-security-engineer-associate","text":"","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Manage-security-operation.html#manage-security-operation","text":"Once you have deployed and secured your Azure environment, learn to monitor, operate, and continuously improve the security of your solutions. This learning path helps prepare you for Exam AZ-500: Microsoft Azure Security Technologies . Configure and manage Azure Monitor Introduction Explore Azure Monitor Configure and monitor metrics and logs Enable Log Analytics Manage connected sources for log analytics Enable Azure monitor Alerts Configure properties for diagnostic logging Perform try-this exercises Knowledge check Summary Enable and manage Microsoft Defender for Cloud Introduction MITRE Attack matrix Implement Microsoft Defender for Cloud Security posture Workload protections Deploy Microsoft Defender for Cloud Azure Arc Azure Arc capabilities Microsoft cloud security benchmark Configure Microsoft Defender for Cloud policies View and edit security policies Manage and implement Microsoft Defender for Cloud recommendations Explore secure score Define brute force attacks Understand just-in-time VM access Implement just-in-time VM access Perform try-this exercises Knowledge check Summary Configure and monitor Microsoft Sentinel Introduction Enable Azure Sentinel Configure data connections to Sentinel Create workbooks for explore Sentinel data Enable rules to create incidents Configure playbooks Hunt and investigate potential breaches Knowledge check Summary","title":"Manage security operation"},{"location":"Cloud/Azure/AZ-500/Manage-security-operation.html#configure-and-manage-azure-monitor","text":"Introduction Azure Monitor is a key resource to keep watch on how all your Azure resources are performing, and to trigger alerts if there is any sort of problem. Monitoring your systems and updating them as needed is as important as your initial secure setup. Explore Azure Monitor Earlier, this course discussed Microsoft Azure Monitor. The following high-level diagram depicts the two fundamental data types that Azure Monitor uses, Metrics and Logs. On the left side of the figure are the sources of monitoring data that populate these data stores. On the right side are the different functions that Azure Monitor performs with this collected data, such as analysis, alerting, and streaming to external systems. For many Azure resources, you\u2019ll find the data that Azure Monitor collects right in the resource\u2019s Overview page in the Azure portal. Check out any virtual machine (VM). for example, and you'll notice several charts displaying performance metrics. Select any of the graphs to open the data in Metrics Explorer, which allows you to chart the values of multiple metrics over time. You can view the charts interactively or pin them to a dashboard to view them with other visualizations. Exporting data to a SIEM Processed events that Microsoft Defender for Cloud produces are published to the Azure activity log, one of the log types available through Azure Monitor. Azure Monitor offers a consolidated pipeline for routing any of your monitoring data into a SIEM tool. This is done by streaming that data to an event hub, where it can then be pulled into a partner tool. This pipe uses the Azure Monitor single pipeline for getting access to the monitoring data from your Azure environment. This allows you to easily set up SIEMs and monitoring tools to consume the data. Currently, the exposed security data from Microsoft Defender for Cloud to a SIEM consists of security alerts. Microsoft Defender for Cloud security alerts Microsoft Defender for Cloud automatically collects, analyzes, and integrates log data from your Azure resources; the network; and connected partner solutions, like firewall and endpoint protection solutions, to detect real threats and reduce false positives. Microsoft Defender for Cloud displays a list of prioritized security alerts along with the information you need to quickly investigate the problem and recommendations for how to remediate an attack. The following sections describe how you can configure data to be streamed to an event hub. The steps assume that you already have Microsoft Defender for Cloud configured in your Azure subscription. Azure Event Hubs Azure Event Hubs is a streaming platform and event ingestion service that can transform and store data by using any real-time analytics provider or batching/storage adapters. Use Event Hubs to stream log data from Azure Monitor to a Microsoft Sentinel or a partner SIEM and monitoring tools. What data can be sent into an event hub? Within your Azure environment, there are several 'tiers' of monitoring data, and the method of accessing data from each tier varies slightly. Typically, these tiers can be described as: Application monitoring data - Data about the performance and functionality of the code you have written and are running on Azure. Examples of application monitoring data include performance traces, application logs, and user telemetry. Application monitoring data is usually collected in one of the following ways: By instrumenting your code with an SDK such as the Application Insights SDK. By running a monitoring agent that listens for new application logs on the machine running your application, such as the Windows Azure Diagnostic Agent or Linux Azure Diagnostic Agent. Guest OS monitoring data - Data about the operating system on which your application is running. Examples of guest OS monitoring data would be Linux syslog or Windows system events. To collect this type of data, you need to install an agent such as the Windows Azure Diagnostic Agent or Linux Azure Diagnostic Agent. Azure resource monitoring data - Data about the operation of an Azure resource. For some Azure resource types, such as virtual machines, there is a guest OS and application(s) to monitor inside of that Azure service. For other Azure resources, such as Network Security Groups, the resource monitoring data is the highest tier of data available (since there is no guest OS or application running in those resources). This data can be collected using resource diagnostic settings. Azure subscription monitoring data - Data about the operation and management of an Azure subscription, as well as data about the health and operation of Azure itself. The activity log contains most subscription monitoring data, such as service health incidents and Azure Resource Manager audits. You can collect this data using a Log Profile. Azure tenant monitoring data - Data about the operation of tenant-level Azure services, such as Azure Active Directory. The Azure Active Directory audits and sign-ins are examples of tenant monitoring data. This data can be collected using a tenant diagnostic setting. Data from any tier can be sent into an event hub, where it can be pulled into a tool. Some sources can be configured to send data directly to an event hub while another process such as a Logic App may be required to retrieve the required data. Connecting to Microsoft Sentinel Microsoft Sentinel is now generally available. With Microsoft Sentinel, enterprises worldwide can now keep pace with the exponential growth in security data, improve security outcomes without adding analyst resources, and reduce hardware and operational costs. Microsoft Sentinel brings together the power of Azure and AI to enable Security Operations Centers to achieve more. Some of the features of Microsoft Sentinel are: More than 100 built-in alert rules Sentinel's alert rule wizard to create your own. Alerts can be triggered by a single event or based on a threshold, or by correlating different datasets or by using built-in machine learning algorithms. Jupyter Notebooks that use a growing collection of hunting queries, exploratory queries, and python libraries. Investigation graph for visualizing and traversing the connections between entities like users, assets, applications, or URLs and related activities like logins, data transfers, or application usage to rapidly understand the scope and impact of an incident. The Microsoft Sentinel GitHub repository has grown to over 400 detection, exploratory, and hunting queries, plus Azure Notebooks samples and related Python libraries, playbooks samples, and parsers. The bulk of these were developed by Microsoft's security researchers based on their vast global security experience and threat intelligence. To on-board Microsoft Sentinel, you first need to enable Microsoft Sentinel, and then connect your data sources. Microsoft Sentinel comes with a number of connectors for Microsoft solutions, available out of the box and providing real-time integration, including Microsoft Threat Protection solutions, Microsoft 365 sources, including Microsoft 365, Azure AD, Azure ATP, and Microsoft Cloud App Security, and more. In addition, there are built-in connectors to the broader security ecosystem for non-Microsoft solutions. You can also use common event format, Syslog or REST-API to connect your data sources with Azure Sentinel. After you connect your data sources, choose from a gallery of expertly created dashboards that surface insights based on your data. These dashboards can be easily customized to your needs. Configure and monitor metrics and logs All data that Azure Monitor collects fits into one of two fundamental types: metrics or logs. Azure Monitor Metrics Azure Monitor Metrics is a feature of Azure Monitor that collects numeric data from monitored resources into a time series database. Metrics are numerical values that are collected at regular intervals and describe some aspect of a system at a particular time. Azure Monitor Metrics - Navigation Example Use Metrics Explorer to interactively analyze the data in your metric database and chart the values of multiple metrics over time. You can pin the charts to a dashboard to view them with other visualizations. You can also retrieve metrics by using the Azure monitoring REST API. Behind the scene, log-based metrics translate into log queries. Their retention matches the retention of events in underlying logs. For Application Insights resources, logs are stored for 90 days. Types of metrics There are multiple types of metrics supported by Azure Monitor Metrics: Native metrics use tools in Azure Monitor for analysis and alerting. Platform metrics are collected from Azure resources. They require no configuration and have no cost. Custom metrics are collected from different sources that you configure, including applications and agents running on virtual machines. Prometheus metrics (preview) are collected from Kubernetes clusters, including Azure Kubernetes Service (AKS), and use industry-standard tools for analyzing and alerting, such as PromQL and Grafana. Additional Background and Information What is Prometheus? Prometheus is an open-source toolkit that collects data for monitoring and alerting. Prometheus Features: A multi-dimensional data model with time series data identified by metric name and key/value pairs PromQL (PromQL component called Prom Kubernetes - an extension to support Prometheus) provides a flexible query language to use this dimensionality. Time series collection happens via a pull model over Hypertext Transfer Protocol (HTTP) Pushing time series is supported via an intermediary gateway Targets are discovered via service discovery or static configuration What is Azure Managed Grafana? Azure Managed Grafana is a data visualization platform built on top of the Grafana software by Grafana Labs. It's built as a fully managed Azure service operated and supported by Microsoft. Grafana helps you combine metrics, logs, and traces into a single user interface. With its extensive support for data sources and graphing capabilities, you can view and analyze your application and infrastructure telemetry data in real-time. Azure Managed Grafana is optimized for the Azure environment. It works seamlessly with many Azure services. Specifically, for the current preview, it provides with the following integration features: Built-in support for Azure Monitor and Azure Data Explorer User authentication and access control using Azure Active Directory identities Direct import of existing charts from the Azure portal Why use Azure Managed Grafana? Managed Grafana lets you bring together all your telemetry data into one place. It can access various data sources supported, including your data stores in Azure and elsewhere. By combining charts, logs, and alerts into one view, you can get a holistic view of your application and infrastructure and correlate information across multiple datasets. As a fully managed service, Azure Managed Grafana lets you deploy Grafana without having to deal with setup. The service provides high availability, service level agreement (SLA) guarantees, and automatic software updates. You can share Grafana dashboards with people inside and outside your organization and allow others to join in for monitoring or troubleshooting. Managed Grafana uses Azure Active Directory (Azure AD)\u2019s centralized identity management, which allows you to control which users can use a Grafana instance, and you can use managed identities to access Azure data stores, such as Azure Monitor. You can create dashboards instantaneously by importing existing charts directly from the Azure portal or by using prebuilt dashboards. The differences between each of the metrics are summarized in the following table. Category Native platform metrics Native custom metrics Prometheus metrics (preview) Sources Azure resources Azure Monitor agent Application Insights Representational State Transfer (REST) Application Programming Interface (API) Azure Kubernetes Service (AKS) cluster Any Kubernetes cluster through remote-write Configuration None Varies by source Enable Azure Monitor managed service for Prometheus Stored Subscription Subscription Azure Monitor workspace Cost No Yes Yes (free during preview) Aggregation pre-aggregated pre-aggregated raw data Analyze Metrics Explorer Metrics Explorer Prometheus Querying (PromQL) LanguageGrafana dashboards Alert metrics alert rule metrics alert rule Prometheus alert rule Visualize WorkbooksAzure dashboardGrafana WorkbooksAzure dashboardGrafana Grafana Retrieve Azure Command-Line Interface (CLI) Azure PowerShell cmdletsRepresentational State Transfer (REST) Application Programming Interface (API) or client library.NETGoJavaJavaScriptPython Azure Command-Line Interface (CLI) Azure PowerShell cmdletsRepresentational State Transfer (REST) Application Programming Interface (API) or client library.NETGoJavaJavaScriptPython Grafana Data collection Azure Monitor collects metrics from the following sources. After these metrics are collected in the Azure Monitor metric database, they can be evaluated together regardless of their source: Azure resources: Platform metrics are created by Azure resources and give you visibility into their health and performance. Each type of resource creates a distinct set of metrics without any configuration required. Platform metrics are collected from Azure resources at a one-minute frequency unless specified otherwise in the metric's definition. Applications: Application Insights creates metrics for your monitored applications to help you detect performance issues and track trends in how your application is used. Values include Server response time and Browser exceptions. Virtual machine agents: Metrics are collected from the guest operating system of a virtual machine. You can enable guest operating system (OS) metrics for Windows virtual machines using the Windows diagnostic extension and Linux virtual machines by using the InfluxData Telegraf agent. Custom metrics: You can define metrics in addition to the standard metrics that are automatically available. You can define custom metrics in your application that are monitored by Application Insights. You can also create custom metrics for an Azure service by using the custom metrics Application Programming Interface (API). Kubernetes clusters: Kubernetes clusters typically send metric data to a local Prometheus server that you must maintain. Azure Monitor managed service for Prometheus provides a managed service that collects metrics from Kubernetes clusters and stores them in Azure Monitor Metrics. A common type of log entry is an event, which is collected sporadically. Events are created by an application or service and typically include enough information to provide complete context on their own. For example, an event can indicate that a particular resource was created or modified, a new host started in response to increased traffic, or an error was detected in an application. Because the format of the data can vary, applications can create custom logs by using the structure that they need. Metric data can even be stored in Logs to combine them with other monitoring data for trending and other data analysis. The following is a list of the different ways that you can use Logs in Azure Monitor. Analyze - Use Log Analytics in the Azure portal to write log queries and interactively analyze log data using the powerful Data Explorer analysis engine. Use the Application Insights analytics console in the Azure portal to write log queries and interactively analyze log data from Application Insights. Visualize - Pin query results rendered as tables or charts to an Azure dashboard. Create a workbook to combine with multiple sets of data in an interactive report. Export the results of a query to Power BI to use different visualizations and share with users outside of Azure. Export the results of a query to Grafana to use its dashboarding and combine with other data sources. Alert - Configure a log alert rule that sends a notification or takes automated action when the results of the query match a particular result. Configure a metric alert rule on certain log data logs extracted as metrics. Retrieve - Access log query results from a command line using Azure command-line interface (CLI). Access log query results from a command line using PowerShell cmdlets. Access log query results from a custom application using Representational State Transfer (REST) Application Programming Interface (API). Export - Build a workflow to retrieve log data and copy it to an external location using Logic Apps. Log queries Data in Azure Monitor Logs is retrieved using a log query written with the Kusto query language, which allows you to quickly retrieve, consolidate, and analyze collected data. Use Log Analytics to write and test log queries in the Azure portal. It allows you to work with results interactively or pin them to a dashboard to view them with other visualizations. Security tools use of Monitor logs Microsoft Defender for Cloud stores data that it collects in a Log Analytics workspace where it can be analyzed with other log data. Azure Sentinel stores data from data sources into a Log Analytics workspace. Enable Log Analytics Log Analytics is part of Microsoft Azure's overall monitoring solution. Log Analytics helps you monitors cloud and on-premises environments to maintain availability and performance. Log Analytics is the primary tool in the Azure portal for writing log queries and interactively analyzing their results. Even if a log query is used elsewhere in Azure Monitor, you'll typically write and test the query first using Log Analytics. You can start Log Analytics from several places in the Azure portal. The scope of the data available to Log Analytics is determined by how you start it. Select Logs from the Azure Monitor menu or Log Analytics workspaces menu. Select Analytics from the Overview page of an Application Insights application. Select Logs from the menu of an Azure resource. In addition to interactively working with log queries and their results in Log Analytics, areas in Azure Monitor where you will use queries include the following: Alert rules. Alert rules proactively identify issues from data in your workspace. Each alert rule is based on a log search that is automatically run at regular intervals. The results are inspected to determine if an alert should be created. Dashboards. You can pin the results of any query into an Azure dashboard which allow you to visualize log and metric data together and optionally share with other Azure users. Views. You can create visualizations of data to be included in user dashboards with View Designer. Log queries provide the data used by tiles and visualization parts in each view. Export. When you import log data from Azure Monitor into Excel or Power BI, you create a log query to define the data to export. PowerShell. Use the results of a log query in a PowerShell script from a command line or an Azure Automation runbook that uses Invoke-AzOperationalInsightsQuery. Azure Monitor Logs API. The Azure Monitor Logs API allows any REST API client to retrieve log data from the workspace. The API request includes a query that is run against Azure Monitor to determine the data to retrieve. At the center of Log Analytics is the Log Analytics workspace, which is hosted in Azure. Log Analytics collects data in the workspace from connected sources by configuring data sources and adding solutions to your subscription. Data sources and solutions each create different record types, each with its own set of properties. But you can still analyze sources and solutions together in queries to the workspace. This capability allows you to use the same tools and methods to work with a variety of data collected by a variety of sources. Use the Log Analytics workspaces menu to create a Log Analytics workspace using the Azure portal. A Log Analytics workspace is a unique environment for Azure Monitor log data. Each workspace has its own data repository and configuration, and data sources and solutions are configured to store their data in a particular workspace. You require a Log Analytics workspace if you intend on collecting data from the following sources: Azure resources in your subscription On-premises computers monitored by System Center Operations Manager Device collections from Configuration Manager Diagnostics or log data from Azure storage Manage connected sources for log analytics The Azure Log Analytics agent was developed for comprehensive management across virtual machines in any cloud, on-premises machines, and those monitored by System Center Operations Manager. The Windows and Linux agents send collected data from different sources to your Log Analytics workspace in Azure Monitor, as well as any unique logs or metrics as defined in a monitoring solution. The Log Analytics agent also supports insights and other services in Azure Monitor such as Azure Monitor for VMs, Microsoft Defender for Cloud, and Azure Automation. Comparison to Azure diagnostics extension The Azure diagnostics extension in Azure Monitor can also be used to collect monitoring data from the guest operating system of Azure virtual machines. You may choose to use either or both depending on your requirements. The key differences to consider are: Azure Diagnostics Extension can be used only with Azure virtual machines. The Log Analytics agent can be used with virtual machines in Azure, other clouds, and on-premises. Azure Diagnostics extension sends data to Azure Storage, Azure Monitor Metrics (Windows only) and Event Hubs. The Log Analytics agent collects data to Azure Monitor Logs. The Log Analytics agent is required for solutions, Azure Monitor for VMs, and other services such as Microsoft Defender for Cloud. Data destinations The Log Analytics agent sends data to a Log Analytics workspace in Azure Monitor. The Windows agent can be multihomed to send data to multiple workspaces and System Center Operations Manager management groups. The Linux agent can send to only a single destination. Other services The agent for Linux and Windows isn't only for connecting to Azure Monitor, it also supports Azure Automation to host the Hybrid Runbook worker role and other services such as Change Tracking, Update Management, and Microsoft Defender for Cloud. Enable Azure monitor Alerts As discussed already, Azure monitor has metrics, logging, and analytics features. Another feature is Monitor Alerts. Responding to critical situations In addition to allowing you to interactively analyze monitoring data, an effective monitoring solution must be able to proactively respond to critical conditions identified in the data that it collects. This could be sending a text or mail to an administrator responsible for investigating an issue. Or you could launch an automated process that attempts to correct an error condition. Alerts Alerts in Azure Monitor proactively notify you of critical conditions and potentially attempt to take corrective action. Alert rules based on metrics provide near real time alerting based on numeric values, while rules based on logs allow for complex logic across data from multiple sources. Alert rules in Azure Monitor use action groups, which contain unique sets of recipients and actions that can be shared across multiple rules. Based on your requirements, action groups can perform such actions as using webhooks to have alerts start external actions or to integrate with your ITSM tools. The unified alert experience in Azure Monitor includes alerts that were previously managed by Log Analytics and Application Insights. In the past, Azure Monitor, Application Insights, Log Analytics, and Service Health had separate alerting capabilities. Over time, Azure improved and combined both the user interface and different methods of alerting. The consolidation is still in process. Overview of Alerts in Azure The diagram below represents the flow of alerts. Alert rules are separated from alerts and the actions taken when an alert fires. The alert rule captures the target and criteria for alerting. The alert rule can be in an enabled or a disabled state. Alerts only fire when enabled. The following are key attributes of an alert rule as shown: Target Resource: Defines the scope and signals available for alerting. A target can be any Azure resource. Example targets: a virtual machine, a storage account, a virtual machine scale set, a Log Analytics workspace, or an Application Insights resource. For certain resources (like virtual machines), you can specify multiple resources as the target of the alert rule. Signal: Emitted by the target resource. Signals can be of the following types: metric, activity log, Application Insights, and log. Criteria: A combination of signal and logic applied on a target resource. Examples: Percentage CPU > 70% Server Response Time > 4 ms Result count of a log query > 100 Alert Name: A specific name for the alert rule configured by the user. Alert Description: A description for the alert rule configured by the user. Severity: The severity of the alert after the criteria specified in the alert rule is met. Severity can range from 0 to 4. Sev 0 = Critical Sev 1 = Error Sev 2 = Warning Sev 3 = Informational Sev 4 = Verbose Action: A specific action taken when the alert is fired. What You Can Alert On You can alert on metrics and logs. These include but are not limited to: - Metric values - Log search queries - Activity log events - Health of the underlying Azure platform - Tests for website availability With the consolidation of alerting services still in process, there are some alerting capabilities that are not yet in the new alerts system. Monitor source Signal type Description Service health Activity log Not supported. View Create activity log alerts on service notifications. Application Insights Web availability tests Not supported. View Web test alerts. Available to any website that's instrumented to send data to Application Insights. Receive a notification when availability or responsiveness of a website is below expectations. Configure properties for diagnostic logging Azure Monitor diagnostic logs are logs produced by an Azure service that provide rich, frequently collected data about the operation of that service. Azure Monitor makes two types of diagnostic logs available: Tenant logs. These logs come from tenant-level services that exist outside an Azure subscription, such as Azure Active Directory (Azure AD). Resource logs. These logs come from Azure services that deploy resources within an Azure subscription, such as Network Security Groups (NSGs) or storage accounts. The content of these logs varies by Azure service and resource type. For example, NSG rule counters and Azure Key Vault audits are two types of diagnostic logs. These logs differ from the activity log. The activity log provides insight into the operations, such as creating a VM or deleting a logic app, that Azure Resource Manager performed on resources in your subscription using. The activity log is a subscription-level log. Resource-level diagnostic logs provide insight into operations that were performed within that resource itself, such as getting a secret from a key vault. These logs also differ from guest operating system (OS)\u2013level diagnostic logs. Guest OS diagnostic logs are those collected by an agent running inside a VM or other supported resource type. Resource-level diagnostic logs require no agent and capture resource-specific data from the Azure platform itself, whereas guest OS\u2013level diagnostic logs capture data from the OS and applications running on a VM. Create diagnostic settings in Azure portal You can configure diagnostic settings in the Azure portal either from the Azure Monitor menu or from the menu for the resource. Uses for diagnostic logs Here are some of the things you can do with diagnostic logs: Save them to a storage account for auditing or manual inspection. You can specify the retention time (in days) by using resource diagnostic settings. Stream them to event hubs for ingestion by a third-party service or custom analytics solution, such as Power BI. Analyze them with Azure Monitor, such that the data is immediately written to Azure Monitor with no need to first write the data to storage. Streaming of diagnostic logs can be enabled programmatically, via the portal, or using the Azure Monitor REST APIs. Either way, you create a diagnostic setting in which you specify an Event Hubs namespace and the log categories and metrics you want to send in to the namespace. An event hub is created in the namespace for each log category you enable. A diagnostic log category is a type of log that a resource may collect. Perform try-this exercises Task 1 - Activity Logs and Alerts In this task, we will configure an alert. Sign into the Portal. Search for and launch Monitor. Review the capabilities of Monitor: Monitor & Visualize Metrics, Query & Analyze Logs, and Setup Alerts & Actions. Select Activity log. Under the filters, click Timespan and review the drop-down choices. Open an event and discuss. Back in the Monitor main page, click Alerts then click + New alert rule. Under Resource click Select. Discuss how alerts can be scoped by subscription, resource type, and location. Select a resource for the alert and then click Done. Under Condition click Add. Select a signal, such as All Administrative operations, and then click Done. Under Action group, click Create. Review how action groups are used. Under Select an action type review the various ways the action group can be alerted. Select Email/SMS/Push/Voice. Review the configuration choices and finish creating your action group. Complete the Alert details and click Create alert rule. On the Alerts page, review how you can search your alerts by resource and time range. Task 2 - Log Analytics This lab requires a virtual machine in a running state. In this task, we will configure Log Analytics and run a query. Sign into the Portal. Search for and launch Log Analytics workspaces. Click Add or Create. On the Basics tab, review and complete the required information. Under the Essentials view, review the Pricing tier detail (example: Pricing tier: Pay-as-you-go) Finish creating the workspace and wait for it to deploy. Go to resource and discuss how Log Analytics is used and configured. Under Workspace Data Sources select Virtual machines. Select a virtual machine and click Connect. While you wait for the connection, under Settings click Advanced settings. Click Connected sources. Discuss the possible sources like virtual machines and storage accounts. Click Data. Review the different data sources. Show how Windows event logs can be collected. Save any changes you make. Back at the Log Analytics workspace, Under General select Logs. Review how log data is stored in tables and can be queried. Select the Event table and then click Run. Review the results. Knowledge check Data collected by Azure Monitor collects fits into which two fundamental types. What are those types of data? Events and Alerts Logs and Metrics ( Ans ) Records and Triggers When running a query of the Log Analytics workspace, which query language is used? Contextual Query Language Embedded SQL Kusto Query Language ( Ans ) To be notified when any virtual machine in the production resource group is deleted, what should be configured? Activity log alert ( Ans ) Application alert Log alert The IT managers would like to use a visualization tool for the Azure Monitor results. Each of the following is available, but there is a need to pick the one that will allow for insights and investigation of the data; which should be used? Dashboard Monitor Metrics ( Ans ) Power BI Enable and manage Microsoft Defender for Cloud Introduction Microsoft Defender for Cloud is your central location for setting and monitoring your organization's security posture. You can see which solutions adhere to security measures and which systems need to be secured. Scenario A security engineer uses Microsoft Defender for Cloud to track, maintain and improve an organization's security posture, you'll work on such tasks as: Implement security recommendations from Microsoft Defender for Cloud. Review your secure score and respond to it. Prevent attacks with Microsoft Defender for Cloud. MITRE Attack matrix The MITRE ATT&CK matrix is a publicly accessible knowledge base for understanding the various tactics and techniques used by attackers during a cyberattack. The knowledge base is organized into several categories: pre-attack, initial access, execution, persistence, privilege escalation, defense evasion, credential access, discovery, lateral movement, collection, exfiltration, and command and control. Tactics (T) represent the \"why\" of an ATT&CK technique or sub-technique. It is the adversary's tactical goal: the reason for performing an action. For example, an adversary may want to achieve credential access. Techniques (T) represent \"how'\" an adversary achieves a tactical goal by performing an action. For example, an adversary may dump credentials to achieve credential access. Common Knowledge (CK) in ATT&CK stands for common knowledge, essentially the documented modus operandi of tactics and techniques executed by adversaries. Defender for Cloud uses the MITRE Attack matrix to associate alerts with their perceived intent, helping formalize security domain knowledge. Example: Pre-Attack MITRE Attack Tactic Description Pre-Attack Pre-Attack could be either an attempt to access a certain resource regardless of a malicious intent, or a failed attempt to gain access to a target system to gather information prior to exploitation. This step is detected as an attempt, originating from outside the network, to scan the target system and identify an entry point. Example: Initial Access MITRE Attack Tactic Description Initial Access Initial Access is the stage where an attacker manages to get a foothold on the attacked resource. This stage is relevant for compute hosts and resources such as user accounts, certificates etc. Threat actors will often be able to control the resource after this stage. Implement Microsoft Defender for Cloud Microsoft Defender for Cloud is a solution for cloud security posture management (CSPM) and cloud workload protection (CWP) that finds weak spots across your cloud configuration, helps strengthen the overall security posture of your environment, and can protect workloads across multicloud and hybrid environments from evolving threats. For an interactive overview of how to Manage your cloud security posture with Microsoft Defender for Cloud, click on the image below. Defender for Cloud fills three vital needs as you manage the security of your resources and workloads in the cloud and on-premises: Defender for Cloud secure score continually assesses your security posture so you can track new security opportunities and precisely report on the progress of your security efforts. Defender for Cloud recommendations secures your workloads with step-by-step actions that protect your workloads from known security risks. Defender for Cloud alerts defends your workloads in real-time so you can react immediately and prevent security events from developing. Strengthen the security posture of your cloud resources Get a continuous assessment of the security of your cloud resources running in Azure, AWS, and Google Cloud. Use built-in policies and prioritized recommendations that are aligned to key industry and regulatory standards or build custom requirements that meet your organization's needs. Gather actionable insights by discovering your complete digital footprint and external attack surface signals and use them to automate recommendations and help ensure that resources are configured securely and meet your compliance needs. Protect cloud and hybrid workloads against threats Microsoft Defender for Cloud enables you to protect against evolving threats across multicloud and hybrid environments. You will be able to understand vulnerabilities with insights from industry-leading security research and secure your critical workloads across VMs, containers, databases, storage, app services, and more. Use many options to automate and streamline your security administration from a single place. Protect your resources and track your security progress Microsoft Defender for Cloud's features covers the two broad pillars of cloud security: Cloud Security Posture Management (CSPM) - Remediate security issues and watch your security posture improve Cloud Workload Protection (CWP) - Identify unique workload security requirements Protect all of your resources under one roof Because Defender for Cloud is an Azure-native service, many Azure services are monitored and protected without needing any deployment, but you can also add resources that are on-premises or in other public clouds. When necessary, Defender for Cloud can automatically deploy a Log Analytics agent to gather security-related data. For Azure machines, deployment is handled directly. For hybrid and multicloud environments, Microsoft Defender plans are extended to non-Azure machines with the help of Azure Arc. Cloud Security Posture Management (CSPM) features are extended to multicloud machines without the need for any agents. Defend your Azure-native resources Defender for Cloud helps you detect threats across: Azure Platform as a Service (PaaS) services - Detect threats targeting Azure services, including Azure App Service, Azure SQL, Azure Storage Account, and more data services. You can also perform anomaly detection on your Azure activity logs using the native integration with Microsoft Defender for Cloud Apps (formerly known as Microsoft Cloud App Security). Azure data services - Defender for Cloud includes capabilities that help you automatically classify your data in Azure SQL. You can also get assessments for potential vulnerabilities across Azure SQL and Storage services and recommendations for how to mitigate them. Networks - Defender for Cloud helps you limit exposure to brute force attacks. By reducing access to virtual machine ports and using just-in-time VM access, you can harden your network by preventing unnecessary access. You can set secure access policies on selected ports for only authorized users, allowed source IP address ranges or IP addresses, and for a limited amount of time. Defend your on-premises resources In addition to defending your Azure environment, you can add Defender for Cloud capabilities to your hybrid cloud environment to protect your non-Azure servers. To help you focus on what matters the most, you'll get customized threat intelligence and prioritized alerts according to your specific environment. To extend protection to on-premises machines, deploy Azure Arc and enable Defender for Cloud's enhanced security features. Defend resources running on other clouds Defender for Cloud can protect resources in other clouds (such as Amazon Web Services AWS and Google Cloud Platform GCP). For example, if you've connected an Amazon Web Services (AWS) account to an Azure subscription, you can enable any of these protections: Defender for Cloud's CSPM features extend to your AWS resources. This agentless plan assesses your AWS resources according to AWS-specific security recommendations, and these are included in your secure score. The resources will also be assessed for compliance with built-in standards specific to AWS (AWS Center for Internet Security (CIS), AWS Payment Card Industry (PCI) Data Security Standards (DSS), and AWS Foundational Security Best Practices). Defender for Cloud's asset inventory page is a multicloud enabled feature helping you manage your AWS resources alongside your Azure resources. Microsoft Defender for Kubernetes extends its container threat detection and advanced defenses to your Amazon Elastic Kubernetes Service (EKS) Linux clusters. Microsoft Defender for Servers brings threat detection and advanced defenses to your Windows and Linux Elastic Compute Cloud 2 (EC2) instances. This plan includes the integrated license for Microsoft Defender for Endpoint, security baselines, and OS level assessments, vulnerability assessment scanning, adaptive application controls (AAC), file integrity monitoring (FIM), and more. Close vulnerabilities before they get exploited Defender for Cloud includes vulnerability assessment solutions for virtual machines, container registries, and SQL servers as part of the enhanced security features. Some of the scanners are powered by Qualys. But you don't need a Qualys license or even a Qualys account - everything's handled seamlessly inside Defender for Cloud. Microsoft Defender for Servers includes automatic, native integration with Microsoft Defender for Endpoint. With this integration enabled, you'll have access to the vulnerability findings from Microsoft Defender Vulnerability Management. Review the findings from these vulnerability scanners and respond to them all from within Defender for Cloud. This broad approach brings Defender for Cloud closer to being the single pane of glass for all of your cloud security efforts. Enforce your security policy from the top down It's a security basic to know and make sure your workloads are secure, and it starts with having tailored security policies in place. Because policies in Defender for Cloud are built on top of Azure Policy controls, you're getting the full range and flexibility of a world-class policy solution. In Defender for Cloud, you can set your policies to run on management groups, across subscriptions, and even for a whole tenant. Defender for Cloud continuously discovers new resources that are being deployed across your workloads and assesses whether they're configured according to security best practices. If not, they're flagged, and you get a prioritized list of recommendations for what you need to fix. Recommendations help you reduce the attack surface across each of your resources. The list of recommendations is enabled and supported by the Microsoft cloud security benchmark. This Microsoft-authored benchmark, based on common compliance frameworks, began with Azure and now provides a set of guidelines for security and compliance best practices for multiple cloud environments. In this way, Defender for Cloud enables you not just to set security policies but to apply secure configuration standards across your resources. Extend Defender for Cloud with Defender plans and external monitoring You can extend the Defender for Cloud protection with the following: Advanced threat protection features for virtual machines, Structured Query Language SQL databases, containers, web applications, your network, and more - Protections include securing the management ports of your VMs with just-in-time access and adaptive application controls to create allowlists for what apps should and shouldn't run on your machines. The Defender plans of Microsoft Defender for Cloud offer comprehensive defenses for the compute, data, and service layers of your environment: Microsoft Defender for Servers Microsoft Defender for Storage Microsoft Defender for Structured Query Language (SQL) Microsoft Defender for Containers Microsoft Defender for App Service Microsoft Defender for Key Vault Microsoft Defender for Resource Manager Microsoft Defender for Domain Name System (DNS) Microsoft Defender for open-source relational databases Microsoft Defender for Azure Cosmos Database (DB) Defender Cloud Security Posture Management (CSPM) Security governance and regulatory compliance Cloud security explorer Attack path analysis Agentless scanning for machines Defender for DevOps Use the advanced protection tiles in the workload protections Azure dashboard to monitor and configure each of these protections. Microsoft Defender for the Internet of Things (IoT) is a separate product. Security alerts - When Defender for Cloud detects a threat in any area of your environment, it generates a security alert. These alerts describe details of the affected resources, suggested remediation steps, and in some cases, an option to trigger a logic app in response. Whether an alert is generated by Defender for Cloud or received by Defender for Cloud from an integrated security product, you can export it. To export your alerts to Microsoft Sentinel, any third-party Security information and event management (SIEM), or any other external tool, follow the instructions in Stream alerts to a SIEM, Security orchestration, automation and response (SOAR), or IT Service Management solution. Defender for Cloud's threat protection includes fusion kill-chain analysis, which automatically correlates alerts in your environment based on cyber kill-chain analysis, to help you better understand the full story of an attack campaign, where it started, and what kind of impact it had on your resources. Defender for Cloud's supported kill chain intents are based on version 9 of the MITRE ATT&CK matrix. Security posture Cloud Security Posture Management (CSPM) - Remediate security issues and watch your security posture improve In Defender for Cloud, the posture management features provide the following: Hardening guidance - to help you efficiently and effectively improve your security Visibility - to help you understand your current security situation Defender for Cloud continually assesses your resources, subscriptions, and organization for security issues and shows your security posture in the secure score, an aggregated score of the security findings that tells you, at a glance, your current security situation: the higher the score, the lower the identified risk level. As soon as you open Defender for Cloud for the first time, Defender for Cloud: Generates a secure score for your subscriptions based on an assessment of your connected resources compared with the guidance in the Microsoft cloud security benchmark. Use the score to understand your security posture and the compliance dashboard to review your compliance with the built-in benchmark. When you've enabled the enhanced security features, you can customize the standards used to assess your compliance and add other regulations, such as the National Institute of Standards and Technology (NIST) and Azure Center for Internet Security (CIS) or organization-specific security requirements. You can also apply recommendations and score based on the AWS Foundational Security Best practices standards. Provides hardening recommendations based on any identified security misconfigurations and weaknesses. Use these security recommendations to strengthen the security posture of your organization's Azure, hybrid, and multicloud resources. Analyzes and secure's your attack paths through the cloud security graph, which is a graph-based context engine that exists within Defender for Cloud. The cloud security graph collects data from your multicloud environment and other data sources. For example, the cloud assets inventory, connections and lateral movement possibilities between resources, exposure to the internet, permissions, network connections, vulnerabilities, and more. The data collected is then used to build a graph representing your multicloud environment. Attack path analysis is a graph-based algorithm that scans the cloud security graph. The scans expose exploitable paths attackers may use to breach your environment to reach your high-impact assets. Attack path analysis exposes those attack paths and suggests recommendations as to how best to remediate the issues that will break the attack path and prevent a successful breach. By taking your environment's contextual information into account, such as internet exposure, permissions, lateral movement, and more. Attack path analysis identifies issues that may lead to a breach in your environment and helps you to remediate the highest risk ones first. Additional background and details National Institute of Standards and Technology (NIST) The National Institute of Standards and Technology (NIST) promotes and maintains measurement standards and guidance to help organizations assess risk. In response to Executive Order 13636 on strengthening the cybersecurity of federal networks and critical infrastructure, NIST released the Framework for Improving Critical Infrastructure Cybersecurity (FICIC) in February 2014. The main priorities of the FICIC were to establish a set of standards and practices to help organizations manage cybersecurity risk, while enabling business efficiency. The NIST Framework addresses cybersecurity risk without imposing additional regulatory requirements for both government and private sector organizations. The FICIC references globally recognized standards including NIST Special Publication 800-53 found in Appendix A of the NIST's Framework for Improving Critical Infrastructure Cybersecurity. Each control within the FICIC framework is mapped to corresponding NIST 800-53 controls within the FedRAMP Moderate Baseline. Center for Internet Security (CIS) The Center for Internet Security is a nonprofit entity whose mission is to identify, develop, validate, promote, and sustain best practice solutions for cyberdefense. It draws on the expertise of cybersecurity and IT professionals from government, business, and academia from around the world. To develop standards and best practices, including CIS benchmarks, controls, and hardened images, they follow a consensus decision-making model. CIS benchmarks are configuration baselines and best practices for securely configuring a system. Each of the guidance recommendations references one or more CIS controls that were developed to help organizations improve their cyberdefense capabilities. CIS controls map to many established standards and regulatory frameworks, including the NIST Cybersecurity Framework (CSF) and NIST SP 800-53, the International Organization for Standardization (ISO) 27000 series of standards, Payment Card Industry (PCI) Data Security Standards (DSS), Health Insurance Portability and Accountability Act of 1996 (HIPAA), and others. Each benchmark undergoes two phases of consensus review. The first occurs during initial development when experts convene to discuss, create, and test working drafts until they reach consensus on the benchmark. During the second phase, after the benchmark has been published, the consensus team reviews the feedback from the internet community for incorporation into the benchmark. The Center for Internet Security (CIS) benchmarks provide two levels of security settings: Level 1 recommends essential basic security requirements that can be configured on any system and should cause little or no interruption of service or reduced functionality. Level 2 recommends security settings for environments requiring greater security that could result in some reduced functionality. CIS Hardened Images are securely configured virtual machine images based on CIS Benchmarks hardened to either a Level 1 or Level 2 CIS benchmark profile. Hardening is a process that helps protect against unauthorized access, denial of service, and other cyberthreats by limiting potential weaknesses that make systems vulnerable to cyberattacks. Workload protections Defender for Cloud offers security alerts that are powered by Microsoft Threat Intelligence. It also includes a range of advanced, intelligent protections for your workloads. The workload protections are provided through Microsoft Defender plans specific to the types of resources in your subscriptions. For example, you can enable Microsoft Defender for Storage to get alerted about suspicious activities related to your storage resources. The Cloud workload dashboard includes the following sections: Microsoft Defender for Cloud coverage - Here you can see the resources types in your subscription that are eligible for protection by Defender for Cloud. Wherever relevant, you can upgrade here as well. If you want to upgrade all possible eligible resources, select Upgrade all. Security alerts - When Defender for Cloud detects a threat in any area of your environment, it generates an alert. These alerts describe details of the affected resources, suggested remediation steps, and in some cases an option to trigger a logic app in response. Selecting anywhere in this graph opens the Security alerts page. Advanced protection - Defender for Cloud includes many advanced threat protection capabilities for virtual machines, Structured Query Language (SQL) databases, containers, web applications, your network, and more. In this advanced protection section, you can see the status of the resources in your selected subscriptions for each of these protections. Select any of them to go directly to the configuration area for that protection type. Insights - This rolling pane of news, suggested reading, and high priority alerts gives Defender for Cloud's insights into pressing security matters that are relevant to you and your subscription. Whether it's a list of high severity Common Vulnerabilities and Exposures (CVEs) discovered on your VMs by a vulnerability analysis tool, or a new blog post by a member of the Defender for Cloud team, you'll find it here in the Insights panel. Deploy Microsoft Defender for Cloud Basic and enhanced security features Basic features When you open Defender for Cloud in the Azure portal for the first time or if you enable it through the Application Programming Interface (API), Defender for Cloud is enabled for free on all your Azure subscriptions. Defender for Cloud provides foundational cloud security and posture management (CSPM) features by default. The foundational CSPM includes a secure score, security policy and basic recommendations, and network security assessment to help you protect your Azure resources. Defender Cloud Security Posture Management (CSPM) plan options Defender for cloud offers foundational multicloud CSPM capabilities for free. These capabilities are automatically enabled by default on any subscription or account that has been onboarded to Defender for Cloud. The foundational CSPM includes asset discovery, continuous assessment and security recommendations for posture hardening, compliance with Microsoft Cloud Security Benchmark (MCSB), and a Secure score which measure the current status of your organization\u2019s posture. The optional Defender CSPM plan provides advanced posture management capabilities such as Attack path analysis, Cloud security explorer, advanced threat hunting, security governance capabilities, and also tools to assess your security compliance with a wide range of benchmarks, regulatory standards, and any custom security policies required in your organization, industry, or region. The following table summarizes each plan and its cloud availability. Enhanced features If you want to try out the enhanced security features, enable enhanced security features for free for the first 30 days. At the end of 30 days, if you decide to continue using the service, we'll automatically start charging for usage. Prerequisites To get started with Defender for Cloud, you'll need a Microsoft Azure subscription with Defender for Cloud enabled. If you don't have an Azure subscription, you can sign up for a free subscription. You can enable Microsoft Defender for Storage accounts at either the subscription level or resource level. You can enable Microsoft Defender for SQL (Structured Query Language) at either the subscription level or resource level. You can enable Microsoft Defender for open-source relational databases at the resource level only. The Microsoft Defender plans available at the workspace level are Microsoft Defender for Servers and Microsoft Defender for SQL servers on machines. When you enabled Defender plans on an entire Azure subscription, the protections are inherited by all resources in the subscription. Microsoft Defender for Cloud uses monitoring components to collect data from your resources. These extensions are automatically deployed when you turn on a Defender plan. Each Defender plan has its own requirements for monitoring components, so it's important that the required extensions are deployed to your resources to get all of the benefits of each plan. The Defender plans show you the monitoring coverage for each Defender plan. If the monitoring coverage is Full, all of the necessary extensions are installed. If the monitoring coverage is Partial, the information tooltip tells you what extensions are missing. For some plans, you can configure specific monitoring settings. Enhanced features When you enable the enhanced security features (paid), Defender for Cloud can provide unified security management and threat protection across your hybrid cloud workloads, including: Microsoft Defender for Endpoint - Microsoft Defender for Servers includes Microsoft Defender for Endpoint for comprehensive endpoint detection and response (EDR). Vulnerability assessment for virtual machines, container registries, and SQL resources - Easily enable vulnerability assessment solutions to discover, manage, and resolve vulnerabilities. View, investigate, and remediate the findings directly from within Defender for Cloud. Multicloud security - Connect your accounts from Amazon Web Services (AWS) and Google Cloud Platform (GCP) to protect resources and workloads on those platforms with a range of Microsoft Defender for Cloud security features. Hybrid security \u2013 Get a unified view of security across all of your on-premises and cloud workloads. Apply security policies and continuously assess the security of your hybrid cloud workloads to ensure compliance with security standards. Collect, search, and analyze security data from multiple sources, including firewalls and other partner solutions. Threat protection alerts - Advanced behavioral analytics and the Microsoft Intelligent Security Graph provide an edge over evolving cyber-attacks. Built-in behavioral analytics and machine learning can identify attacks and zero-day exploits. Monitor networks, machines, data stores (SQL servers hosted inside and outside Azure, Azure SQL databases, Azure SQL Managed Instance, and Azure Storage), and cloud services for incoming attacks and post-breach activity. Streamline investigation with interactive tools and contextual threat intelligence. Track compliance with a range of standards - Defender for Cloud continuously assesses your hybrid cloud environment to analyze the risk factors according to the controls and best practices in the Microsoft cloud security benchmark. When you enable enhanced security features, you can apply a range of other industry standards, regulatory standards, and benchmarks according to your organization's needs. Add standards and track your compliance with them from the regulatory compliance dashboard. Access and application controls - Block malware and other unwanted applications by applying machine learning-powered recommendations adapted to your specific workloads to create allowlists and blocklists. Reduce the network attack surface with just-in-time, controlled access to management ports on Azure VMs. Access and application control drastically reduce exposure to brute force and other network attacks. Container security features - Benefit from vulnerability management and real-time threat protection in your containerized environments. Charges are based on the number of unique container images pushed to your connected registry. After an image has been scanned once, you won't be charged for it again unless it's modified and pushed once more. Breadth threat protection for resources connected to Azure - Cloud-native threat protection for the Azure services common to all of your resources: Azure Resource Manager, Azure Domain Name System (DNS), Azure network layer, and Azure Key Vault. Defender for Cloud has unique visibility into the Azure management layer and the Azure DNS layer and can therefore protect cloud resources that are connected to those layers. Manage your Cloud Security Posture Management (CSPM) - CSPM offers you the ability to remediate security issues and review your security posture through the tools provided. These tools include: - Security governance and regulatory compliance - What is Security governance and regulatory compliance? Security governance and regulatory compliance refer to the policies and processes which organizations have in place to ensure that they comply with laws, rules, and regulations put in place by external bodies (government) that control activity in a given jurisdiction. Defender for Cloud allows you to view your regulatory compliance through the regulatory compliance dashboard. Defender for Cloud continuously assesses your hybrid cloud environment to analyze the risk factors according to the controls and best practices in the standards you've applied to your subscriptions. The dashboard reflects the status of your compliance with these standards. - Cloud security graph - What is a cloud security graph? The cloud security graph is a graph-based context engine that exists within Defender for Cloud. The cloud security graph collects data from your multicloud environment and other data sources. For example, the cloud assets inventory, connections and lateral movement possibilities between resources, exposure to the internet, permissions, network connections, vulnerabilities, and more. The data collected is then used to build a graph representing your multicloud environment. Defender for Cloud then uses the generated graph to perform an attack path analysis and find the issues with the highest risk that exist within your environment. You can also query the graph using the cloud security explorer. - Attack path analysis - What is Attack path analysis? Attack path analysis helps you to address the security issues that pose immediate threats with the greatest potential of being exploited in your environment. Defender for Cloud analyzes which security issues are part of potential attack paths that attackers could use to breach your environment. It also highlights the security recommendations that need to be resolved in order to mitigate the issue. - Agentless scanning for machines - What is agentless scanning for machines? Microsoft Defender for Cloud maximizes coverage on OS posture issues and extends beyond the reach of agent-based assessments. With agentless scanning for VMs, you can get frictionless, wide, and instant visibility on actionable posture issues without installed agents, network connectivity requirements, or machine performance impact. Agentless scanning for VMs provides vulnerability assessment and software inventory powered by Defender vulnerability management in Azure and Amazon AWS environments. Agentless scanning is available in Defender Cloud Security Posture Management (CSPM) and Defender for Servers. Azure Arc Today, companies struggle to control and govern increasingly complex environments that extend across data centers, multiple clouds, and edge. Each environment and cloud possesses its own set of management tools, and new DevOps and IT operations (ITOps) operational models can be hard to implement across resources. Azure Arc simplifies governance and management by delivering a consistent multicloud and on-premises management platform. Azure Arc provides a centralized, unified way to: Manage your entire environment together by projecting your existing non-Azure and/or on-premises resources into Azure Resource Manager. Manage virtual machines, Kubernetes clusters, and databases as if they are running in Azure. Use familiar Azure services and management capabilities, regardless of where they live. Continue using traditional IT operations (ITOps) while introducing DevOps practices to support new cloud-native patterns in your environment. Configure custom locations as an abstraction layer on top of Azure Arc-enabled Kubernetes clusters and cluster extensions. Azure Arc capabilities Currently, Azure Arc allows you to manage the following resource types hosted outside of Azure: Servers: Manage Windows and Linux physical servers and virtual machines hosted outside of Azure. Kubernetes clusters: Attach and configure Kubernetes clusters running anywhere with multiple supported distributions. Azure data services: Run Azure data services on-premises, at the edge, and in public clouds using Kubernetes and the infrastructure of your choice. SQL Managed Instance and PostgreSQL server (preview) services are currently available. SQL Server: Extend Azure services to SQL Server instances hosted outside of Azure. Virtual machines (preview): Provision, resize, delete, and manage virtual machines based on VMware vSphere or Azure Stack hyper-converged infrastructure (HCI) and enable VM self-service through role-based access. Key features and benefits Some of the key scenarios that Azure Arc supports are: Implement consistent inventory, management, governance, and security for servers across your environment. Configure Azure VM extensions to use Azure management services to monitor, secure, and update your servers. Manage and govern Kubernetes clusters at scale. Use GitOps to deploy configuration across one or more clusters from Git repositories. Zero-touch compliance and configuration for Kubernetes clusters using Azure Policy. Run Azure data services on any Kubernetes environment as if it runs in Azure (specifically Azure SQL Managed Instance and Azure Database for PostgreSQL server, with benefits such as upgrades, updates, security, and monitoring). Use elastic scale and apply updates without any application downtime, even without continuous connection to Azure. Create custom locations on top of your Azure Arc-enabled Kubernetes clusters, using them as target locations for deploying Azure services instances. Deploy your Azure service cluster extensions for Azure Arc-enabled data services, App services on Azure Arc (including web, function, and logic apps), and Event Grid on Kubernetes. Perform virtual machine lifecycle and management operations for VMware vSphere and Azure Stack hyper-converged infrastructure (HCI) environments. A unified experience viewing your Azure Arc-enabled resources, whether you are using the Azure portal, the Azure CLI, Azure PowerShell, or Azure REST API. Microsoft cloud security benchmark The Microsoft cloud security benchmark (MCSB) provides prescriptive best practices and recommendations to help improve the security of workloads, data, and services on Azure and your multi-cloud environment, focusing on cloud-centric control areas with input from a set of holistic Microsoft and industry security guidance that includes: Cloud Adoption Framework: Guidance on security, including strategy, roles and responsibilities, Azure Top 10 Security Best Practices, and reference implementation. Azure Well-Architected Framework: Guidance on securing your workloads on Azure. The Chief Information Security Officer (CISO) Workshop: Program guidance and reference strategies to accelerate security modernization using Zero Trust principles. Other industry and cloud service provider's security best practice standards and framework: Examples include the Amazon Web Services (AWS) Well-Architected Framework, Center for Internet Security (CIS) Controls, National Institute of Standards and Technology (NIST), and Payment Card Industry Data Security Standard (PCI-DSS). Microsoft cloud security benchmark features Comprehensive multi-cloud security framework: Organizations often have to build an internal security standard to reconcile security controls across multiple cloud platforms to meet security and compliance requirements on each of them. This often requires security teams to repeat the same implementation, monitoring, and assessment across the different cloud environments (often for different compliance standards). This creates unnecessary overhead, cost, and effort. To address this concern, we enhanced the Azure Security Benchmark (ASB) to the Microsoft cloud security benchmark (MCSB) to help you quickly work with different clouds by: Providing a single control framework to easily meet the security controls across clouds Providing consistent user experience for monitoring and enforcing the multi-cloud security benchmark in Defender for Cloud Staying aligned with Industry Standards (e.g., Center for Internet Security, National Institute of Standards and Technology, Payment Card Industry) Automated control monitoring for AWS in Microsoft Defender for Cloud: You can use Microsoft Defender for Cloud Regulatory Compliance Dashboard to monitor your AWS environment against Microsoft cloud security benchmark (MCSB), just like how you monitor your Azure environment. We developed approximately 180 AWS checks for the new AWS security guidance in MCSB, allowing you to monitor your AWS environment and resources in Microsoft Defender for Cloud. Example: Microsoft Defender for Cloud - Regulatory compliance dashboard Azure guidance and security principles: Azure security guidance, security principles, features, and capabilities. Controls Control Domains Description Network security (NS) Network Security covers controls to secure and protect networks, including securing virtual networks, establishing private connections, preventing and mitigating external attacks, and securing Domain Name System (DNS). Identity Management (IM) Identity Management covers controls to establish a secure identity and access controls using identity and access management systems, including the use of single sign-on, strong authentications, managed identities (and service principals) for applications, conditional access, and account anomalies monitoring. Privileged Access (PA) Privileged Access covers controls to protect privileged access to your tenant and resources, including a range of controls to protect your administrative model, administrative accounts, and privileged access workstations against deliberate and inadvertent risk. Data Protection (DP) Data Protection covers control of data protection at rest, in transit, and via authorized access mechanisms, including discover, classify, protect, and monitoring sensitive data assets using access control, encryption, key management, and certificate management. Asset Management (AM) Asset Management covers controls to ensure security visibility and governance over your resources, including recommendations on permissions for security personnel, security access to asset inventory and managing approvals for services and resources (inventory, track, and correct). Logging and Threat Detection (LT) Logging and Threat Detection covers controls for detecting threats on the cloud and enabling, collecting, and storing audit logs for cloud services, including enabling detection, investigation, and remediation processes with controls to generate high-quality alerts with native threat detection in cloud services; it also includes collecting logs with a cloud monitoring service, centralizing security analysis with a security event management (SEM), time synchronization, and log retention. Incident Response (IR) Incident Response covers controls in the incident response life cycle - preparation, detection and analysis, containment, and post-incident activities, including using Azure services (such as Microsoft Defender for Cloud and Sentinel) and/or other cloud services to automate the incident response process. Posture and Vulnerability Management (PV) Posture and Vulnerability Management focuses on controls for assessing and improving the cloud security posture, including vulnerability scanning, penetration testing, and remediation, as well as security configuration tracking, reporting, and correction in cloud resources. Endpoint Security (ES) Endpoint Security covers controls in endpoint detection and response, including the use of endpoint detection and response (EDR) and anti-malware service for endpoints in cloud environments. Backup and Recovery (BR) Backup and Recovery covers controls to ensure that data and configuration backups at the different service tiers are performed, validated, and protected. DevOps Security (DS) DevOps Security covers the controls related to the security engineering and operations in the DevOps processes, including deployment of critical security checks (such as static application security testing and vulnerability management) prior to the deployment phase to ensure the security throughout the DevOps process; it also includes common topics such as threat modeling and software supply security. Governance and Strategy (GS) Governance and Strategy provides guidance for ensuring a coherent security strategy and documented governance approach to guide and sustain security assurance, including establishing roles and responsibilities for the different cloud security functions, unified technical strategy, and supporting policies and standards. Configure Microsoft Defender for Cloud policies What are security policies, initiatives Microsoft Defender for Cloud applies security initiatives to your subscriptions. These initiatives contain one or more security policies. Each of those policies results in a security recommendation for improving your security posture. What is a security initiative? A security initiative is a collection of Azure Policy definitions or rules that are grouped together towards a specific goal or purpose. Security initiatives simplify the management of your policies by grouping a set of policies together, logically, as a single item. A security initiative defines the desired configuration of your workloads and helps ensure you comply with the security requirements of your company or regulators. Like security policies, Defender for Cloud initiatives are also created in Azure Policy. You can use Azure Policy to manage your policies, build initiatives, and assign initiatives to multiple subscriptions or entire management groups. The default initiative automatically assigned to every subscription in Microsoft Defender for Cloud is the Microsoft cloud security benchmark. This benchmark is the Microsoft-authored set of guidelines for security and compliance best practices based on common compliance frameworks. This widely respected benchmark builds on the controls from the Center for Internet Security (CIS) and the National Institute of Standards and Technology (NIST) with a focus on cloud-centric security. Defender for Cloud offers the following options for working with security initiatives and policies: View and edit the built-in default initiative - When you enable Defender for Cloud, the initiative named 'Microsoft cloud security benchmark' is automatically assigned to all Defender for Cloud registered subscriptions. To customize this initiative, you can enable or disable individual policies within it by editing a policy's parameters. Add your own custom initiatives - If you want to customize the security initiatives applied to your subscription, you can do so within Defender for Cloud. You'll then receive recommendations if your machines don't follow the policies you create. Add regulatory compliance standards as initiatives - Defender for Cloud's regulatory compliance dashboard shows the status of all the assessments within your environment in the context of a particular standard or regulation (such as Azure Center for Internet Security (CIS), National Institute of Standards and Technology (NIST) Special Publications (SP) SP 800-53 Rev.4, Swift\u2019s Customer Security Program (CSP) Call Session Control Function (CSCF) v2020). Example: Builtin security initiative What is a security policy? An Azure Policy definition, created in Azure Policy, is a rule about specific security conditions you want to be controlled. Built-in definitions include things like controlling what type of resources can be deployed or enforcing the use of tags on all resources. You can also create your own custom policy definitions. To implement these policy definitions (whether built-in or custom), you'll need to assign them. You can assign any of these policies through the Azure portal, PowerShell, or Azure CLI. Policies can be disabled or enabled from Azure Policy. There are different types of policies in Azure Policy. Defender for Cloud mainly uses 'Audit' policies that check specific conditions and configurations and then report on compliance. There are also \"Enforce' policies that can be used to apply security settings. Example: Built-in security policy View and edit security policies Defender for Cloud uses Azure role-based access control (Azure RBAC), which provides built-in roles you can assign to Azure users, groups, and services. When users open Defender for Cloud, they see only information related to the resources they can access. Users are assigned the owner, contributor, or reader role to the resource's subscription. There are two specific roles for Defender for Cloud: Security Administrator: Has the same view rights as security reader. Can also update the security policy and dismiss alerts. Security reader: Has rights to view Defender for Cloud items such as recommendations, alerts, policy, and health. Can't make changes. You can edit security policies through the Azure Policy portal via Representational State Transfer Application Programming Interface (REST API) or using Windows PowerShell. The Security Policy screen reflects the action taken by the policies assigned to the subscription or management group you selected. Use the links at the top to open a policy assignment that applies to the subscription or management group. These links let you access the assignment and edit or disable the policy. For example, if you see that a particular policy assignment is effectively denying endpoint protection, use the link to edit or disable the policy. In the list of policies, you can see the effective application of the policy on your subscription or management group. The settings of each policy that apply to the scope are taken into consideration, and the cumulative outcome of actions taken by the policy is shown. For example, if one assignment of the policy is disabled, but in another, it's set to AuditIfNotExist, then the cumulative effect applies AuditIfNotExist. The more active effect always takes precedence. The policies' effect can be: Append, Audit, AuditIfNotExists, Deny, DeployIfNotExists, or Disabled. Manage and implement Microsoft Defender for Cloud recommendations What is a security recommendation? Using the policies, Defender for Cloud periodically analyzes the compliance status of your resources to identify potential security misconfigurations and weaknesses. It then provides you with recommendations on how to remediate those issues. Recommendations result from assessing your resources against the relevant policies and identifying resources that aren't meeting your defined requirements. Defender for Cloud makes its security recommendations based on your chosen initiatives. When a policy from your initiative is compared against your resources and finds one or more that isn't compliant, it's presented as a recommendation in Defender for Cloud. Example: Microsoft Defender for Cloud - All recommendations Recommendations are actions for you to take to secure and harden your resources. Each recommendation provides you with the following information: A short description of the issue The remediation steps to carry out in order to implement the recommendation The affected resources In practice, it works like this: Microsoft Cloud security benchmark is an initiative that contains requirements. For example, Azure Storage accounts must restrict network access to reduce their attack surface. The initiative includes multiple policies, each requiring a specific resource type. These policies enforce the requirements in the initiative. To continue the example, the storage requirement is enforced with the policy \"Storage accounts should restrict network access using virtual network rules.\" Microsoft Defender for Cloud continually assesses your connected subscriptions. If it finds a resource that doesn't satisfy a policy, it displays a recommendation to fix that situation and harden the security of resources that aren't meeting your security requirements. For example, if an Azure Storage account on your protected subscriptions isn't protected with virtual network rules, you see the recommendation to harden those resources. So, (1) an initiative includes (2) policies that generate (3) environment-specific recommendations. Security recommendation details Security recommendations contain details that help you understand its significance and how to handle it. The recommendation details shown are: For supported recommendations, the top toolbar shows any or all of the following buttons: Enforce and Deny View the policy definition to go directly to the Azure Policy entry for the underlying policy. Open query - You can view the detailed information about the affected resources using Azure Resource Graph Explorer. Severity indicator Freshness interval Count of exempted resources if exemptions exist for a recommendation; this shows the number of resources that have been exempted with a link to view the specific resources. Mapping to MITRE ATT&CK tactics and techniques if a recommendation has defined tactics and techniques, select the icon for links to the relevant pages on MITRE's site. This applies only to Azure-scored recommendations. 6. Description - A short description of the security issue. 7. When relevant, the details page also includes a table of related recommendations: The relationship types are: - Prerequisite - A recommendation that must be completed before the selected recommendation - Alternative - A different recommendation that provides another way of achieving the goals of the selected recommendation - Dependent - A recommendation for which the selected recommendation is a prerequisite For each related recommendation, the number of unhealthy resources is shown in the \"Affected resources\" column. 8. Remediation steps - A description of the manual steps required to remediate the security issue on the affected resources. For recommendations with the Fix option, you can select View remediation logic before applying the suggested fix to your resources. 9. Affected resources - Your resources are grouped into tabs: - Healthy resources \u2013 Relevant resources that either aren't impacted or on which you have already remediated the issue. - Unhealthy resources \u2013 Resources that are still impacted by the identified issue. - Not applicable resources \u2013 Resources for which the recommendation can't give a definitive answer. The not applicable tab also includes reasons for each resource. 10. Action buttons to remediate the recommendation or trigger a logic app. Viewing the relationship between a recommendation and a policy As mentioned above, Defender for Cloud's built-in recommendations are based on the Microsoft cloud security benchmark. Almost every recommendation has an underlying policy that is derived from a requirement in the benchmark. When you're reviewing the details of a recommendation, it's often helpful to be able to see the underlying policy. For every recommendation supported by a policy, use the View policy definition link from the recommendation details page to go directly to the Azure Policy entry for the relevant policy: Use this link to view the policy definition and review the evaluation logic. If you're reviewing the list of recommendations on our Security recommendations reference guide, you'll also see links to the policy definition pages: Explore secure score Overview of secure score Microsoft Defender for Cloud has two main goals: to help you understand your current security situation to help you efficiently and effectively improve your security The central feature in Defender for Cloud that enables you to achieve those goals is the secure score. Defender for Cloud continually assesses your cross-cloud resources for security issues. It then aggregates all the findings into a single score so that you can tell, at a glance, your current security situation: the higher the score, the lower the identified risk level. In the Azure portal pages, the secure score is shown as a percentage value, and the underlying values are also clearly presented: In the Azure mobile app, the secure score is shown as a percentage value, and you can tap the secure score to see the details that explain the score: To increase your security, review Defender for Cloud's recommendations page and remediate the recommendation by implementing the remediation instructions for each issue. Recommendations are grouped into security controls. Each control is a logical group of related security recommendations and reflects your vulnerable attack surfaces. Your score only improves when you remediate all of the recommendations for a single resource within a control. To see how well your organization is securing each individual attack surface, review the scores for each security control. How your secure score is calculated To get all the possible points for security control, all of your resources must comply with all of the security recommendations within the security control. For example, Defender for Cloud has multiple recommendations regarding how to secure your management ports. You'll need to remediate them all to make a difference to your secure score. Example scores for a control In this example: Remediate vulnerabilities security control - This control group has multiple recommendations related to discovering and resolving known vulnerabilities. Max score - The maximum number of points you can gain by completing all recommendations within a control. The maximum score for a control indicates the relative significance of that control and is fixed for every environment. Use the max score values to triage the issues to work on first. Current score - The current score for this control. Current score = [Score per resource] * [Number of healthy resources] Each control contributes towards the total score. In this example, the control is contributing 2.00 points to the current total secure score. Potential score increase - The remaining points available to you are within your control. If you remediate all the recommendations in this control, your score will increase by 9%. Potential score increase = [Score per resource] * [Number of unhealthy resources] Insights - Gives you extra details for each recommendation, such as: Preview recommendation - This recommendation won't affect your secure score until it's GA. Fix - From within the recommendation details page, you can use 'Fix' to resolve this issue. Enforce - From within the recommendation details page, you can automatically deploy a policy to fix this issue whenever someone creates a non-compliant resource. Deny - From within the recommendation details page, you can prevent new resources from being created with this issue. Which recommendations are included in the secure score calculations? Only built-in recommendations have an impact on the secure score. Recommendations flagged as Preview aren't included in the calculations of your secure score. They should still be remediated wherever possible so that when the preview period ends, they'll contribute towards your score. Preview recommendations are marked with: Improve your secure score To improve your secure score, remediate security recommendations from your recommendations list. You can remediate each recommendation manually for each resource or use the Fix option (when available) to resolve an issue on multiple resources quickly. You can also configure the Enforce and Deny options on the relevant recommendations to improve your score and make sure your users don't create resources that negatively impact your score. Frequently asked questions (FAQ) Secure score If I address only three out of four recommendations in security control, will my secure score change? No. It won't change until you remediate all of the recommendations for a single resource. To get the maximum score for a control, you must remediate all recommendations for all resources. If a recommendation isn't applicable to me, and I disable it in the policy, will my security control be fulfilled and my secure score updated? Yes. We recommend disabling recommendations when they're inapplicable in your environment. If a security control offers me zero points toward my secure score, should I ignore it? In some cases, you'll see a control max score greater than zero, but the impact is zero. When the incremental score for fixing resources is negligible, it's rounded to zero. Don't ignore these recommendations because they still bring security improvements. The only exception is the \"Additional Best Practice\" control. Remediating these recommendations won't increase your score, but it will enhance your overall security. Define brute force attacks Brute force attacks A brute force attack is a type of hacking technique in which an attacker tries to gain access to a network or system by guessing the username and password combination through an automated process. The attacker typically uses a program that generates a large number of login attempts in a short period of time to try every possible combination of characters until the correct one is discovered. This type of attack can be very effective against weak passwords and security systems with no protection against brute force attacks, but it is time-consuming and can be detected by security measures such as account lockouts after a certain number of failed login attempts. Management services, ports, and protocols Adversaries without prior knowledge of legitimate credentials within the system or environment may guess passwords to attempt access to accounts. Without knowledge of the password for an account, an adversary may opt to systematically guess the password using a repetitive or iterative mechanism. An adversary may guess login credentials without prior knowledge of system or environment passwords during an operation by using a list of common passwords. Password guessing may or may not take into account the target's policies on password complexity or use policies that may lock accounts out after a number of failed attempts. Typically, management services over commonly used ports are used when guessing passwords. Commonly targeted services include the following: Brute force attack programs and use cases These programs can be used individually or in combination to launch a successful brute force attack on a target network or system. There are several types of brute force attack programs used by attackers, including: Indications of an attack Extreme counts of failed sign-ins from many unknown usernames Never previously successfully authenticated from multiple remote desktop protocol (RDP) connections or from new source IP addresses Example: Potential SQL Brute Force attempt alert Practices to blunt a Brute Force Attacks To counteract brute-force attacks, you can take multiple measures such as: Disable the public IP address and use one of these connection methods: Use a point-to-site virtual private network (VPN) Create a site-to-site VPN Use Azure ExpressRoute to create secure links from your on-premises network to Azure. Require two-factor authentication Increase password length and complexity Limit login attempts Implement Captcha About CAPTCHAs - Any time you let people register on your site or even enter a name and URL (like for a blog comment), you might get a flood of fake names. These are often left by automated programs (bots) that try to leave URLs on every website they can find. (A common motivation is to post the URLs of products for sale.) You can help make sure that a user is a real person and not a computer program by using a CAPTCHA to validate users when they register or otherwise enter their name and site. CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. A CAPTCHA is a challenge-response test in which the user is asked to do something that is easy for a person to do but hard for an automated program to do. The most common type of CAPTCHA is one where you see distorted letters and are asked to type them. (The distortion is supposed to make it hard for bots to decipher the letters.) Limit the amount of time that the ports are open. Understand just-in-time VM access The risk of open management ports on a virtual machine Threat actors actively hunt accessible machines with open management ports, like remote desktop protocol (RDP) or secure shell protocol (SSH). All of your virtual machines are potential targets for an attack. When a VM is successfully compromised, it's used as the entry point to attack further resources within your environment. Why JIT VM access is the solution As with all cybersecurity prevention techniques, your goal should be to reduce the attack surface. In this case, that means having fewer open ports, especially management ports. Your legitimate users also use these ports, so it's not practical to keep them closed. To solve this dilemma, Microsoft Defender for Cloud offers JIT. With JIT, you can lock down the inbound traffic to your VMs, reducing exposure to attacks while providing easy access to connect to VMs when needed. How JIT operates with network resources in Azure and AWS In Azure, you can block inbound traffic on specific ports, by enabling just-in-time VM access. Defender for Cloud ensures \"deny all inbound traffic\" rules exist for your selected ports in the network security group (NSG) and Azure Firewall rules. These rules restrict access to your Azure VMs\u2019 management ports and defend them from attack. If other rules already exist for the selected ports, then those existing rules take priority over the new \"deny all inbound traffic\" rules. If there are no existing rules on the selected ports, then the new rules take top priority in the NSG and Azure Firewall. In AWS, by enabling JIT-access the relevant rules in the attached Amazon Elastic Compute Cloud (Amazon EC2) security groups, for the selected ports, are revoked which blocks inbound traffic on those specific ports. When a user requests access to a VM, Defender for Cloud checks that the user has Azure role-based access control (Azure RBAC) permissions for that VM. If the request is approved, Defender for Cloud configures the NSGs and Azure Firewall to allow inbound traffic to the selected ports from the relevant IP address (or range), for the amount of time that was specified. In AWS, Defender for Cloud creates a new EC2 security group that allows inbound traffic to the specified ports. After the time has expired, Defender for Cloud restores the NSGs to their previous states. Connections that are already established are not interrupted. Just-in-time VM enabled on an Azure Virtual Machine. Example: Azure virtual machine The diagram shows the logic that Defender for Cloud applies when deciding how to categorize your supported VMs (i.e., Azure Virtual Machine) Just-in-time VM enabled on an AWS EC2 Instance. The diagram shows the logic that Defender for Cloud applies when deciding how to categorize your supported VMs (i.e., AWS EC2 instance) Example: AWS EC2 Instance Added to the recommendation\u2019s Unhealthy resources tab The diagram shows the logic Defender for Cloud applies when deciding how to categorize your supported VM. When Defender for Cloud finds a machine that can benefit from JIT, it adds that machine to the recommendation's Unhealthy resources tab. Example: Affected resources Implement just-in-time VM access Just-in-time (JIT) virtual machine (VM) access is used to lock down inbound traffic to your Azure VMs, reducing exposure to attacks while providing easy access to connect to VMs when needed. When you enable JIT VM Access for your VMs, you next create a policy that determines the ports to help protect, how long ports should remain open, and the approved IP addresses that can access these ports. The policy helps you stay in control of what users can do when they request access. Requests are logged in the Azure activity log, so you can easily monitor and audit access. The policy will also help you quickly identify the existing VMs that have JIT VM Access enabled and the VMs where JIT VM Access is recommended. How JIT VM Access works To use Just-in-Time VM access, you must enable Microsoft Defender for Cloud. After you enable Defender, you can view which virtual machines have JIT configured. Enable JIT on any virtual machine that is not Healthy. For each virtual machine, you are recommended specific ports and access. You can accept the recommendations or Add other ports of your choosing. Once everything is in place, users must request access to the virtual machine. You can also monitor the usage of each virtual machine. Perform try-this exercises Task 1: Microsoft Defender for Cloud overview and recommendations In this task, you will review Microsoft Defender for Cloud. In the Portal, navigate to Microsoft Defender for Cloud. Under General, select Overview. Discuss the Overview page. Under Management, select Environment settings. Select your subscription, and then review the Microsoft Defender for Cloud features. Return to the main Microsoft Defender for Cloud blade. Under General, select Recommendations. Review Secure Score, Recommendations status, and Resource Health. From the main Microsoft Defender for Cloud blade, under Cloud Security, select Regulatory compliance. Review the compliance assessment and available compliance controls. Scroll down and, under Microsoft cloud security benchmark, expand each compliance control to review several recommendations. For example, NS. Network Security, DP. Data Protection, and ES. Endpoint Security. From the main Microsoft Defender for Cloud blade, under Cloud Security, select Workload protections. Review the Defender for Cloud coverage, Security alerts, and Advanced protection features. Knowledge check Which tasks are not included in the Microsoft Defender for Cloud free tier? Monitor IoT hubs and resources Monitor network access and endpoint security Monitor non-Azure resources ( Ans ) An organization compliance group requires client authentication using Azure AD and Key Vault diagnostic logs. What is the easiest way to implement the requirement for client authentication? Configure management groups Implement Microsoft Defender for Cloud policies ( Ans ) Create Desired Configuration State scripts An organization is working with an outside agency that needs to access a virtual machine. There's a real concern about brute-force login attacks targeted at virtual machine management ports. Which of the following components would open the management ports for a defined time range? Select one. Azure Firewall Bastion service Just-in-Time virtual machine access ( Ans ) When using Microsoft Defender for Cloud to provide visibility into virtual machine security settings, the monitoring system will notify administrators as issues arise. Which incident below would require a different monitoring tool to discover it? A newer operating system version is available. ( Ans ) System security updates and critical updates that are missing. Disk encryption is applied on virtual machines. Summary Microsoft Defender for Cloud and Secure Score is the keys to keeping your solutions and data secure. You should be able to: - Define the most common types of cyber-attacks - Configure Microsoft Defender for Cloud based on your security posture - Review Secure Score and raise it - Lock down your solutions using Microsoft Defender for Cloud recommendations Configure and monitor Microsoft Sentinel Introduction In the world we live in today, there is always the chance that your system may be hacked. What do you do at this point to find and evaluate a breach? Microsoft Sentinel is a tool to help you locate a breach and track what the intruder did while on your systems. Scenario A security engineer uses Microsoft Sentinel to discover and track a breach, you will work on such tasks as: Configure Microsoft Sentinel. Build queries to find breaches. Track incidents with investigation and hunting. Enable Azure Sentinel Microsoft Sentinel is a scalable, cloud-native, security information event management (SIEM) and security orchestration automated response (SOAR) solution. Microsoft Sentinel delivers intelligent security analytics and threat intelligence across the enterprise, providing a single solution for alert detection, threat visibility, proactive hunting, and threat response. Value of a security information and event management tool Security Operations (SecOps) teams are inundated with a high volume of alerts and spend far too much time in tasks like infrastructure setup and maintenance. As a result, many legitimate threats go unnoticed. According to the \u201cCybersecurity Jobs Report, 2018-2021\u201d by Cybersecurity Ventures, An expected shortfall of 3.5M security professionals by 2021 will further increase the challenges for security operations teams. Alert fatigue is real. Security analysts face a huge burden of triage as they not only have to sift through a sea of alerts but also correlate alerts from different products manually or using a traditional correlation engine. Microsoft Azure Sentinel Microsoft Sentinel offers nearly limitless cloud scale and speed to address your security needs. Think of Microsoft Sentinel as the first SIEM-as-a-service that brings the power of the cloud and artificial intelligence to help security operations teams efficiently identify and stop cyber-attacks before they cause harm. Microsoft Sentinel enriches your investigation and detection by providing both Microsoft's threat intelligence stream and external threat intelligence streams. Microsoft Sentinel integrates with Microsoft 365 solution and correlates millions of signals from different products such as Azure Identity Protection, Microsoft Cloud App Security, and soon Azure Advanced Threat Protection, Windows Advanced Threat Protection, M365 Advanced Threat Protection, Intune, and Azure Information Protection. It enables the following services: - Collect data at cloud scale across all users, devices, applications, and infrastructure, both on-premises and in multiple clouds. - Detect previously undetected threats, and minimize false positives using Microsoft's analytics and unparalleled threat intelligence. - Investigate threats with artificial intelligence, and hunt for suspicious activities at scale, tapping into years of cyber security work at Microsoft. - Respond to incidents rapidly with built-in orchestration and automation of common tasks Building on the full range of existing Azure services, Microsoft Sentinel natively incorporates proven foundations, like Log Analytics, and Logic Apps. Microsoft Sentinel enriches your investigation and detection with AI, and provides Microsoft's threat intelligence stream, and enables you to bring your own threat intelligence. Configure data connections to Sentinel To onboard Microsoft Sentinel, you first need to connect to your security sources. Microsoft Sentinel comes with a number of connectors for Microsoft solutions, available out of the box and providing real-time integration, including Microsoft Threat Protection solutions, and Microsoft 365 sources, including Microsoft 365, Azure AD, Azure ATP, and Microsoft Cloud App Security, and more. In addition, there are built-in connectors to the broader security ecosystem for non-Microsoft solutions. You can also use common event format, Syslog or REST-API to connect your data sources with Microsoft Sentinel as well. Data connection methods The following data connection methods are supported by Microsoft Sentinel: Service to service integration: Some services are connected natively, such as AWS and Microsoft services, these services leverage the Azure foundation for out-of-the-box integration, the following solutions can be connected in a few clicks: Amazon Web Services - CloudTrail Azure Activity Azure AD audit logs and sign-ins Azure AD Identity Protection Azure Advanced Threat Protection Azure Information Protection Microsoft Defender for Cloud Cloud App SecurityCloud App Security Domain name server Microsoft 365 Microsoft Defender ATP Microsoft web application firewall Windows firewall Windows security events External solutions via API Some data sources are connected using APIs that are provided by the connected data source. Typically, most security technologies provide a set of APIs through which event logs can be retrieved. The APIs connect to Microsoft Sentinel and gather specific data types and send them to Azure Log Analytics External solutions via an agen Microsoft Sentinel can be connected to all other data sources that can perform real-time log streaming using the Syslog protocol, via an agent. The Microsoft Sentinel agent, which is based on the Log Analytics agent, converts CEF formatted logs into a format that can be ingested by Log Analytics. Depending on the appliance type, the agent is installed either directly on the appliance, or on a dedicated Linux server. Agent connection options To connect your external appliance to Microsoft Sentinel, the agent must be deployed on a dedicated machine (VM or on-premises) to support the communication between the appliance and Microsoft Sentinel. You can deploy the agent automatically or manually. Automatic deployment is only available if your dedicated machine is a new VM you are creating in Azure. Alternatively, you can deploy the agent manually on an existing Azure VM, on a VM in another cloud, or on an on-premises machine. Global prerequisites Active Azure Subscription Log Analytics workspace. To enable Microsoft Sentinel, you need contributor permissions to the subscription in which the Microsoft Sentinel workspace resides. To use Microsoft Sentinel, you need either contributor or reader permissions on the resource group that the workspace belongs to. Additional permissions may be needed to connect specific data sources. Microsoft Sentinel is a paid service. Create workbooks for explore Sentinel data After you connect your data sources to Microsoft Sentinel, you can monitor the data using the Microsoft Sentinel integration with Azure Monitor Workbooks, which provides versatility in creating custom workbooks. While Workbooks are displayed differently in Microsoft Sentinel, it may be helpful for you to determine how to create interactive reports with Azure Monitor Workbooks. Microsoft Sentinel allows you to create custom workbooks across your data and comes with built-in workbook templates to quickly gain insights across your data as soon as you connect a data source. Workbooks combine text, Analytics queries, Azure Metrics, and parameters into rich interactive reports. Workbooks are editable by other team members who have access to the same Azure resources. Workbooks are helpful for scenarios like: Exploring the usage of your app when you don't know the metrics of interest in advance: numbers of users, retention rates, conversion rates, etc. Unlike other usage analytics tools, workbooks let you combine multiple kinds of visualizations and analyses, making them great for this kind of free-form exploration. Explaining to your team how a newly released feature is performing by showing user counts for key interactions and other metrics. Sharing the results of an A/B experiment in your app with other members of your team. You can explain the goals for the experiment with text, then show each usage metric and Analytics query used to evaluate the experiment, along with clear call-outs for whether each metric was above- or below-target. Reporting the impact of an outage on the usage of your app, combining data, text explanation, and a discussion of next steps to prevent outages in the future. Saving and sharing workbooks with your team Workbooks are saved within an Application Insights resource, either in the My Reports section that's private to you or in the Shared Reports section accessible to everyone with access to the Application Insights resource. A workbook can be shared with a link or via email. Keep in mind that recipients of the link need access to this resource in the Azure portal to view the workbook. To make edits, recipients need at least Contributor permissions for the resource. Analytics To help you reduce noise and minimize the number of alerts you have to review and investigate, Microsoft Sentinel uses analytics to correlate alerts into incidents. Incidents are groups of related alerts that together create a possible actionable threat that you can investigate and resolve. Use the built-in correlation rules as-is, or use them as a starting point to build your own. Microsoft Sentinel also provides machine learning rules to map your network behavior and then look for anomalies across your resources. These analytics connect the dots by combining low-fidelity alerts about different entities into potential high-fidelity security incidents. Enable rules to create incidents Alerts triggered in Microsoft security solutions that are connected to Microsoft Sentinel, such as Microsoft Defender for Cloud Apps and Azure Advanced Threat Protection, do not automatically create incidents in Microsoft Sentinel. By default, when you connect a Microsoft solution to Microsoft Sentinel, any alert generated in that service will be stored as raw data in Microsoft Sentinel, in the Security Alert table in your Microsoft Sentinel workspace. You can then use that data like any other raw data you connect to Sentinel. Prerequisites You must connect Microsoft security solutions to enable incident creation from security service alerts. Using Microsoft Security incident creation analytic rules Use the built-in rules available in Microsoft Sentinel to choose which connected Microsoft security solutions should create Microsoft Sentinel incidents automatically in real-time. You can also edit the rules to define more specific options for filtering which of the alerts generated by the Microsoft security solution should create incidents in Microsoft Sentinel. For example, you can choose to create Microsoft Sentinel incidents automatically only from high-severity Microsoft Defender for Cloud alerts. You can create more than one Microsoft Security analytic rule per Microsoft security service type. This action does not create duplicate incidents since each rule is used as a filter. Even if an alert matches more than one Microsoft Security analytic rule, it creates just one Microsoft Sentinel incident. When you connect a Microsoft security solution, you can select whether you want the alerts from the security solution to automatically generate incidents in Microsoft Sentinel automatically. Configure playbooks Security automation and orchestration lets you automate your common tasks and simplify security orchestration with playbooks that integrate with Azure services as well as your existing tools. Built on the foundation of Azure Logic Apps, Azure Sentinel's automation and orchestration solution provides a highly-extensible architecture that enables scalable automation as new technologies and threats emerge. To build playbooks with Azure Logic Apps, you can choose from a growing gallery of built-in playbooks. These include 200+ connectors for services such as Azure functions. The connectors allow you to apply any custom logic in code, ServiceNow, Jira, Zendesk, HTTP requests, Microsoft Teams, Slack, Windows Defender ATP, and Cloud App Security. For example, if you use the ServiceNow ticketing system, you can use the tools provided to use Azure Logic Apps to automate your workflows and open a ticket in ServiceNow each time a particular event is detected. Hunt and investigate potential breaches Currently in preview, Microsoft Sentinel deep investigation tools help you to understand the scope and find the root cause, of a potential security threat. You can choose an entity on the interactive graph to ask interesting questions for a specific entity, and drill down into that entity and its connections to get to the root cause of the threat. An incident can include multiple alerts. It's an aggregation of all the relevant evidence for a specific investigation. An incident is created based on analytic rules that you created in the Analytics page. The properties related to the alerts, such as severity, and status, are set at the incident level. After you let Microsoft Sentinel know what kinds of threats you're looking for and how to find them, you can monitor detected threats by investigating incidents. Use the investigation graph to deep dive The investigation graph enables analysts to ask the right questions for each investigation. The investigation graph helps you understand the scope, and identify the root cause, of a potential security threat by correlating relevant data with any involved entity. You can dive deeper and investigate any entity presented in the graph by selecting it and choosing between different expansion options. The investigation graph provides you with: - Visual context from raw data: The live, visual graph displays entity relationships extracted automatically from the raw data. This enables you to easily view connections across different data sources. - Full investigation scope discovery: Expand your investigation scope using built-in exploration queries to surface the full scope of a breach. - Built-in investigation steps: Use predefined exploration options to make sure you are asking the right questions in the face of a threat. To use the investigation graph: Select an incident, then select Investigate. This takes you to the investigation graph. The graph provides an illustrative map of the entities directly connected to the alert and each resource connected further. You'll only be able to investigate the incident if you used the entity mapping fields when you set up your analytic rule. The investigation graph requires that your original incident includes entities. Hunting Use Microsoft's Sentinel's powerful hunting search-and-query tools, based on the MITRE framework, which enable you to proactively hunt for security threats across your organization\u2019s data sources, before an alert is triggered. After you discover which hunting query provides high-value insights into possible attacks, you can also create custom detection rules based on your query, and surface those insights as alerts to your security incident responders. While hunting, you can create bookmarks for interesting events, enabling you to return to them later, share them with others, and group them with other correlating events to create a compelling incident for investigation. For example, one built-in query provides data about the most uncommon processes running on your infrastructure. You may not want an alert each time they are run. With Microsoft Sentinel hunting, you can take advantage of the following capabilities: Built-in queries: To get you started, a starting page provides preloaded query examples designed to get you started and get you familiar with the tables and the query language. These built-in hunting queries are developed by Microsoft security researchers on a continuous basis, adding new queries, and fine-tuning existing queries to provide you with an entry point to look for new detections and figure out where to start hunting for the beginnings of new attacks. Powerful query language with IntelliSense: Built on top of a query language that gives you the flexibility you need to take hunting to the next level. Create your own bookmarks: During the hunting process, you may come across matches or findings, dashboards, or activities that look unusual or suspicious. In order to mark those items so you can come back to them in the future, use the bookmark functionality. Bookmarks let you save items for later, to be used to create an incident for investigation. Use notebooks to automate investigation: Notebooks are like step-by-step playbooks that you can build to walk through the steps of an investigation and hunt. Notebooks encapsulate all the hunting steps in a reusable playbook that can be shared with others in your organization. Query the stored data: The data is accessible in tables for you to query. For example, you can query process creation, DNS events, and many other event types. Links to the community: Leverage the power of the greater community to find additional queries and data sources. Community The Microsoft Sentinel community is a powerful resource for threat detection and automation. Our Microsoft security analysts constantly create and add new workbooks, playbooks, hunting queries, and more, posting them to the community for you to use in your environment. You can download sample content from the private community GitHub repository to create custom workbooks, hunting queries, notebooks, and playbooks for Microsoft Sentinel. Knowledge check Where can custom security alerts be created and managed? Azure Security Center Azure Sentinel ( Ans ) Azure Storage Which of the items below would exceed the capabilities of an Azure Sentinel playbook? A Sentinel playbook can help automate and orchestrate an incident response. A Sentinel playbook be run manually or set to run automatically when specific alerts are triggered. A Sentinel playbook be created to handle several subscriptions at once. ( Ans ) Sentinel is being used to investigate an incident. When viewing the incident detailed information, which value has to be assigned, instead of being included in the data? Incident ID Incident owner ( Ans ) Number of entities involved When creating roles within a security operations team to grant appropriate access to Azure Sentinel. Which role below would have to be created versus being built-in? Azure Sentinel reader Azure Sentinel responder Azure Sentinel owner ( Ans ) An investigator wants to be proactive about looking for security threats. The security officer has read about Sentinel\u2019s hunting capabilities and notebooks. What is an Azure Sentinel notebook? A step-by-step playbook that provides the ability to walk through the steps of an investigation and hunt. ( Ans ) A table to query and locate actions like DNS events. A saved item for the creation of an incident for investigation.","title":"Configure and manage Azure Monitor"},{"location":"Cloud/Azure/AZ-500/Secure-your-data-and-applications.html","text":"AZ-500: Microsoft Certified: Azure Security Engineer Associate # Secure your data and applications # Application running within Azure access your confidential data need to be locked down. Learn to secure your applications, storage, databases, and key vaults. This learning path helps prepare you for Exam AZ-500: Microsoft Azure Security Technologies . Deploy and secure Azure Key Vault Introduction Explore Azure Key Vault Configure Key Vault access Review a secure Key Vault example Deploy and manage Key Vault certificates Create Key Vault keys Manage customer managed keys Enable Key Vault secrets Configure key rotation Manage Key Vault safety and recovery features Perform Try-This exercises Explore the Azure Hardware Security Module Knowledge check Summary Configure application security features Introduction Review the Microsoft identity platform Explore Azure AD application scenarios Register an application with App Registration Configure Microsoft Graph permissions Enable managed identities Azure App Services App Service Environment Azure App Service plan App Service Environment networking Availability Zone Support for App Service Environments App Service Environment Certificates Perform Try-This exercises Knowledge check Summary Implement storage security Introduction Define data sovereignty Configure Azure storage access Deploy shared access signatures Manage Azure AD storage authentication Implement storage service encryption Configure blob data retention policies Configure Azure files authentication Enable the secure transfer required\u200b property Perform Try-This exercises Knowledge check Summary Configure and manage SQL database security Introduction Enable SQL database authentication Configure SQL database firewalls Enable and monitor database auditing Implement data discovery and classification Microsoft Defender for SQL Vulnerability assessment for SQL Server SQL Advanced Threat Protection Explore detection of a suspicious event SQL vulnerability assessment express and classic configurations Configure dynamic data masking Implement transparent data encryption Deploy always encrypted\u200b features Deploy an always encrypted implementation Perform Try-This exercises Knowledge check Summary Explore Azure Key Vault # Protecting your keys is essential to protecting your identity and data in the cloud. Azure Key Vault helps safeguard cryptographic keys and secrets that cloud applications and services use. Key Vault streamlines the key management process and enables you to maintain control of keys that access and encrypt your data. Developers can create keys for development and testing in minutes, and then migrate them to production keys. Security administrators can grant (and revoke) permission to keys, as needed. You can use Key Vault to create multiple secure containers, called vaults. Vaults help reduce the chances of accidental loss of security information by centralizing application secrets storage. Key vaults also control and log the access to anything stored in them. Azure Key Vault can manage requesting and renewing TLS certificates. It provides features for a robust solution for certificate lifecycle management. Azure Key Vault helps address the following issues: Secrets management. You can use Azure Key Vault to securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets. Key management. You use Azure Key Vault as a key management solution, making it easier to create and control the encryption keys used to encrypt your data. Certificate management. Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private SSL/TLS certificates for use with Azure and your internal connected resources. Store secrets backed by hardware security modules (HSMs). The secrets and keys can be protected either by software, or FIPS 140-2 Level 2 validates HSMs. Azure Key Vault is designed to support application keys and secrets. Key Vault is not intended as storage for user passwords. The following table lists security best practices for using Key Vault. Best practice Solution Grant access to users, groups, and applications at a specific scope. Use RBAC\u2019s predefined roles. For example, to grant access to a user to manage key vaults, you would assign the predefined role Key Vault Contributor to this user at a specific scope. The scope in this case would be a subscription, a resource group, or just a specific key vault. If the predefined roles don\u2019t fit your needs, you can define your own roles. Control what users have access to. Access to a key vault is controlled through two separate interfaces: management plane, and data plane. The management plane and data plane access controls work independently. Use RBAC to control what users have access to. For example, if you want to grant an application access to use keys in a key vault, you only need to grant data plane access permissions by using key vault access policies, and no management plane access is needed for this application. Conversely, if you want a user to be able to read vault properties and tags but not have any access to keys, secrets, or certificates, you can grant this user read access by using RBAC, and no access to the data plane is required. Store certificates in your key vault. Azure Resource Manager can securely deploy certificates stored in Azure Key Vault to Azure VMs when the VMs are deployed. By setting appropriate access policies for the key vault, you also control who gets access to your certificate. Another benefit is that you manage all your certificates in one place in Azure Key Vault. Ensure that you can recover a deletion of key vaults or key vault objects. Deletion of key vaults or key vault objects can be either inadvertent or malicious. Enable the soft delete and purge protection features of Key Vault, particularly for keys that are used to encrypt data at rest. Deletion of these keys is equivalent to data loss, so you can recover deleted vaults and vault objects if needed. Practice Key Vault recovery operations on a regular basis. Azure Key Vault is offered in two service tiers\u2014standard and premium The main difference between Standard and Premium is that Premium supports HSM-protected keys. Important If a user has contributor permissions (RBAC) to a key vault management plane, they can grant themselves access to the data plane by setting a key vault access policy. We recommend that you tightly control who has contributor access to your key vaults, to ensure that only authorized persons can access and manage your key vaults, keys, secrets, and certificates. Configure Key Vault access Access to a key vault is controlled through two interfaces: the management plane, and the data plane. The management plane is where you manage Key Vault itself. Operations in this plane include creating and deleting key vaults, retrieving Key Vault properties, and updating access policies. The data plane is where you work with the data stored in a key vault. You can add, delete, and modify keys, secrets, and certificates from here. To access a key vault in either plane, all callers (users or applications) must have proper authentication and authorization. Authentication establishes the identity of the caller. Authorization determines which operations the caller can execute. Both planes use Azure AD for authentication. For authorization, the management plane uses RBAC, and the data plane can use either newly added RBAC or a Key Vault access policy. Active Directory authentication When you create a key vault in an Azure subscription, its automatically associated with the Azure AD tenant of the subscription. All callers in both planes must register in this tenant and authenticate to access the key vault. In both cases, applications can access Key Vault in two ways: User plus application access. The application accesses Key Vault on behalf of a signed-in user. Examples of this type of access include Azure PowerShell and the Azure portal. User access is granted in two ways. They can either access Key Vault from any application, or they must use a specific application (referred to as compound identity). Application-only access. The application runs as a daemon service or background job. The application identity is granted access to the key vault. For both types of access, the application authenticates with Azure AD. The application uses any supported authentication method based on the application type. The application acquires a token for a resource in the plane to grant access. The resource is an endpoint in the management or data plane, based on the Azure environment. The application uses the token and sends a REST API request to Key Vault. To learn more, review the whole authentication flow. Benefits The model of a single mechanism for authentication to both planes has several benefits: Organizations can centrally control access to all key vaults in their organization. If a user leaves, they instantly lose access to all key vaults in the organization. Organizations can customize authentication by using the options in Azure AD, such as to enable multifactor authentication for added security. Review a secure Key Vault example In this example, we're developing an application that uses a certificate for SSL, Azure Storage to store data, and an RSA 2,048-bit key for sign operations. Our application runs in an Azure virtual machine (VM) (or a virtual machine scale set). We can use a key vault to store the application secrets. We can store the bootstrap certificate that's used by the application to authenticate with Azure AD. We need access to the following stored keys and secrets: SSL certificate - Used for SSL. Storage key - Used to access the Storage account. RSA 2,048-bit key - Used for sign operations. Bootstrap certificate - Used to authenticate with Azure AD. After access is granted, we can fetch the storage key and use the RSA key for signing. We need to define the following roles to specify who can manage, deploy, and audit our application: Security team - IT staff from the office of the CSO (Chief Security Officer) or similar contributors. The security team is responsible for the proper safekeeping of secrets. The secrets can include SSL certificates, RSA keys for signing, connection strings, and storage account keys. Developers and operators - The staff who develop the application and deploy it in Azure. The members of this team aren't part of the security staff. They shouldn't have access to sensitive data like SSL certificates and RSA keys. Only the application that they deploy should have access to sensitive data. Auditors - This role is for contributors who aren't members of the development or general IT staff. They review the use and maintenance of certificates, keys, and secrets to ensure compliance with security standards. There is another role that is outside the scope of our application: the subscription (or resource group) administrator. The subscription admin sets up initial access permissions for the security team. They grant access to the security team by using a resource group that has the resources required by the application. Security team Create key vaults. Turn on Key Vault logging. Add keys and secrets. Create backups of keys for disaster recovery. Set Key Vault access policies to grant permissions to users and applications for specific operations. Roll the keys and secrets periodically. Developers and operators Get references from the security team for the bootstrap and SSL certificates (thumbprints), storage key (secret URI), and RSA key (key URI) for signing. Develop and deploy the application to access keys and secrets programmatically. Auditors Review the Key Vault logs to confirm proper use of keys and secrets, and compliance with data security standards. The following table summarizes the access permissions for our roles and application. Role Management plane permissions Data plane permissions Security team Key Vault Contributor Keys: backup, create, delete, get, import, list, restore. Secrets: all operations Developers and operators Key Vault deploy permission Note: This permission allows deployed VMs to fetch secrets from a key vault. None Auditors None Keys: list Secrets: list. Note: This permission enables auditors to inspect attributes (tags, activation dates, expiration dates) for keys and secrets not emitted in the logs. Application None Keys: sign Secrets: get The three team roles need access to other resources along with Key Vault permissions. To deploy VMs (or the Web Apps feature of Azure App Service), developers and operators need Contributor access to those resource types. Auditors need read access to the Storage account where the Key Vault logs are stored. Deploy and manage Key Vault certificates Key Vault certificates support provides for management of your x509 certificates and enables: A certificate owner to create a certificate through a Key Vault creation process or through the import of an existing certificate. Includes both self-signed and CA-generated certificates. A Key Vault certificate owner to implement secure storage and management of X509 certificates without interaction with private key material. A certificate owner to create a policy that directs Key Vault to manage the life-cycle of a certificate. Certificate owners to provide contact information for notification about lifecycle events of expiration and renewal of certificate. Automatic renewal with selected issuers - Key Vault partner X509 certificate providers and CAs. When a Key Vault certificate is created, an addressable key and secret are also created with the same name. The Key Vault key allows key operations and the Key Vault secret allows retrieval of the certificate value as a secret. A Key Vault certificate also contains public x509 certificate metadata. The identifier and version of certificates is similar to that of keys and secrets. A specific version of an addressable key and secret created with the Key Vault certificate version is available in the Key Vault certificate response. When a Key Vault certificate is created, it can be retrieved from the addressable secret with the private key in either PFX or PEM format. However, the policy used to create the certificate must indicate that the key is exportable. If the policy indicates non-exportable, then the private key isn't a part of the value when retrieved as a secret. The addressable key becomes more relevant with non-exportable Key Vault certificates. The addressable Key Vault key\u2019s operations are mapped from the keyusage field of the Key Vault certificate policy used to create the Key Vault certificate. If a Key Vault certificate expires, it\u2019s addressable key and secret become inoperable. Two types of key are supported \u2013 RSA or RSA HSM with certificates. Exportable is only allowed with RSA, and is not supported by RSA HSM. Certificate policy A certificate policy contains information on how to create and manage the Key Vault certificate lifecycle. When a certificate with private key is imported into the Key Vault, a default policy is created by reading the x509 certificate. When a Key Vault certificate is created from scratch, a policy needs to be supplied. This policy specifies how to create the Key Vault certificate version, or the next Key Vault certificate version. After a policy has been established, it\u2019s not required with successive create operations for future versions. There's only one instance of a policy for all the versions of a Key Vault certificate. At a high level, a certificate policy contains the following information: X509 certificate properties. Contains subject name, subject alternate names, and other properties used to create an x509 certificate request. Key Properties. Contains key type, key length, exportable, and reuse key fields. These fields instruct key vault on how to generate a key. Secret properties. Contains secret properties such as content type of addressable secret to generate the secret value, for retrieving certificate as a secret. Lifetime Actions. Contains lifetime actions for the Key Vault certificate. Each lifetime action contains: Trigger, which specifies via days before expiry or lifetime span percentage. Action, which specifies the action type: emailContacts, or autoRenew. Issuer: Contains the parameters about the certificate issuer to use to issue x509 certificates. Policy attributes: Contains attributes associated with the policy. Certificate Issuer Before you can create a certificate issuer in a Key Vault, the following two prerequisite steps must be completed successfully: Onboard to CA providers: An organization administrator must onboard their company with at least one CA provider. Admin creates requester credentials for Key Vault to enroll (and renew) SSL certificates: Provides the configuration to be used to create an issuer object of the provider in the key vault. Certificate contacts Certificate contacts contain contact information to send notifications triggered by certificate lifetime events. The contacts information is shared by all the certificates in the key vault. A notification is sent to all the specified contacts for an event for any certificate in the key vault. If a certificate's policy is set to auto renewal, then a notification is sent for the following events: Before certificate renewal After certificate renewal, and stating if the certificate was successfully renewed, or if there was an error, requiring manual renewal of the certificate When it\u2019s time to renew a certificate for a certificate policy that is set to manually renew (email only) Certificate access control The Key Vault that contains certificates manages access control for those same certificates. The access control policy for certificates is distinct from the access control policies for keys and secrets in the same Key Vault. Users might create one or more vaults to hold certificates, to maintain scenario appropriate segmentation and management of certificates. The following permissions closely mirror the operations allowed on a secret object, and can be used on a per-principal basis in the secrets access control entry on a key vault: Permissions for certificate management operations: get: Get the current certificate version, or any version of a certificate. list: List the current certificates, or versions of a certificate. update: Update a certificate. create: Create a Key Vault certificate. import: Import certificate material into a Key Vault certificate. delete: Delete a certificate, its policy, and all of its versions. recover: Recover a deleted certificate. backup: Back up a certificate in a key vault. restore: Restore a backed-up certificate to a key vault. managecontacts: Manage Key Vault certificate contacts. manageissuers: Manage Key Vault certificate authorities/issuers. getissuers: Get a certificate's authorities/issuers. listissuers: List a certificate's authorities/issuers. setissuers: Create or update a Key Vault certificate's authorities/issuers. deleteissuers: Delete a Key Vault certificate's authorities/issuers. Permissions for privileged operations: purge: Purge (permanently delete) a deleted certificate. Create Key Vault keys Cryptographic keys in Key Vault are represented as JSON Web Key (JWK) objects. There are two types of keys, depending on how they were created. Soft keys: A key processed in software by Key Vault, but is encrypted at rest using a system key that is in a Hardware Security Module (HSM). Clients may import an existing RSA or EC (Elliptic Curve) key, or request that Key Vault generates one. Hard keys: A key processed in an HSM (Hardware Security Module). These keys are protected in one of the Key Vault HSM Security Worlds (there's one Security World per geography to maintain isolation). Clients may import an RSA or EC key, in soft form or by exporting from a compatible HSM device. Clients may also request Key Vault to generate a key. Key operations Key Vault supports many operations on key objects. Here are a few: Create: Allows a client to create a key in Key Vault. The value of the key is generated by Key Vault and stored, and isn't released to the client. Asymmetric keys may be created in Key Vault. Import: Allows a client to import an existing key to Key Vault. Asymmetric keys may be imported to Key Vault using many different packaging methods within a JWK construct. Update: Allows a client with sufficient permissions to modify the metadata (key attributes) associated with a key previously stored within Key Vault. Delete: Allows a client with sufficient permissions to delete a key from Key Vault Cryptographic operations Once a key has been created in Key Vault, the following cryptographic operations may be performed using the key. For best application performance, verify that operations are performed locally. Sign and Verify: Strictly, this operation is \"sign hash\" or \"verify hash\", as Key Vault doesn't support hashing of content as part of signature creation. Applications should hash the data to be signed locally, then request that Key Vault signs the hash. Verification of signed hashes is supported as a convenience operation for applications that may not have access to [public] key material. Key Encryption / Wrapping: A key stored in Key Vault may be used to protect another key, typically a symmetric content encryption key (CEK). When the key in Key Vault is asymmetric, key encryption is used. When the key in Key Vault is symmetric, key wrapping is used. Encrypt and Decrypt: A key stored in Key Vault may be used to encrypt or decrypt a single block of data. The size of the block is determined using the key type and selected encryption algorithm. The Encrypt operation is provided for convenience, for applications that may not have access to [public] key material. Application services plan More organizations are adopting secrets management policies, where secrets are stored centrally with expectations around expiration and access control. Azure Key Vault provides these management capabilities to your applications in Azure, but some applications can\u2019t easily take on code changes to start integrating with it. Key Vault references are a way to introduce secrets management into your app without code changes. Apps hosted in App Service and Azure Functions can now define a reference to a secret managed in Key Vault as part of their application settings. The app\u2019s system-assigned identity is used to securely fetch the secret and make it available to the app as an environment variable. Teams can replace existing secrets stored in app settings with references to the same secret in Key Vault, and the app continues to operate as normal. Configure a hardware security module key-generation solution For added assurance, when you use Azure Key Vault, you can import or generate keys in hardware security modules (HSMs) that never leave the HSM boundary. This scenario is often referred to as Bring Your Own Key (BYOK). The HSMs are FIPS 140-2 Level 2 validated. Azure Key Vault uses Thales nShield family of HSMs to protect your keys. (This functionality isn't available for Azure China.) Generating and transferring an HSM-protected key over the Internet: You generate the key from an offline workstation, which reduces the attack surface. The key is encrypted with a Key Exchange Key (KEK), which stays encrypted until transferred to the Azure Key Vault HSMs. Only the encrypted version of your key leaves the original workstation. The toolset sets properties on your tenant key that binds your key to the Azure Key Vault security world. After the Azure Key Vault HSMs receive and decrypt your key, only these HSMs can use it. Your key can't be exported. This binding is enforced using the Thales HSMs. The KEK that encrypts your key is generated inside the Azure Key Vault HSMs, and isn't exportable. The HSMs enforce that there can be no clear version of the KEK outside the HSMs. In addition, the toolset includes attestation from Thales that the KEK isn't exportable and was generated inside a genuine HSM manufactured by Thales. The toolset includes attestation from Thales that the Azure Key Vault security world was also generated on a genuine HSM manufactured by Thales. Microsoft uses separate KEKs and separate security worlds in each geographical region. This separation ensures that your key can be used only in data centers in the region in which you encrypted it. For example, a key from a European customer can't be used in data centers in North American or Asia. Manage customer managed keys Once you have created your Key Vault and have populated it with keys and secrets. The next step is to set up a rotation strategy for the values you store as Key Vault secrets. Secrets can be rotated in several ways: As part of a manual process Programmatically by using REST API calls Through an Azure Automation script Example of storage service encryption with customer-managed Keys. This service uses Azure Key Vault that provides highly available and scalable secure storage for RSA cryptographic keys backed by FIPS 140-2 Level 2 validated HSMs (Hardware Security Modules). Key Vault streamlines the key management process and enables customers to maintain control of keys that are used to encrypt data, manage, and audit their key usage, in order to protect sensitive data as part of their regulatory or compliance needs, HIPAA and BAA compliant. Customers can generate/import their RSA key to Azure Key Vault and enable Storage Service Encryption. Azure Storage handles the encryption and decryption in a fully transparent fashion using envelope encryption in which data is encrypted using an AES-based key, which in turn is protected using the Customer-Managed Key stored in Azure Key Vault. Customers can rotate their key in Azure Key Vault as per their compliance policies. When they rotate their key, Azure Storage detects the new key version and re-encrypts the Account Encryption Key for that storage account. Key rotation doesn't result in re-encryption of all data and there's no other action required from user. Customers can also revoke access to the storage account by revoking access on their key in Azure Key Vault. There are several ways to revoke access to your keys. Revoking access effectively blocks access to all blobs in the storage account as the Account Encryption Key is inaccessible by Azure Storage. Customers can enable this feature on all available redundancy types of Azure Blob storage including premium storage and can toggle from using Microsoft managed to using customer-managed keys. There's no extra charge for enabling this feature. You can enable this feature on any Azure Resource Manager storage account using the Azure portal, Azure PowerShell, Azure CLI, or the Microsoft Azure Storage Resource Provider API. Enable Key Vault secrets Key Vault provides secure storage of secrets, such as passwords and database connection strings. From a developer's perspective, Key Vault APIs accept and return secret values as strings. Internally, Key Vault stores and manages secrets as sequences of octets (8-bit bytes), with a maximum size of 25k bytes each. The Key Vault service doesn't provide semantics for secrets. It merely accepts the data, encrypts it, stores it, and returns a secret identifier (\"ID\"). The identifier can be used to retrieve the secret at a later time. For highly sensitive data, clients should consider additional layers of protection for data. Encrypting data using a separate protection key prior to storage in Key Vault is one example. Key Vault also supports a contentType field for secrets. Clients may specify the content type of a secret to assist in interpreting the secret data when it's retrieved. The maximum length of this field is 255 characters. There are no pre-defined values. The suggested usage is as a hint for interpreting the secret data. For instance, an implementation may store both passwords and certificates as secrets, then use this field to differentiate. There are no predefined values. As shown above, the values for Key Vault Secrets are: Name-value pair - Name must be unique in the Vault Value can be any Unicode Transformation Format (UTF-8) string - max of 25 KB in size Manual or certificate creation Activation date Expiration date Encryption All secrets in your Key Vault are stored encrypted. Key Vault encrypts secrets at rest with a hierarchy of encryption keys, with all keys in that hierarchy are protected by modules that are Federal Information Processing Standards (FIPS) 140-2 compliant. This encryption is transparent, and requires no action from the user. The Azure Key Vault service encrypts your secrets when you add them, and decrypts them automatically when you read them. The encryption leaf key of the key hierarchy is unique to each key vault. The encryption root key of the key hierarchy is unique to the security world, and its protection level varies between regions: China: root key is protected by a module that is validated for FIPS 140-2 Level 1. Other regions: root key is protected by a module that is validated for FIPS 140-2 Level 2 or higher. Secret attributes In addition to the secret data, the following attributes may be specified: exp: IntDate, optional, default is forever. The exp (expiration time) attribute identifies the expiration time on or after which the secret data SHOULD NOT be retrieved, except in particular situations. This field is for informational purposes only as it informs users of key vault service that a particular secret may not be used. Its value MUST be a number containing an IntDate value. nbf: IntDate, optional, default is now. The nbf (not before) attribute identifies the time before which the secret data SHOULD NOT be retrieved, except in particular situations. This field is for informational purposes only. Its value MUST be a number containing an IntDate value. enabled: boolean, optional, default is true. This attribute specifies whether the secret data can be retrieved. The enabled attribute is used with nbf and exp when an operation occurs between nbf and exp, it will only be permitted if enabled is set to true. Operations outside the nbf and exp window are automatically disallowed, except in particular situations. There are more read-only attributes that are included in any response that includes secret attributes: created: IntDate, optional. The created attribute indicates when this version of the secret was created. This value is null for secrets created prior to the addition of this attribute. Its value must be a number containing an IntDate value. updated: IntDate, optional. The updated attribute indicates when this version of the secret was updated. This value is null for secrets that were last updated prior to the addition of this attribute. Its value must be a number containing an IntDate value. For information on common attributes for each key vault object type, see Azure Key Vault keys, secrets and certificates overview. Date-time controlled operations A secret's get operation will work for not-yet-valid and expired secrets, outside the nbf / exp window. Calling a secret's get operation, for a not-yet-valid secret, can be used for test purposes. Retrieving (getting) an expired secret, can be used for recovery operations. Secret access control Access Control for secrets managed in Key Vault, is provided at the level of the Key Vault that contains those secrets. The access control policy for secrets is distinct from the access control policy for keys in the same Key Vault. Users may create one or more vaults to hold secrets, and are required to maintain scenario appropriate segmentation and management of secrets. The following permissions can be used, on a per-principal basis, in the secrets access control entry on a vault, and closely mirror the operations allowed on a secret object: Permissions for secret management operations get: Read a secret list: List the secrets or versions of a secret stored in a Key Vault set: Create a secret delete: Delete a secret recover: Recover a deleted secret backup: Back up a secret in a key vault restore: Restore a backed up secret to a key vault Permissions for privileged operations purge: Purge (permanently delete) a deleted secret Secret tags You can specify more application-specific metadata in the form of tags. Key Vault supports up to 15 tags, each of which can have a 256 character name and a 256 character value. Note Tags are readable by a caller if they have the list or get permission. Usage Scenarios When to use Examples Securely store, manage lifecycle, and monitor credentials for service-to-service communication like passwords, access keys, service principal client secrets. Use Azure Key Vault with a Virtual MachineUse Azure Key Vault with an Azure Web Application Configure key rotation Once you have keys and secrets stored in the key vault it's important to think about a rotation strategy. There are several ways to rotate the values: As part of a manual process Programmatically by using API calls Through an Azure Automation script This diagram shows how Event Grid and Function Apps can be used to automate the process. Thirty days before the expiration date of a secret, Key Vault publishes the \"near expiry\" event to Event Grid. Event Grid checks the event subscriptions and uses HTTP POST to call the function app endpoint subscribed to the event. The function app receives the secret information, generates a new random password, and creates a new version for the secret with the new password in Key Vault. The function app updates SQL Server with the new password. Manage Key Vault safety and recovery features Key Vault's soft-delete feature allows recovery of the deleted vaults and deleted key vault objects (for example, keys, secrets, certificates), known as soft-delete. Specifically, we address the following scenarios: This safeguard offer the following protections: Once a secret, key, certificate, or key vault is deleted, it remains recoverable for a configurable period of 7 to 90 calendar days. If no configuration is specified, the default recovery period is set to 90 days. Users are provided with sufficient time to notice an accidental secret deletion and respond. Two operations must be made to permanently delete a secret. First a user must delete the object, which puts it into the soft-deleted state. Second, a user must purge the object in the soft-deleted state. The purge operation requires extra access policy permissions. These extra protections reduce the risk of a user accidentally or maliciously deleting a secret or a key vault. To purge a secret in the soft-deleted state, a service principal must be granted another \"purge\" access policy permission. The purge access policy permission isn't granted by default to any service principal including key vault and subscription owners and must be deliberately set. By requiring an elevated access policy permission to purge a soft-deleted secret, it reduces the probability of accidentally deleting a secret. Supporting interfaces The soft-delete feature is available through the REST API, the Azure CLI, PowerShell, .NET/C# interfaces, and ARM templates. Scenarios Azure Key Vaults are tracked resources, managed by Azure Resource Manager. Azure Resource Manager also specifies a well-defined behavior for deletion, which requires that a successful DELETE operation must result in that resource not being accessible anymore. The soft-delete feature addresses the recovery of the deleted object, whether the deletion was accidental or intentional. In the typical scenario, a user may have inadvertently deleted a key vault or a key vault object. If that key vault or key vault object were to be recoverable for a predetermined period, the user may undo the deletion and recover their data. In a different scenario, a rogue user may attempt to delete a key vault or a key vault object, such as a key inside a vault, to cause a business disruption. Separation and deletion of the key vault or key vault object from the actual deletion of the underlying data can be used as a safety measure by, for instance, restricting permissions on data deletion to a different, trusted role. This approach effectively requires quorum for an operation that might otherwise result in an immediate data loss. Soft-delete behavior When soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion. When creating a new key vault, soft-delete is on by default. Once soft-delete is enabled on a key vault, it can't be disabled. The default retention period is 90 days but, during key vault creation, it's possible to set the retention policy interval to a value from 7 to 90 days through the Azure portal. The purge protection retention policy uses the same interval. Once set, the retention policy interval can't be changed. You can't reuse the name of a key vault that has been soft-deleted until the retention period has passed. Purge protection Permanently deleting, purging, a key vault is possible via a POST operation on the proxy resource and requires special privileges. Generally, only the subscription owner is able to purge a key vault. The POST operation triggers the immediate and irrecoverable deletion of that vault. Exceptions are: When the Azure subscription has been marked as undeletable. In this case, only the service may then perform the actual deletion, and does so as a scheduled process. When the enable-purge-protection argument is enabled on the vault itself. In this case, Key Vault waits for 90 days from when the original secret object was marked for deletion to permanently delete the object. Key vault recovery Upon deleting a key vault object, such as a key, the service will place the object in a deleted state, making it inaccessible to any retrieval operations. While in this state, the key vault object can only be listed, recovered, or forcefully/permanently deleted. To view the objects, use the Azure CLI az keyvault key list-deleted command, or the PowerShell Get-AzKeyVault -InRemovedState command. At the same time, Key Vault will schedule the deletion of the underlying data corresponding to the deleted key vault or key vault object for execution after a predetermined retention interval. The DNS record corresponding to the vault is also retained during the retention interval. Soft-delete retention period Soft-deleted resources are retained for a set period of time, 90 days. During the soft-delete retention interval, the following apply: You may list all of the key vaults and key vault objects in the soft-delete state for your subscription as well as access deletion and recovery information about them. Only users with special permissions can list deleted vaults. We recommend that our users create a custom role with these special permissions for handling deleted vaults. A key vault with the same name can't be created in the same location; correspondingly, a key vault object can't be created in a given vault if that key vault contains an object with the same name and which is in a deleted state. Only a privileged user may restore a key vault or key vault object by issuing a recover command on the corresponding proxy resource. The user, member of the custom role, who has the privilege to create a key vault under the resource group can restore the vault. Only a privileged user may forcibly delete a key vault or key vault object by issuing a delete command on the corresponding proxy resource. Unless a key vault or key vault object is recovered, at the end of the retention interval the service performs a purge of the soft-deleted key vault or key vault object and its content. Resource deletion may not be rescheduled. Billing implications In general, when an object (a key vault or a key or a secret) is in deleted state, there are only two operations possible: 'purge' and 'recover'. All the other operations fail. Therefore, even though the object exists, no operations can be performed and hence no usage will occur, so no bill. However there are following exceptions: In general, when an object (a key vault or a key or a secret) is in deleted state, there are only two operations possible: 'purge' and 'recover'. All the other operations fail. Therefore, even though the object exists, no operations can be performed and hence no usage will occur, so no bill. However there are following exceptions: If the object is an HSM-key, the 'HSM Protected key' charge per key version per month charge applies if a key version has been used in last 30 days. After that, since the object is in deleted state no operations can be performed against it, so no charge will apply. Key vault soft-delete on by default If a secret is deleted and the key vault doesn't have soft-deleted protection, it's deleted permanently. Although users can currently opt out of soft-delete during key vault creation, this ability is deprecated. In February 2025, Microsoft enables soft-delete protection on all key vaults, and users are no longer be able to opt out of or turn off soft-delete. This, protect secrets from accidental or malicious deletion by a user. This diagram shows how the process flow of deleting a key with and without soft-delete protection. When a secret is deleted from a key vault without soft-delete protection, the secret is permanently deleted. Users can currently opt out of soft-delete during key vault creation. However, Microsoft enables soft-delete protection on all key vaults to protect secrets from accidental or malicious deletion by a user. Users are no longer be able to opt out of or turn off soft-delete. Key vault backup Azure Key Vault automatically provides features to help you maintain availability and prevent data loss. Back up secrets only if you have a critical business justification. Backing up secrets in your key vault may introduce operational challenges such as maintaining multiple sets of logs, permissions, and backups when secrets expire or rotate. Key Vault maintains availability in disaster scenarios and will automatically fail over requests to a paired region without any intervention from a user. If you want protection against accidental or malicious deletion of your secrets, configure soft-delete and purge protection features on your key vault. Limitations Important Key Vault does not support the ability to backup more than 500 past versions of a key, secret, or certificate object. Attempting to backup a key, secret, or certificate object may result in an error. It is not possible to delete previous versions of a key, secret, or certificate. Key Vault doesn't currently provide a way to back up an entire key vault in a single operation. Any attempt to use the commands listed in this document to do an automated backup of a key vault may result in errors and not supported by Microsoft or the Azure Key Vault team. Also consider the following consequences: Backing up secrets that have multiple versions might cause time-out errors. A backup creates a point-in-time snapshot. Secrets might renew during a backup, causing a mismatch of encryption keys. If you exceed key vault service limits for requests per second, your key vault is throttled, and the backup fails. Design considerations When you back up a key vault object, such as a secret, key, or certificate, the backup operation downloads the object as an encrypted blob. This blob can't be decrypted outside of Azure. To get usable data from this blob, you must restore the blob into a key vault within the same Azure subscription and Azure geography. Prerequisites To back up a key vault object, you must have: Contributor-level or higher permissions on an Azure subscription. A primary key vault that contains the secrets you want to back up. A secondary key vault where secrets are restored. Back up and restore from the Azure portal Back up 1. Navigate to the Azure portal. 2. Select your key vault. 3. Navigate to the key you want to back up. Select the object Select Download Back up Select Download Store the encrypted blob in a secure location. Restore 1. Navigate to the Azure portal. 2. Select your key vault. 3. Navigate to the key you want to restore. 4. Select Restore Backup. Navigate to the location where you stored the encrypted blob. Select OK. Perform Try-This exercises Task 1: Create a key vault In this task, we'll create a key vault. Sign in to the Azure portal and search for Key Vaults. On the Key vaults page, click + Create. On the Basics tab, fill out the required information. Discuss the Pricing tier selections, Standard and Premium. Premium supports HSM-backed keys. Discuss Soft delete and Retention period. Click Review and Create and then Create. Wait for the new key vault to be created, or move to a key vault that has already been created. Task 2: Review key vault settings In this task, we'll review key vault settings. In the Portal, navigate to the key vault. Under the Name list, click the newly created Key Vault. Under the Objects, click Keys. Click Generate/Import and review the Keys configuration information. Under Settings, click Secrets. Click Generate/Import, review the Secrets configuration information, and click Create. View the new Secret and note that keys support versioning. Under Settings, click Certificates. Click Generate/Import and review the Certificates configuration information. Task 3: Configure access policies Note To complete this demonstration you will need a non-privileged test user. In this task, we'll configure access policies and test access. Continue in the Portal with your key vault. Under Settings, click Access Policies. Review the Enable access to choices: Azure Virtual Machines for deployment, Azure Resource Manager for template deployment, and Azure Disk Encryption for volume encryption. Review the creator account Key Permissions. Note the Cryptographic operation permissions aren't assigned. Review the creator account Secret Permissions. Note the Purge permission. Review the creator account Certificate Permissions. Open the Cloud Shell with the Bash option. You should be signed in as a Global Administrator. Use your key information to verify the secret you created in the previous task displays successfully for this role. az keyvault secret show --name <secret_name> --vault-name <keyvault_name>' In another browser tab, open the portal, and sign-in as the test user. Open the Cloud Shell with the Bash option. Verify that the secret doesn't display for the test user. Access is denied. az keyvault secret show --name <secret_name> --vault-name <keyvault_name> Return to the Global Administrator account in the portal. Add the Key Vault Contributor role to your test user. Try the test user's access. Access is denied. az keyvault secret show --name <secret_name> --vault-name <keyvault_name> Explain that adding the RBAC role grants access to the Key Vault control plane. It doesn't grant access to the date in the Key Vault. Return to your Key Vault and create an access policy. Under Settings, select Access policies and then Add Access Policy. Configure from the template (optional): Key, Secret, & Certificate Management Key permissions: none Secret permissions: Get, List Certificate permissions: none Select principal: select your test user Be sure to Add your new access policy. And to Save your changes. Try the test user's access. The user should now have access and the key should display. az keyvault secret show --name <secret_name> --vault-name <keyvault_name> As you have time, return to the Secret configuration settings and change Enabled to No. Be sure to save your changes, then try access the key again. Explore the Azure Hardware Security Module Azure Dedicated HSM is an Azure service that provides cryptographic key storage in Azure. Dedicated HSM meets the most stringent security requirements. It's the ideal solution for customers who require FIPS 140-2 Level 3-validated devices and complete and exclusive control of the HSM appliance. Best fit Azure Dedicated HSM is most suitable for \u201clift-and-shift\u201d scenarios that require direct and sole access to HSM devices. Examples include: Migrating applications from on-premises to Azure Virtual Machines Migrating applications from Amazon AWS EC2 to virtual machines that use the AWS Cloud HSM Classic service Running shrink-wrapped software such as Apache/Ngnix SSL Offload, Oracle TDE, and ADCS in Azure Virtual Machines Not a fit Azure Dedicated HSM is not a good fit for the following type of scenario: Microsoft cloud services that support encryption with customer-managed keys (such as Azure Information Protection, Azure Disk Encryption, Azure Data Lake Store, Azure Storage, Azure SQL Database, and Customer Key for Office 365) that are not integrated with Azure Dedicated HSM. Knowledge check Which of the following items should be stored in Azure Key Vault? Secret ( Ans ) Links to external certificate Identity management A select group of users must be able to create and delete keys in the key vault. When authenticating to the data plane using Azure AD, what security tool should be used the authorize access at a role level to these users? Key vault access policies Role-based Access Control ( Ans ) Azure AD authentication Which of these statements best describes Azure Key Vault's authentication and authorization process? Applications authenticate to a vault with the lead developer\u2019s username and password and have full access to all secrets in the vault. Applications and users authenticate to a vault with their Azure Active Directory identities and are authorized to perform actions on all secrets in the vault. ( Ans ) Applications and users authenticate to a vault with a Microsoft account and are authorized to access specific secrets. How does Azure Key Vault help protect your secrets after they're loaded by your app? Azure Key Vault automatically generates a new secret after every use. Azure Key Vault double-encrypts secrets, requiring your app to decrypt them locally every time they're used. It doesn't protect your secrets. Secrets are unprotected once they're loaded by your application. ( Ans ) A manager wants to know more about software-protected keys and hardware-protected keys. Pick the correct topic you could explain to your manager? Only hardware-protected keys are encrypted at rest. Software-protected keys aren't isolated from the application. Software-protected cryptographic operations are performed in software and Hardware-protected cryptographic operations are performed within the HSM. ( Ans ) Configure application security features Introduction Application security is one extra layer in the defense-in-depth strategy. You ensure that your application is registered, so it has a unique identity in Azure AD, then you can manage and control both access to the application and what the application can do. Scenario A security engineer uses application security to protect the usage of your application and prevent the loss of data, you will work on such tasks as: Register applications with Azure AD. Assign permissions for the application and the users to access it. Use certificates and secrets to protect operations and data. Review the Microsoft identity platform Microsoft identity platform is an evolution of the Azure Active Directory (Azure AD) developer platform. It allows developers to build applications that sign in users, get tokens to call APIs, such as Microsoft Graph or APIs that developers have built. It consists of an authentication service, open-source libraries, application registration and configuration (through a developer portal and application API), full developer documentation, quickstart samples, code samples, tutorials, how-to guides, and other developer content. The Microsoft identity platform supports industry-standard protocols such as OAuth 2.0 and OpenID Connect. Up until now, most developers have worked with the Azure AD v1.0 platform to authenticate work and school accounts (provisioned by Azure AD) by requesting tokens from the Azure AD v1.0 endpoint, using Azure AD Authentication Library (ADAL), Azure portal for application registration and configuration, and the Microsoft Graph API for programmatic application configuration. With the unified Microsoft identity platform (v2.0), you can write code once and authenticate any Microsoft identity into your application. For several platforms, the fully supported open-source Microsoft Authentication Library (MSAL) is recommended for use against the identity platform endpoints. MSAL is simple to use, provides great single sign-on (SSO) experiences for your users, helps you achieve high reliability and performance, and is developed using Microsoft Secure Development Lifecycle (SDL). When calling APIs, you can configure your application to take advantage of incremental consent, which allows you to delay the request for consent for more invasive scopes until the application\u2019s usage warrants this at runtime. MSAL also supports Azure Active Directory B2C, so your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs. With the Microsoft identity platform, one can expand their reach to these kinds of users: Work and school accounts (Azure AD provisioned accounts) Personal accounts (such as Outlook.com or Hotmail.com) Your customers who bring their own email or social identity (such as LinkedIn, Facebook, and Google) via MSAL and Azure AD business-to-consumer (B2C) You can use the Azure portal to register and configure your application and use the Microsoft Graph API for programmatic application configuration. Microsoft identity platform The following diagram depicts the Microsoft identity experience at a high level, including the app registration experience, software development kits (SDKs), endpoints, and supported identities. The Microsoft identity platform has two endpoints (v1.0 and v2.0); however, when developing a new application, consider it's highly recommended that you use the v2.0 (default) endpoint to benefit from the latest features and capabilities: The Microsoft Authentication Library can be used in many application scenarios, including the following: Single-page applications (JavaScript) Web app signing in users Web application signing in a user and calling a web API on behalf of the user Protecting a web API so only authenticated users can access it Web API calling another downstream Web API on behalf of the signed-in user Desktop application calling a web API on behalf of the signed-in user Mobile application calling a web API on behalf of the user who's signed in interactively. Desktop/service daemon application calling web API on behalf of itself Languages and frameworks Library Supported platforms and frameworks MSAL for Android Android MSAL Angular Single-page apps with Angular and Angular.js frameworks MSAL for iOS and macOS iOS and macOS MSAL Go (Preview) Windows, macOS, Linux MSAL Java Windows, macOS, Linux MSAL.js JavaScript/TypeScript frameworks such as Vue.js, Ember.js, or Durandal.js MSAL.NET .NET Framework, .NET Core, Xamarin Android, Xamarin iOS, Universal Windows Platform MSAL Node Web apps with Express, desktop apps with Electron, Cross-platform console apps MSAL Python Windows, macOS, Linux MSAL React Single-page apps with React and React-based libraries (Next.js, Gatsby.js) Migrate apps that use ADAL to MSAL Active Directory Authentication Library (ADAL) integrates with the Azure AD for developers (v1.0) endpoint, where MSAL integrates with the Microsoft identity platform. The v1.0 endpoint supports work accounts but not personal accounts. The v2.0 endpoint is unifying Microsoft personal accounts and works accounts into a single authentication system. Additionally, with MSAL, you can also get authentications for Azure AD B2C. Explore Azure AD application scenarios Any application that outsources authentication to Azure AD needs to be registered in a directory. This step involves telling Azure AD about your application, including: Azure AD application scenarios Frontend Authentication Backend Single page application are frontends that run in a browser Azure AD Authorization Endpoint Web API Web apps are applications that authenticate a user in a web browser to a web application Azure AD WS-Federation or SAML Endpoint Web application Native apps are applications that call a web API on behalf of a user Azure AD Authorization Endpoint and Azure AD Token Endpoint Web API Web API apps are web applications that need to get resources from a web API Azure AD Authorization Endpoint and Azure AD Token Endpoint Web application and Web API Service-to-service applications are daemon or server application that needs to get resources from a web API Azure AD Authorization Endpoint and Azure AD Token Endpoint Web API Azure AD represents applications following a specific model that's designed to fulfill two main functions: Identify the app according to the authentication protocols it supports. This involves enumerating all the identifiers, URLs, secrets, and related information that Azure AD needs at authentication time. Here, Azure AD: Holds all the data needed to support authentication at run time. Holds all the data for deciding which resources an app might need to access, whether it should fulfill a particular request, and under what circumstances it should fulfill the request. Supplies the infrastructure for implementing app provisioning both within the app developer's tenant and to any other Azure AD tenant. Handle user consent during token request time and facilitate the dynamic provisioning of apps across tenants. Here, Azure AD: Enables users and administrators to dynamically grant or deny consent for the app to access resources on their behalf. Enables administrators to ultimately decide what apps are allowed to do, which users can use specific apps, and how directory resources are accessed. In Azure AD, an application object describes an application as an abstract entity. Developers work with applications. At deployment time, Azure AD uses a specific application object as a blueprint to create a service principal, which represents a concrete instance of an application within a directory or tenant. It's the service principal that defines what the app can do in a specific target directory, who can use it, what resources it has access to, and so on. Azure AD creates a service principal from an application object through consent. The following diagram depicts a simplified Azure AD provisioning flow driven by consent. In this provisioning flow: A user from B tries to sign in with the app. Azure AD gets and verifies the user credentials. Azure AD prompts the user to consent for the app to gain access to tenant B. Azure AD uses the application object in A as a blueprint for creating a service principal in B. The user receives the requested token. You can repeat this process as many times as you want for other tenants (C, D, and so on). Directory A keeps the blueprint for the app (application object). Users and admins of all the other tenants where the app is given consent to retain control over what the application can do through the corresponding service principal object in each tenant. When an application is given permission to access resources in a tenant (upon registration or consent), a service principal object is created. The Microsoft Graph ServicePrincipal entity defines the schema for a service principal object's properties. Register an application with App Registration For the most secure operation, register your app with the Microsoft identity platform. Before your app can get a token from the Microsoft identity platform, it must be registered in the Azure portal. Registration integrates your app with the Microsoft identity platform and establishes the information that it uses to get tokens, including: Application ID: A unique identifier assigned by the Microsoft identity platform. Redirect URI/URL: One or more endpoints at which your app will receive responses from the Microsoft identity platform. (For native and mobile apps, this is a URI assigned by the Microsoft identity platform.) Application Secret: A password or a public/private key pair that your app uses to authenticate with the Microsoft identity platform. (Not needed for native or mobile apps.) Getting an access token Like most developers, you will probably use authentication libraries to manage your token interactions with the Microsoft identity platform. Authentication libraries abstract many protocol details, like validation, cookie handling, token caching, and maintaining secure connections, away from the developer and let you focus your development on your app. Microsoft publishes open-source client libraries and server middleware. Configure Microsoft Graph permissions Microsoft Graph exposes granular permissions that control the access that apps have to resources, like users, groups, and mail. As a developer, you decide which permissions to request for Microsoft Graph. When a user signs in to your app they, or, in some cases, an administrator, are given a chance to consent to these permissions. If the user consents, your app is given access to the resources and APIs that it has requested. For apps that don't take a signed-in user, permissions can be pre-consented to by an administrator when the app is installed. Microsoft Graph has two types of permissions: Delegated permissions are used by apps that have a signed-in user present. For these apps, either the user or an administrator consents to the permissions that the app requests, and the app can act as the signed-in user when making calls to Microsoft Graph. Some delegated permissions can be consented by non-administrative users, but some higher-privileged permissions require administrator consent. Application permissions are used by apps that run without a signed-in user present; for example, apps that run as background services or daemons. Application permissions can only be consented by an administrator. Effective permissions are the permissions that your app will have when making requests to Microsoft Graph. It is important to understand the difference between the delegated and application permissions that your app is granted and its effective permissions when making calls to Microsoft Graph. For delegated permissions, the effective permissions of your app will be the intersection of the delegated permissions the app has been granted (via consent) and the privileges of the currently signed-in user. Your app can never have more privileges than the signed-in user. Within organizations, the privileges of the signed-in user can be determined by policy or by membership in one or more administrator roles. For example, assume your app has been granted the User.ReadWrite.All delegated permission. This permission nominally grants your app permission to read and update the profile of every user in an organization. If the signed-in user is a global administrator, your app will be able to update the profile of every user in the organization. However, if the signed-in user is not in an administrator role, your app will be able to update only the profile of the signed-in user. It will not be able to update the profiles of other users in the organization because the user that it has permission to act on behalf of does not have those privileges. For application permissions, the effective permissions of your app will be the full level of privileges implied by the permission. For example, an app that has the User.ReadWrite.All application permission can update the profile of every user in the organization. Microsoft Graph API You can use the Microsoft Graph Security API to connect Microsoft security products, services, and partners to streamline security operations and improve threat protection, detection, and response capabilities. The Microsoft Graph Security API is an intermediary service (or broker) that provides a single programmatic interface to connect multiple Microsoft Graph Security providers (also called security providers or providers). The Microsoft Graph Security API federates requests to all providers in the Microsoft Graph Security ecosystem. This is based on the security provider consent provided by the application, as shown in the following diagram. The consent workflow only applies to non-Microsoft providers. The following is a description of the flow: The application user signs in to the provider application to view the consent form from the provider. This consent form experience or UI is owned by the provider and applies to non-Microsoft providers only to get explicit consent from their customers to send requests to Microsoft Graph Security API. The client consent is stored on the provider side. The provider consent service calls the Microsoft Graph Security API to inform consent approval for the respective customer. The application sends a request to the Microsoft Graph Security API. The Microsoft Graph Security API checks for the consent information for this customer mapped to various providers. The Microsoft Graph Security API calls all those providers the customer has given explicit consent to via the provider consent experience. The response is returned from all the consented providers for that client. The result set response is returned to the application. If the customer has not consented to any provider, no results from those providers are included in the response. The Microsoft Graph Security API makes it easy to connect with security solutions from Microsoft and partners. It allows you to more readily realize and enrich the value of these solutions. You can connect easily with the Microsoft Graph Security API by using one of the following approaches, depending on your requirements: Why use the Microsoft Graph Security API? Write code \u2013 Find code samples in C#, Java, NodeJS, and more. Connect using scripts \u2013 Find PowerShell samples. Drag and drop into workflows and playbooks \u2013 Use Microsoft Graph Security connectors for Azure Logic Apps, Microsoft Flow, and PowerApps. Get data into reports and dashboards \u2013 Use the Microsoft Graph Security connector for Power BI. Connect using Jupyter notebooks \u2013 Find Jupyter notebook samples. Unify and standardize alert tracking Connect once to integrate alerts from any Microsoft Graph-integrated security solution and keep alert status and assignments in sync across all solutions. You can also stream alerts to security information and event management (SIEM) solutions, such as Splunk using Microsoft Graph Security API connectors. Correlate security alerts to improve threat protection and response Correlate alerts across security solutions more easily with a unified alert schema. This not only allows you to receive actionable alert information but allows security analysts to pivot and enrich alerts with asset and user information, enabling faster response to threats and asset protection. Update alert tags, status, and assignments Tag alerts with additional context or threat intelligence to inform response and remediation. Ensure that comments and feedback on alerts are captured for visibility to all workflows. Keep alert status and assignments in sync so that all integrated solutions reflect the current state. Use webhook subscriptions to get notified of changes. Unlock security context to drive investigation Dive deep into related security-relevant inventory (like users, hosts, and apps), then add organizational context from other Microsoft Graph providers (Azure AD, Microsoft Intune, Microsoft 365) to bring business and security contexts together and improve threat response. Enable managed identities A common challenge when building cloud applications is how to manage the credentials in your code for authenticating to cloud services. Keeping the credentials secure is an important task. Ideally, the credentials never appear on developer workstations and aren't checked into source control. Azure Key Vault provides a way to securely store credentials, secrets, and other keys, but your code has to authenticate to Key Vault to retrieve them. Managed Identities for Azure resources is the new name for the service formerly known as Managed Service Identity (MSI) for Azure resources feature in Azure Active Directory (Azure AD) solves the above noted problem. The feature provides Azure services with an automatically managed identity in Azure AD. You can use the identity to authenticate to any service that supports Azure AD authentication, including Key Vault, without any credentials in your code. The managed identities for Azure resources feature is free with Azure AD for Azure subscriptions. There's no additional cost. Terminology The following terms are used throughout the managed identities for Azure resources documentation set: Client ID - a unique identifier generated by Azure AD that is tied to an application and service principal during its initial provisioning. Principal ID - the object ID of the service principal object for your managed identity that is used to grant role-based access to an Azure resource. Azure Instance Metadata Service (IMDS) - a REST endpoint accessible to all IaaS VMs created via the Azure Resource Manager. The endpoint is available at a well-known non-routable IP address (169.254.169.254) that can be accessed only from within the VM. How managed identities for Azure resources works There are two types of managed identities: A system-assigned managed identity is enabled directly on an Azure service instance. When the identity is enabled, Azure creates an identity for the instance in the Azure AD tenant that's trusted by the subscription of the instance. After the identity is created, the credentials are provisioned onto the instance. The lifecycle of a system-assigned identity is directly tied to the Azure service instance that it's enabled on. If the instance is deleted, Azure automatically cleans up the credentials and the identity in Azure AD. A user-assigned managed identity is created as a standalone Azure resource. Through a create process, Azure creates an identity in the Azure AD tenant that's trusted by the subscription in use. After the identity is created, the identity can be assigned to one or more Azure service instances. The lifecycle of a user-assigned identity is managed separately from the lifecycle of the Azure service instances to which it's assigned. Internally, managed identities are service principals of a special type, which are locked to only be used with Azure resources. When the managed identity is deleted, the corresponding service principal is automatically removed. Also, when a User-Assigned or System-Assigned Identity is created, the Managed Identity Resource Provider (MSRP) issues a certificate internally to that identity. Your code can use a managed identity to request access tokens for services that support Azure AD authentication. Azure takes care of rolling the credentials that are used by the service instance. Credential rotation Credential rotation is controlled by the resource provider that hosts the Azure resource. The default rotation of the credential occurs every 46 days. It's up to the resource provider to call for new credentials, so the resource provider could wait longer than 46 days. The following diagram shows how managed service identities work with Azure virtual machines (VMs): How a system-assigned managed identity works with an Azure VM Azure Resource Manager receives a request to enable the system-assigned managed identity on a VM. Azure Resource Manager creates a service principal in Azure AD for the identity of the VM. The service principal is created in the Azure AD tenant that's trusted by the subscription. Azure Resource Manager configures the identity on the VM by updating the Azure Instance Metadata Service identity endpoint with the service principal client ID and certificate. After the VM has an identity, use the service principal information to grant the VM access to Azure resources. To call Azure Resource Manager, use role-based access control (RBAC) in Azure AD to assign the appropriate role to the VM service principal. To call Key Vault, grant your code access to the specific secret or key in Key Vault. Your code that's running on the VM can request a token from the Azure Instance Metadata service endpoint, accessible only from within the VM: http://169.254.169.254/metadata/identity/oauth2/token The resource parameter specifies the service to which the token is sent. To authenticate to Azure Resource Manager, use resource=https://management.azure.com/. API version parameter specifies the IMDS version, use api-version=2018-02-01 or greater. A call is made to Azure AD to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Azure AD returns a JSON Web Token (JWT) access token. Your code sends the access token on a call to a service that supports Azure AD authentication Azure App Services Azure App Service is an HTTP-based service for hosting web applications, REST APIs, and mobile backends. You can develop in your favorite language, be it .NET, .NET Core, Java, Ruby, Node.js, PHP, or Python. Applications run and scale with ease on both Windows and Linux-based environments. App Service not only adds the power of Microsoft Azure to your application, such as security, load balancing, autoscaling, and automated management. You can also use its DevOps capabilities, such as continuous deployment from Azure DevOps, GitHub, Docker Hub, and other sources, package management, staging environments, custom domain, and TLS/SSL certificates. With App Service, you pay for the Azure compute resources you use. The compute resources you use are determined by the App Service plan that you run your apps on. For more information, see Azure App Service plans overview. Why use App Service? Azure App Service is a fully managed platform as a service (PaaS) offering for developers. Here are some key features of App Service: Multiple languages and frameworks - App Service has first-class support for ASP.NET, ASP.NET Core, Java, Ruby, Node.js, PHP, or Python. You can also run PowerShell and other scripts or executables as background services. Managed production environment - App Service automatically patches and maintains the OS and language frameworks for you. Spend time writing great apps and let Azure worry about the platform. Containerization and Docker - Dockerize your app and host a custom Windows or Linux container in App Service. Run multi-container apps with Docker Compose. Migrate your Docker skills directly to App Service. DevOps optimization - Set up continuous integration and deployment with Azure DevOps, GitHub, BitBucket, Docker Hub, or Azure Container Registry. Promote updates through test and staging environments. Manage your apps in App Service by using Azure PowerShell or the cross-platform command-line interface (CLI). Global scale with high availability - Scale up or out manually or automatically. Host your apps anywhere in Microsoft's global data center infrastructure, and the App Service SLA promises high availability. Connections to SaaS platforms and on-premises data - Choose from more than 50 connectors for enterprise systems (such as SAP), SaaS services (such as Salesforce), and internet services (such as Facebook). Access on-premises data using Hybrid Connections and Azure Virtual Networks. Security and compliance - The App Service is ISO, SOC, and PCI compliant. Authenticate users with Azure Active Directory, Google, Facebook, Twitter, or Microsoft account. Create IP address restrictions and manage service identities. Prevent subdomain takeovers. Application templates - Choose from an extensive list of application templates in the Azure Marketplace, such as WordPress, Joomla, and Drupal. Visual Studio and Visual Studio Code integration - Dedicated tools in Visual Studio and Visual Studio Code streamline the work of creating, deploying, and debugging. API and mobile features - App Service provides turn-key CORS support for RESTful API scenarios and simplifies mobile app scenarios by enabling authentication, offline data sync, push notifications, and more. Serverless code - Run a code snippet or script on-demand without having to explicitly provision or manage infrastructure and pay only for the compute time your code actually uses. Besides App Service, Azure offers other services that can be used for hosting websites and web applications. For most scenarios, App Service is the best choice. App Service Environment An App Service Environment is an Azure App Service feature that provides a fully isolated and dedicated environment for running App Service apps securely at high scale. An App Service Environment can host: Windows web apps Linux web apps Docker containers (Windows and Linux) Functions Logic apps (Standard) App Service Environments are appropriate for application workloads that require: High scale. Isolation and secure network access. High memory utilization. High requests per second (RPS). You can create multiple App Service Environments in a single Azure region or across multiple Azure regions. This flexibility makes an App Service Environment ideal for horizontally scaling stateless applications with a high RPS requirement. An App Service Environment can host applications from only one customer, and they do so on one of their virtual networks. Customers have fine-grained control over inbound and outbound application network traffic. Applications can establish high-speed secure connections over VPNs to on-premises corporate resources. Usage scenarios App Service Environments have many use cases, including: Internal line-of-business applications. Applications that need more than 30 App Service plan instances. Single-tenant systems to satisfy internal compliance or security requirements. Network-isolated application hosting. Multi-tier applications. There are many networking features that enable apps in a multi-tenant App Service to reach network-isolated resources or become network-isolated themselves. These features are enabled at the application level. With an App Service Environment, no added configuration is required for the apps to be on a virtual network. The apps are deployed into a network-isolated environment that's already on a virtual network. If you really need a complete isolation story, you can also deploy your App Service Environment onto dedicated hardware. Dedicated environment An App Service Environment is a single-tenant deployment of Azure App Service that runs on your virtual network. Applications are hosted in App Service plans, which are created in an App Service Environment. An App Service plan is essentially a provisioning profile for an application host. As you scale out your App Service plan, you create more application hosts with all the apps in that App Service plan on each host. A single App Service Environment v3 can have up to 200 total App Service plan instances across all the App Service plans combined. A single App Service Isolated v2 (Iv2) plan can have up to 100 instances by itself. When you're deploying onto dedicated hardware (hosts), you're limited in scaling across all App Service plans to the number of cores in this type of environment. An App Service Environment that's deployed on dedicated hosts has 132 vCores available. I1v2 uses two vCores, I2v2 uses four vCores, and I3v2 uses eight vCores per instance. Azure App Service plan An app service always runs in an App Serviceplan. In addition, Azure Functions also has the option of running in an App Service plan. An App Service plan defines a set of compute resources for a web app to run. These compute resources are analogous to the server farm in conventional web hosting. One or more apps can be configured to run on the same computing resources (or in the same App Service plan). When you create an App Service plan in a certain region (for example, West Europe), a set of compute resources is created for that plan in that region. Whatever apps you put into this App Service plan run on these compute resources as defined by your App Service plan. Each App Service plan defines: Operating System (Windows, Linux) Region (West US, East US, etc.) Number of VM instances Size of VM instances (Small, Medium, Large) Pricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, Isolated, IsolatedV2) The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. The pricing tiers available to your App Service plan depend on the operating system selected at creation time. There are a few categories of pricing tiers: Shared compute: Free and Shared, the two base tiers, runs an app on the same Azure VM as other App Service apps, including apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources cannot scale out. Dedicated compute: The Basic, Standard, Premium, PremiumV2, and PremiumV3 tiers run apps on dedicated Azure VMs. Only apps in the same App Service plan share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out. Isolated: This Isolated and IsolatedV2 tiers run dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities. App Service Environment networking App Service Environment is a single-tenant deployment of Azure App Service that hosts Windows and Linux containers, web apps, API apps, logic apps, and function apps. When you install an App Service Environment, you pick the Azure virtual network that you want it to be deployed in. All of the inbound and outbound application traffic is inside the virtual network you specify. You deploy into a single subnet in your virtual network, and nothing else can be deployed into that subnet. Subnet requirements You must delegate the subnet to Microsoft.Web/hostingEnvironments, and the subnet must be empty. The size of the subnet can affect the scaling limits of the App Service plan instances within the App Service Environment. It's a good idea to use a /24 address space (256 addresses) for your subnet to ensure enough addresses to support production scale. Note Windows Containers uses an additional IP address per app for each App Service plan instance, and you need to size the subnet accordingly. If your App Service Environment has, for example, 2 Windows Container App Service plans, each with 25 instances and each with 5 apps running, you will need 300 IP addresses and additional addresses to support horizontal (up/down) scale. If you use a smaller subnet, be aware of the following limitations: Any particular subnet has five addresses reserved for management purposes. In addition to the management addresses, App Service Environment dynamically scales the supporting infrastructure and uses between 4 and 27 addresses, depending on the configuration and load. You can use the remaining addresses for instances in the App Service plan. The minimal size of your subnet is a /27 address space (32 addresses). If you run out of addresses within your subnet, you can be restricted from scaling out your App Service plans in the App Service Environment. Another possibility is that you can experience increased latency during intensive traffic load if Microsoft can't scale the supporting infrastructure. Addresses App Service Environment has the following network information at creation: Address type Description App Service Environment virtual network The virtual network deployed into. App Service Environment subnet The subnet deployed into. Domain suffix The domain suffix that is used by the apps made. Virtual IP (VIP) The VIP type is used. The two possible values are internal and external. Inbound address The inbound address is the address at which your apps are reached. If you have an internal VIP, it's an address in your App Service Environment subnet. If the address is external, it's a public-facing address. Default outbound addresses The apps use this address, by default, when making outbound calls to the internet. As you scale your App Service plans in your App Service Environment, you'll use more addresses from your subnet. The number of addresses you use varies based on the number of App Service plan instances you have and how much traffic there is. Apps in the App Service Environment don't have dedicated addresses in the subnet. The specific addresses an app uses in the subnet will change over time. Availability Zone Support for App Service Environments Azure App Service Environment can be deployed across availability zones (AZ) to help you achieve resiliency and reliability for your business-critical workloads. This architecture is also known as zone redundancy. When you configure it to be zone redundant, the platform automatically spreads the instances of the Azure App Service plan across three zones in the selected region. This means that the minimum App Service Plan instance count will always be three. If you specify a capacity larger than three, and the number of instances is divisible by three, the instances are spread evenly. Otherwise, instance counts beyond 3*N are spread across the remaining one or two zones. Prerequisites You configure availability zones when you create your App Service Environment. All App Service plans created in that App Service Environment will need a minimum of 3 instances, and those will automatically be zone redundant. You can only specify availability zones when creating a new App Service Environment. A pre-existing App Service Environment can't be converted to use availability zones. Availability zones are only supported in a subset of regions. Downtime requirements Downtime will be dependent on how you decide to carry out the migration. Since you can't convert pre-existing App Service Environments to use availability zones, migration will consist of a side-by-side deployment where you'll create a new App Service Environment with availability zones enabled. Downtime will depend on how you choose to redirect traffic from your old to your new availability zone-enabled App Service Environment. For example, if you're using an Application Gateway, a custom domain, or Azure Front Door, downtime will be dependent on the time it takes to update those respective services with your new app's information. Alternatively, you can route traffic to multiple apps at the same time using a service such as Azure Traffic Manager and only fully cutover to your new availability zone-enabled apps when everything is deployed and fully tested. For more information on App Service Environment migration options, see App Service Environment migration. If you're already using App Service Environment v3, disregard the information about migration from previous versions and focus on the app migration strategies. App Service Environment Certificates Azure App Service provides a highly scalable, self-patching web hosting service. Once the certificate is added to your App Service app or function app, you can secure a custom Domain Name System (DNS) name with it or use it in your application code. Note A certificate uploaded into an app is stored in a deployment unit that is bound to the app service plan's resource group and region combination (internally called a webspace). This makes the certificate accessible to other apps in the same resource group and region combination. The following lists are options for adding certificates in App Service: Create a free App Service managed certificate: A private certificate that's free of charge and easy to use if you just need to secure your custom domain in App Service. Purchase an App Service certificate: A private certificate that's managed by Azure. It combines the simplicity of automated certificate management and the flexibility of renewal and export options. Import a certificate from Key Vault: Useful if you use Azure Key Vault to manage your Public-Key Cryptography Standards #12 (PKCS12) certificates. Upload a private certificate: If you already have a private certificate from a third-party provider, you can upload it. Upload a public certificate: Public certificates are not used to secure custom domains, but you can load them into your code if you need them to access remote resources. Prerequisites Create an App Service app. For a private certificate, make sure that it satisfies all requirements from App Service. Free certificate only: Map the domain you want a certificate for to App Service. For a root domain (like contoso.com), make sure your app doesn't have any IP restrictions configured. Both certificate creation and its periodic renewal for a root domain depends on your app being reachable from the internet. Configure an app registration via the Azure portal Note The application registration process is constantly being updated and improved. Validate before your demo In this exercise, we will demo how to register an application. In the Portal search for and select Azure Active Directory. Under Manage select App registrations. Click New registration. Name: AZ500 app Review the Supported app types Select Accounts in this organizational directory only (Single tenant) Redirect URL > Web: https://oauth.pstmn.io/v1/browser-callback Click Register Wait for the application to register. On the Overview tab, review the Application (Client ID), Directory (tenant ID), and Object ID. Under Manage click Certificates and Secrets. Review the use of client secrets that an application uses to prove its identity when requesting a token. Click New client secret. Description: key1 Expires: In 1 year Click Add Wait for the application credentials to update. Knowledge check What method does Microsoft Azure App Service use to obtain credentials for users attempting to access an app? Credentials are stored in the browser Pass-through authentication Redirection to a provider endpoint ( Ans ) What type of Managed Service Identities can you create? Application-assigned and VM-assigned Database-assigned and unsigned System-assigned and User-assigned ( Ans ) An App Service application stores page graphics in an Azure storage account. The app needs to authenticate programmatically to the storage account, what should be configured? Create an Azure AD system user Create a managed identity ( Ans ) Create an RBAC role assignment How does using managed identities for Azure resources change the way an app authenticates to Azure Key Vault? The app gets tokens from a token service instead of Azure Active Directory. ( Ans ) The app uses a certificate to authenticate instead of a secret. Managed identities are automatically recognized by Azure Key Vault and authenticated automatically. Implement storage security Introduction Every organization has data. That data needs to be protected at rest, in transit, and while it's being used within an application. Azure provides a set of security features to protect your data, no matter where it's located. Scenario A security engineer will protect data within databases, file shares, or anywhere it resides, you'll work on such tasks as: Lock down access to storage to specific users and applications. Encrypt data where it's stored and while it's moved around. Use identity to protect data when it's accessed and used. Define data sovereignty What is Data Sovereignty? - Data sovereignty is the concept that information, which has been converted and stored in binary digital form, is subject to the laws of the country or region in which it is located. Many of the current concerns that surround data sovereignty relate to enforcing privacy regulations and preventing data stored in a foreign country or region from being subpoenaed by the host country or region\u2019s government. In Azure, customer data might be replicated within a selected geographic area for enhanced data durability during a major data center disaster, and in some cases will not be replicated outside it. Paired regions Azure operates in multiple geographies around the world. An Azure geography is a defined area of the world that contains at least one Azure Region. An Azure region is an area within a geography, containing one or more datacenters. Each Azure region is paired with another region within the same geography, forming a regional pair. The exception is Brazil South, which is paired with a region outside its geography. Across the region pairs Azure serializes platform updates (or planned maintenance), so that only one paired region is updated at a time. If an outage affecting multiple regions, one region in each pair will be prioritized for recovery. We recommend that you configure business continuity and disaster recovery (BCDR) across regional pairs to benefit from Azure\u2019s isolation and VM policies. For applications that support multiple active regions, we recommend using both regions in a region pair where possible. Multiple regions will ensure optimal availability for applications and minimized recovery time in a disaster. Benefits of Azure paired regions Physical isolation - When possible, Azure services prefer at least 300 miles of separation between datacenters in a regional pair (although longer distance isn't practical or possible in all geographies). Physical datacenter separation reduces the likelihood of both regions being affected simultaneously as a result of natural disasters, civil unrest, power outages, or physical network outages. Isolation is subject to the constraints within the geography, such as geography size, power and network infrastructure availability, and regulations. Platform-provided replication - Some services such as geo-redundant storage provide automatic replication to the paired region. Region recovery order - If a broad outage occurs, recovery of one region is prioritized out of every pair. Applications that are deployed across paired regions are guaranteed to have one of the regions recovered with priority. If an application is deployed across regions that are not paired, recovery might be delayed. In the worst case, the chosen regions might be the last two to be recovered. Sequential updates - Planned Azure system updates are rolled out to paired regions sequentially, not at the same time. A staged roll-out helps minimize downtime, the effect of bugs, and logical failures in the rare event of a bad update. Data residency - To meet data residency requirements for tax and law enforcement jurisdiction purposes, a region resides within the same geography as its pair (except for Brazil South). Configure Azure storage access Every request made against a secured resource in the Blob, File, Queue, or Table service must be authorized. Authorization ensures that resources in your storage account are accessible only when you want them to be, and only to those users or applications to whom you grant access. Options for authorizing requests to Azure Storage include Azure AD - Azure Storage provides integration with Azure Active Directory (Azure AD) for identity-based authorization of requests to the Blob and Queue services. With Azure AD, you can use role-based access control (RBAC) to grant access to blob and queue resources to users, groups, or applications. You can grant permissions that are scoped to the level of an individual container or queue. Authorizing access to blob and queue data with Azure AD provides superior security and ease of use over other authorization options. When you use Azure AD to authorize requests make from your applications, you avoid having to store your account access key with your code, as you do with Shared Key authorization. While you can continue to use Shared Key authorization with your blob and queue applications, Microsoft recommends moving to Azure AD where possible. Azure Active Directory Domain Services (Azure AD DS) authorization for Azure Files. Azure Files supports identity-based authorization over Server Message Block (SMB) through Azure AD DS. You can use RBAC for fine-grained control over a client's access to Azure Files resources in a storage account Shared Key - Shared Key authorization relies on your account access keys and other parameters to produce an encrypted signature string that is passed on via the request in the Authorization header. Shared Access Signatures - A shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients who should not be trusted with your storage account key but to whom you wish to delegate access to certain storage account resources. By distributing a shared access signature URI to these clients, you can grant them access to a resource for a specified period of time, with a specified set of permissions. The URI query parameters comprising the SAS token incorporate all of the information necessary to grant controlled access to a storage resource. A client who is in possession of the SAS can make a request against Azure Storage with just the SAS URI, and the information contained in the SAS token is used to authorize the request. Anonymous access to containers and blobs - You can enable anonymous, public read access to a container and its blobs in Azure Blob storage. By doing so, you can grant read-only access to these resources without sharing your account key, and without requiring a shared access signature (SAS).Public read access is best for scenarios where you want certain blobs to always be available for anonymous read access. For more fine-grained control, look to using the shared access signature, described above. Authenticating and authorizing access to blob and queue data with Azure AD provides superior security and ease of use over other authorization options. For example, by using Azure AD, you avoid having to store your account access key with your code, as you do with Shared Key authorization. While you can continue to use Shared Key authorization with your blob and queue applications, Microsoft recommends moving to Azure AD where possible. Similarly, you can continue to use shared access signatures (SAS) to grant fine-grained access to resources in your storage account, but Azure AD offers similar capabilities without the need to manage SAS tokens or worry about revoking a compromised SAS. Important Where possible use authorizing applications that access Azure Storage using Azure AD. It provides better security and ease of use over other authorization options. Deploy shared access signatures As a best practice, you shouldn't share storage account keys with external third-party applications. If these apps need access to your data, you'll need to secure their connections without using storage account keys. For untrusted clients, use a shared access signature (SAS). A shared access signature is a string that contains a security token that can be attached to a URI. Use a shared access signature to delegate access to storage objects and specify constraints, such as the permissions and the time range of access. You can give a customer a shared access signature token, for example, so they can upload pictures to a file system in Blob storage. Separately, you can give a web application permission to read those pictures. In both cases, you allow only the access that the application needs to do the task. Types of shared access signatures You can use a service-level shared access signature to allow access to specific resources in a storage account. You'd use this type of shared access signature, for example, to allow an app to retrieve a list of files in a file system or to download a file. Use an account-level shared access signature to allow access to anything that a service-level shared access signature can allow, plus additional resources and abilities. For example, you can use an account-level shared access signature to allow the ability to create file systems. A user delegation SAS, introduced with version 2018-11-09. A user delegation SAS is secured with Azure AD credentials. This type of SAS is supported for the Blob service only and can be used to grant access to containers and blobs. Additionally, a service SAS can reference a stored access policy that provides an additional level of control over a set of signatures, including the ability to modify or revoke access to the resource if necessary. One would typically use a shared access signature for a service where users read and write their data to your storage account. Accounts that store user data have two typical designs: Clients upload and download data through a front-end proxy service, which performs authentication. This front-end proxy service has the advantage of allowing validation of business rules. But if the service must handle large amounts of data or high-volume transactions, you might find it complicated or expensive to scale this service to match demand. A lightweight service authenticates the client as needed. Then it generates a shared access signature. After receiving the shared access signature, the client can access storage account resources directly. The shared access signature defines the client's permissions and access interval. The shared access signature reduces the need to route all data through the front-end proxy service. Manage Azure AD storage authentication In addition to Shared Key and Shared Access Signatures, Azure Storage supports using Azure Active Directory (Azure AD) to authorize requests to blob data. With Azure AD, you can use Azure role-based access control (Azure RBAC) to grant permissions to a security principal, which may be a user, group, or application service principal. The security principal is authenticated by Azure AD to return an OAuth 2.0 token. The token can then be used to authorize a request against the Blob service. To authorize requests against Azure Storage with Azure AD provides superior security and ease of use over Shared Key authorization. Microsoft recommends using Azure AD authorization with your blob applications when possible to assure access with minimum required privileges. Authorization with Azure AD is available for all general-purpose and Blob storage accounts in all public regions and national clouds. Only storage accounts created with the Azure Resource Manager deployment model support Azure AD authorization. Blob storage additionally supports creating shared access signatures (SAS) that is signed with Azure AD credentials. A few more details When a security principal (a user, group, or application) attempts to access a queue resource, the request must be authorized. With Azure AD, access to a resource is a two-step process. First, the security principal's identity is authenticated, and an OAuth 2.0 token is returned. Next, the token is passed as part of a request to the Queue service and used by the service to authorize access to the specified resource. The authentication step requires that an application request an OAuth 2.0 access token at runtime. If an application is running from within an Azure entity, such as an Azure VM, a Virtual Machine Scale Set, or an Azure Functions app, it can use a managed identity to access queues. The authorization step requires one or more Azure roles to be assigned to the security principal. Azure Storage provides Azure roles that encompass common sets of permissions for queue data. The roles that are assigned to a security principal determine the permissions that that principal will have. Native and web applications that request the Azure Queue service can also authorize access with Azure AD. Implement storage service encryption Azure Storage security is a key part to defense in depth. Azure Storage provides a comprehensive set of security capabilities that together enable developers to build secure applications: All data (including metadata) written to Azure Storage is automatically encrypted using Storage Service Encryption (SSE). Azure Active Directory (Azure AD) and Role-Based Access Control (RBAC) are supported for Azure Storage for both resource management operations and data operations, as follows: You can assign RBAC roles scoped to the storage account to security principals and use Azure AD to authorize resource management operations such as key management. Azure AD integration is supported for blob and queue data operations. You can assign RBAC roles scoped to a subscription, resource group, storage account, or an individual container or queue to a security principal or a managed identity for Azure resources. Data can be secured in transit between an application and Azure by using Client-Side Encryption, HTTPS, or SMB 3.0. OS and data disks used by Azure virtual machines can be encrypted using Azure Disk Encryption. Delegated access to the data objects in Azure Storage can be granted using a shared access signature. Azure Storage encryption for data at rest Azure Storage automatically encrypts your data when persisting it to the cloud. Encryption protects your data and to help you to meet your organizational security and compliance commitments. Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant. Azure Storage encryption is similar to BitLocker encryption on Windows. Azure Storage encryption is enabled for all new and existing storage accounts and cannot be disabled. Because your data is secured by default, you don't need to modify your code or applications to take advantage of Azure Storage encryption. Storage accounts are encrypted regardless of their performance tier (standard or premium) or deployment model (Azure Resource Manager or classic). All Azure Storage redundancy options support encryption, and all copies of a storage account are encrypted. All Azure Storage resources are encrypted, including blobs, disks, files, queues, and tables. All object metadata is also encrypted. Encryption does not affect Azure Storage performance. There is no additional cost for Azure Storage encryption. Encryption key management You can rely on Microsoft-managed keys for the encryption of your storage account, or you can manage encryption with your own keys. If you choose to manage encryption with your own keys, you have two options: You can specify a customer-managed key to use for encrypting and decrypting all data in the storage account. A customer-managed key is used to encrypt all data in all services in your storage account. You can specify a customer-provided key on Blob storage operations. A client making a read or write request against Blob storage can include an encryption key on the request for granular control over how blob data is encrypted and decrypted. The following table compares key management options for Azure Storage encryption. Microsoft-managed keys Customer-managed keys Customer-provided keys Encryption/decryption operations Azure Azure Azure Azure Storage services supported All Blob storage, Azure Files Blob storage Key storage Microsoft key store Azure Key Vault Azure Key Vault or any other key store Key rotation responsibility Microsoft Customer Customer Key usage Microsoft Azure portal, Storage Resource Provider REST API, Azure Storage management libraries, PowerShell, CLI Azure Storage REST API (Blob storage), Azure Storage client libraries Key access Microsoft only Microsoft, Customer Customer only Configure blob data retention policies Immutable storage for Azure Blob storage enables users to store business-critical data objects in a WORM (Write Once, Read Many) state. This state makes the data non-erasable and non-modifiable for a user-specified interval. For the duration of the retention interval, blobs can be created and read, but cannot be modified or deleted. Immutable storage is available for general-purpose v2 and Blob storage accounts in all Azure regions. Time-based vs Legal hold policies Time-based retention policy support: Users can set policies to store data for a specified interval. When a time-based retention policy is set, blobs can be created and read, but not modified or deleted. After the retention period has expired, blobs can be deleted but not overwritten. When a time-based retention policy is applied on a container, all blobs in the container will stay in the immutable state for the duration of the effective retention period. The effective retention period for blobs is equal to the difference between the blob's creation time and the user-specified retention interval. Because users can extend the retention interval, immutable storage uses the most recent value of the user-specified retention interval to calculate the effective retention period. Legal hold policy support: If the retention interval is not known, users can set legal holds to store immutable data until the legal hold is cleared. When a legal hold policy is set, blobs can be created and read, but not modified or deleted. Each legal hold is associated with a user-defined alphanumeric tag (such as a case ID, event name, etc.) that is used as an identifier string. Legal holds are temporary holds that can be used for legal investigation purposes or general protection policies. Each legal hold policy needs to be associated with one or more tags. Tags are used as a named identifier, such as a case ID or event, to categorize and describe the purpose of the hold. Other immutable storage features Support for all blob tiers: WORM policies are independent of the Azure Blob storage tier and apply to all the tiers: hot, cool, and archive. Users can transition data to the most cost-optimized tier for their workloads while maintaining data immutability. Container-level configuration: Users can configure time-based retention policies and legal hold tags at the container level. By using simple container-level settings, users can create and lock time-based retention policies, extend retention intervals, set and clear legal holds, and more. These policies apply to all the blobs in the container, both existing and new. Audit logging support: Each container includes a policy audit log. It shows up to seven time-based retention commands for locked time-based retention policies and contains the user ID, command type, time stamps, and retention interval. For legal holds, the log contains the user ID, command type, time stamps, and legal hold tags. This log is retained for the lifetime of the policy, in accordance with the SEC 17a-4(f) regulatory guidelines. The Azure Activity Log shows a more comprehensive log of all the control plane activities; while enabling Azure Resource Logs retains and shows data plane operations. It is the user's responsibility to store those logs persistently, as might be required for regulatory or other purposes. Important A container can have both a legal hold and a time-based retention policy at the same time. All blobs in that container stay in the immutable state until all legal holds are cleared, even if their effective retention period has expired. Conversely, a blob stays in an immutable state until the effective retention period expires, even though all legal holds have been cleared. Configure Azure files authentication Azure Files supports identity-based authentication over Server Message Block (SMB) through on-premises Active Directory Domain Services (AD DS) and Azure Active Directory Domain Services (Azure AD DS). This article focuses on how Azure file shares can use domain services, either on-premises or in Azure, to support identity-based access to Azure file shares over SMB. Enabling identity-based access for your Azure file shares allows you to replace existing file servers with Azure file shares without replacing your existing directory service, maintaining seamless user access to shares. Azure Files enforces authorization on user access to both the share and the directory/file levels. Share-level permission assignment can be performed on Azure Active Directory (Azure AD) users or groups managed through the role-based access control (RBAC) model. With RBAC, the credentials you use for file access should be available or synced to Azure AD. You can assign built-in RBAC roles like Storage File Data SMB Share Reader to users or groups in Azure AD to grant read access to an Azure file share. At the directory/file level, Azure Files supports preserving, inheriting, and enforcing Windows DACLs just like any Windows file servers. You can choose to keep Windows DACLs when copying data over SMB between your existing file share and your Azure file shares. Whether you plan to enforce authorization or not, you can use Azure file shares to back up ACLs along with your data. Advantages of identity-based authentication Identity-based authentication for Azure Files offers several benefits over using Shared Key authentication: Extend the traditional identity-based file share access experience to the cloud with on-premises AD DS and Azure AD DS. If you plan to lift and shift your application to the cloud, replacing traditional file servers with Azure file shares, then you may want your application to authenticate with either on-premises AD DS or Azure AD DS credentials to access file data. Azure Files supports using both on-premises AD DS or Azure AD DS credentials to access Azure file shares over SMB from either on-premises AD DS or Azure AD DS domain-joined VMs. Enforce granular access control on Azure file shares. You can grant permissions to a specific identity at the share, directory, or file level. For example, suppose that you have several teams using a single Azure file share for project collaboration. You can grant all teams access to non-sensitive directories while limiting access to directories containing sensitive financial data to your Finance team only. Back up Windows ACLs (also known as NTFS) along with your data. You can use Azure file shares to back up your existing on-premises file shares. Azure Files preserves your ACLs along with your data when you back up a file share to Azure file shares over SMB. Identity-based authentication data flow How it works Azure file shares leverages Kerberos protocol for authenticating with either on-premises AD DS or Azure AD DS. When an identity associated with a user or application running on a client attempts to access data in Azure file shares, the request is sent to the domain service, either AD DS or Azure AD DS, to authenticate the identity. If authentication is successful, it returns a Kerberos token. The client sends a request that includes the Kerberos token and Azure file shares use that token to authorize the request. Azure file shares only receive the Kerberos token, not access credentials. Preserve directory and file ACLs when importing data to Azure file shares Azure Files supports preserving directory or file level ACLs when copying data to Azure file shares. You can copy ACLs on a directory or file to Azure file shares using either Azure File Sync or common file movement toolsets. For example, you can use robocopy with the /copy:s flag to copy data as well as ACLs to an Azure file share. ACLs are preserved by default, you are not required to enable identity-based authentication on your storage account to preserve ACLs. Enable the secure transfer required property You can configure your storage account to accept requests from secure connections only by setting the Secure transfer required property for the storage account. When you require secure transfer, any requests originating from an insecure connection are rejected. Microsoft recommends that you always require secure transfer for all of your storage accounts. When secure transfer is required, a call to an Azure Storage REST API operation must be made over HTTPS. Any request made over HTTP is rejected. Connecting to an Azure File share over SMB without encryption fails when secure transfer is required for the storage account. Examples of insecure connections include those made over SMB 2.1, SMB 3.0 without encryption, or some versions of the Linux SMB client. By default, the Secure transfer required property is enabled when you create a storage account. Azure Storage doesn't support HTTPS for custom domain names, this option is not applied when you're using a custom domain name. Require secure transfer for a new storage account Important Azure Files connections require encryption (SMB) Perform Try-This exercises Use this Try-This exercise to gain some hands-on experience with Azure. In this demonstration, we'll explore storage security configurations. Task 1: Generate SAS tokens Note This demonstration requires a storage account with a blob container and an uploaded file. For the best results, upload a PNG or JPEG file. In this task, we'll generate and test a Shared Access Signature. Open the Azure portal. Navigate to your Storage Account. Under Settings, select Access keys. Explain how Storage Account access keys can be used. Review regenerating keys. Under Settings, select Shared access signature. Explain how an account-level SAS can be used. Review the configuration settings, including Allowed services, Allowed resource type, Allowed permissions, and Start and expiry date/times. Back at the Storage Account page, under Blob service, select Containers. Right-click the blob file that you want to share and select Generate SAS. Click Generate SAS token and URL. Copy the Blob SAS URL. There's a clipboard icon on the far right of the text box. Copy the URL into a browser, and your file should display. Task 2: Key Rollover Note Always use the latest version of Azure Storage Explorer. In this task, we'll use Storage Explorer to test key rollover. Download and install Azure Storage Explorer - https://azure.microsoft.com/features/storage-explorer/ After the installation, launch the tool. Review the Release Notes and menu options. If this is your first time using the tool, you'll need to re-enter your credentials. After you've been authenticated, you can select the subscriptions of interest. Explain Storage Explorer can also be used for Local and attached accounts. Right-click Storage Accounts and select Connect to Azure storage. Discuss the various connection options. Select Use a storage account name and key. In the portal, select your storage account. Under Settings, select Access Keys. Retrieve the Storage account name and key1 key. In Storage Explorer, provide the account and key information, then click Connect. Verify that you can navigate to your storage account content. In the portal and your storage account. Under Settings, select Access Keys. Next to key1 click the Regenerate icon. Acknowledge the message that the current key will become immediately invalid and isn't recoverable. In Storage Explorer, refresh the storage account. You should receive an error that the server failed to authenticate the request. Reconnect so you can continue with the demonstration. Task 3: Storage Access Policies In this task, we'll create a blob storage access policy. In the Portal, navigate to your Blob container. Under Settings, select Access Policy. Review the two policies: Storage access policies and Blob immutable storage. Under Stored access polices click Add policy. Create a policy with Read and List permissions and usable for a restricted period of time. Under Blob immutable storage, click Add policy. Review the two policy types: Time-based retention and Legal hold. Create a policy based on time-based retention. Be sure to Save your changes. In Storage Explorer, right-click your container and select Get shared access signature. The Access Policy drop-down enables you to create a SAS based on a pre-defined configuration. As you have time, show how Storage Explorer can be used to perform security tasks. Task 4: Azure AD User Account Authentication In this task, we will configure Azure AD user account authentication for storage. In the portal, navigate to and select your blob container. Notice at the top the authentication method. There are two choices: Access key and Azure AD User Account. Explain the differences between the two methods. Switch to Azure AD User Account. You should receive an error stating you don't have access permissions. Click Access Control (IAM). Select Add role assignment. Select the Storage Blob Data Owner role. Discuss the other storage roles that are shown. Assign the role to your account and Save your changes. Return to the Overview blade. Switch to Azure AD User Account. Notice that you're now able to view the container. Take a minute to select Change access level and review the Public access level choices. Task 5: Storage Endpoints (if you haven't already done this in the Network lesson) Note This task requires a storage account and virtual network with subnet. Storage Explorer is also required. In this task, we'll secure a storage endpoint. In the Portal. Locate your storage account. Create a file share, and upload a file. Use the Shared Access Signature blade to Generate SAS and connection string. Use Storage Explorer and the connection string to access the file share. Ensure you can view your uploaded file. Locate your virtual network, and then select a subnet in the virtual network. Under Service Endpoints, view the Services drop-down and the different services that can be secured with an endpoint. Check the Microsoft.Storage option. Save your changes. Return to your storage account. Select Firewalls and virtual networks. Change to Selected networks. Add your virtual network and verify your subnet with the new service endpoint is listed. Save your changes. Return to the Storage Explorer. Refresh the storage account. Verify you can no longer access the file share. Knowledge check There is a need to provide a contingent staff employee temporary read-only access to the contents of an Azure storage account container named \u201cMedia\u201d. It is important to grant access while adhering to the security principle of least-privilege. What should be configured? Set the public access level to container. Generate a shared access signature (SAS) token for the container. ( Ans ) Share the container entity tag (Etag) with the contingent staff member. A company has both a development and production environment. The development environment needs time-limited access to storage. The production environment needs unrestricted access to storage resources. To configure storage access to meet the requirements, what configuration choices should be done? Use shared access signatures for the development apps. And use access keys for the production apps. ( Ans ) Use shared access signatures for the production apps. Then, use access keys for the development apps. Use Stored Access Policies for the production apps. Also, use Cross Origin Resource Sharing for the development apps. A company is being audited. It is not known how long the audit will take, but during that time files must not be changed or removed. It is okay to read or create new files. What should be done to set this up? Add a time-based retention policy to the blob container. And create a tag to identify items being protected. Add legal hold retention policy to the blob container. Also, identify a tag for the items that are being protected. ( Ans ) Configure a retention time period of two weeks with an option to renew. Then, add a time-based retention policy to the blob container. Which statements are factual when configuring an Azure file share for a business group? Azure Files can authenticate to Azure Active Directory Domain Services. ( Ans ) Azure Files cannot authenticate to on-premises Active Directory Domain Services. Azure Files can use RBAC for share-level or directory/file permissions. When configuring Secure transfer, the Compliance office wants to understand how connections are secured when REST API operational calls are made to an Azure Storage account. What information would be provided? Requests to storage can be HTTPS or HTTP. Requests to storage must be SMB with data access flag enabled. By default, new storage accounts have secure transfer required enabled. ( Ans ) Configure and manage SQL database security # Configure SQL database firewalls Start your database security process by configuring a SQL Database firewall. Azure SQL Database and Azure Synapse Analytics, previously SQL Data Warehouse, (both referred to as SQL Database in this lesson) provide a relational database service for Azure and other internet-based applications. To help protect your data, firewalls prevent all access to your database server until you specify which computers have permission. The firewall grants access to databases based on the originating IP address of each request. In addition to IP rules, the firewall also manages virtual network rules. Virtual network rules are based on virtual network service endpoints. Virtual network rules might be preferable to IP rules in some cases. Overview Initially, all access to your Azure SQL Database is blocked by the SQL Database firewall. To access a database server, you must specify one or more server-level IP firewall rules that enable access to your Azure SQL Database. Use the IP firewall rules to specify which IP address ranges from the internet are allowed, and whether Azure applications can attempt to connect to your Azure SQL Database. To selectively grant access to just one of the databases in your Azure SQL Database, you must create a database-level rule for the required database. Specify an IP address range for the database IP firewall rule that is beyond the IP address range specified in the server-level IP firewall rule, and ensure that the IP address of the client falls in the range specified in the database-level rule. Note Azure Synapse Analytics only supports server-level IP firewall rules, and not database-level IP firewall rules. Connecting from the internet When a computer attempts to connect to your database server from the internet, the firewall first checks the originating IP address of the request against the database-level IP firewall rules for the database that the connection is requesting: If the IP address of the request is within one of the ranges specified in the database-level IP firewall rules, the connection is granted to the SQL Database containing the rule. If the IP address of the request is not within one of the ranges specified in the database-level IP firewall rules, the firewall checks the server-level IP firewall rules. If the IP address of the request is within one of the ranges specified in the server-level IP firewall rules, the connection is granted. Server-level IP firewall rules apply to all SQL databases on the Azure SQL Database. If the IP address of the request is not within the ranges specified in any of the database-level or server-level IP firewall rules, the connection request fails. Connecting from Azure To allow applications from Azure to connect to your Azure SQL Database, Azure connections must be enabled. When an application from Azure attempts to connect to your database server, the firewall verifies that Azure connections are allowed. A firewall setting with starting and ending addresses equal to 0.0.0.0 indicates Azure connections are allowed. If the connection attempt is not allowed, the request does not reach the Azure SQL Database server. This option configures the firewall to allow all connections from Azure including connections from the subscriptions of other customers. When selecting this option, make sure your sign-in and user permissions limit access to authorized users only. Server-level IP firewall rules Server-level IP firewall rules enable clients to access your entire Azure SQL Database\u2014that is, all the databases within the same SQL Database server. These rules are stored in the master database. You can configure server-level IP firewall rules using the Azure portal, PowerShell, or by using Transact-SQL statements. To create server-level IP firewall rules using the Azure portal or PowerShell, you must be the subscription owner or a subscription contributor. To create a server-level IP firewall rule using Transact-SQL, you must connect to the SQL Database instance as the server-level principal login or the Azure Active Directory (Azure AD) administrator (which means that a server-level IP firewall rule must have first been created by a user with Azure-level permissions). Database-level IP firewall rules Database-level IP firewall rules enable clients to access certain secure databases within the same SQL Database server. You can create these rules for each database (including the master database), and they are stored in the individual databases. You can only create and manage database-level IP firewall rules for master databases and user databases by using Transact-SQL statements, and only after you have configured the first server-level firewall. If you specify an IP address range in the database-level IP firewall rule that is outside the range specified in the server-level IP firewall rule, only those clients that have IP addresses in the database-level range can access the database. You can have a maximum of 128 database-level IP firewall rules for a database. Important Whenever possible, as a best practice, use database-level IP firewall rules to enhance security and to make your database more portable. Use server-level IP firewall rules for administrators and when you have several databases with the same access requirements, and you don't want to spend time configuring each database individually. Enable and monitor database auditing Auditing for Azure SQL Database and Azure Synapse Analytics tracks database events and writes them to an audit log in your Azure storage account, Log Analytics workspace or Event Hubs. Auditing also: Helps you maintain regulatory compliance, understand database activity, and gain insight into discrepancies and anomalies that could indicate business concerns or suspected security violations. Enables and facilitates adherence to compliance standards, although it doesn't guarantee compliance. Overview You can use SQL database auditing to: Retain an audit trail of selected events. You can define categories of database actions to be audited. Report on database activity. You can use pre-configured reports and a dashboard to get started quickly with activity and event reporting. Analyze reports. You can find suspicious events, unusual activity, and trends. Define server-level vs. database-level auditing policy An auditing policy can be defined for a specific database or as a default server policy: A server policy applies to all existing and newly created databases on the server. If server auditing is enabled, it always applies to the database. The database will be audited, regardless of the database auditing settings. Enabling auditing on the database or data warehouse, in addition to enabling it on the server, does not override or change any of the settings of the server auditing. Both audits will exist side by side. In other words, the database is audited twice in parallel; once by the server policy and once by the database policy. Shown below is the configuration of auditing using the Azure portal. Summary of database auditing Retain an audit trail of selected events Report on database activity and analyze results Configure policies for the server or database level Configure audit log destination A new server policy applies to all existing and newly created databases Implement data discovery and classification Data Discovery & Classification is built into Azure SQL Database. It provides advanced capabilities for discovering, classifying, labeling, and reporting the sensitive data in your databases. Your most sensitive data might include business, financial, healthcare, or personal information. Discovering and classifying this data can play a pivotal role in your organization's information-protection approach. It can serve as infrastructure for: Helping to meet standards for data privacy and requirements for regulatory compliance. Various security scenarios, such as monitoring (auditing) and alerting on anomalous access to sensitive data. Controlling access to and hardening the security of databases that contain highly sensitive data. Data Discovery & Classification is part of the Advanced Data Security offering, which is a unified package for advanced SQL security capabilities. You can access and manage Data Discovery & Classification via the central SQL Advanced Data Security section of the Azure portal. Classifying your data and identifying your data protection needs helps you select the right cloud solution for your organization. Data classification enables organizations to find storage optimizations that might not be possible when all data is assigned the same value. Classifying (or categorizing) stored data by sensitivity and business impact helps organizations determine the risks associated with the data. After your data has been classified, organizations can manage their data in ways that reflect their internal value instead of treating all data the same way. Data classification can yield benefits such as compliance efficiencies, improved ways to manage the organization\u2019s resources, and facilitation of migration to the cloud. Some data protection solutions\u2014such as encryption, rights management, and data loss prevention\u2014have moved to the cloud and can help mitigate cloud risks. However, organization must be sure to address data classification rules for data retention when moving to the cloud. Data exists in one of three basic states: at rest, in process, and in transit. All three states require unique technical solutions for data classification, but the applied principles of data classification should be the same for each. Data that is classified as confidential needs to stay confidential when at rest, in process, or in transit. Data can also be either structured or unstructured. Typical classification processes for structured data found in databases and spreadsheets are less complex and time-consuming to manage than those for unstructured data such as documents, source code, and email. Generally, organizations will have more unstructured data than structured data. Regardless of whether data is structured or unstructured, it\u2019s important for organizations to manage data sensitivity. When properly implemented, data classification helps ensure that sensitive or confidential data assets are managed with greater oversight than data assets that are considered public distribution. Protect data at rest Data encryption at rest is a mandatory step toward data privacy, compliance, and data sovereignty. Best practice Solution Apply disk encryption to help safeguard your data. Use Microsoft Azure Disk Encryption, which enables IT administrators to encrypt both Windows infrastructure as a service (IaaS) and Linux IaaS virtual machine (VM) disks. Disk encryption combines the industry-standard BitLocker feature and the Linux DM-Crypt feature to provide volume encryption for the operating system (OS) and the data disks. Azure Storage and Azure SQL Database encrypt data at rest by default, and many services offer encryption as an option. You can use Azure Key Vault to maintain control of keys that access and encrypt your data. Use encryption to help mitigate risks related to unauthorized data access. Encrypt your drives before you write sensitive data to them. Organizations that don\u2019t enforce data encryption are risk greater exposure to data-integrity issues. For example, unauthorized users or malicious hackers might steal data in compromised accounts or gain unauthorized access to data coded in Clear Format. To comply with industry regulations, companies also must prove that they are diligent and using correct security controls to enhance their data security. Protect data in transit Protecting data in transit should be an essential part of your data protection strategy. Because data is moving back and forth from many locations, we generally recommend that you always use SSL/TLS protocols to exchange data across different locations. In some circumstances, you might want to isolate the entire communication channel between your on-premises and cloud infrastructures by using a VPN. For data moving between your on-premises infrastructure and Azure, consider appropriate safeguards such as HTTPS or VPN. When sending encrypted traffic between an Azure virtual network and an on-premises location over the public internet, use Azure VPN Gateway. The following table lists best practices specific to using Azure VPN Gateway, SSL/TLS, and HTTPS. Best practice Solution Secure access from multiple workstations located on-premises to an Azure virtual network Use site-to-site VPN. Secure access from an individual workstation located on-premises to an Azure virtual network Use point-to-site VPN. Move larger data sets over a dedicated high-speed wide area network (WAN) link Use Azure ExpressRoute. If you choose to use ExpressRoute, you can also encrypt the data at the application level by using SSL/TLS or other protocols for added protection. Interact with Azure Storage through the Azure portal All transactions occur via HTTPS. You can also use Storage REST API over HTTPS to interact with Azure Storage and Azure SQL Database. Organizations that fail to protect data in transit are more susceptible to man-in-the-middle attacks, eavesdropping, and session hijacking. These attacks can be the first step in gaining access to confidential data. Now that we\u2019ve covered the physical aspects of data classification, let\u2019s look at the classification based on discovery and classification. Data Discovery Data discovery and classification provides advanced capabilities built into Azure SQL Database for discovering, classifying, labeling and protecting sensitive data (such as business, personal data, and financial information) in your databases. Discovering and classifying this data can play a pivotal role in your organizational information protection stature. It can serve as infrastructure for: Helping meet data privacy standards and regulatory compliance requirements. Addressing various security scenarios such as monitoring, auditing, and alerting on anomalous access to sensitive data. Controlling access to and hardening the security of databases containing highly sensitive data. Data discovery and classification is part of the Advanced Data Security offering, which is a unified package for advanced Microsoft SQL Server security capabilities. You access and manage data discovery and classification via the central SQL Advanced Data Security portal. Data discovery and classification introduces a set of advanced services and SQL capabilities, forming a SQL Information Protection paradigm aimed at protecting the data, not just the database: Discovery and recommendations - The classification engine scans your database and identifies columns containing potentially sensitive data. It then provides you with an easier way to review and apply the appropriate classification recommendations via the Azure portal. Labeling - Sensitivity classification labels can be persistently tagged on columns using new classification metadata attributes introduced into the SQL Server Engine. This metadata can then be utilized for advanced sensitivity-based auditing and protection scenarios. Query result set sensitivity - The sensitivity of the query result set is calculated in real time for auditing purposes. Visibility - You can view the database classification state in a detailed dashboard in the Azure portal. Additionally, you can download a report (in Microsoft Excel format) that you can use for compliance and auditing purposes, in addition to other needs. Steps for discovery, classification, and labeling Classifications have two metadata attributes: Labels - These are the main classification attributes used to define the sensitivity level of the data stored in the column. Information Types - These provide additional granularity into the type of data stored in the column. SQL data discovery and classification comes with a built-in set of sensitivity labels and information types, and discovery logic. You can now customize this taxonomy and define a set and ranking of classification constructs specifically for your environment. Definition and customization of your classification taxonomy takes place in one central location for your entire Azure Tenant. That location is in Microsoft Defender for Cloud, as part of your Security Policy. Only a user with administrative rights on the Tenant root management group can perform this task. As part of Azure Information Protection policy management, you can define custom labels, rank them, and associate them with a selected set of information types. You can also add your own custom information types and configure them with string patterns, which are added to the discovery logic for identifying this type of data in your databases. Learn more about customizing and managing your policy in the Information Protection policy how-to guide. After you\u2019ve defined the tenant-wide policy, you can continue with classifying individual databases using your customized policy. Microsoft Defender for SQL Applies to: Azure SQL Database | Azure SQL Managed Instance | Azure Synapse Analytics Microsoft Defender for SQL is a Defender plan in Microsoft Defender for Cloud. Microsoft Defender for SQL includes functionality for surfacing and mitigating potential database vulnerabilities and detecting anomalous activities that could indicate a threat to your database. It provides a single go-to location for enabling and managing these capabilities. What are the benefits of Microsoft Defender for SQL? Microsoft Defender for SQL provides a set of advanced SQL security capabilities, including SQL Vulnerability Assessment and Advanced Threat Protection. Vulnerability Assessment is an easy-to-configure service that can discover, track, and help you remediate potential database vulnerabilities. It provides visibility into your security state, and it includes actionable steps to resolve security issues and enhance your database fortifications. Advanced Threat Protection detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit your database. It continuously monitors your database for suspicious activities, and it provides immediate security alerts on potential vulnerabilities, Azure SQL injection attacks, and anomalous database access patterns. Advanced Threat Protection alerts provide details of suspicious activity and recommend action on how to investigate and mitigate the threat. Enable Microsoft Defender for SQL once to enable all these included features. With one selection, you can enable Microsoft Defender for all databases on your server in Azure or in your SQL Managed Instance. Enabling or managing Microsoft Defender for SQL settings requires belonging to the SQL security manager role or one of the database or server admin roles. Example: Database selection options 4/4 Vulnerability assessment for SQL Server What is SQL vulnerability assessment? SQL vulnerability assessment is a service that provides visibility into your security state. Vulnerability assessment includes actionable steps to resolve security issues and enhance your database security. It can help you to monitor a dynamic database environment where changes are difficult to track and improve your SQL security posture. Vulnerability assessment is a scanning service built into Azure SQL Database. The service employs a knowledge base of rules that flag security vulnerabilities. It highlights deviations from best practices, such as misconfigurations, excessive permissions, and unprotected sensitive data. The rules are based on Microsoft's best practices and focus on the security issues that present the biggest risks to your database and its valuable data. They cover database-level issues and server-level security issues, like server firewall settings and server-level permissions. The results of the scan include actionable steps to resolve each issue and provide customized remediation scripts where applicable. You can customize an assessment report for your environment by setting an acceptable baseline for: Permission configurations Feature configurations Database settings SQL vulnerability assessment is an easy-to-configure service that can discover, track, and help you remediate potential database vulnerabilities. Use it to proactively improve your database security for: Azure SQL Database Azure SQL Managed Instance Azure Synapse Analytics Vulnerability assessment is part of Microsoft Defender for Azure SQL, which is a unified package for advanced SQL security capabilities. Vulnerability assessment can be accessed and managed from each SQL database resource in the Azure portal. SQL Advanced Threat Protection Advanced Threat Protection for Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics, SQL Server on Azure Virtual Machines and Azure Arc-enabled SQL Server detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases. Advanced Threat Protection is part of the Microsoft Defender for SQL offering, which is a unified package for advanced SQL security capabilities. Advanced Threat Protection can be accessed and managed via the central Microsoft Defender for SQL portal. Overview Advanced Threat Protection provides a new layer of security, which enables customers to detect and respond to potential threats as they occur by providing security alerts on anomalous activities. Users receive an alert upon suspicious database activities, potential vulnerabilities, and SQL injection attacks, as well as anomalous database access and queries patterns. Advanced Threat Protection integrates alerts with Microsoft Defender for Cloud, which include details of suspicious activity and recommend action on how to investigate and mitigate the threat. Advanced Threat Protection makes it simple to address potential threats to the database without the need to be a security expert or manage advanced security monitoring systems. For a full investigation experience, it is recommended to enable auditing, which writes database events to an audit log in your Azure storage account. Explore detection of a suspicious event You receive an email notification upon detection of anomalous database activities. The email provides information on the suspicious security event, including the nature of the anomalous activities, database name, server name, application name, and event time. In addition, the email provides information on possible causes and recommended actions to investigate and mitigate the potential threat to the database. Example: Email notification providing information on a suspicious security event. Click the View recent SQL alerts link in the email to launch the Azure portal and show the Microsoft Defender for Cloud alerts page, which provides an overview of active threats detected on the database. Click a specific alert to get additional details and actions for investigating this threat and remediating future threats. For example, SQL injection is one of the most common Web application security issues on the Internet that is used to attack data-driven applications. Attackers take advantage of application vulnerabilities to inject malicious SQL statements into application entry fields, breaching or modifying data in the database. For SQL Injection alerts, the alert's details include the vulnerable SQL statement that was exploited. Explore alerts in the Azure portal Advanced Threat Protection integrates its alerts with Microsoft Defender for Cloud. Live SQL Advanced Threat Protection tiles within the database and SQL Microsoft Defender for Cloud blades in the Azure portal track the status of active threats. Click the Advanced Threat Protection alert to launch the Microsoft Defender for Cloud alerts page and get an overview of active SQL threats detected on the database. SQL vulnerability assessment express and classic configurations What are the SQL vulnerability assessments express and classic configurations? You can configure vulnerability assessment for your SQL databases with either: Express configuration (preview) \u2013 The default procedure that lets you configure vulnerability assessment without dependency on external storage to store baseline and scan result data. Classic configuration \u2013 The legacy procedure that requires you to manage an Azure storage account to store baseline and scan result data. Configuration modes benefits and limitations comparison: Configure dynamic data masking SQL Database dynamic data masking (DDM) limits sensitive data exposure by masking it to non-privileged users. Dynamic data masking helps prevent unauthorized access to sensitive data by enabling customers to designate how much of the sensitive data to reveal with minimal impact on the application layer. It\u2019s a policy-based security feature that hides the sensitive data in the result set of a query over designated database fields, while the data in the database is not changed. For example, a service representative at a call center may identify callers by several digits of their credit card number, but those data items should not be fully exposed to the service representative. A masking rule can be defined that masks all but the last four digits of any credit card number in the result set of any query. As another example, an appropriate data mask can be defined to protect personal data, so that a developer can query production environments for troubleshooting purposes without violating compliance regulations. Dynamic data masking basics You set up a dynamic data masking policy in the Azure portal by selecting the dynamic data masking operation in your SQL Database configuration blade or settings blade. This feature cannot be set by using portal for Azure Synapse Dynamic data masking policy SQL users excluded from masking - A set of SQL users or Azure AD identities that get unmasked data in the SQL query results. Users with administrator privileges are always excluded from masking, and view the original data without any mask. Masking rules - A set of rules that define the designated fields to be masked and the masking function that is used. The designated fields can be defined using a database schema name, table name, and column name. Masking functions - A set of methods that control the exposure of data for different scenarios. Recommended fields to mask The DDM recommendations engine, flags certain fields from your database as potentially sensitive fields, which may be good candidates for masking. In the Dynamic Data Masking blade in the portal, you can review the recommended columns for your database. All you need to do is click Add Mask for one or more columns and then Save to apply a mask for these fields. Implement transparent data encryption Transparent data encryption (TDE) helps protect Azure SQL Database, Azure SQL Managed Instance, and Synapse SQL in Azure Synapse Analytics against the threat of malicious offline activity by encrypting data at rest. It performs real-time encryption and decryption of the database, associated backups, and transaction log files at rest without requiring changes to the application. By default, TDE is enabled for all newly deployed Azure SQL databases and needs to be manually enabled for older databases of Azure SQL Database, Azure SQL Managed Instance, or Azure Synapse. TDE performs real-time I/O encryption and decryption of the data at the page level. Each page is decrypted when it's read into memory and then encrypted before being written to disk. TDE encrypts the storage of an entire database by using a symmetric key called the Database Encryption Key (DEK). On database startup, the encrypted DEK is decrypted and then used for decryption and re-encryption of the database files in the SQL Server Database Engine process. DEK is protected by the TDE protector. TDE protector is either a service-managed certificate (service-managed transparent data encryption) or an asymmetric key stored in Azure Key Vault (customer-managed transparent data encryption). For Azure SQL Database and Azure Synapse, the TDE protector is set at the logical SQL server level and is inherited by all databases associated with that server. For Azure SQL Managed Instance (BYOK feature in preview), the TDE protector is set at the instance level and it is inherited by all encrypted databases on that instance. The term server refers both to server and instance throughout this document, unless stated differently. Service-managed transparent data encryption In Azure, the default setting for TDE is that the DEK is protected by a built-in server certificate. The built-in server certificate is unique for each server and the encryption algorithm used is AES 256. If a database is in a geo-replication relationship, both the primary and geo-secondary databases are protected by the primary database's parent server key. If two databases are connected to the same server, they also share the same built-in certificate. Microsoft automatically rotates these certificates in compliance with the internal security policy and the root key is protected by a Microsoft internal secret store. Customers can verify SQL Database compliance with internal security policies in independent third-party audit reports available on the Microsoft Trust Center. Microsoft also seamlessly moves and manages the keys as needed for geo-replication and restores. Customer-managed transparent data encryption - Bring Your Own Key Customer-managed TDE is also referred to as Bring Your Own Key (BYOK) support for TDE. In this scenario, the TDE Protector that encrypts the DEK is a customer-managed asymmetric key, which is stored in a customer-owned and managed Azure Key Vault (Azure's cloud-based external key management system) and never leaves the key vault. The TDE Protector can be generated by the key vault or transferred to the key vault from an on premises hardware security module (HSM) device. SQL Database needs to be granted permissions to the customer-owned key vault to decrypt and encrypt the DEK. If permissions of the logical SQL server to the key vault are revoked, a database will be inaccessible, and all data is encrypted With TDE with Azure Key Vault integration, users can control key management tasks including key rotations, key vault permissions, key backups, and enable auditing/reporting on all TDE protectors using Azure Key Vault functionality. Key Vault provides central key management, leverages tightly monitored HSMs, and enables separation of duties between management of keys and data to help meet compliance with security policies. Manage TDE in the Azure portal To configure TDE through the Azure portal, you must be connected as the Azure Owner, Contributor, or SQL Security Manager. You turn TDE on and off on the database level. To enable TDE on a database, go to the Azure portal and sign in with your Azure Administrator or Contributor account. Find the TDE settings under your user database. By default, service-managed transparent data encryption is used. A TDE certificate is automatically generated for the server that contains the database. For Azure SQL Managed Instance use T-SQL to turn TDE on and off on a database. Deploy always encrypted features Ensure that your database is always encrypted. SQL Database Always Encrypted Always Encrypted is a feature designed to protect sensitive data, such as credit card numbers or national/regional identification numbers (for example, U.S. social security numbers), stored in Azure SQL Database or SQL Server databases. Always Encrypted allows clients to encrypt sensitive data inside client applications and never reveal the encryption keys to the Database Engine (SQL Database or SQL Server). As a result, Always Encrypted provides a separation between users who own the data (and can view it) and users who manage the data (but should have no access). By ensuring on-premises database administrators, cloud database operators, or other high-privileged, but unauthorized users, cannot access the encrypted data. Always Encrypted enables customers to confidently store sensitive data outside of their direct control. So, Always Encrypted allows organizations to encrypt data at rest and in use for storage in Azure, to enable delegation of on-premises database administration to third parties, or to reduce security clearance requirements for their own DBA staff. Always Encrypted makes encryption transparent to applications. An Always Encrypted-enabled driver installed on the client computer achieves this by automatically encrypting and decrypting sensitive data in the client application. The driver encrypts the data in sensitive columns before passing the data to the Database Engine, and automatically rewrites queries so that the semantics to the application are preserved. Similarly, the driver transparently decrypts data, stored in encrypted database columns, contained in query results. Example usage scenarios Client On-Premises with Data in Azure A customer has an on-premises client application at their business location. The application operates on sensitive data stored in a database hosted in Azure (SQL Database or SQL Server running in a virtual machine on Microsoft Azure). The customer uses Always Encrypted and stores Always Encrypted keys in a trusted key store hosted on-premises, to ensure Microsoft cloud administrators have no access to sensitive data. Client and Data in Azure A customer has a client application, hosted in Microsoft Azure (for example, in a worker role or a web role), which operates on sensitive data stored in a database hosted in Azure (SQL Database or SQL Server running in a virtual machine on Microsoft Azure). Although Always Encrypted does not provide complete isolation of data from cloud administrators, as both the data and keys are exposed to cloud administrators of the platform hosting the client tier, the customer still benefits from reducing the security attack surface area (the data is always encrypted in the database). Always Encrypted Features The Database Engine never operates on plaintext data stored in encrypted columns, but it still supports some queries on encrypted data, depending on the encryption type for the column. Always Encrypted supports two types of encryption: randomized encryption and deterministic encryption. Deterministic encryption always generates the same encrypted value for any given plain text value. Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns. However, it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column, especially if there is a small set of possible encrypted values, such as True/False, or North/South/East/West region. Deterministic encryption must use a column collation with a binary2 sort order for character columns. Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns. Use deterministic encryption for columns that will be used as search or grouping parameters, for example a government ID number. Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables. Deploy an always encrypted implementation A proper always encrypting implementation is important to a secure database. Configuring Always Encrypted The initial setup of Always Encrypted in a database involves generating Always Encrypted keys, creating key metadata, configuring encryption properties of selected database columns, and/or encrypting data that may already exist in columns that need to be encrypted. Remember that some of these tasks aren't supported in Transact-SQL and require the use of client-side tools. As Always Encrypted keys and protected sensitive data are never revealed in plaintext to the server, the Database Engine can't be involved in key provisioning and perform data encryption or decryption operations. You can use SQL Server Management Studio (SSMS) or PowerShell to accomplish such tasks. Task SSMS PowerShell SQL Provisioning column master keys, column encryption keys and encrypted column encryption keys with their corresponding column master keys Yes Yes No Creating key metadata in the database Yes Yes Yes Creating new tables with encrypted columns Yes Yes Yes Encrypting existing data in selected database columns Yes Yes No When setting up encryption for a column, you specify the information about the encryption algorithm and cryptographic keys used to protect the data in the column. Always Encrypted uses two types of keys: column encryption keys and column master keys. A column encryption key is used to encrypt data in an encrypted column. A column master key is a key-protecting key that encrypts one or more column encryption keys. The Database Engine stores encryption configuration for each column in database metadata. Note, however, the Database Engine never stores or uses the keys of either type in plaintext. It only stores encrypted values of column encryption keys and the information about the location of column master keys, which are stored in external trusted key stores, such as Azure Key Vault, Windows Certificate Store on a client machine, or a hardware security module. To access data stored in an encrypted column in plaintext, an application must use an Always Encrypted enabled client driver. When an application issues a parameterized query, the driver transparently collaborates with the Database Engine to determine which parameters target encrypted columns and, thus, should be encrypted. For each parameter that needs to be encrypted, the driver obtains the information about the encryption algorithm and the encrypted value of the column encryption key for the column, the parameter targets, as well as the location of its corresponding column master key. Next, the driver contacts the key store, containing the column master key, in order to decrypt the encrypted column encryption key value and then, it uses the plaintext column encryption key to encrypt the parameter. The resultant plaintext column encryption key is cached to reduce the number of round trips to the key store on subsequent uses of the same column encryption key. The driver substitutes the plaintext values of the parameters targeting encrypted columns with their encrypted values, and it sends the query to the server for processing. The server computes the result set, and for any encrypted columns included in the result set, the driver attaches the encryption metadata for the column, including the information about the encryption algorithm and the corresponding keys. The driver first tries to find the plaintext column encryption key in the local cache, and only makes a round to the column master key if it can't find the key in the cache. Next, the driver decrypts the results and returns plaintext values to the application. A client driver interacts with a key store, containing a column master key, using a column master key store provider, which is a client-side software component that encapsulates a key store containing the column master key. Providers for common types of key stores are available in client-side driver libraries from Microsoft or as standalone downloads. You can also implement your own provider. Always Encrypted capabilities, including built-in column master key store providers vary by a driver library and its version. Perform Try-This exercises Note These demonstrations require an Azure SQL database with sample data. In Task 1, there are instructions to install the AdventureWorks sample database. Also, Task 3 requires SQL Server Management Studio. Task 1 - Azure SQL: Advanced Data Security and Auditing In this task, we will explore vulnerability assessments, data discovery and classification, and auditing. Install the AdventureWorks sample database In the Portal, search for and a select SQL databases. On the Basics tab, give your database a name, and create a new server. On the Additional settings tab, select Sample for Use existing data. Also, Enable advanced data security and Start free trial. Review & create, and then Create. Wait for the database to deploy. Review Vulnerability Assessments Navigate to your SQL database. Under Security select Microsoft Defender for Cloud. Select Vulnerability Assessment. Review vulnerability assessments and the risk levels. Click Scan. The scan doesn't need to be fully complete for results to show. Review the Findings. Click any Security Check to get more details. Review the Passed checks. Notice Export Scan Results and Scan History Review Data Discovery and Classification Return to the Security blade. Select Data Discovery & Classification. On the Classification tab, select Add classification. Schema name: SalesLT Table name: Customer Column name: Phone Information type: Contact Info Sensitivity label: Confidential When finished click Add classification. Click the blue bar columns with classification recommendations. Notice the data that has been recommended for classification. Select the data of interest and then click Accept selected recommendations. Save your changes. Review Auditing Return to your SQL database. Under Security select Auditing. Select On for auditing. Click Storage for the destination. Select on the Storage account for logs. Set Retention day to 45 days. Set storage access key to Primary. Save your changes. Discuss Server level auditing and when how it could be used. Task 2 - Azure SQL: Diagnostics Note This demonstration requires an Azure SQL database. In this task, we will review and configure SQL database diagnostics. In the Portal, search for and launch SQL databases. From the Overview blade, review the Compute utilization data graphic. Data is available for different time frames (1 hour, 24 hours, 7 days). Under Monitoring select Diagnostic settings. Click Add diagnostic setting. Give your setting a name. Under Destination details select Send to Log Analytics. Make a note of the Log Analytics workspace that will be used. Under Destination details select Archive to Storage Account. Select the Errors log. Select the Automatic tuning log. Select the Basic metric. Give each item a retention time of 45 days. Retention only applies to storage account. Save your diagnostic setting. In the Portal, search for and launch the Log Analytics workspace. Select the workspace that is being using for your database diagnostics. Under General select Usage and estimated costs. Click Data retention. Use the slider to show how to increase the data retention time. Discuss how additional charges can incur, depending on the pricing plan. Under General select Workspace summary. Click Add and then search the Marketplace for Azure SQL. This feature may be in Preview. Explain the benefits of using this product. Select and then create Azure SQL Analytics. It will take few minutes for the product to deploy. Click Go to resource once the deployment is completed. Click Azure SQL databases. Review the additional metrics that are provided by this product. You can drill into any graphic for additional details. Task 3 - Azure SQL: Azure AD Authentication Note This task requires an Azure SQL database that has not had Azure AD configured. This task also requires SQL Server Management Studio. In this task, we will configure Azure AD authentication. In the Portal. Navigate to your SQL database. On the Overview page, there's an Active Directory admin box that shows the current status, configured or not configured. Under Settings select Active Directory admin. Click Set admin. Search for and Select the new Active Directory admin. Remember this user you'll need in following steps. Be sure to Save your changes. In SQL Server Management Studio connect to the database server using your credentials. Select the SQL database you configured with a new Active Directory admin. Construct a query to create a new user. Insert the admin user and domain. For example, user@contoso.com Create user [user@contoso.com] from external provider; Run the query and ensure it completes successfully. In the Object Explorer navigate your database and Security and Users folder. Verify that the new admin user is shown. Connect to the new database with the new admin credentials. Verify that you can successfully access the database. Knowledge check Choose the best response for each of the questions. Then select Check your answers. An SQL database administrator has recently read about SQL injection attacks. They ask what can be done to minimize the risk of this type of attack. Implementing which of the following features will help protect the database? Advanced Threat Protection ( Ans ) Data Discovery and Classification Dynamic Data Masking An organization provides a Help Desk for its customers. Service representatives need to identify callers using the last four numbers of their credit card. There is a need to ensure the complete credit card number isn't fully exposed to the service representatives. Which of the following features should be implemented? Always Encrypted Data Classification Dynamic Data Masking ( Ans ) Auditors need to be assured that sensitive database data always remains encrypted at rest, in transit, and in use. To assure the auditors this is being done, which of the below features is configured? Always Encrypted ( Ans ) Disk Encryption Dynamic Data Masking An App Service web application uses an SQL database. Users need to authenticate to the database with their Azure AD credentials. Which of the following configuration tasks would enable this? Create an SQL Database Administrator Create an Azure AD Database Administrator Create users in each database ( Ans ) What type of firewall rules can be configured for an Azure SQL database? Datacenter-level firewall rules Server-level firewall rules ( Ans ) Table-level firewall rules","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Secure-your-data-and-applications.html#az-500-microsoft-certified-azure-security-engineer-associate","text":"","title":"AZ-500: Microsoft Certified: Azure Security Engineer Associate"},{"location":"Cloud/Azure/AZ-500/Secure-your-data-and-applications.html#secure-your-data-and-applications","text":"Application running within Azure access your confidential data need to be locked down. Learn to secure your applications, storage, databases, and key vaults. This learning path helps prepare you for Exam AZ-500: Microsoft Azure Security Technologies . Deploy and secure Azure Key Vault Introduction Explore Azure Key Vault Configure Key Vault access Review a secure Key Vault example Deploy and manage Key Vault certificates Create Key Vault keys Manage customer managed keys Enable Key Vault secrets Configure key rotation Manage Key Vault safety and recovery features Perform Try-This exercises Explore the Azure Hardware Security Module Knowledge check Summary Configure application security features Introduction Review the Microsoft identity platform Explore Azure AD application scenarios Register an application with App Registration Configure Microsoft Graph permissions Enable managed identities Azure App Services App Service Environment Azure App Service plan App Service Environment networking Availability Zone Support for App Service Environments App Service Environment Certificates Perform Try-This exercises Knowledge check Summary Implement storage security Introduction Define data sovereignty Configure Azure storage access Deploy shared access signatures Manage Azure AD storage authentication Implement storage service encryption Configure blob data retention policies Configure Azure files authentication Enable the secure transfer required\u200b property Perform Try-This exercises Knowledge check Summary Configure and manage SQL database security Introduction Enable SQL database authentication Configure SQL database firewalls Enable and monitor database auditing Implement data discovery and classification Microsoft Defender for SQL Vulnerability assessment for SQL Server SQL Advanced Threat Protection Explore detection of a suspicious event SQL vulnerability assessment express and classic configurations Configure dynamic data masking Implement transparent data encryption Deploy always encrypted\u200b features Deploy an always encrypted implementation Perform Try-This exercises Knowledge check Summary","title":"Secure your data and applications"},{"location":"Cloud/Azure/AZ-500/Secure-your-data-and-applications.html#explore-azure-key-vault","text":"Protecting your keys is essential to protecting your identity and data in the cloud. Azure Key Vault helps safeguard cryptographic keys and secrets that cloud applications and services use. Key Vault streamlines the key management process and enables you to maintain control of keys that access and encrypt your data. Developers can create keys for development and testing in minutes, and then migrate them to production keys. Security administrators can grant (and revoke) permission to keys, as needed. You can use Key Vault to create multiple secure containers, called vaults. Vaults help reduce the chances of accidental loss of security information by centralizing application secrets storage. Key vaults also control and log the access to anything stored in them. Azure Key Vault can manage requesting and renewing TLS certificates. It provides features for a robust solution for certificate lifecycle management. Azure Key Vault helps address the following issues: Secrets management. You can use Azure Key Vault to securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets. Key management. You use Azure Key Vault as a key management solution, making it easier to create and control the encryption keys used to encrypt your data. Certificate management. Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private SSL/TLS certificates for use with Azure and your internal connected resources. Store secrets backed by hardware security modules (HSMs). The secrets and keys can be protected either by software, or FIPS 140-2 Level 2 validates HSMs. Azure Key Vault is designed to support application keys and secrets. Key Vault is not intended as storage for user passwords. The following table lists security best practices for using Key Vault. Best practice Solution Grant access to users, groups, and applications at a specific scope. Use RBAC\u2019s predefined roles. For example, to grant access to a user to manage key vaults, you would assign the predefined role Key Vault Contributor to this user at a specific scope. The scope in this case would be a subscription, a resource group, or just a specific key vault. If the predefined roles don\u2019t fit your needs, you can define your own roles. Control what users have access to. Access to a key vault is controlled through two separate interfaces: management plane, and data plane. The management plane and data plane access controls work independently. Use RBAC to control what users have access to. For example, if you want to grant an application access to use keys in a key vault, you only need to grant data plane access permissions by using key vault access policies, and no management plane access is needed for this application. Conversely, if you want a user to be able to read vault properties and tags but not have any access to keys, secrets, or certificates, you can grant this user read access by using RBAC, and no access to the data plane is required. Store certificates in your key vault. Azure Resource Manager can securely deploy certificates stored in Azure Key Vault to Azure VMs when the VMs are deployed. By setting appropriate access policies for the key vault, you also control who gets access to your certificate. Another benefit is that you manage all your certificates in one place in Azure Key Vault. Ensure that you can recover a deletion of key vaults or key vault objects. Deletion of key vaults or key vault objects can be either inadvertent or malicious. Enable the soft delete and purge protection features of Key Vault, particularly for keys that are used to encrypt data at rest. Deletion of these keys is equivalent to data loss, so you can recover deleted vaults and vault objects if needed. Practice Key Vault recovery operations on a regular basis. Azure Key Vault is offered in two service tiers\u2014standard and premium The main difference between Standard and Premium is that Premium supports HSM-protected keys. Important If a user has contributor permissions (RBAC) to a key vault management plane, they can grant themselves access to the data plane by setting a key vault access policy. We recommend that you tightly control who has contributor access to your key vaults, to ensure that only authorized persons can access and manage your key vaults, keys, secrets, and certificates. Configure Key Vault access Access to a key vault is controlled through two interfaces: the management plane, and the data plane. The management plane is where you manage Key Vault itself. Operations in this plane include creating and deleting key vaults, retrieving Key Vault properties, and updating access policies. The data plane is where you work with the data stored in a key vault. You can add, delete, and modify keys, secrets, and certificates from here. To access a key vault in either plane, all callers (users or applications) must have proper authentication and authorization. Authentication establishes the identity of the caller. Authorization determines which operations the caller can execute. Both planes use Azure AD for authentication. For authorization, the management plane uses RBAC, and the data plane can use either newly added RBAC or a Key Vault access policy. Active Directory authentication When you create a key vault in an Azure subscription, its automatically associated with the Azure AD tenant of the subscription. All callers in both planes must register in this tenant and authenticate to access the key vault. In both cases, applications can access Key Vault in two ways: User plus application access. The application accesses Key Vault on behalf of a signed-in user. Examples of this type of access include Azure PowerShell and the Azure portal. User access is granted in two ways. They can either access Key Vault from any application, or they must use a specific application (referred to as compound identity). Application-only access. The application runs as a daemon service or background job. The application identity is granted access to the key vault. For both types of access, the application authenticates with Azure AD. The application uses any supported authentication method based on the application type. The application acquires a token for a resource in the plane to grant access. The resource is an endpoint in the management or data plane, based on the Azure environment. The application uses the token and sends a REST API request to Key Vault. To learn more, review the whole authentication flow. Benefits The model of a single mechanism for authentication to both planes has several benefits: Organizations can centrally control access to all key vaults in their organization. If a user leaves, they instantly lose access to all key vaults in the organization. Organizations can customize authentication by using the options in Azure AD, such as to enable multifactor authentication for added security. Review a secure Key Vault example In this example, we're developing an application that uses a certificate for SSL, Azure Storage to store data, and an RSA 2,048-bit key for sign operations. Our application runs in an Azure virtual machine (VM) (or a virtual machine scale set). We can use a key vault to store the application secrets. We can store the bootstrap certificate that's used by the application to authenticate with Azure AD. We need access to the following stored keys and secrets: SSL certificate - Used for SSL. Storage key - Used to access the Storage account. RSA 2,048-bit key - Used for sign operations. Bootstrap certificate - Used to authenticate with Azure AD. After access is granted, we can fetch the storage key and use the RSA key for signing. We need to define the following roles to specify who can manage, deploy, and audit our application: Security team - IT staff from the office of the CSO (Chief Security Officer) or similar contributors. The security team is responsible for the proper safekeeping of secrets. The secrets can include SSL certificates, RSA keys for signing, connection strings, and storage account keys. Developers and operators - The staff who develop the application and deploy it in Azure. The members of this team aren't part of the security staff. They shouldn't have access to sensitive data like SSL certificates and RSA keys. Only the application that they deploy should have access to sensitive data. Auditors - This role is for contributors who aren't members of the development or general IT staff. They review the use and maintenance of certificates, keys, and secrets to ensure compliance with security standards. There is another role that is outside the scope of our application: the subscription (or resource group) administrator. The subscription admin sets up initial access permissions for the security team. They grant access to the security team by using a resource group that has the resources required by the application. Security team Create key vaults. Turn on Key Vault logging. Add keys and secrets. Create backups of keys for disaster recovery. Set Key Vault access policies to grant permissions to users and applications for specific operations. Roll the keys and secrets periodically. Developers and operators Get references from the security team for the bootstrap and SSL certificates (thumbprints), storage key (secret URI), and RSA key (key URI) for signing. Develop and deploy the application to access keys and secrets programmatically. Auditors Review the Key Vault logs to confirm proper use of keys and secrets, and compliance with data security standards. The following table summarizes the access permissions for our roles and application. Role Management plane permissions Data plane permissions Security team Key Vault Contributor Keys: backup, create, delete, get, import, list, restore. Secrets: all operations Developers and operators Key Vault deploy permission Note: This permission allows deployed VMs to fetch secrets from a key vault. None Auditors None Keys: list Secrets: list. Note: This permission enables auditors to inspect attributes (tags, activation dates, expiration dates) for keys and secrets not emitted in the logs. Application None Keys: sign Secrets: get The three team roles need access to other resources along with Key Vault permissions. To deploy VMs (or the Web Apps feature of Azure App Service), developers and operators need Contributor access to those resource types. Auditors need read access to the Storage account where the Key Vault logs are stored. Deploy and manage Key Vault certificates Key Vault certificates support provides for management of your x509 certificates and enables: A certificate owner to create a certificate through a Key Vault creation process or through the import of an existing certificate. Includes both self-signed and CA-generated certificates. A Key Vault certificate owner to implement secure storage and management of X509 certificates without interaction with private key material. A certificate owner to create a policy that directs Key Vault to manage the life-cycle of a certificate. Certificate owners to provide contact information for notification about lifecycle events of expiration and renewal of certificate. Automatic renewal with selected issuers - Key Vault partner X509 certificate providers and CAs. When a Key Vault certificate is created, an addressable key and secret are also created with the same name. The Key Vault key allows key operations and the Key Vault secret allows retrieval of the certificate value as a secret. A Key Vault certificate also contains public x509 certificate metadata. The identifier and version of certificates is similar to that of keys and secrets. A specific version of an addressable key and secret created with the Key Vault certificate version is available in the Key Vault certificate response. When a Key Vault certificate is created, it can be retrieved from the addressable secret with the private key in either PFX or PEM format. However, the policy used to create the certificate must indicate that the key is exportable. If the policy indicates non-exportable, then the private key isn't a part of the value when retrieved as a secret. The addressable key becomes more relevant with non-exportable Key Vault certificates. The addressable Key Vault key\u2019s operations are mapped from the keyusage field of the Key Vault certificate policy used to create the Key Vault certificate. If a Key Vault certificate expires, it\u2019s addressable key and secret become inoperable. Two types of key are supported \u2013 RSA or RSA HSM with certificates. Exportable is only allowed with RSA, and is not supported by RSA HSM. Certificate policy A certificate policy contains information on how to create and manage the Key Vault certificate lifecycle. When a certificate with private key is imported into the Key Vault, a default policy is created by reading the x509 certificate. When a Key Vault certificate is created from scratch, a policy needs to be supplied. This policy specifies how to create the Key Vault certificate version, or the next Key Vault certificate version. After a policy has been established, it\u2019s not required with successive create operations for future versions. There's only one instance of a policy for all the versions of a Key Vault certificate. At a high level, a certificate policy contains the following information: X509 certificate properties. Contains subject name, subject alternate names, and other properties used to create an x509 certificate request. Key Properties. Contains key type, key length, exportable, and reuse key fields. These fields instruct key vault on how to generate a key. Secret properties. Contains secret properties such as content type of addressable secret to generate the secret value, for retrieving certificate as a secret. Lifetime Actions. Contains lifetime actions for the Key Vault certificate. Each lifetime action contains: Trigger, which specifies via days before expiry or lifetime span percentage. Action, which specifies the action type: emailContacts, or autoRenew. Issuer: Contains the parameters about the certificate issuer to use to issue x509 certificates. Policy attributes: Contains attributes associated with the policy. Certificate Issuer Before you can create a certificate issuer in a Key Vault, the following two prerequisite steps must be completed successfully: Onboard to CA providers: An organization administrator must onboard their company with at least one CA provider. Admin creates requester credentials for Key Vault to enroll (and renew) SSL certificates: Provides the configuration to be used to create an issuer object of the provider in the key vault. Certificate contacts Certificate contacts contain contact information to send notifications triggered by certificate lifetime events. The contacts information is shared by all the certificates in the key vault. A notification is sent to all the specified contacts for an event for any certificate in the key vault. If a certificate's policy is set to auto renewal, then a notification is sent for the following events: Before certificate renewal After certificate renewal, and stating if the certificate was successfully renewed, or if there was an error, requiring manual renewal of the certificate When it\u2019s time to renew a certificate for a certificate policy that is set to manually renew (email only) Certificate access control The Key Vault that contains certificates manages access control for those same certificates. The access control policy for certificates is distinct from the access control policies for keys and secrets in the same Key Vault. Users might create one or more vaults to hold certificates, to maintain scenario appropriate segmentation and management of certificates. The following permissions closely mirror the operations allowed on a secret object, and can be used on a per-principal basis in the secrets access control entry on a key vault: Permissions for certificate management operations: get: Get the current certificate version, or any version of a certificate. list: List the current certificates, or versions of a certificate. update: Update a certificate. create: Create a Key Vault certificate. import: Import certificate material into a Key Vault certificate. delete: Delete a certificate, its policy, and all of its versions. recover: Recover a deleted certificate. backup: Back up a certificate in a key vault. restore: Restore a backed-up certificate to a key vault. managecontacts: Manage Key Vault certificate contacts. manageissuers: Manage Key Vault certificate authorities/issuers. getissuers: Get a certificate's authorities/issuers. listissuers: List a certificate's authorities/issuers. setissuers: Create or update a Key Vault certificate's authorities/issuers. deleteissuers: Delete a Key Vault certificate's authorities/issuers. Permissions for privileged operations: purge: Purge (permanently delete) a deleted certificate. Create Key Vault keys Cryptographic keys in Key Vault are represented as JSON Web Key (JWK) objects. There are two types of keys, depending on how they were created. Soft keys: A key processed in software by Key Vault, but is encrypted at rest using a system key that is in a Hardware Security Module (HSM). Clients may import an existing RSA or EC (Elliptic Curve) key, or request that Key Vault generates one. Hard keys: A key processed in an HSM (Hardware Security Module). These keys are protected in one of the Key Vault HSM Security Worlds (there's one Security World per geography to maintain isolation). Clients may import an RSA or EC key, in soft form or by exporting from a compatible HSM device. Clients may also request Key Vault to generate a key. Key operations Key Vault supports many operations on key objects. Here are a few: Create: Allows a client to create a key in Key Vault. The value of the key is generated by Key Vault and stored, and isn't released to the client. Asymmetric keys may be created in Key Vault. Import: Allows a client to import an existing key to Key Vault. Asymmetric keys may be imported to Key Vault using many different packaging methods within a JWK construct. Update: Allows a client with sufficient permissions to modify the metadata (key attributes) associated with a key previously stored within Key Vault. Delete: Allows a client with sufficient permissions to delete a key from Key Vault Cryptographic operations Once a key has been created in Key Vault, the following cryptographic operations may be performed using the key. For best application performance, verify that operations are performed locally. Sign and Verify: Strictly, this operation is \"sign hash\" or \"verify hash\", as Key Vault doesn't support hashing of content as part of signature creation. Applications should hash the data to be signed locally, then request that Key Vault signs the hash. Verification of signed hashes is supported as a convenience operation for applications that may not have access to [public] key material. Key Encryption / Wrapping: A key stored in Key Vault may be used to protect another key, typically a symmetric content encryption key (CEK). When the key in Key Vault is asymmetric, key encryption is used. When the key in Key Vault is symmetric, key wrapping is used. Encrypt and Decrypt: A key stored in Key Vault may be used to encrypt or decrypt a single block of data. The size of the block is determined using the key type and selected encryption algorithm. The Encrypt operation is provided for convenience, for applications that may not have access to [public] key material. Application services plan More organizations are adopting secrets management policies, where secrets are stored centrally with expectations around expiration and access control. Azure Key Vault provides these management capabilities to your applications in Azure, but some applications can\u2019t easily take on code changes to start integrating with it. Key Vault references are a way to introduce secrets management into your app without code changes. Apps hosted in App Service and Azure Functions can now define a reference to a secret managed in Key Vault as part of their application settings. The app\u2019s system-assigned identity is used to securely fetch the secret and make it available to the app as an environment variable. Teams can replace existing secrets stored in app settings with references to the same secret in Key Vault, and the app continues to operate as normal. Configure a hardware security module key-generation solution For added assurance, when you use Azure Key Vault, you can import or generate keys in hardware security modules (HSMs) that never leave the HSM boundary. This scenario is often referred to as Bring Your Own Key (BYOK). The HSMs are FIPS 140-2 Level 2 validated. Azure Key Vault uses Thales nShield family of HSMs to protect your keys. (This functionality isn't available for Azure China.) Generating and transferring an HSM-protected key over the Internet: You generate the key from an offline workstation, which reduces the attack surface. The key is encrypted with a Key Exchange Key (KEK), which stays encrypted until transferred to the Azure Key Vault HSMs. Only the encrypted version of your key leaves the original workstation. The toolset sets properties on your tenant key that binds your key to the Azure Key Vault security world. After the Azure Key Vault HSMs receive and decrypt your key, only these HSMs can use it. Your key can't be exported. This binding is enforced using the Thales HSMs. The KEK that encrypts your key is generated inside the Azure Key Vault HSMs, and isn't exportable. The HSMs enforce that there can be no clear version of the KEK outside the HSMs. In addition, the toolset includes attestation from Thales that the KEK isn't exportable and was generated inside a genuine HSM manufactured by Thales. The toolset includes attestation from Thales that the Azure Key Vault security world was also generated on a genuine HSM manufactured by Thales. Microsoft uses separate KEKs and separate security worlds in each geographical region. This separation ensures that your key can be used only in data centers in the region in which you encrypted it. For example, a key from a European customer can't be used in data centers in North American or Asia. Manage customer managed keys Once you have created your Key Vault and have populated it with keys and secrets. The next step is to set up a rotation strategy for the values you store as Key Vault secrets. Secrets can be rotated in several ways: As part of a manual process Programmatically by using REST API calls Through an Azure Automation script Example of storage service encryption with customer-managed Keys. This service uses Azure Key Vault that provides highly available and scalable secure storage for RSA cryptographic keys backed by FIPS 140-2 Level 2 validated HSMs (Hardware Security Modules). Key Vault streamlines the key management process and enables customers to maintain control of keys that are used to encrypt data, manage, and audit their key usage, in order to protect sensitive data as part of their regulatory or compliance needs, HIPAA and BAA compliant. Customers can generate/import their RSA key to Azure Key Vault and enable Storage Service Encryption. Azure Storage handles the encryption and decryption in a fully transparent fashion using envelope encryption in which data is encrypted using an AES-based key, which in turn is protected using the Customer-Managed Key stored in Azure Key Vault. Customers can rotate their key in Azure Key Vault as per their compliance policies. When they rotate their key, Azure Storage detects the new key version and re-encrypts the Account Encryption Key for that storage account. Key rotation doesn't result in re-encryption of all data and there's no other action required from user. Customers can also revoke access to the storage account by revoking access on their key in Azure Key Vault. There are several ways to revoke access to your keys. Revoking access effectively blocks access to all blobs in the storage account as the Account Encryption Key is inaccessible by Azure Storage. Customers can enable this feature on all available redundancy types of Azure Blob storage including premium storage and can toggle from using Microsoft managed to using customer-managed keys. There's no extra charge for enabling this feature. You can enable this feature on any Azure Resource Manager storage account using the Azure portal, Azure PowerShell, Azure CLI, or the Microsoft Azure Storage Resource Provider API. Enable Key Vault secrets Key Vault provides secure storage of secrets, such as passwords and database connection strings. From a developer's perspective, Key Vault APIs accept and return secret values as strings. Internally, Key Vault stores and manages secrets as sequences of octets (8-bit bytes), with a maximum size of 25k bytes each. The Key Vault service doesn't provide semantics for secrets. It merely accepts the data, encrypts it, stores it, and returns a secret identifier (\"ID\"). The identifier can be used to retrieve the secret at a later time. For highly sensitive data, clients should consider additional layers of protection for data. Encrypting data using a separate protection key prior to storage in Key Vault is one example. Key Vault also supports a contentType field for secrets. Clients may specify the content type of a secret to assist in interpreting the secret data when it's retrieved. The maximum length of this field is 255 characters. There are no pre-defined values. The suggested usage is as a hint for interpreting the secret data. For instance, an implementation may store both passwords and certificates as secrets, then use this field to differentiate. There are no predefined values. As shown above, the values for Key Vault Secrets are: Name-value pair - Name must be unique in the Vault Value can be any Unicode Transformation Format (UTF-8) string - max of 25 KB in size Manual or certificate creation Activation date Expiration date Encryption All secrets in your Key Vault are stored encrypted. Key Vault encrypts secrets at rest with a hierarchy of encryption keys, with all keys in that hierarchy are protected by modules that are Federal Information Processing Standards (FIPS) 140-2 compliant. This encryption is transparent, and requires no action from the user. The Azure Key Vault service encrypts your secrets when you add them, and decrypts them automatically when you read them. The encryption leaf key of the key hierarchy is unique to each key vault. The encryption root key of the key hierarchy is unique to the security world, and its protection level varies between regions: China: root key is protected by a module that is validated for FIPS 140-2 Level 1. Other regions: root key is protected by a module that is validated for FIPS 140-2 Level 2 or higher. Secret attributes In addition to the secret data, the following attributes may be specified: exp: IntDate, optional, default is forever. The exp (expiration time) attribute identifies the expiration time on or after which the secret data SHOULD NOT be retrieved, except in particular situations. This field is for informational purposes only as it informs users of key vault service that a particular secret may not be used. Its value MUST be a number containing an IntDate value. nbf: IntDate, optional, default is now. The nbf (not before) attribute identifies the time before which the secret data SHOULD NOT be retrieved, except in particular situations. This field is for informational purposes only. Its value MUST be a number containing an IntDate value. enabled: boolean, optional, default is true. This attribute specifies whether the secret data can be retrieved. The enabled attribute is used with nbf and exp when an operation occurs between nbf and exp, it will only be permitted if enabled is set to true. Operations outside the nbf and exp window are automatically disallowed, except in particular situations. There are more read-only attributes that are included in any response that includes secret attributes: created: IntDate, optional. The created attribute indicates when this version of the secret was created. This value is null for secrets created prior to the addition of this attribute. Its value must be a number containing an IntDate value. updated: IntDate, optional. The updated attribute indicates when this version of the secret was updated. This value is null for secrets that were last updated prior to the addition of this attribute. Its value must be a number containing an IntDate value. For information on common attributes for each key vault object type, see Azure Key Vault keys, secrets and certificates overview. Date-time controlled operations A secret's get operation will work for not-yet-valid and expired secrets, outside the nbf / exp window. Calling a secret's get operation, for a not-yet-valid secret, can be used for test purposes. Retrieving (getting) an expired secret, can be used for recovery operations. Secret access control Access Control for secrets managed in Key Vault, is provided at the level of the Key Vault that contains those secrets. The access control policy for secrets is distinct from the access control policy for keys in the same Key Vault. Users may create one or more vaults to hold secrets, and are required to maintain scenario appropriate segmentation and management of secrets. The following permissions can be used, on a per-principal basis, in the secrets access control entry on a vault, and closely mirror the operations allowed on a secret object: Permissions for secret management operations get: Read a secret list: List the secrets or versions of a secret stored in a Key Vault set: Create a secret delete: Delete a secret recover: Recover a deleted secret backup: Back up a secret in a key vault restore: Restore a backed up secret to a key vault Permissions for privileged operations purge: Purge (permanently delete) a deleted secret Secret tags You can specify more application-specific metadata in the form of tags. Key Vault supports up to 15 tags, each of which can have a 256 character name and a 256 character value. Note Tags are readable by a caller if they have the list or get permission. Usage Scenarios When to use Examples Securely store, manage lifecycle, and monitor credentials for service-to-service communication like passwords, access keys, service principal client secrets. Use Azure Key Vault with a Virtual MachineUse Azure Key Vault with an Azure Web Application Configure key rotation Once you have keys and secrets stored in the key vault it's important to think about a rotation strategy. There are several ways to rotate the values: As part of a manual process Programmatically by using API calls Through an Azure Automation script This diagram shows how Event Grid and Function Apps can be used to automate the process. Thirty days before the expiration date of a secret, Key Vault publishes the \"near expiry\" event to Event Grid. Event Grid checks the event subscriptions and uses HTTP POST to call the function app endpoint subscribed to the event. The function app receives the secret information, generates a new random password, and creates a new version for the secret with the new password in Key Vault. The function app updates SQL Server with the new password. Manage Key Vault safety and recovery features Key Vault's soft-delete feature allows recovery of the deleted vaults and deleted key vault objects (for example, keys, secrets, certificates), known as soft-delete. Specifically, we address the following scenarios: This safeguard offer the following protections: Once a secret, key, certificate, or key vault is deleted, it remains recoverable for a configurable period of 7 to 90 calendar days. If no configuration is specified, the default recovery period is set to 90 days. Users are provided with sufficient time to notice an accidental secret deletion and respond. Two operations must be made to permanently delete a secret. First a user must delete the object, which puts it into the soft-deleted state. Second, a user must purge the object in the soft-deleted state. The purge operation requires extra access policy permissions. These extra protections reduce the risk of a user accidentally or maliciously deleting a secret or a key vault. To purge a secret in the soft-deleted state, a service principal must be granted another \"purge\" access policy permission. The purge access policy permission isn't granted by default to any service principal including key vault and subscription owners and must be deliberately set. By requiring an elevated access policy permission to purge a soft-deleted secret, it reduces the probability of accidentally deleting a secret. Supporting interfaces The soft-delete feature is available through the REST API, the Azure CLI, PowerShell, .NET/C# interfaces, and ARM templates. Scenarios Azure Key Vaults are tracked resources, managed by Azure Resource Manager. Azure Resource Manager also specifies a well-defined behavior for deletion, which requires that a successful DELETE operation must result in that resource not being accessible anymore. The soft-delete feature addresses the recovery of the deleted object, whether the deletion was accidental or intentional. In the typical scenario, a user may have inadvertently deleted a key vault or a key vault object. If that key vault or key vault object were to be recoverable for a predetermined period, the user may undo the deletion and recover their data. In a different scenario, a rogue user may attempt to delete a key vault or a key vault object, such as a key inside a vault, to cause a business disruption. Separation and deletion of the key vault or key vault object from the actual deletion of the underlying data can be used as a safety measure by, for instance, restricting permissions on data deletion to a different, trusted role. This approach effectively requires quorum for an operation that might otherwise result in an immediate data loss. Soft-delete behavior When soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion. When creating a new key vault, soft-delete is on by default. Once soft-delete is enabled on a key vault, it can't be disabled. The default retention period is 90 days but, during key vault creation, it's possible to set the retention policy interval to a value from 7 to 90 days through the Azure portal. The purge protection retention policy uses the same interval. Once set, the retention policy interval can't be changed. You can't reuse the name of a key vault that has been soft-deleted until the retention period has passed. Purge protection Permanently deleting, purging, a key vault is possible via a POST operation on the proxy resource and requires special privileges. Generally, only the subscription owner is able to purge a key vault. The POST operation triggers the immediate and irrecoverable deletion of that vault. Exceptions are: When the Azure subscription has been marked as undeletable. In this case, only the service may then perform the actual deletion, and does so as a scheduled process. When the enable-purge-protection argument is enabled on the vault itself. In this case, Key Vault waits for 90 days from when the original secret object was marked for deletion to permanently delete the object. Key vault recovery Upon deleting a key vault object, such as a key, the service will place the object in a deleted state, making it inaccessible to any retrieval operations. While in this state, the key vault object can only be listed, recovered, or forcefully/permanently deleted. To view the objects, use the Azure CLI az keyvault key list-deleted command, or the PowerShell Get-AzKeyVault -InRemovedState command. At the same time, Key Vault will schedule the deletion of the underlying data corresponding to the deleted key vault or key vault object for execution after a predetermined retention interval. The DNS record corresponding to the vault is also retained during the retention interval. Soft-delete retention period Soft-deleted resources are retained for a set period of time, 90 days. During the soft-delete retention interval, the following apply: You may list all of the key vaults and key vault objects in the soft-delete state for your subscription as well as access deletion and recovery information about them. Only users with special permissions can list deleted vaults. We recommend that our users create a custom role with these special permissions for handling deleted vaults. A key vault with the same name can't be created in the same location; correspondingly, a key vault object can't be created in a given vault if that key vault contains an object with the same name and which is in a deleted state. Only a privileged user may restore a key vault or key vault object by issuing a recover command on the corresponding proxy resource. The user, member of the custom role, who has the privilege to create a key vault under the resource group can restore the vault. Only a privileged user may forcibly delete a key vault or key vault object by issuing a delete command on the corresponding proxy resource. Unless a key vault or key vault object is recovered, at the end of the retention interval the service performs a purge of the soft-deleted key vault or key vault object and its content. Resource deletion may not be rescheduled. Billing implications In general, when an object (a key vault or a key or a secret) is in deleted state, there are only two operations possible: 'purge' and 'recover'. All the other operations fail. Therefore, even though the object exists, no operations can be performed and hence no usage will occur, so no bill. However there are following exceptions: In general, when an object (a key vault or a key or a secret) is in deleted state, there are only two operations possible: 'purge' and 'recover'. All the other operations fail. Therefore, even though the object exists, no operations can be performed and hence no usage will occur, so no bill. However there are following exceptions: If the object is an HSM-key, the 'HSM Protected key' charge per key version per month charge applies if a key version has been used in last 30 days. After that, since the object is in deleted state no operations can be performed against it, so no charge will apply. Key vault soft-delete on by default If a secret is deleted and the key vault doesn't have soft-deleted protection, it's deleted permanently. Although users can currently opt out of soft-delete during key vault creation, this ability is deprecated. In February 2025, Microsoft enables soft-delete protection on all key vaults, and users are no longer be able to opt out of or turn off soft-delete. This, protect secrets from accidental or malicious deletion by a user. This diagram shows how the process flow of deleting a key with and without soft-delete protection. When a secret is deleted from a key vault without soft-delete protection, the secret is permanently deleted. Users can currently opt out of soft-delete during key vault creation. However, Microsoft enables soft-delete protection on all key vaults to protect secrets from accidental or malicious deletion by a user. Users are no longer be able to opt out of or turn off soft-delete. Key vault backup Azure Key Vault automatically provides features to help you maintain availability and prevent data loss. Back up secrets only if you have a critical business justification. Backing up secrets in your key vault may introduce operational challenges such as maintaining multiple sets of logs, permissions, and backups when secrets expire or rotate. Key Vault maintains availability in disaster scenarios and will automatically fail over requests to a paired region without any intervention from a user. If you want protection against accidental or malicious deletion of your secrets, configure soft-delete and purge protection features on your key vault. Limitations Important Key Vault does not support the ability to backup more than 500 past versions of a key, secret, or certificate object. Attempting to backup a key, secret, or certificate object may result in an error. It is not possible to delete previous versions of a key, secret, or certificate. Key Vault doesn't currently provide a way to back up an entire key vault in a single operation. Any attempt to use the commands listed in this document to do an automated backup of a key vault may result in errors and not supported by Microsoft or the Azure Key Vault team. Also consider the following consequences: Backing up secrets that have multiple versions might cause time-out errors. A backup creates a point-in-time snapshot. Secrets might renew during a backup, causing a mismatch of encryption keys. If you exceed key vault service limits for requests per second, your key vault is throttled, and the backup fails. Design considerations When you back up a key vault object, such as a secret, key, or certificate, the backup operation downloads the object as an encrypted blob. This blob can't be decrypted outside of Azure. To get usable data from this blob, you must restore the blob into a key vault within the same Azure subscription and Azure geography. Prerequisites To back up a key vault object, you must have: Contributor-level or higher permissions on an Azure subscription. A primary key vault that contains the secrets you want to back up. A secondary key vault where secrets are restored. Back up and restore from the Azure portal Back up 1. Navigate to the Azure portal. 2. Select your key vault. 3. Navigate to the key you want to back up. Select the object Select Download Back up Select Download Store the encrypted blob in a secure location. Restore 1. Navigate to the Azure portal. 2. Select your key vault. 3. Navigate to the key you want to restore. 4. Select Restore Backup. Navigate to the location where you stored the encrypted blob. Select OK. Perform Try-This exercises Task 1: Create a key vault In this task, we'll create a key vault. Sign in to the Azure portal and search for Key Vaults. On the Key vaults page, click + Create. On the Basics tab, fill out the required information. Discuss the Pricing tier selections, Standard and Premium. Premium supports HSM-backed keys. Discuss Soft delete and Retention period. Click Review and Create and then Create. Wait for the new key vault to be created, or move to a key vault that has already been created. Task 2: Review key vault settings In this task, we'll review key vault settings. In the Portal, navigate to the key vault. Under the Name list, click the newly created Key Vault. Under the Objects, click Keys. Click Generate/Import and review the Keys configuration information. Under Settings, click Secrets. Click Generate/Import, review the Secrets configuration information, and click Create. View the new Secret and note that keys support versioning. Under Settings, click Certificates. Click Generate/Import and review the Certificates configuration information. Task 3: Configure access policies Note To complete this demonstration you will need a non-privileged test user. In this task, we'll configure access policies and test access. Continue in the Portal with your key vault. Under Settings, click Access Policies. Review the Enable access to choices: Azure Virtual Machines for deployment, Azure Resource Manager for template deployment, and Azure Disk Encryption for volume encryption. Review the creator account Key Permissions. Note the Cryptographic operation permissions aren't assigned. Review the creator account Secret Permissions. Note the Purge permission. Review the creator account Certificate Permissions. Open the Cloud Shell with the Bash option. You should be signed in as a Global Administrator. Use your key information to verify the secret you created in the previous task displays successfully for this role. az keyvault secret show --name <secret_name> --vault-name <keyvault_name>' In another browser tab, open the portal, and sign-in as the test user. Open the Cloud Shell with the Bash option. Verify that the secret doesn't display for the test user. Access is denied. az keyvault secret show --name <secret_name> --vault-name <keyvault_name> Return to the Global Administrator account in the portal. Add the Key Vault Contributor role to your test user. Try the test user's access. Access is denied. az keyvault secret show --name <secret_name> --vault-name <keyvault_name> Explain that adding the RBAC role grants access to the Key Vault control plane. It doesn't grant access to the date in the Key Vault. Return to your Key Vault and create an access policy. Under Settings, select Access policies and then Add Access Policy. Configure from the template (optional): Key, Secret, & Certificate Management Key permissions: none Secret permissions: Get, List Certificate permissions: none Select principal: select your test user Be sure to Add your new access policy. And to Save your changes. Try the test user's access. The user should now have access and the key should display. az keyvault secret show --name <secret_name> --vault-name <keyvault_name> As you have time, return to the Secret configuration settings and change Enabled to No. Be sure to save your changes, then try access the key again. Explore the Azure Hardware Security Module Azure Dedicated HSM is an Azure service that provides cryptographic key storage in Azure. Dedicated HSM meets the most stringent security requirements. It's the ideal solution for customers who require FIPS 140-2 Level 3-validated devices and complete and exclusive control of the HSM appliance. Best fit Azure Dedicated HSM is most suitable for \u201clift-and-shift\u201d scenarios that require direct and sole access to HSM devices. Examples include: Migrating applications from on-premises to Azure Virtual Machines Migrating applications from Amazon AWS EC2 to virtual machines that use the AWS Cloud HSM Classic service Running shrink-wrapped software such as Apache/Ngnix SSL Offload, Oracle TDE, and ADCS in Azure Virtual Machines Not a fit Azure Dedicated HSM is not a good fit for the following type of scenario: Microsoft cloud services that support encryption with customer-managed keys (such as Azure Information Protection, Azure Disk Encryption, Azure Data Lake Store, Azure Storage, Azure SQL Database, and Customer Key for Office 365) that are not integrated with Azure Dedicated HSM. Knowledge check Which of the following items should be stored in Azure Key Vault? Secret ( Ans ) Links to external certificate Identity management A select group of users must be able to create and delete keys in the key vault. When authenticating to the data plane using Azure AD, what security tool should be used the authorize access at a role level to these users? Key vault access policies Role-based Access Control ( Ans ) Azure AD authentication Which of these statements best describes Azure Key Vault's authentication and authorization process? Applications authenticate to a vault with the lead developer\u2019s username and password and have full access to all secrets in the vault. Applications and users authenticate to a vault with their Azure Active Directory identities and are authorized to perform actions on all secrets in the vault. ( Ans ) Applications and users authenticate to a vault with a Microsoft account and are authorized to access specific secrets. How does Azure Key Vault help protect your secrets after they're loaded by your app? Azure Key Vault automatically generates a new secret after every use. Azure Key Vault double-encrypts secrets, requiring your app to decrypt them locally every time they're used. It doesn't protect your secrets. Secrets are unprotected once they're loaded by your application. ( Ans ) A manager wants to know more about software-protected keys and hardware-protected keys. Pick the correct topic you could explain to your manager? Only hardware-protected keys are encrypted at rest. Software-protected keys aren't isolated from the application. Software-protected cryptographic operations are performed in software and Hardware-protected cryptographic operations are performed within the HSM. ( Ans ) Configure application security features Introduction Application security is one extra layer in the defense-in-depth strategy. You ensure that your application is registered, so it has a unique identity in Azure AD, then you can manage and control both access to the application and what the application can do. Scenario A security engineer uses application security to protect the usage of your application and prevent the loss of data, you will work on such tasks as: Register applications with Azure AD. Assign permissions for the application and the users to access it. Use certificates and secrets to protect operations and data. Review the Microsoft identity platform Microsoft identity platform is an evolution of the Azure Active Directory (Azure AD) developer platform. It allows developers to build applications that sign in users, get tokens to call APIs, such as Microsoft Graph or APIs that developers have built. It consists of an authentication service, open-source libraries, application registration and configuration (through a developer portal and application API), full developer documentation, quickstart samples, code samples, tutorials, how-to guides, and other developer content. The Microsoft identity platform supports industry-standard protocols such as OAuth 2.0 and OpenID Connect. Up until now, most developers have worked with the Azure AD v1.0 platform to authenticate work and school accounts (provisioned by Azure AD) by requesting tokens from the Azure AD v1.0 endpoint, using Azure AD Authentication Library (ADAL), Azure portal for application registration and configuration, and the Microsoft Graph API for programmatic application configuration. With the unified Microsoft identity platform (v2.0), you can write code once and authenticate any Microsoft identity into your application. For several platforms, the fully supported open-source Microsoft Authentication Library (MSAL) is recommended for use against the identity platform endpoints. MSAL is simple to use, provides great single sign-on (SSO) experiences for your users, helps you achieve high reliability and performance, and is developed using Microsoft Secure Development Lifecycle (SDL). When calling APIs, you can configure your application to take advantage of incremental consent, which allows you to delay the request for consent for more invasive scopes until the application\u2019s usage warrants this at runtime. MSAL also supports Azure Active Directory B2C, so your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs. With the Microsoft identity platform, one can expand their reach to these kinds of users: Work and school accounts (Azure AD provisioned accounts) Personal accounts (such as Outlook.com or Hotmail.com) Your customers who bring their own email or social identity (such as LinkedIn, Facebook, and Google) via MSAL and Azure AD business-to-consumer (B2C) You can use the Azure portal to register and configure your application and use the Microsoft Graph API for programmatic application configuration. Microsoft identity platform The following diagram depicts the Microsoft identity experience at a high level, including the app registration experience, software development kits (SDKs), endpoints, and supported identities. The Microsoft identity platform has two endpoints (v1.0 and v2.0); however, when developing a new application, consider it's highly recommended that you use the v2.0 (default) endpoint to benefit from the latest features and capabilities: The Microsoft Authentication Library can be used in many application scenarios, including the following: Single-page applications (JavaScript) Web app signing in users Web application signing in a user and calling a web API on behalf of the user Protecting a web API so only authenticated users can access it Web API calling another downstream Web API on behalf of the signed-in user Desktop application calling a web API on behalf of the signed-in user Mobile application calling a web API on behalf of the user who's signed in interactively. Desktop/service daemon application calling web API on behalf of itself Languages and frameworks Library Supported platforms and frameworks MSAL for Android Android MSAL Angular Single-page apps with Angular and Angular.js frameworks MSAL for iOS and macOS iOS and macOS MSAL Go (Preview) Windows, macOS, Linux MSAL Java Windows, macOS, Linux MSAL.js JavaScript/TypeScript frameworks such as Vue.js, Ember.js, or Durandal.js MSAL.NET .NET Framework, .NET Core, Xamarin Android, Xamarin iOS, Universal Windows Platform MSAL Node Web apps with Express, desktop apps with Electron, Cross-platform console apps MSAL Python Windows, macOS, Linux MSAL React Single-page apps with React and React-based libraries (Next.js, Gatsby.js) Migrate apps that use ADAL to MSAL Active Directory Authentication Library (ADAL) integrates with the Azure AD for developers (v1.0) endpoint, where MSAL integrates with the Microsoft identity platform. The v1.0 endpoint supports work accounts but not personal accounts. The v2.0 endpoint is unifying Microsoft personal accounts and works accounts into a single authentication system. Additionally, with MSAL, you can also get authentications for Azure AD B2C. Explore Azure AD application scenarios Any application that outsources authentication to Azure AD needs to be registered in a directory. This step involves telling Azure AD about your application, including: Azure AD application scenarios Frontend Authentication Backend Single page application are frontends that run in a browser Azure AD Authorization Endpoint Web API Web apps are applications that authenticate a user in a web browser to a web application Azure AD WS-Federation or SAML Endpoint Web application Native apps are applications that call a web API on behalf of a user Azure AD Authorization Endpoint and Azure AD Token Endpoint Web API Web API apps are web applications that need to get resources from a web API Azure AD Authorization Endpoint and Azure AD Token Endpoint Web application and Web API Service-to-service applications are daemon or server application that needs to get resources from a web API Azure AD Authorization Endpoint and Azure AD Token Endpoint Web API Azure AD represents applications following a specific model that's designed to fulfill two main functions: Identify the app according to the authentication protocols it supports. This involves enumerating all the identifiers, URLs, secrets, and related information that Azure AD needs at authentication time. Here, Azure AD: Holds all the data needed to support authentication at run time. Holds all the data for deciding which resources an app might need to access, whether it should fulfill a particular request, and under what circumstances it should fulfill the request. Supplies the infrastructure for implementing app provisioning both within the app developer's tenant and to any other Azure AD tenant. Handle user consent during token request time and facilitate the dynamic provisioning of apps across tenants. Here, Azure AD: Enables users and administrators to dynamically grant or deny consent for the app to access resources on their behalf. Enables administrators to ultimately decide what apps are allowed to do, which users can use specific apps, and how directory resources are accessed. In Azure AD, an application object describes an application as an abstract entity. Developers work with applications. At deployment time, Azure AD uses a specific application object as a blueprint to create a service principal, which represents a concrete instance of an application within a directory or tenant. It's the service principal that defines what the app can do in a specific target directory, who can use it, what resources it has access to, and so on. Azure AD creates a service principal from an application object through consent. The following diagram depicts a simplified Azure AD provisioning flow driven by consent. In this provisioning flow: A user from B tries to sign in with the app. Azure AD gets and verifies the user credentials. Azure AD prompts the user to consent for the app to gain access to tenant B. Azure AD uses the application object in A as a blueprint for creating a service principal in B. The user receives the requested token. You can repeat this process as many times as you want for other tenants (C, D, and so on). Directory A keeps the blueprint for the app (application object). Users and admins of all the other tenants where the app is given consent to retain control over what the application can do through the corresponding service principal object in each tenant. When an application is given permission to access resources in a tenant (upon registration or consent), a service principal object is created. The Microsoft Graph ServicePrincipal entity defines the schema for a service principal object's properties. Register an application with App Registration For the most secure operation, register your app with the Microsoft identity platform. Before your app can get a token from the Microsoft identity platform, it must be registered in the Azure portal. Registration integrates your app with the Microsoft identity platform and establishes the information that it uses to get tokens, including: Application ID: A unique identifier assigned by the Microsoft identity platform. Redirect URI/URL: One or more endpoints at which your app will receive responses from the Microsoft identity platform. (For native and mobile apps, this is a URI assigned by the Microsoft identity platform.) Application Secret: A password or a public/private key pair that your app uses to authenticate with the Microsoft identity platform. (Not needed for native or mobile apps.) Getting an access token Like most developers, you will probably use authentication libraries to manage your token interactions with the Microsoft identity platform. Authentication libraries abstract many protocol details, like validation, cookie handling, token caching, and maintaining secure connections, away from the developer and let you focus your development on your app. Microsoft publishes open-source client libraries and server middleware. Configure Microsoft Graph permissions Microsoft Graph exposes granular permissions that control the access that apps have to resources, like users, groups, and mail. As a developer, you decide which permissions to request for Microsoft Graph. When a user signs in to your app they, or, in some cases, an administrator, are given a chance to consent to these permissions. If the user consents, your app is given access to the resources and APIs that it has requested. For apps that don't take a signed-in user, permissions can be pre-consented to by an administrator when the app is installed. Microsoft Graph has two types of permissions: Delegated permissions are used by apps that have a signed-in user present. For these apps, either the user or an administrator consents to the permissions that the app requests, and the app can act as the signed-in user when making calls to Microsoft Graph. Some delegated permissions can be consented by non-administrative users, but some higher-privileged permissions require administrator consent. Application permissions are used by apps that run without a signed-in user present; for example, apps that run as background services or daemons. Application permissions can only be consented by an administrator. Effective permissions are the permissions that your app will have when making requests to Microsoft Graph. It is important to understand the difference between the delegated and application permissions that your app is granted and its effective permissions when making calls to Microsoft Graph. For delegated permissions, the effective permissions of your app will be the intersection of the delegated permissions the app has been granted (via consent) and the privileges of the currently signed-in user. Your app can never have more privileges than the signed-in user. Within organizations, the privileges of the signed-in user can be determined by policy or by membership in one or more administrator roles. For example, assume your app has been granted the User.ReadWrite.All delegated permission. This permission nominally grants your app permission to read and update the profile of every user in an organization. If the signed-in user is a global administrator, your app will be able to update the profile of every user in the organization. However, if the signed-in user is not in an administrator role, your app will be able to update only the profile of the signed-in user. It will not be able to update the profiles of other users in the organization because the user that it has permission to act on behalf of does not have those privileges. For application permissions, the effective permissions of your app will be the full level of privileges implied by the permission. For example, an app that has the User.ReadWrite.All application permission can update the profile of every user in the organization. Microsoft Graph API You can use the Microsoft Graph Security API to connect Microsoft security products, services, and partners to streamline security operations and improve threat protection, detection, and response capabilities. The Microsoft Graph Security API is an intermediary service (or broker) that provides a single programmatic interface to connect multiple Microsoft Graph Security providers (also called security providers or providers). The Microsoft Graph Security API federates requests to all providers in the Microsoft Graph Security ecosystem. This is based on the security provider consent provided by the application, as shown in the following diagram. The consent workflow only applies to non-Microsoft providers. The following is a description of the flow: The application user signs in to the provider application to view the consent form from the provider. This consent form experience or UI is owned by the provider and applies to non-Microsoft providers only to get explicit consent from their customers to send requests to Microsoft Graph Security API. The client consent is stored on the provider side. The provider consent service calls the Microsoft Graph Security API to inform consent approval for the respective customer. The application sends a request to the Microsoft Graph Security API. The Microsoft Graph Security API checks for the consent information for this customer mapped to various providers. The Microsoft Graph Security API calls all those providers the customer has given explicit consent to via the provider consent experience. The response is returned from all the consented providers for that client. The result set response is returned to the application. If the customer has not consented to any provider, no results from those providers are included in the response. The Microsoft Graph Security API makes it easy to connect with security solutions from Microsoft and partners. It allows you to more readily realize and enrich the value of these solutions. You can connect easily with the Microsoft Graph Security API by using one of the following approaches, depending on your requirements: Why use the Microsoft Graph Security API? Write code \u2013 Find code samples in C#, Java, NodeJS, and more. Connect using scripts \u2013 Find PowerShell samples. Drag and drop into workflows and playbooks \u2013 Use Microsoft Graph Security connectors for Azure Logic Apps, Microsoft Flow, and PowerApps. Get data into reports and dashboards \u2013 Use the Microsoft Graph Security connector for Power BI. Connect using Jupyter notebooks \u2013 Find Jupyter notebook samples. Unify and standardize alert tracking Connect once to integrate alerts from any Microsoft Graph-integrated security solution and keep alert status and assignments in sync across all solutions. You can also stream alerts to security information and event management (SIEM) solutions, such as Splunk using Microsoft Graph Security API connectors. Correlate security alerts to improve threat protection and response Correlate alerts across security solutions more easily with a unified alert schema. This not only allows you to receive actionable alert information but allows security analysts to pivot and enrich alerts with asset and user information, enabling faster response to threats and asset protection. Update alert tags, status, and assignments Tag alerts with additional context or threat intelligence to inform response and remediation. Ensure that comments and feedback on alerts are captured for visibility to all workflows. Keep alert status and assignments in sync so that all integrated solutions reflect the current state. Use webhook subscriptions to get notified of changes. Unlock security context to drive investigation Dive deep into related security-relevant inventory (like users, hosts, and apps), then add organizational context from other Microsoft Graph providers (Azure AD, Microsoft Intune, Microsoft 365) to bring business and security contexts together and improve threat response. Enable managed identities A common challenge when building cloud applications is how to manage the credentials in your code for authenticating to cloud services. Keeping the credentials secure is an important task. Ideally, the credentials never appear on developer workstations and aren't checked into source control. Azure Key Vault provides a way to securely store credentials, secrets, and other keys, but your code has to authenticate to Key Vault to retrieve them. Managed Identities for Azure resources is the new name for the service formerly known as Managed Service Identity (MSI) for Azure resources feature in Azure Active Directory (Azure AD) solves the above noted problem. The feature provides Azure services with an automatically managed identity in Azure AD. You can use the identity to authenticate to any service that supports Azure AD authentication, including Key Vault, without any credentials in your code. The managed identities for Azure resources feature is free with Azure AD for Azure subscriptions. There's no additional cost. Terminology The following terms are used throughout the managed identities for Azure resources documentation set: Client ID - a unique identifier generated by Azure AD that is tied to an application and service principal during its initial provisioning. Principal ID - the object ID of the service principal object for your managed identity that is used to grant role-based access to an Azure resource. Azure Instance Metadata Service (IMDS) - a REST endpoint accessible to all IaaS VMs created via the Azure Resource Manager. The endpoint is available at a well-known non-routable IP address (169.254.169.254) that can be accessed only from within the VM. How managed identities for Azure resources works There are two types of managed identities: A system-assigned managed identity is enabled directly on an Azure service instance. When the identity is enabled, Azure creates an identity for the instance in the Azure AD tenant that's trusted by the subscription of the instance. After the identity is created, the credentials are provisioned onto the instance. The lifecycle of a system-assigned identity is directly tied to the Azure service instance that it's enabled on. If the instance is deleted, Azure automatically cleans up the credentials and the identity in Azure AD. A user-assigned managed identity is created as a standalone Azure resource. Through a create process, Azure creates an identity in the Azure AD tenant that's trusted by the subscription in use. After the identity is created, the identity can be assigned to one or more Azure service instances. The lifecycle of a user-assigned identity is managed separately from the lifecycle of the Azure service instances to which it's assigned. Internally, managed identities are service principals of a special type, which are locked to only be used with Azure resources. When the managed identity is deleted, the corresponding service principal is automatically removed. Also, when a User-Assigned or System-Assigned Identity is created, the Managed Identity Resource Provider (MSRP) issues a certificate internally to that identity. Your code can use a managed identity to request access tokens for services that support Azure AD authentication. Azure takes care of rolling the credentials that are used by the service instance. Credential rotation Credential rotation is controlled by the resource provider that hosts the Azure resource. The default rotation of the credential occurs every 46 days. It's up to the resource provider to call for new credentials, so the resource provider could wait longer than 46 days. The following diagram shows how managed service identities work with Azure virtual machines (VMs): How a system-assigned managed identity works with an Azure VM Azure Resource Manager receives a request to enable the system-assigned managed identity on a VM. Azure Resource Manager creates a service principal in Azure AD for the identity of the VM. The service principal is created in the Azure AD tenant that's trusted by the subscription. Azure Resource Manager configures the identity on the VM by updating the Azure Instance Metadata Service identity endpoint with the service principal client ID and certificate. After the VM has an identity, use the service principal information to grant the VM access to Azure resources. To call Azure Resource Manager, use role-based access control (RBAC) in Azure AD to assign the appropriate role to the VM service principal. To call Key Vault, grant your code access to the specific secret or key in Key Vault. Your code that's running on the VM can request a token from the Azure Instance Metadata service endpoint, accessible only from within the VM: http://169.254.169.254/metadata/identity/oauth2/token The resource parameter specifies the service to which the token is sent. To authenticate to Azure Resource Manager, use resource=https://management.azure.com/. API version parameter specifies the IMDS version, use api-version=2018-02-01 or greater. A call is made to Azure AD to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Azure AD returns a JSON Web Token (JWT) access token. Your code sends the access token on a call to a service that supports Azure AD authentication Azure App Services Azure App Service is an HTTP-based service for hosting web applications, REST APIs, and mobile backends. You can develop in your favorite language, be it .NET, .NET Core, Java, Ruby, Node.js, PHP, or Python. Applications run and scale with ease on both Windows and Linux-based environments. App Service not only adds the power of Microsoft Azure to your application, such as security, load balancing, autoscaling, and automated management. You can also use its DevOps capabilities, such as continuous deployment from Azure DevOps, GitHub, Docker Hub, and other sources, package management, staging environments, custom domain, and TLS/SSL certificates. With App Service, you pay for the Azure compute resources you use. The compute resources you use are determined by the App Service plan that you run your apps on. For more information, see Azure App Service plans overview. Why use App Service? Azure App Service is a fully managed platform as a service (PaaS) offering for developers. Here are some key features of App Service: Multiple languages and frameworks - App Service has first-class support for ASP.NET, ASP.NET Core, Java, Ruby, Node.js, PHP, or Python. You can also run PowerShell and other scripts or executables as background services. Managed production environment - App Service automatically patches and maintains the OS and language frameworks for you. Spend time writing great apps and let Azure worry about the platform. Containerization and Docker - Dockerize your app and host a custom Windows or Linux container in App Service. Run multi-container apps with Docker Compose. Migrate your Docker skills directly to App Service. DevOps optimization - Set up continuous integration and deployment with Azure DevOps, GitHub, BitBucket, Docker Hub, or Azure Container Registry. Promote updates through test and staging environments. Manage your apps in App Service by using Azure PowerShell or the cross-platform command-line interface (CLI). Global scale with high availability - Scale up or out manually or automatically. Host your apps anywhere in Microsoft's global data center infrastructure, and the App Service SLA promises high availability. Connections to SaaS platforms and on-premises data - Choose from more than 50 connectors for enterprise systems (such as SAP), SaaS services (such as Salesforce), and internet services (such as Facebook). Access on-premises data using Hybrid Connections and Azure Virtual Networks. Security and compliance - The App Service is ISO, SOC, and PCI compliant. Authenticate users with Azure Active Directory, Google, Facebook, Twitter, or Microsoft account. Create IP address restrictions and manage service identities. Prevent subdomain takeovers. Application templates - Choose from an extensive list of application templates in the Azure Marketplace, such as WordPress, Joomla, and Drupal. Visual Studio and Visual Studio Code integration - Dedicated tools in Visual Studio and Visual Studio Code streamline the work of creating, deploying, and debugging. API and mobile features - App Service provides turn-key CORS support for RESTful API scenarios and simplifies mobile app scenarios by enabling authentication, offline data sync, push notifications, and more. Serverless code - Run a code snippet or script on-demand without having to explicitly provision or manage infrastructure and pay only for the compute time your code actually uses. Besides App Service, Azure offers other services that can be used for hosting websites and web applications. For most scenarios, App Service is the best choice. App Service Environment An App Service Environment is an Azure App Service feature that provides a fully isolated and dedicated environment for running App Service apps securely at high scale. An App Service Environment can host: Windows web apps Linux web apps Docker containers (Windows and Linux) Functions Logic apps (Standard) App Service Environments are appropriate for application workloads that require: High scale. Isolation and secure network access. High memory utilization. High requests per second (RPS). You can create multiple App Service Environments in a single Azure region or across multiple Azure regions. This flexibility makes an App Service Environment ideal for horizontally scaling stateless applications with a high RPS requirement. An App Service Environment can host applications from only one customer, and they do so on one of their virtual networks. Customers have fine-grained control over inbound and outbound application network traffic. Applications can establish high-speed secure connections over VPNs to on-premises corporate resources. Usage scenarios App Service Environments have many use cases, including: Internal line-of-business applications. Applications that need more than 30 App Service plan instances. Single-tenant systems to satisfy internal compliance or security requirements. Network-isolated application hosting. Multi-tier applications. There are many networking features that enable apps in a multi-tenant App Service to reach network-isolated resources or become network-isolated themselves. These features are enabled at the application level. With an App Service Environment, no added configuration is required for the apps to be on a virtual network. The apps are deployed into a network-isolated environment that's already on a virtual network. If you really need a complete isolation story, you can also deploy your App Service Environment onto dedicated hardware. Dedicated environment An App Service Environment is a single-tenant deployment of Azure App Service that runs on your virtual network. Applications are hosted in App Service plans, which are created in an App Service Environment. An App Service plan is essentially a provisioning profile for an application host. As you scale out your App Service plan, you create more application hosts with all the apps in that App Service plan on each host. A single App Service Environment v3 can have up to 200 total App Service plan instances across all the App Service plans combined. A single App Service Isolated v2 (Iv2) plan can have up to 100 instances by itself. When you're deploying onto dedicated hardware (hosts), you're limited in scaling across all App Service plans to the number of cores in this type of environment. An App Service Environment that's deployed on dedicated hosts has 132 vCores available. I1v2 uses two vCores, I2v2 uses four vCores, and I3v2 uses eight vCores per instance. Azure App Service plan An app service always runs in an App Serviceplan. In addition, Azure Functions also has the option of running in an App Service plan. An App Service plan defines a set of compute resources for a web app to run. These compute resources are analogous to the server farm in conventional web hosting. One or more apps can be configured to run on the same computing resources (or in the same App Service plan). When you create an App Service plan in a certain region (for example, West Europe), a set of compute resources is created for that plan in that region. Whatever apps you put into this App Service plan run on these compute resources as defined by your App Service plan. Each App Service plan defines: Operating System (Windows, Linux) Region (West US, East US, etc.) Number of VM instances Size of VM instances (Small, Medium, Large) Pricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, Isolated, IsolatedV2) The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. The pricing tiers available to your App Service plan depend on the operating system selected at creation time. There are a few categories of pricing tiers: Shared compute: Free and Shared, the two base tiers, runs an app on the same Azure VM as other App Service apps, including apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources cannot scale out. Dedicated compute: The Basic, Standard, Premium, PremiumV2, and PremiumV3 tiers run apps on dedicated Azure VMs. Only apps in the same App Service plan share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out. Isolated: This Isolated and IsolatedV2 tiers run dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities. App Service Environment networking App Service Environment is a single-tenant deployment of Azure App Service that hosts Windows and Linux containers, web apps, API apps, logic apps, and function apps. When you install an App Service Environment, you pick the Azure virtual network that you want it to be deployed in. All of the inbound and outbound application traffic is inside the virtual network you specify. You deploy into a single subnet in your virtual network, and nothing else can be deployed into that subnet. Subnet requirements You must delegate the subnet to Microsoft.Web/hostingEnvironments, and the subnet must be empty. The size of the subnet can affect the scaling limits of the App Service plan instances within the App Service Environment. It's a good idea to use a /24 address space (256 addresses) for your subnet to ensure enough addresses to support production scale. Note Windows Containers uses an additional IP address per app for each App Service plan instance, and you need to size the subnet accordingly. If your App Service Environment has, for example, 2 Windows Container App Service plans, each with 25 instances and each with 5 apps running, you will need 300 IP addresses and additional addresses to support horizontal (up/down) scale. If you use a smaller subnet, be aware of the following limitations: Any particular subnet has five addresses reserved for management purposes. In addition to the management addresses, App Service Environment dynamically scales the supporting infrastructure and uses between 4 and 27 addresses, depending on the configuration and load. You can use the remaining addresses for instances in the App Service plan. The minimal size of your subnet is a /27 address space (32 addresses). If you run out of addresses within your subnet, you can be restricted from scaling out your App Service plans in the App Service Environment. Another possibility is that you can experience increased latency during intensive traffic load if Microsoft can't scale the supporting infrastructure. Addresses App Service Environment has the following network information at creation: Address type Description App Service Environment virtual network The virtual network deployed into. App Service Environment subnet The subnet deployed into. Domain suffix The domain suffix that is used by the apps made. Virtual IP (VIP) The VIP type is used. The two possible values are internal and external. Inbound address The inbound address is the address at which your apps are reached. If you have an internal VIP, it's an address in your App Service Environment subnet. If the address is external, it's a public-facing address. Default outbound addresses The apps use this address, by default, when making outbound calls to the internet. As you scale your App Service plans in your App Service Environment, you'll use more addresses from your subnet. The number of addresses you use varies based on the number of App Service plan instances you have and how much traffic there is. Apps in the App Service Environment don't have dedicated addresses in the subnet. The specific addresses an app uses in the subnet will change over time. Availability Zone Support for App Service Environments Azure App Service Environment can be deployed across availability zones (AZ) to help you achieve resiliency and reliability for your business-critical workloads. This architecture is also known as zone redundancy. When you configure it to be zone redundant, the platform automatically spreads the instances of the Azure App Service plan across three zones in the selected region. This means that the minimum App Service Plan instance count will always be three. If you specify a capacity larger than three, and the number of instances is divisible by three, the instances are spread evenly. Otherwise, instance counts beyond 3*N are spread across the remaining one or two zones. Prerequisites You configure availability zones when you create your App Service Environment. All App Service plans created in that App Service Environment will need a minimum of 3 instances, and those will automatically be zone redundant. You can only specify availability zones when creating a new App Service Environment. A pre-existing App Service Environment can't be converted to use availability zones. Availability zones are only supported in a subset of regions. Downtime requirements Downtime will be dependent on how you decide to carry out the migration. Since you can't convert pre-existing App Service Environments to use availability zones, migration will consist of a side-by-side deployment where you'll create a new App Service Environment with availability zones enabled. Downtime will depend on how you choose to redirect traffic from your old to your new availability zone-enabled App Service Environment. For example, if you're using an Application Gateway, a custom domain, or Azure Front Door, downtime will be dependent on the time it takes to update those respective services with your new app's information. Alternatively, you can route traffic to multiple apps at the same time using a service such as Azure Traffic Manager and only fully cutover to your new availability zone-enabled apps when everything is deployed and fully tested. For more information on App Service Environment migration options, see App Service Environment migration. If you're already using App Service Environment v3, disregard the information about migration from previous versions and focus on the app migration strategies. App Service Environment Certificates Azure App Service provides a highly scalable, self-patching web hosting service. Once the certificate is added to your App Service app or function app, you can secure a custom Domain Name System (DNS) name with it or use it in your application code. Note A certificate uploaded into an app is stored in a deployment unit that is bound to the app service plan's resource group and region combination (internally called a webspace). This makes the certificate accessible to other apps in the same resource group and region combination. The following lists are options for adding certificates in App Service: Create a free App Service managed certificate: A private certificate that's free of charge and easy to use if you just need to secure your custom domain in App Service. Purchase an App Service certificate: A private certificate that's managed by Azure. It combines the simplicity of automated certificate management and the flexibility of renewal and export options. Import a certificate from Key Vault: Useful if you use Azure Key Vault to manage your Public-Key Cryptography Standards #12 (PKCS12) certificates. Upload a private certificate: If you already have a private certificate from a third-party provider, you can upload it. Upload a public certificate: Public certificates are not used to secure custom domains, but you can load them into your code if you need them to access remote resources. Prerequisites Create an App Service app. For a private certificate, make sure that it satisfies all requirements from App Service. Free certificate only: Map the domain you want a certificate for to App Service. For a root domain (like contoso.com), make sure your app doesn't have any IP restrictions configured. Both certificate creation and its periodic renewal for a root domain depends on your app being reachable from the internet. Configure an app registration via the Azure portal Note The application registration process is constantly being updated and improved. Validate before your demo In this exercise, we will demo how to register an application. In the Portal search for and select Azure Active Directory. Under Manage select App registrations. Click New registration. Name: AZ500 app Review the Supported app types Select Accounts in this organizational directory only (Single tenant) Redirect URL > Web: https://oauth.pstmn.io/v1/browser-callback Click Register Wait for the application to register. On the Overview tab, review the Application (Client ID), Directory (tenant ID), and Object ID. Under Manage click Certificates and Secrets. Review the use of client secrets that an application uses to prove its identity when requesting a token. Click New client secret. Description: key1 Expires: In 1 year Click Add Wait for the application credentials to update. Knowledge check What method does Microsoft Azure App Service use to obtain credentials for users attempting to access an app? Credentials are stored in the browser Pass-through authentication Redirection to a provider endpoint ( Ans ) What type of Managed Service Identities can you create? Application-assigned and VM-assigned Database-assigned and unsigned System-assigned and User-assigned ( Ans ) An App Service application stores page graphics in an Azure storage account. The app needs to authenticate programmatically to the storage account, what should be configured? Create an Azure AD system user Create a managed identity ( Ans ) Create an RBAC role assignment How does using managed identities for Azure resources change the way an app authenticates to Azure Key Vault? The app gets tokens from a token service instead of Azure Active Directory. ( Ans ) The app uses a certificate to authenticate instead of a secret. Managed identities are automatically recognized by Azure Key Vault and authenticated automatically. Implement storage security Introduction Every organization has data. That data needs to be protected at rest, in transit, and while it's being used within an application. Azure provides a set of security features to protect your data, no matter where it's located. Scenario A security engineer will protect data within databases, file shares, or anywhere it resides, you'll work on such tasks as: Lock down access to storage to specific users and applications. Encrypt data where it's stored and while it's moved around. Use identity to protect data when it's accessed and used. Define data sovereignty What is Data Sovereignty? - Data sovereignty is the concept that information, which has been converted and stored in binary digital form, is subject to the laws of the country or region in which it is located. Many of the current concerns that surround data sovereignty relate to enforcing privacy regulations and preventing data stored in a foreign country or region from being subpoenaed by the host country or region\u2019s government. In Azure, customer data might be replicated within a selected geographic area for enhanced data durability during a major data center disaster, and in some cases will not be replicated outside it. Paired regions Azure operates in multiple geographies around the world. An Azure geography is a defined area of the world that contains at least one Azure Region. An Azure region is an area within a geography, containing one or more datacenters. Each Azure region is paired with another region within the same geography, forming a regional pair. The exception is Brazil South, which is paired with a region outside its geography. Across the region pairs Azure serializes platform updates (or planned maintenance), so that only one paired region is updated at a time. If an outage affecting multiple regions, one region in each pair will be prioritized for recovery. We recommend that you configure business continuity and disaster recovery (BCDR) across regional pairs to benefit from Azure\u2019s isolation and VM policies. For applications that support multiple active regions, we recommend using both regions in a region pair where possible. Multiple regions will ensure optimal availability for applications and minimized recovery time in a disaster. Benefits of Azure paired regions Physical isolation - When possible, Azure services prefer at least 300 miles of separation between datacenters in a regional pair (although longer distance isn't practical or possible in all geographies). Physical datacenter separation reduces the likelihood of both regions being affected simultaneously as a result of natural disasters, civil unrest, power outages, or physical network outages. Isolation is subject to the constraints within the geography, such as geography size, power and network infrastructure availability, and regulations. Platform-provided replication - Some services such as geo-redundant storage provide automatic replication to the paired region. Region recovery order - If a broad outage occurs, recovery of one region is prioritized out of every pair. Applications that are deployed across paired regions are guaranteed to have one of the regions recovered with priority. If an application is deployed across regions that are not paired, recovery might be delayed. In the worst case, the chosen regions might be the last two to be recovered. Sequential updates - Planned Azure system updates are rolled out to paired regions sequentially, not at the same time. A staged roll-out helps minimize downtime, the effect of bugs, and logical failures in the rare event of a bad update. Data residency - To meet data residency requirements for tax and law enforcement jurisdiction purposes, a region resides within the same geography as its pair (except for Brazil South). Configure Azure storage access Every request made against a secured resource in the Blob, File, Queue, or Table service must be authorized. Authorization ensures that resources in your storage account are accessible only when you want them to be, and only to those users or applications to whom you grant access. Options for authorizing requests to Azure Storage include Azure AD - Azure Storage provides integration with Azure Active Directory (Azure AD) for identity-based authorization of requests to the Blob and Queue services. With Azure AD, you can use role-based access control (RBAC) to grant access to blob and queue resources to users, groups, or applications. You can grant permissions that are scoped to the level of an individual container or queue. Authorizing access to blob and queue data with Azure AD provides superior security and ease of use over other authorization options. When you use Azure AD to authorize requests make from your applications, you avoid having to store your account access key with your code, as you do with Shared Key authorization. While you can continue to use Shared Key authorization with your blob and queue applications, Microsoft recommends moving to Azure AD where possible. Azure Active Directory Domain Services (Azure AD DS) authorization for Azure Files. Azure Files supports identity-based authorization over Server Message Block (SMB) through Azure AD DS. You can use RBAC for fine-grained control over a client's access to Azure Files resources in a storage account Shared Key - Shared Key authorization relies on your account access keys and other parameters to produce an encrypted signature string that is passed on via the request in the Authorization header. Shared Access Signatures - A shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients who should not be trusted with your storage account key but to whom you wish to delegate access to certain storage account resources. By distributing a shared access signature URI to these clients, you can grant them access to a resource for a specified period of time, with a specified set of permissions. The URI query parameters comprising the SAS token incorporate all of the information necessary to grant controlled access to a storage resource. A client who is in possession of the SAS can make a request against Azure Storage with just the SAS URI, and the information contained in the SAS token is used to authorize the request. Anonymous access to containers and blobs - You can enable anonymous, public read access to a container and its blobs in Azure Blob storage. By doing so, you can grant read-only access to these resources without sharing your account key, and without requiring a shared access signature (SAS).Public read access is best for scenarios where you want certain blobs to always be available for anonymous read access. For more fine-grained control, look to using the shared access signature, described above. Authenticating and authorizing access to blob and queue data with Azure AD provides superior security and ease of use over other authorization options. For example, by using Azure AD, you avoid having to store your account access key with your code, as you do with Shared Key authorization. While you can continue to use Shared Key authorization with your blob and queue applications, Microsoft recommends moving to Azure AD where possible. Similarly, you can continue to use shared access signatures (SAS) to grant fine-grained access to resources in your storage account, but Azure AD offers similar capabilities without the need to manage SAS tokens or worry about revoking a compromised SAS. Important Where possible use authorizing applications that access Azure Storage using Azure AD. It provides better security and ease of use over other authorization options. Deploy shared access signatures As a best practice, you shouldn't share storage account keys with external third-party applications. If these apps need access to your data, you'll need to secure their connections without using storage account keys. For untrusted clients, use a shared access signature (SAS). A shared access signature is a string that contains a security token that can be attached to a URI. Use a shared access signature to delegate access to storage objects and specify constraints, such as the permissions and the time range of access. You can give a customer a shared access signature token, for example, so they can upload pictures to a file system in Blob storage. Separately, you can give a web application permission to read those pictures. In both cases, you allow only the access that the application needs to do the task. Types of shared access signatures You can use a service-level shared access signature to allow access to specific resources in a storage account. You'd use this type of shared access signature, for example, to allow an app to retrieve a list of files in a file system or to download a file. Use an account-level shared access signature to allow access to anything that a service-level shared access signature can allow, plus additional resources and abilities. For example, you can use an account-level shared access signature to allow the ability to create file systems. A user delegation SAS, introduced with version 2018-11-09. A user delegation SAS is secured with Azure AD credentials. This type of SAS is supported for the Blob service only and can be used to grant access to containers and blobs. Additionally, a service SAS can reference a stored access policy that provides an additional level of control over a set of signatures, including the ability to modify or revoke access to the resource if necessary. One would typically use a shared access signature for a service where users read and write their data to your storage account. Accounts that store user data have two typical designs: Clients upload and download data through a front-end proxy service, which performs authentication. This front-end proxy service has the advantage of allowing validation of business rules. But if the service must handle large amounts of data or high-volume transactions, you might find it complicated or expensive to scale this service to match demand. A lightweight service authenticates the client as needed. Then it generates a shared access signature. After receiving the shared access signature, the client can access storage account resources directly. The shared access signature defines the client's permissions and access interval. The shared access signature reduces the need to route all data through the front-end proxy service. Manage Azure AD storage authentication In addition to Shared Key and Shared Access Signatures, Azure Storage supports using Azure Active Directory (Azure AD) to authorize requests to blob data. With Azure AD, you can use Azure role-based access control (Azure RBAC) to grant permissions to a security principal, which may be a user, group, or application service principal. The security principal is authenticated by Azure AD to return an OAuth 2.0 token. The token can then be used to authorize a request against the Blob service. To authorize requests against Azure Storage with Azure AD provides superior security and ease of use over Shared Key authorization. Microsoft recommends using Azure AD authorization with your blob applications when possible to assure access with minimum required privileges. Authorization with Azure AD is available for all general-purpose and Blob storage accounts in all public regions and national clouds. Only storage accounts created with the Azure Resource Manager deployment model support Azure AD authorization. Blob storage additionally supports creating shared access signatures (SAS) that is signed with Azure AD credentials. A few more details When a security principal (a user, group, or application) attempts to access a queue resource, the request must be authorized. With Azure AD, access to a resource is a two-step process. First, the security principal's identity is authenticated, and an OAuth 2.0 token is returned. Next, the token is passed as part of a request to the Queue service and used by the service to authorize access to the specified resource. The authentication step requires that an application request an OAuth 2.0 access token at runtime. If an application is running from within an Azure entity, such as an Azure VM, a Virtual Machine Scale Set, or an Azure Functions app, it can use a managed identity to access queues. The authorization step requires one or more Azure roles to be assigned to the security principal. Azure Storage provides Azure roles that encompass common sets of permissions for queue data. The roles that are assigned to a security principal determine the permissions that that principal will have. Native and web applications that request the Azure Queue service can also authorize access with Azure AD. Implement storage service encryption Azure Storage security is a key part to defense in depth. Azure Storage provides a comprehensive set of security capabilities that together enable developers to build secure applications: All data (including metadata) written to Azure Storage is automatically encrypted using Storage Service Encryption (SSE). Azure Active Directory (Azure AD) and Role-Based Access Control (RBAC) are supported for Azure Storage for both resource management operations and data operations, as follows: You can assign RBAC roles scoped to the storage account to security principals and use Azure AD to authorize resource management operations such as key management. Azure AD integration is supported for blob and queue data operations. You can assign RBAC roles scoped to a subscription, resource group, storage account, or an individual container or queue to a security principal or a managed identity for Azure resources. Data can be secured in transit between an application and Azure by using Client-Side Encryption, HTTPS, or SMB 3.0. OS and data disks used by Azure virtual machines can be encrypted using Azure Disk Encryption. Delegated access to the data objects in Azure Storage can be granted using a shared access signature. Azure Storage encryption for data at rest Azure Storage automatically encrypts your data when persisting it to the cloud. Encryption protects your data and to help you to meet your organizational security and compliance commitments. Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant. Azure Storage encryption is similar to BitLocker encryption on Windows. Azure Storage encryption is enabled for all new and existing storage accounts and cannot be disabled. Because your data is secured by default, you don't need to modify your code or applications to take advantage of Azure Storage encryption. Storage accounts are encrypted regardless of their performance tier (standard or premium) or deployment model (Azure Resource Manager or classic). All Azure Storage redundancy options support encryption, and all copies of a storage account are encrypted. All Azure Storage resources are encrypted, including blobs, disks, files, queues, and tables. All object metadata is also encrypted. Encryption does not affect Azure Storage performance. There is no additional cost for Azure Storage encryption. Encryption key management You can rely on Microsoft-managed keys for the encryption of your storage account, or you can manage encryption with your own keys. If you choose to manage encryption with your own keys, you have two options: You can specify a customer-managed key to use for encrypting and decrypting all data in the storage account. A customer-managed key is used to encrypt all data in all services in your storage account. You can specify a customer-provided key on Blob storage operations. A client making a read or write request against Blob storage can include an encryption key on the request for granular control over how blob data is encrypted and decrypted. The following table compares key management options for Azure Storage encryption. Microsoft-managed keys Customer-managed keys Customer-provided keys Encryption/decryption operations Azure Azure Azure Azure Storage services supported All Blob storage, Azure Files Blob storage Key storage Microsoft key store Azure Key Vault Azure Key Vault or any other key store Key rotation responsibility Microsoft Customer Customer Key usage Microsoft Azure portal, Storage Resource Provider REST API, Azure Storage management libraries, PowerShell, CLI Azure Storage REST API (Blob storage), Azure Storage client libraries Key access Microsoft only Microsoft, Customer Customer only Configure blob data retention policies Immutable storage for Azure Blob storage enables users to store business-critical data objects in a WORM (Write Once, Read Many) state. This state makes the data non-erasable and non-modifiable for a user-specified interval. For the duration of the retention interval, blobs can be created and read, but cannot be modified or deleted. Immutable storage is available for general-purpose v2 and Blob storage accounts in all Azure regions. Time-based vs Legal hold policies Time-based retention policy support: Users can set policies to store data for a specified interval. When a time-based retention policy is set, blobs can be created and read, but not modified or deleted. After the retention period has expired, blobs can be deleted but not overwritten. When a time-based retention policy is applied on a container, all blobs in the container will stay in the immutable state for the duration of the effective retention period. The effective retention period for blobs is equal to the difference between the blob's creation time and the user-specified retention interval. Because users can extend the retention interval, immutable storage uses the most recent value of the user-specified retention interval to calculate the effective retention period. Legal hold policy support: If the retention interval is not known, users can set legal holds to store immutable data until the legal hold is cleared. When a legal hold policy is set, blobs can be created and read, but not modified or deleted. Each legal hold is associated with a user-defined alphanumeric tag (such as a case ID, event name, etc.) that is used as an identifier string. Legal holds are temporary holds that can be used for legal investigation purposes or general protection policies. Each legal hold policy needs to be associated with one or more tags. Tags are used as a named identifier, such as a case ID or event, to categorize and describe the purpose of the hold. Other immutable storage features Support for all blob tiers: WORM policies are independent of the Azure Blob storage tier and apply to all the tiers: hot, cool, and archive. Users can transition data to the most cost-optimized tier for their workloads while maintaining data immutability. Container-level configuration: Users can configure time-based retention policies and legal hold tags at the container level. By using simple container-level settings, users can create and lock time-based retention policies, extend retention intervals, set and clear legal holds, and more. These policies apply to all the blobs in the container, both existing and new. Audit logging support: Each container includes a policy audit log. It shows up to seven time-based retention commands for locked time-based retention policies and contains the user ID, command type, time stamps, and retention interval. For legal holds, the log contains the user ID, command type, time stamps, and legal hold tags. This log is retained for the lifetime of the policy, in accordance with the SEC 17a-4(f) regulatory guidelines. The Azure Activity Log shows a more comprehensive log of all the control plane activities; while enabling Azure Resource Logs retains and shows data plane operations. It is the user's responsibility to store those logs persistently, as might be required for regulatory or other purposes. Important A container can have both a legal hold and a time-based retention policy at the same time. All blobs in that container stay in the immutable state until all legal holds are cleared, even if their effective retention period has expired. Conversely, a blob stays in an immutable state until the effective retention period expires, even though all legal holds have been cleared. Configure Azure files authentication Azure Files supports identity-based authentication over Server Message Block (SMB) through on-premises Active Directory Domain Services (AD DS) and Azure Active Directory Domain Services (Azure AD DS). This article focuses on how Azure file shares can use domain services, either on-premises or in Azure, to support identity-based access to Azure file shares over SMB. Enabling identity-based access for your Azure file shares allows you to replace existing file servers with Azure file shares without replacing your existing directory service, maintaining seamless user access to shares. Azure Files enforces authorization on user access to both the share and the directory/file levels. Share-level permission assignment can be performed on Azure Active Directory (Azure AD) users or groups managed through the role-based access control (RBAC) model. With RBAC, the credentials you use for file access should be available or synced to Azure AD. You can assign built-in RBAC roles like Storage File Data SMB Share Reader to users or groups in Azure AD to grant read access to an Azure file share. At the directory/file level, Azure Files supports preserving, inheriting, and enforcing Windows DACLs just like any Windows file servers. You can choose to keep Windows DACLs when copying data over SMB between your existing file share and your Azure file shares. Whether you plan to enforce authorization or not, you can use Azure file shares to back up ACLs along with your data. Advantages of identity-based authentication Identity-based authentication for Azure Files offers several benefits over using Shared Key authentication: Extend the traditional identity-based file share access experience to the cloud with on-premises AD DS and Azure AD DS. If you plan to lift and shift your application to the cloud, replacing traditional file servers with Azure file shares, then you may want your application to authenticate with either on-premises AD DS or Azure AD DS credentials to access file data. Azure Files supports using both on-premises AD DS or Azure AD DS credentials to access Azure file shares over SMB from either on-premises AD DS or Azure AD DS domain-joined VMs. Enforce granular access control on Azure file shares. You can grant permissions to a specific identity at the share, directory, or file level. For example, suppose that you have several teams using a single Azure file share for project collaboration. You can grant all teams access to non-sensitive directories while limiting access to directories containing sensitive financial data to your Finance team only. Back up Windows ACLs (also known as NTFS) along with your data. You can use Azure file shares to back up your existing on-premises file shares. Azure Files preserves your ACLs along with your data when you back up a file share to Azure file shares over SMB. Identity-based authentication data flow How it works Azure file shares leverages Kerberos protocol for authenticating with either on-premises AD DS or Azure AD DS. When an identity associated with a user or application running on a client attempts to access data in Azure file shares, the request is sent to the domain service, either AD DS or Azure AD DS, to authenticate the identity. If authentication is successful, it returns a Kerberos token. The client sends a request that includes the Kerberos token and Azure file shares use that token to authorize the request. Azure file shares only receive the Kerberos token, not access credentials. Preserve directory and file ACLs when importing data to Azure file shares Azure Files supports preserving directory or file level ACLs when copying data to Azure file shares. You can copy ACLs on a directory or file to Azure file shares using either Azure File Sync or common file movement toolsets. For example, you can use robocopy with the /copy:s flag to copy data as well as ACLs to an Azure file share. ACLs are preserved by default, you are not required to enable identity-based authentication on your storage account to preserve ACLs. Enable the secure transfer required property You can configure your storage account to accept requests from secure connections only by setting the Secure transfer required property for the storage account. When you require secure transfer, any requests originating from an insecure connection are rejected. Microsoft recommends that you always require secure transfer for all of your storage accounts. When secure transfer is required, a call to an Azure Storage REST API operation must be made over HTTPS. Any request made over HTTP is rejected. Connecting to an Azure File share over SMB without encryption fails when secure transfer is required for the storage account. Examples of insecure connections include those made over SMB 2.1, SMB 3.0 without encryption, or some versions of the Linux SMB client. By default, the Secure transfer required property is enabled when you create a storage account. Azure Storage doesn't support HTTPS for custom domain names, this option is not applied when you're using a custom domain name. Require secure transfer for a new storage account Important Azure Files connections require encryption (SMB) Perform Try-This exercises Use this Try-This exercise to gain some hands-on experience with Azure. In this demonstration, we'll explore storage security configurations. Task 1: Generate SAS tokens Note This demonstration requires a storage account with a blob container and an uploaded file. For the best results, upload a PNG or JPEG file. In this task, we'll generate and test a Shared Access Signature. Open the Azure portal. Navigate to your Storage Account. Under Settings, select Access keys. Explain how Storage Account access keys can be used. Review regenerating keys. Under Settings, select Shared access signature. Explain how an account-level SAS can be used. Review the configuration settings, including Allowed services, Allowed resource type, Allowed permissions, and Start and expiry date/times. Back at the Storage Account page, under Blob service, select Containers. Right-click the blob file that you want to share and select Generate SAS. Click Generate SAS token and URL. Copy the Blob SAS URL. There's a clipboard icon on the far right of the text box. Copy the URL into a browser, and your file should display. Task 2: Key Rollover Note Always use the latest version of Azure Storage Explorer. In this task, we'll use Storage Explorer to test key rollover. Download and install Azure Storage Explorer - https://azure.microsoft.com/features/storage-explorer/ After the installation, launch the tool. Review the Release Notes and menu options. If this is your first time using the tool, you'll need to re-enter your credentials. After you've been authenticated, you can select the subscriptions of interest. Explain Storage Explorer can also be used for Local and attached accounts. Right-click Storage Accounts and select Connect to Azure storage. Discuss the various connection options. Select Use a storage account name and key. In the portal, select your storage account. Under Settings, select Access Keys. Retrieve the Storage account name and key1 key. In Storage Explorer, provide the account and key information, then click Connect. Verify that you can navigate to your storage account content. In the portal and your storage account. Under Settings, select Access Keys. Next to key1 click the Regenerate icon. Acknowledge the message that the current key will become immediately invalid and isn't recoverable. In Storage Explorer, refresh the storage account. You should receive an error that the server failed to authenticate the request. Reconnect so you can continue with the demonstration. Task 3: Storage Access Policies In this task, we'll create a blob storage access policy. In the Portal, navigate to your Blob container. Under Settings, select Access Policy. Review the two policies: Storage access policies and Blob immutable storage. Under Stored access polices click Add policy. Create a policy with Read and List permissions and usable for a restricted period of time. Under Blob immutable storage, click Add policy. Review the two policy types: Time-based retention and Legal hold. Create a policy based on time-based retention. Be sure to Save your changes. In Storage Explorer, right-click your container and select Get shared access signature. The Access Policy drop-down enables you to create a SAS based on a pre-defined configuration. As you have time, show how Storage Explorer can be used to perform security tasks. Task 4: Azure AD User Account Authentication In this task, we will configure Azure AD user account authentication for storage. In the portal, navigate to and select your blob container. Notice at the top the authentication method. There are two choices: Access key and Azure AD User Account. Explain the differences between the two methods. Switch to Azure AD User Account. You should receive an error stating you don't have access permissions. Click Access Control (IAM). Select Add role assignment. Select the Storage Blob Data Owner role. Discuss the other storage roles that are shown. Assign the role to your account and Save your changes. Return to the Overview blade. Switch to Azure AD User Account. Notice that you're now able to view the container. Take a minute to select Change access level and review the Public access level choices. Task 5: Storage Endpoints (if you haven't already done this in the Network lesson) Note This task requires a storage account and virtual network with subnet. Storage Explorer is also required. In this task, we'll secure a storage endpoint. In the Portal. Locate your storage account. Create a file share, and upload a file. Use the Shared Access Signature blade to Generate SAS and connection string. Use Storage Explorer and the connection string to access the file share. Ensure you can view your uploaded file. Locate your virtual network, and then select a subnet in the virtual network. Under Service Endpoints, view the Services drop-down and the different services that can be secured with an endpoint. Check the Microsoft.Storage option. Save your changes. Return to your storage account. Select Firewalls and virtual networks. Change to Selected networks. Add your virtual network and verify your subnet with the new service endpoint is listed. Save your changes. Return to the Storage Explorer. Refresh the storage account. Verify you can no longer access the file share. Knowledge check There is a need to provide a contingent staff employee temporary read-only access to the contents of an Azure storage account container named \u201cMedia\u201d. It is important to grant access while adhering to the security principle of least-privilege. What should be configured? Set the public access level to container. Generate a shared access signature (SAS) token for the container. ( Ans ) Share the container entity tag (Etag) with the contingent staff member. A company has both a development and production environment. The development environment needs time-limited access to storage. The production environment needs unrestricted access to storage resources. To configure storage access to meet the requirements, what configuration choices should be done? Use shared access signatures for the development apps. And use access keys for the production apps. ( Ans ) Use shared access signatures for the production apps. Then, use access keys for the development apps. Use Stored Access Policies for the production apps. Also, use Cross Origin Resource Sharing for the development apps. A company is being audited. It is not known how long the audit will take, but during that time files must not be changed or removed. It is okay to read or create new files. What should be done to set this up? Add a time-based retention policy to the blob container. And create a tag to identify items being protected. Add legal hold retention policy to the blob container. Also, identify a tag for the items that are being protected. ( Ans ) Configure a retention time period of two weeks with an option to renew. Then, add a time-based retention policy to the blob container. Which statements are factual when configuring an Azure file share for a business group? Azure Files can authenticate to Azure Active Directory Domain Services. ( Ans ) Azure Files cannot authenticate to on-premises Active Directory Domain Services. Azure Files can use RBAC for share-level or directory/file permissions. When configuring Secure transfer, the Compliance office wants to understand how connections are secured when REST API operational calls are made to an Azure Storage account. What information would be provided? Requests to storage can be HTTPS or HTTP. Requests to storage must be SMB with data access flag enabled. By default, new storage accounts have secure transfer required enabled. ( Ans )","title":"Explore Azure Key Vault"},{"location":"Cloud/Azure/AZ-500/Secure-your-data-and-applications.html#configure-and-manage-sql-database-security","text":"Configure SQL database firewalls Start your database security process by configuring a SQL Database firewall. Azure SQL Database and Azure Synapse Analytics, previously SQL Data Warehouse, (both referred to as SQL Database in this lesson) provide a relational database service for Azure and other internet-based applications. To help protect your data, firewalls prevent all access to your database server until you specify which computers have permission. The firewall grants access to databases based on the originating IP address of each request. In addition to IP rules, the firewall also manages virtual network rules. Virtual network rules are based on virtual network service endpoints. Virtual network rules might be preferable to IP rules in some cases. Overview Initially, all access to your Azure SQL Database is blocked by the SQL Database firewall. To access a database server, you must specify one or more server-level IP firewall rules that enable access to your Azure SQL Database. Use the IP firewall rules to specify which IP address ranges from the internet are allowed, and whether Azure applications can attempt to connect to your Azure SQL Database. To selectively grant access to just one of the databases in your Azure SQL Database, you must create a database-level rule for the required database. Specify an IP address range for the database IP firewall rule that is beyond the IP address range specified in the server-level IP firewall rule, and ensure that the IP address of the client falls in the range specified in the database-level rule. Note Azure Synapse Analytics only supports server-level IP firewall rules, and not database-level IP firewall rules. Connecting from the internet When a computer attempts to connect to your database server from the internet, the firewall first checks the originating IP address of the request against the database-level IP firewall rules for the database that the connection is requesting: If the IP address of the request is within one of the ranges specified in the database-level IP firewall rules, the connection is granted to the SQL Database containing the rule. If the IP address of the request is not within one of the ranges specified in the database-level IP firewall rules, the firewall checks the server-level IP firewall rules. If the IP address of the request is within one of the ranges specified in the server-level IP firewall rules, the connection is granted. Server-level IP firewall rules apply to all SQL databases on the Azure SQL Database. If the IP address of the request is not within the ranges specified in any of the database-level or server-level IP firewall rules, the connection request fails. Connecting from Azure To allow applications from Azure to connect to your Azure SQL Database, Azure connections must be enabled. When an application from Azure attempts to connect to your database server, the firewall verifies that Azure connections are allowed. A firewall setting with starting and ending addresses equal to 0.0.0.0 indicates Azure connections are allowed. If the connection attempt is not allowed, the request does not reach the Azure SQL Database server. This option configures the firewall to allow all connections from Azure including connections from the subscriptions of other customers. When selecting this option, make sure your sign-in and user permissions limit access to authorized users only. Server-level IP firewall rules Server-level IP firewall rules enable clients to access your entire Azure SQL Database\u2014that is, all the databases within the same SQL Database server. These rules are stored in the master database. You can configure server-level IP firewall rules using the Azure portal, PowerShell, or by using Transact-SQL statements. To create server-level IP firewall rules using the Azure portal or PowerShell, you must be the subscription owner or a subscription contributor. To create a server-level IP firewall rule using Transact-SQL, you must connect to the SQL Database instance as the server-level principal login or the Azure Active Directory (Azure AD) administrator (which means that a server-level IP firewall rule must have first been created by a user with Azure-level permissions). Database-level IP firewall rules Database-level IP firewall rules enable clients to access certain secure databases within the same SQL Database server. You can create these rules for each database (including the master database), and they are stored in the individual databases. You can only create and manage database-level IP firewall rules for master databases and user databases by using Transact-SQL statements, and only after you have configured the first server-level firewall. If you specify an IP address range in the database-level IP firewall rule that is outside the range specified in the server-level IP firewall rule, only those clients that have IP addresses in the database-level range can access the database. You can have a maximum of 128 database-level IP firewall rules for a database. Important Whenever possible, as a best practice, use database-level IP firewall rules to enhance security and to make your database more portable. Use server-level IP firewall rules for administrators and when you have several databases with the same access requirements, and you don't want to spend time configuring each database individually. Enable and monitor database auditing Auditing for Azure SQL Database and Azure Synapse Analytics tracks database events and writes them to an audit log in your Azure storage account, Log Analytics workspace or Event Hubs. Auditing also: Helps you maintain regulatory compliance, understand database activity, and gain insight into discrepancies and anomalies that could indicate business concerns or suspected security violations. Enables and facilitates adherence to compliance standards, although it doesn't guarantee compliance. Overview You can use SQL database auditing to: Retain an audit trail of selected events. You can define categories of database actions to be audited. Report on database activity. You can use pre-configured reports and a dashboard to get started quickly with activity and event reporting. Analyze reports. You can find suspicious events, unusual activity, and trends. Define server-level vs. database-level auditing policy An auditing policy can be defined for a specific database or as a default server policy: A server policy applies to all existing and newly created databases on the server. If server auditing is enabled, it always applies to the database. The database will be audited, regardless of the database auditing settings. Enabling auditing on the database or data warehouse, in addition to enabling it on the server, does not override or change any of the settings of the server auditing. Both audits will exist side by side. In other words, the database is audited twice in parallel; once by the server policy and once by the database policy. Shown below is the configuration of auditing using the Azure portal. Summary of database auditing Retain an audit trail of selected events Report on database activity and analyze results Configure policies for the server or database level Configure audit log destination A new server policy applies to all existing and newly created databases Implement data discovery and classification Data Discovery & Classification is built into Azure SQL Database. It provides advanced capabilities for discovering, classifying, labeling, and reporting the sensitive data in your databases. Your most sensitive data might include business, financial, healthcare, or personal information. Discovering and classifying this data can play a pivotal role in your organization's information-protection approach. It can serve as infrastructure for: Helping to meet standards for data privacy and requirements for regulatory compliance. Various security scenarios, such as monitoring (auditing) and alerting on anomalous access to sensitive data. Controlling access to and hardening the security of databases that contain highly sensitive data. Data Discovery & Classification is part of the Advanced Data Security offering, which is a unified package for advanced SQL security capabilities. You can access and manage Data Discovery & Classification via the central SQL Advanced Data Security section of the Azure portal. Classifying your data and identifying your data protection needs helps you select the right cloud solution for your organization. Data classification enables organizations to find storage optimizations that might not be possible when all data is assigned the same value. Classifying (or categorizing) stored data by sensitivity and business impact helps organizations determine the risks associated with the data. After your data has been classified, organizations can manage their data in ways that reflect their internal value instead of treating all data the same way. Data classification can yield benefits such as compliance efficiencies, improved ways to manage the organization\u2019s resources, and facilitation of migration to the cloud. Some data protection solutions\u2014such as encryption, rights management, and data loss prevention\u2014have moved to the cloud and can help mitigate cloud risks. However, organization must be sure to address data classification rules for data retention when moving to the cloud. Data exists in one of three basic states: at rest, in process, and in transit. All three states require unique technical solutions for data classification, but the applied principles of data classification should be the same for each. Data that is classified as confidential needs to stay confidential when at rest, in process, or in transit. Data can also be either structured or unstructured. Typical classification processes for structured data found in databases and spreadsheets are less complex and time-consuming to manage than those for unstructured data such as documents, source code, and email. Generally, organizations will have more unstructured data than structured data. Regardless of whether data is structured or unstructured, it\u2019s important for organizations to manage data sensitivity. When properly implemented, data classification helps ensure that sensitive or confidential data assets are managed with greater oversight than data assets that are considered public distribution. Protect data at rest Data encryption at rest is a mandatory step toward data privacy, compliance, and data sovereignty. Best practice Solution Apply disk encryption to help safeguard your data. Use Microsoft Azure Disk Encryption, which enables IT administrators to encrypt both Windows infrastructure as a service (IaaS) and Linux IaaS virtual machine (VM) disks. Disk encryption combines the industry-standard BitLocker feature and the Linux DM-Crypt feature to provide volume encryption for the operating system (OS) and the data disks. Azure Storage and Azure SQL Database encrypt data at rest by default, and many services offer encryption as an option. You can use Azure Key Vault to maintain control of keys that access and encrypt your data. Use encryption to help mitigate risks related to unauthorized data access. Encrypt your drives before you write sensitive data to them. Organizations that don\u2019t enforce data encryption are risk greater exposure to data-integrity issues. For example, unauthorized users or malicious hackers might steal data in compromised accounts or gain unauthorized access to data coded in Clear Format. To comply with industry regulations, companies also must prove that they are diligent and using correct security controls to enhance their data security. Protect data in transit Protecting data in transit should be an essential part of your data protection strategy. Because data is moving back and forth from many locations, we generally recommend that you always use SSL/TLS protocols to exchange data across different locations. In some circumstances, you might want to isolate the entire communication channel between your on-premises and cloud infrastructures by using a VPN. For data moving between your on-premises infrastructure and Azure, consider appropriate safeguards such as HTTPS or VPN. When sending encrypted traffic between an Azure virtual network and an on-premises location over the public internet, use Azure VPN Gateway. The following table lists best practices specific to using Azure VPN Gateway, SSL/TLS, and HTTPS. Best practice Solution Secure access from multiple workstations located on-premises to an Azure virtual network Use site-to-site VPN. Secure access from an individual workstation located on-premises to an Azure virtual network Use point-to-site VPN. Move larger data sets over a dedicated high-speed wide area network (WAN) link Use Azure ExpressRoute. If you choose to use ExpressRoute, you can also encrypt the data at the application level by using SSL/TLS or other protocols for added protection. Interact with Azure Storage through the Azure portal All transactions occur via HTTPS. You can also use Storage REST API over HTTPS to interact with Azure Storage and Azure SQL Database. Organizations that fail to protect data in transit are more susceptible to man-in-the-middle attacks, eavesdropping, and session hijacking. These attacks can be the first step in gaining access to confidential data. Now that we\u2019ve covered the physical aspects of data classification, let\u2019s look at the classification based on discovery and classification. Data Discovery Data discovery and classification provides advanced capabilities built into Azure SQL Database for discovering, classifying, labeling and protecting sensitive data (such as business, personal data, and financial information) in your databases. Discovering and classifying this data can play a pivotal role in your organizational information protection stature. It can serve as infrastructure for: Helping meet data privacy standards and regulatory compliance requirements. Addressing various security scenarios such as monitoring, auditing, and alerting on anomalous access to sensitive data. Controlling access to and hardening the security of databases containing highly sensitive data. Data discovery and classification is part of the Advanced Data Security offering, which is a unified package for advanced Microsoft SQL Server security capabilities. You access and manage data discovery and classification via the central SQL Advanced Data Security portal. Data discovery and classification introduces a set of advanced services and SQL capabilities, forming a SQL Information Protection paradigm aimed at protecting the data, not just the database: Discovery and recommendations - The classification engine scans your database and identifies columns containing potentially sensitive data. It then provides you with an easier way to review and apply the appropriate classification recommendations via the Azure portal. Labeling - Sensitivity classification labels can be persistently tagged on columns using new classification metadata attributes introduced into the SQL Server Engine. This metadata can then be utilized for advanced sensitivity-based auditing and protection scenarios. Query result set sensitivity - The sensitivity of the query result set is calculated in real time for auditing purposes. Visibility - You can view the database classification state in a detailed dashboard in the Azure portal. Additionally, you can download a report (in Microsoft Excel format) that you can use for compliance and auditing purposes, in addition to other needs. Steps for discovery, classification, and labeling Classifications have two metadata attributes: Labels - These are the main classification attributes used to define the sensitivity level of the data stored in the column. Information Types - These provide additional granularity into the type of data stored in the column. SQL data discovery and classification comes with a built-in set of sensitivity labels and information types, and discovery logic. You can now customize this taxonomy and define a set and ranking of classification constructs specifically for your environment. Definition and customization of your classification taxonomy takes place in one central location for your entire Azure Tenant. That location is in Microsoft Defender for Cloud, as part of your Security Policy. Only a user with administrative rights on the Tenant root management group can perform this task. As part of Azure Information Protection policy management, you can define custom labels, rank them, and associate them with a selected set of information types. You can also add your own custom information types and configure them with string patterns, which are added to the discovery logic for identifying this type of data in your databases. Learn more about customizing and managing your policy in the Information Protection policy how-to guide. After you\u2019ve defined the tenant-wide policy, you can continue with classifying individual databases using your customized policy. Microsoft Defender for SQL Applies to: Azure SQL Database | Azure SQL Managed Instance | Azure Synapse Analytics Microsoft Defender for SQL is a Defender plan in Microsoft Defender for Cloud. Microsoft Defender for SQL includes functionality for surfacing and mitigating potential database vulnerabilities and detecting anomalous activities that could indicate a threat to your database. It provides a single go-to location for enabling and managing these capabilities. What are the benefits of Microsoft Defender for SQL? Microsoft Defender for SQL provides a set of advanced SQL security capabilities, including SQL Vulnerability Assessment and Advanced Threat Protection. Vulnerability Assessment is an easy-to-configure service that can discover, track, and help you remediate potential database vulnerabilities. It provides visibility into your security state, and it includes actionable steps to resolve security issues and enhance your database fortifications. Advanced Threat Protection detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit your database. It continuously monitors your database for suspicious activities, and it provides immediate security alerts on potential vulnerabilities, Azure SQL injection attacks, and anomalous database access patterns. Advanced Threat Protection alerts provide details of suspicious activity and recommend action on how to investigate and mitigate the threat. Enable Microsoft Defender for SQL once to enable all these included features. With one selection, you can enable Microsoft Defender for all databases on your server in Azure or in your SQL Managed Instance. Enabling or managing Microsoft Defender for SQL settings requires belonging to the SQL security manager role or one of the database or server admin roles. Example: Database selection options 4/4 Vulnerability assessment for SQL Server What is SQL vulnerability assessment? SQL vulnerability assessment is a service that provides visibility into your security state. Vulnerability assessment includes actionable steps to resolve security issues and enhance your database security. It can help you to monitor a dynamic database environment where changes are difficult to track and improve your SQL security posture. Vulnerability assessment is a scanning service built into Azure SQL Database. The service employs a knowledge base of rules that flag security vulnerabilities. It highlights deviations from best practices, such as misconfigurations, excessive permissions, and unprotected sensitive data. The rules are based on Microsoft's best practices and focus on the security issues that present the biggest risks to your database and its valuable data. They cover database-level issues and server-level security issues, like server firewall settings and server-level permissions. The results of the scan include actionable steps to resolve each issue and provide customized remediation scripts where applicable. You can customize an assessment report for your environment by setting an acceptable baseline for: Permission configurations Feature configurations Database settings SQL vulnerability assessment is an easy-to-configure service that can discover, track, and help you remediate potential database vulnerabilities. Use it to proactively improve your database security for: Azure SQL Database Azure SQL Managed Instance Azure Synapse Analytics Vulnerability assessment is part of Microsoft Defender for Azure SQL, which is a unified package for advanced SQL security capabilities. Vulnerability assessment can be accessed and managed from each SQL database resource in the Azure portal. SQL Advanced Threat Protection Advanced Threat Protection for Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics, SQL Server on Azure Virtual Machines and Azure Arc-enabled SQL Server detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases. Advanced Threat Protection is part of the Microsoft Defender for SQL offering, which is a unified package for advanced SQL security capabilities. Advanced Threat Protection can be accessed and managed via the central Microsoft Defender for SQL portal. Overview Advanced Threat Protection provides a new layer of security, which enables customers to detect and respond to potential threats as they occur by providing security alerts on anomalous activities. Users receive an alert upon suspicious database activities, potential vulnerabilities, and SQL injection attacks, as well as anomalous database access and queries patterns. Advanced Threat Protection integrates alerts with Microsoft Defender for Cloud, which include details of suspicious activity and recommend action on how to investigate and mitigate the threat. Advanced Threat Protection makes it simple to address potential threats to the database without the need to be a security expert or manage advanced security monitoring systems. For a full investigation experience, it is recommended to enable auditing, which writes database events to an audit log in your Azure storage account. Explore detection of a suspicious event You receive an email notification upon detection of anomalous database activities. The email provides information on the suspicious security event, including the nature of the anomalous activities, database name, server name, application name, and event time. In addition, the email provides information on possible causes and recommended actions to investigate and mitigate the potential threat to the database. Example: Email notification providing information on a suspicious security event. Click the View recent SQL alerts link in the email to launch the Azure portal and show the Microsoft Defender for Cloud alerts page, which provides an overview of active threats detected on the database. Click a specific alert to get additional details and actions for investigating this threat and remediating future threats. For example, SQL injection is one of the most common Web application security issues on the Internet that is used to attack data-driven applications. Attackers take advantage of application vulnerabilities to inject malicious SQL statements into application entry fields, breaching or modifying data in the database. For SQL Injection alerts, the alert's details include the vulnerable SQL statement that was exploited. Explore alerts in the Azure portal Advanced Threat Protection integrates its alerts with Microsoft Defender for Cloud. Live SQL Advanced Threat Protection tiles within the database and SQL Microsoft Defender for Cloud blades in the Azure portal track the status of active threats. Click the Advanced Threat Protection alert to launch the Microsoft Defender for Cloud alerts page and get an overview of active SQL threats detected on the database. SQL vulnerability assessment express and classic configurations What are the SQL vulnerability assessments express and classic configurations? You can configure vulnerability assessment for your SQL databases with either: Express configuration (preview) \u2013 The default procedure that lets you configure vulnerability assessment without dependency on external storage to store baseline and scan result data. Classic configuration \u2013 The legacy procedure that requires you to manage an Azure storage account to store baseline and scan result data. Configuration modes benefits and limitations comparison: Configure dynamic data masking SQL Database dynamic data masking (DDM) limits sensitive data exposure by masking it to non-privileged users. Dynamic data masking helps prevent unauthorized access to sensitive data by enabling customers to designate how much of the sensitive data to reveal with minimal impact on the application layer. It\u2019s a policy-based security feature that hides the sensitive data in the result set of a query over designated database fields, while the data in the database is not changed. For example, a service representative at a call center may identify callers by several digits of their credit card number, but those data items should not be fully exposed to the service representative. A masking rule can be defined that masks all but the last four digits of any credit card number in the result set of any query. As another example, an appropriate data mask can be defined to protect personal data, so that a developer can query production environments for troubleshooting purposes without violating compliance regulations. Dynamic data masking basics You set up a dynamic data masking policy in the Azure portal by selecting the dynamic data masking operation in your SQL Database configuration blade or settings blade. This feature cannot be set by using portal for Azure Synapse Dynamic data masking policy SQL users excluded from masking - A set of SQL users or Azure AD identities that get unmasked data in the SQL query results. Users with administrator privileges are always excluded from masking, and view the original data without any mask. Masking rules - A set of rules that define the designated fields to be masked and the masking function that is used. The designated fields can be defined using a database schema name, table name, and column name. Masking functions - A set of methods that control the exposure of data for different scenarios. Recommended fields to mask The DDM recommendations engine, flags certain fields from your database as potentially sensitive fields, which may be good candidates for masking. In the Dynamic Data Masking blade in the portal, you can review the recommended columns for your database. All you need to do is click Add Mask for one or more columns and then Save to apply a mask for these fields. Implement transparent data encryption Transparent data encryption (TDE) helps protect Azure SQL Database, Azure SQL Managed Instance, and Synapse SQL in Azure Synapse Analytics against the threat of malicious offline activity by encrypting data at rest. It performs real-time encryption and decryption of the database, associated backups, and transaction log files at rest without requiring changes to the application. By default, TDE is enabled for all newly deployed Azure SQL databases and needs to be manually enabled for older databases of Azure SQL Database, Azure SQL Managed Instance, or Azure Synapse. TDE performs real-time I/O encryption and decryption of the data at the page level. Each page is decrypted when it's read into memory and then encrypted before being written to disk. TDE encrypts the storage of an entire database by using a symmetric key called the Database Encryption Key (DEK). On database startup, the encrypted DEK is decrypted and then used for decryption and re-encryption of the database files in the SQL Server Database Engine process. DEK is protected by the TDE protector. TDE protector is either a service-managed certificate (service-managed transparent data encryption) or an asymmetric key stored in Azure Key Vault (customer-managed transparent data encryption). For Azure SQL Database and Azure Synapse, the TDE protector is set at the logical SQL server level and is inherited by all databases associated with that server. For Azure SQL Managed Instance (BYOK feature in preview), the TDE protector is set at the instance level and it is inherited by all encrypted databases on that instance. The term server refers both to server and instance throughout this document, unless stated differently. Service-managed transparent data encryption In Azure, the default setting for TDE is that the DEK is protected by a built-in server certificate. The built-in server certificate is unique for each server and the encryption algorithm used is AES 256. If a database is in a geo-replication relationship, both the primary and geo-secondary databases are protected by the primary database's parent server key. If two databases are connected to the same server, they also share the same built-in certificate. Microsoft automatically rotates these certificates in compliance with the internal security policy and the root key is protected by a Microsoft internal secret store. Customers can verify SQL Database compliance with internal security policies in independent third-party audit reports available on the Microsoft Trust Center. Microsoft also seamlessly moves and manages the keys as needed for geo-replication and restores. Customer-managed transparent data encryption - Bring Your Own Key Customer-managed TDE is also referred to as Bring Your Own Key (BYOK) support for TDE. In this scenario, the TDE Protector that encrypts the DEK is a customer-managed asymmetric key, which is stored in a customer-owned and managed Azure Key Vault (Azure's cloud-based external key management system) and never leaves the key vault. The TDE Protector can be generated by the key vault or transferred to the key vault from an on premises hardware security module (HSM) device. SQL Database needs to be granted permissions to the customer-owned key vault to decrypt and encrypt the DEK. If permissions of the logical SQL server to the key vault are revoked, a database will be inaccessible, and all data is encrypted With TDE with Azure Key Vault integration, users can control key management tasks including key rotations, key vault permissions, key backups, and enable auditing/reporting on all TDE protectors using Azure Key Vault functionality. Key Vault provides central key management, leverages tightly monitored HSMs, and enables separation of duties between management of keys and data to help meet compliance with security policies. Manage TDE in the Azure portal To configure TDE through the Azure portal, you must be connected as the Azure Owner, Contributor, or SQL Security Manager. You turn TDE on and off on the database level. To enable TDE on a database, go to the Azure portal and sign in with your Azure Administrator or Contributor account. Find the TDE settings under your user database. By default, service-managed transparent data encryption is used. A TDE certificate is automatically generated for the server that contains the database. For Azure SQL Managed Instance use T-SQL to turn TDE on and off on a database. Deploy always encrypted features Ensure that your database is always encrypted. SQL Database Always Encrypted Always Encrypted is a feature designed to protect sensitive data, such as credit card numbers or national/regional identification numbers (for example, U.S. social security numbers), stored in Azure SQL Database or SQL Server databases. Always Encrypted allows clients to encrypt sensitive data inside client applications and never reveal the encryption keys to the Database Engine (SQL Database or SQL Server). As a result, Always Encrypted provides a separation between users who own the data (and can view it) and users who manage the data (but should have no access). By ensuring on-premises database administrators, cloud database operators, or other high-privileged, but unauthorized users, cannot access the encrypted data. Always Encrypted enables customers to confidently store sensitive data outside of their direct control. So, Always Encrypted allows organizations to encrypt data at rest and in use for storage in Azure, to enable delegation of on-premises database administration to third parties, or to reduce security clearance requirements for their own DBA staff. Always Encrypted makes encryption transparent to applications. An Always Encrypted-enabled driver installed on the client computer achieves this by automatically encrypting and decrypting sensitive data in the client application. The driver encrypts the data in sensitive columns before passing the data to the Database Engine, and automatically rewrites queries so that the semantics to the application are preserved. Similarly, the driver transparently decrypts data, stored in encrypted database columns, contained in query results. Example usage scenarios Client On-Premises with Data in Azure A customer has an on-premises client application at their business location. The application operates on sensitive data stored in a database hosted in Azure (SQL Database or SQL Server running in a virtual machine on Microsoft Azure). The customer uses Always Encrypted and stores Always Encrypted keys in a trusted key store hosted on-premises, to ensure Microsoft cloud administrators have no access to sensitive data. Client and Data in Azure A customer has a client application, hosted in Microsoft Azure (for example, in a worker role or a web role), which operates on sensitive data stored in a database hosted in Azure (SQL Database or SQL Server running in a virtual machine on Microsoft Azure). Although Always Encrypted does not provide complete isolation of data from cloud administrators, as both the data and keys are exposed to cloud administrators of the platform hosting the client tier, the customer still benefits from reducing the security attack surface area (the data is always encrypted in the database). Always Encrypted Features The Database Engine never operates on plaintext data stored in encrypted columns, but it still supports some queries on encrypted data, depending on the encryption type for the column. Always Encrypted supports two types of encryption: randomized encryption and deterministic encryption. Deterministic encryption always generates the same encrypted value for any given plain text value. Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns. However, it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column, especially if there is a small set of possible encrypted values, such as True/False, or North/South/East/West region. Deterministic encryption must use a column collation with a binary2 sort order for character columns. Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns. Use deterministic encryption for columns that will be used as search or grouping parameters, for example a government ID number. Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables. Deploy an always encrypted implementation A proper always encrypting implementation is important to a secure database. Configuring Always Encrypted The initial setup of Always Encrypted in a database involves generating Always Encrypted keys, creating key metadata, configuring encryption properties of selected database columns, and/or encrypting data that may already exist in columns that need to be encrypted. Remember that some of these tasks aren't supported in Transact-SQL and require the use of client-side tools. As Always Encrypted keys and protected sensitive data are never revealed in plaintext to the server, the Database Engine can't be involved in key provisioning and perform data encryption or decryption operations. You can use SQL Server Management Studio (SSMS) or PowerShell to accomplish such tasks. Task SSMS PowerShell SQL Provisioning column master keys, column encryption keys and encrypted column encryption keys with their corresponding column master keys Yes Yes No Creating key metadata in the database Yes Yes Yes Creating new tables with encrypted columns Yes Yes Yes Encrypting existing data in selected database columns Yes Yes No When setting up encryption for a column, you specify the information about the encryption algorithm and cryptographic keys used to protect the data in the column. Always Encrypted uses two types of keys: column encryption keys and column master keys. A column encryption key is used to encrypt data in an encrypted column. A column master key is a key-protecting key that encrypts one or more column encryption keys. The Database Engine stores encryption configuration for each column in database metadata. Note, however, the Database Engine never stores or uses the keys of either type in plaintext. It only stores encrypted values of column encryption keys and the information about the location of column master keys, which are stored in external trusted key stores, such as Azure Key Vault, Windows Certificate Store on a client machine, or a hardware security module. To access data stored in an encrypted column in plaintext, an application must use an Always Encrypted enabled client driver. When an application issues a parameterized query, the driver transparently collaborates with the Database Engine to determine which parameters target encrypted columns and, thus, should be encrypted. For each parameter that needs to be encrypted, the driver obtains the information about the encryption algorithm and the encrypted value of the column encryption key for the column, the parameter targets, as well as the location of its corresponding column master key. Next, the driver contacts the key store, containing the column master key, in order to decrypt the encrypted column encryption key value and then, it uses the plaintext column encryption key to encrypt the parameter. The resultant plaintext column encryption key is cached to reduce the number of round trips to the key store on subsequent uses of the same column encryption key. The driver substitutes the plaintext values of the parameters targeting encrypted columns with their encrypted values, and it sends the query to the server for processing. The server computes the result set, and for any encrypted columns included in the result set, the driver attaches the encryption metadata for the column, including the information about the encryption algorithm and the corresponding keys. The driver first tries to find the plaintext column encryption key in the local cache, and only makes a round to the column master key if it can't find the key in the cache. Next, the driver decrypts the results and returns plaintext values to the application. A client driver interacts with a key store, containing a column master key, using a column master key store provider, which is a client-side software component that encapsulates a key store containing the column master key. Providers for common types of key stores are available in client-side driver libraries from Microsoft or as standalone downloads. You can also implement your own provider. Always Encrypted capabilities, including built-in column master key store providers vary by a driver library and its version. Perform Try-This exercises Note These demonstrations require an Azure SQL database with sample data. In Task 1, there are instructions to install the AdventureWorks sample database. Also, Task 3 requires SQL Server Management Studio. Task 1 - Azure SQL: Advanced Data Security and Auditing In this task, we will explore vulnerability assessments, data discovery and classification, and auditing. Install the AdventureWorks sample database In the Portal, search for and a select SQL databases. On the Basics tab, give your database a name, and create a new server. On the Additional settings tab, select Sample for Use existing data. Also, Enable advanced data security and Start free trial. Review & create, and then Create. Wait for the database to deploy. Review Vulnerability Assessments Navigate to your SQL database. Under Security select Microsoft Defender for Cloud. Select Vulnerability Assessment. Review vulnerability assessments and the risk levels. Click Scan. The scan doesn't need to be fully complete for results to show. Review the Findings. Click any Security Check to get more details. Review the Passed checks. Notice Export Scan Results and Scan History Review Data Discovery and Classification Return to the Security blade. Select Data Discovery & Classification. On the Classification tab, select Add classification. Schema name: SalesLT Table name: Customer Column name: Phone Information type: Contact Info Sensitivity label: Confidential When finished click Add classification. Click the blue bar columns with classification recommendations. Notice the data that has been recommended for classification. Select the data of interest and then click Accept selected recommendations. Save your changes. Review Auditing Return to your SQL database. Under Security select Auditing. Select On for auditing. Click Storage for the destination. Select on the Storage account for logs. Set Retention day to 45 days. Set storage access key to Primary. Save your changes. Discuss Server level auditing and when how it could be used. Task 2 - Azure SQL: Diagnostics Note This demonstration requires an Azure SQL database. In this task, we will review and configure SQL database diagnostics. In the Portal, search for and launch SQL databases. From the Overview blade, review the Compute utilization data graphic. Data is available for different time frames (1 hour, 24 hours, 7 days). Under Monitoring select Diagnostic settings. Click Add diagnostic setting. Give your setting a name. Under Destination details select Send to Log Analytics. Make a note of the Log Analytics workspace that will be used. Under Destination details select Archive to Storage Account. Select the Errors log. Select the Automatic tuning log. Select the Basic metric. Give each item a retention time of 45 days. Retention only applies to storage account. Save your diagnostic setting. In the Portal, search for and launch the Log Analytics workspace. Select the workspace that is being using for your database diagnostics. Under General select Usage and estimated costs. Click Data retention. Use the slider to show how to increase the data retention time. Discuss how additional charges can incur, depending on the pricing plan. Under General select Workspace summary. Click Add and then search the Marketplace for Azure SQL. This feature may be in Preview. Explain the benefits of using this product. Select and then create Azure SQL Analytics. It will take few minutes for the product to deploy. Click Go to resource once the deployment is completed. Click Azure SQL databases. Review the additional metrics that are provided by this product. You can drill into any graphic for additional details. Task 3 - Azure SQL: Azure AD Authentication Note This task requires an Azure SQL database that has not had Azure AD configured. This task also requires SQL Server Management Studio. In this task, we will configure Azure AD authentication. In the Portal. Navigate to your SQL database. On the Overview page, there's an Active Directory admin box that shows the current status, configured or not configured. Under Settings select Active Directory admin. Click Set admin. Search for and Select the new Active Directory admin. Remember this user you'll need in following steps. Be sure to Save your changes. In SQL Server Management Studio connect to the database server using your credentials. Select the SQL database you configured with a new Active Directory admin. Construct a query to create a new user. Insert the admin user and domain. For example, user@contoso.com Create user [user@contoso.com] from external provider; Run the query and ensure it completes successfully. In the Object Explorer navigate your database and Security and Users folder. Verify that the new admin user is shown. Connect to the new database with the new admin credentials. Verify that you can successfully access the database. Knowledge check Choose the best response for each of the questions. Then select Check your answers. An SQL database administrator has recently read about SQL injection attacks. They ask what can be done to minimize the risk of this type of attack. Implementing which of the following features will help protect the database? Advanced Threat Protection ( Ans ) Data Discovery and Classification Dynamic Data Masking An organization provides a Help Desk for its customers. Service representatives need to identify callers using the last four numbers of their credit card. There is a need to ensure the complete credit card number isn't fully exposed to the service representatives. Which of the following features should be implemented? Always Encrypted Data Classification Dynamic Data Masking ( Ans ) Auditors need to be assured that sensitive database data always remains encrypted at rest, in transit, and in use. To assure the auditors this is being done, which of the below features is configured? Always Encrypted ( Ans ) Disk Encryption Dynamic Data Masking An App Service web application uses an SQL database. Users need to authenticate to the database with their Azure AD credentials. Which of the following configuration tasks would enable this? Create an SQL Database Administrator Create an Azure AD Database Administrator Create users in each database ( Ans ) What type of firewall rules can be configured for an Azure SQL database? Datacenter-level firewall rules Server-level firewall rules ( Ans ) Table-level firewall rules","title":"Configure and manage SQL database security"},{"location":"Cloud/Gcp/gcp.html","text":"","title":"Gcp"},{"location":"Cloud/Ibm/ibm.html","text":"","title":"Ibm"},{"location":"Cloud/Oci/oci.html","text":"","title":"Oci"},{"location":"DevSecOps/devsecops.html","text":"","title":"Devsecops"},{"location":"DevSecOps/Containerimagescan/Containerimagescan.html","text":"","title":"Containerimagescan"},{"location":"DevSecOps/CyberArk/CyberArk.html","text":"","title":"CyberArk"},{"location":"DevSecOps/DAST/dast.html","text":"","title":"Dast"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault-instalation.html","text":"Hashicorp vault installation kubernetes minikube # Install the Vault Helm chart 1. Add the HashiCorp Helm repository. $ helm repo add hashicorp https://helm.releases.hashicorp.com Update all the repositories to ensure helm is aware of the latest versions. $ helm repo update To verify, search repositories for vault in charts. $ helm search repo hashicorp/vault Install the latest version of the Vault Helm chart with Integrated Storage. Create a file named helm-vault-raft-values.yml with the following contents: $ cat > helm-vault-raft-values.yml <<EOF server: affinity: \"\" ha: enabled: true raft: enabled: true EOF Run this command: $ helm install vault hashicorp/vault --values helm-vault-raft-values.yml NAME: vault LAST DEPLOYED: Sun Jul 16 20:52:39 2023 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing HashiCorp Vault! Now that you have deployed Vault, you should look over the docs on using Vault with Kubernetes available here: https://www.vaultproject.io/docs/ Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get manifest vault This creates three Vault server instances with an Integrated Storage (Raft) backend. Display all the pods within the default namespace. $ kubectl -n default get pod,svc NAME READY STATUS RESTARTS AGE pod/vault-0 0/1 Running 0 116s pod/vault-1 0/1 Running 0 116s pod/vault-2 0/1 Running 0 116s pod/vault-agent-injector-86bbf55bd8-ks72t 1/1 Running 0 117s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 20h service/vault ClusterIP 10.106.90.5 <none> 8200/TCP,8201/TCP 117s service/vault-active ClusterIP 10.102.35.129 <none> 8200/TCP,8201/TCP 117s service/vault-agent-injector-svc ClusterIP 10.103.217.175 <none> 443/TCP 117s service/vault-internal ClusterIP None <none> 8200/TCP,8201/TCP 117s service/vault-standby ClusterIP 10.102.232.168 <none> 8200/TCP,8201/TCP 117s Initialize vault-0 with one key share and one key threshold. $ kubectl exec vault-0 -- vault operator init \\ -key-shares=1 \\ -key-threshold=1 \\ -format=json > cluster-keys.json The operator init command generates a root key that it disassembles into key shares -key-shares=1 and then sets the number of key shares required to unseal Vault -key-threshold=1. These key shares are written to the output as unseal keys in JSON format -format=json. Here the output is redirected to a file named cluster-keys.json. Display the unseal key found in cluster-keys.json. $ jq -r \".unseal_keys_b64[]\" cluster-keys.json Note ``` Insecure operation Do not run an unsealed Vault in production with a single key share and a single key threshold. This approach is only used here to simplify the unsealing process for this demonstration. ``` Create a variable named VAULT_UNSEAL_KEY to capture the Vault unseal key. $ VAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json) After initialization, Vault is configured to know where and how to access the storage, but does not know how to decrypt any of it. Unsealing is the process of constructing the root key necessary to read the decryption key to decrypt the data, allowing access to the Vault. Unseal Vault running on the vault-0 pod. $ kubectl exec vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY Note ``` Insecure operation Providing the unseal key with the command writes the key to your shell's history. This approach is only used here to simplify the unsealing process for this demonstration. ``` The Vault server is initialized and unsealed. Join the vault-1 pod to the Raft cluster. $ kubectl exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 Key Value --- ----- Joined true Join the vault-2 pod to the Raft cluster. $ kubectl exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 Key Value --- ----- Joined true Use the unseal key from above to unseal vault-1. $ kubectl exec -ti vault-1 -- vault operator unseal $VAULT_UNSEAL_KEY Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 1 Threshold 1 Unseal Progress 0/1 Unseal Nonce n/a Version 1.14.0 Build Date 2023-06-19T11:40:23Z Storage Type raft HA Enabled true Use the unseal key from above to unseal vault-2. $ kubectl exec -ti vault-2 -- vault operator unseal $VAULT_UNSEAL_KEY Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 1 Threshold 1 Unseal Progress 0/1 Unseal Nonce n/a Version 1.14.0 Build Date 2023-06-19T11:40:23Z Storage Type raft HA Enabled true Expose the service $ minikube service list |-------------|--------------------------|--------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|--------------------------|--------------|-----| | default | kubernetes | No node port | | default | vault | No node port | | default | vault-active | No node port | | default | vault-agent-injector-svc | No node port | | default | vault-internal | No node port | | default | vault-standby | No node port | | kube-system | kube-dns | No node port | |-------------|--------------------------|--------------|-----| $ minikube service expose vault |-----------|-------|-------------|--------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|--------------| | default | vault | | No node port | |-----------|-------|-------------|--------------| \ud83d\ude3f service default/vault has no node port \ud83c\udfc3 Starting tunnel for service vault. |-----------|-------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|------------------------| | default | vault | | http://127.0.0.1:62499 | | | | | http://127.0.0.1:62500 | |-----------|-------|-------------|------------------------| [default vault http://127.0.0.1:62499 http://127.0.0.1:62500] \u2757 Because you are using a Docker driver on darwin, the terminal needs to be open to run it. Set a secret in Vault # Vault generated an initial root token when it was initialized. Display the root token found in cluster-keys.json. $ jq -r \".root_token\" cluster-keys.json Login to vault in CLI $ export VAULT_ADDR=\"http://127.0.0.1:62499\" $ vault login Token (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token xxxxxxxxxxxxxxxxxxxxxxxx token_accessor xxxxxxxxxxxxxxxxxxxxxxxx token_duration \u221e token_renewable false token_policies [\"root\"] identity_policies [] policies [\"root\"] Enable an instance of the kv-v2 secrets engine at the path secret. $ vault secrets enable -path=secret kv-v2 Success! Enabled the kv-v2 secrets engine at: secret/ Create a secret at path secret/webapp/config with a username and password. $ vault kv put secret/webapp/config username=\"static-user\" password=\"static-password\" ====== Secret Path ====== secret/data/webapp/config ======= Metadata ======= Key Value --- ----- created_time 2023-07-16T16:02:49.336297792Z custom_metadata <nil> deletion_time n/a destroyed false version 1 Verify that the secret is defined at the path secret/webapp/config. $ vault kv get secret/webapp/config ====== Secret Path ====== secret/data/webapp/config ======= Metadata ======= Key Value --- ----- created_time 2023-07-16T16:02:49.336297792Z custom_metadata <nil> deletion_time n/a destroyed false version 1 ====== Data ====== Key Value --- ----- password static-password username static-user ```` ## **Configure Kubernetes authentication** The initial root token is a privileged user that can perform any operation at any path. The web application only requires the ability to read secrets defined at a single path. This application should authenticate and be granted a token with limited access. We recommend that root tokens are used only for initial setup of an authentication method and policies. Afterwards they should be revoked. Vault provides a Kubernetes authentication method that enables clients to authenticate with a Kubernetes Service Account Token. 1. **Enable the Kubernetes authentication method.** $ vault auth enable kubernetes Success! Enabled kubernetes auth method at: kubernetes/ Vault accepts this service token from any client within the Kubernetes cluster. During authentication, Vault verifies that the service account token is valid by querying a configured Kubernetes endpoint. 3. **Configure the Kubernetes authentication method to use the location of the Kubernetes API.** For the best compatibility with recent Kubernetes versions, ensure you are using Vault v1.9.3 or greater. $ vault write auth/kubernetes/config kubernetes_host=\"http://127.0.0.1:62499\" Success! Data written to: auth/kubernetes/config The environment variable KUBERNETES_PORT_443_TCP_ADDR is defined and references the internal network address of the Kubernetes host. For a client to access the secret data defined, at secret/webapp/config, requires that the read capability be granted for the path secret/data/webapp/config. This is an example of a policy. A policy defines a set of capabilities. 4. **Write out the policy named webapp that enables the read capability for secrets at path secret/data/webapp/config.** $ vault policy write webapp - <<EOF path \"secret/data/webapp/config\" { capabilities = [\"read\"] } EOF Success! Uploaded policy: webapp Define an auth method role that uses the webapp policy. A role binds policies and environment parameters together to create a login for the web application. 5. **Create a Kubernetes authentication role, named webapp, that connects the Kubernetes service account name and webapp policy.** $ vault write auth/kubernetes/role/webapp \\ bound_service_account_names=vault \\ bound_service_account_namespaces=default \\ policies=webapp \\ ttl=24h Success! Data written to: auth/kubernetes/role/webapp The role connects the Kubernetes service account, vault, and namespace, default, with the Vault policy, webapp. The tokens returned after authentication are valid for 24 hours. ## **Launch a web application** We've created a web application, published it to DockerHub, and created a Kubernetes deployment that will run the application in your existing cluster. The example web application performs the single function of listening for HTTP requests. During a request it reads the Kubernetes service token, logs into Vault, and then requests the secret. 1. **Use your preferred text editor and review the contents of deployment-01-webapp.yml.** $ vi deployment-01-webapp.yml apiVersion: apps/v1 kind: Deployment metadata: name: webapp labels: app: webapp spec: replicas: 1 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: serviceAccountName: vault containers: - name: app image: hashieducation/simple-vault-client:latest imagePullPolicy: Always env: - name: VAULT_ADDR value: 'http://127.0.0.1:62499' - name: JWT_PATH value: '/var/run/secrets/kubernetes.io/serviceaccount/token' - name: SERVICE_PORT value: '8080' The web application deployment defines a list of environment variables. - **JWT_PATH** sets the path of the JSON web token (JWT) issued by Kubernetes. This token is used by the web application to authenticate with Vault. - **VAULT_ADDR** sets the address of the Vault service. The Helm chart defined a Kubernetes service named vault that forwards requests to its endpoints (i.e. The pods named vault-0, vault-1, and vault-2). - **SERVICE_PORT** sets the port that the service listens for incoming HTTP requests. 2. **Deploy the webapp in Kubernetes by applying the file deployment-01-webapp.yml.** $ kubectl apply --filename deployment-01-webapp.yml deployment.apps/webapp created The webapp runs as a pod within the default namespace. 3. **Get all the pods within the default namespace.** $ kubectl get pods NAME READY STATUS RESTARTS AGE vault-0 1/1 Running 0 72m vault-1 1/1 Running 0 72m vault-2 1/1 Running 0 72m vault-agent-injector-86bbf55bd8-ks72t 1/1 Running 0 72m webapp-b4b9578bb-9s6tq 1/1 Running 0 6m32s ``` The webapp pod is displayed here as the pod prefixed with webapp. The deployment of the service requires the retrieval of the web application container from Docker Hub. This displays the STATUS of ContainerCreating. The pod reports that it is not ready (0/1). Wait until the webapp pod is running and ready (1/1). The webapp pod runs an HTTP service that is listening on port 8080.","title":"Hashicorp vault instalation"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault-instalation.html#hashicorp-vault-installation-kubernetes-minikube","text":"Install the Vault Helm chart 1. Add the HashiCorp Helm repository. $ helm repo add hashicorp https://helm.releases.hashicorp.com Update all the repositories to ensure helm is aware of the latest versions. $ helm repo update To verify, search repositories for vault in charts. $ helm search repo hashicorp/vault Install the latest version of the Vault Helm chart with Integrated Storage. Create a file named helm-vault-raft-values.yml with the following contents: $ cat > helm-vault-raft-values.yml <<EOF server: affinity: \"\" ha: enabled: true raft: enabled: true EOF Run this command: $ helm install vault hashicorp/vault --values helm-vault-raft-values.yml NAME: vault LAST DEPLOYED: Sun Jul 16 20:52:39 2023 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing HashiCorp Vault! Now that you have deployed Vault, you should look over the docs on using Vault with Kubernetes available here: https://www.vaultproject.io/docs/ Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get manifest vault This creates three Vault server instances with an Integrated Storage (Raft) backend. Display all the pods within the default namespace. $ kubectl -n default get pod,svc NAME READY STATUS RESTARTS AGE pod/vault-0 0/1 Running 0 116s pod/vault-1 0/1 Running 0 116s pod/vault-2 0/1 Running 0 116s pod/vault-agent-injector-86bbf55bd8-ks72t 1/1 Running 0 117s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 20h service/vault ClusterIP 10.106.90.5 <none> 8200/TCP,8201/TCP 117s service/vault-active ClusterIP 10.102.35.129 <none> 8200/TCP,8201/TCP 117s service/vault-agent-injector-svc ClusterIP 10.103.217.175 <none> 443/TCP 117s service/vault-internal ClusterIP None <none> 8200/TCP,8201/TCP 117s service/vault-standby ClusterIP 10.102.232.168 <none> 8200/TCP,8201/TCP 117s Initialize vault-0 with one key share and one key threshold. $ kubectl exec vault-0 -- vault operator init \\ -key-shares=1 \\ -key-threshold=1 \\ -format=json > cluster-keys.json The operator init command generates a root key that it disassembles into key shares -key-shares=1 and then sets the number of key shares required to unseal Vault -key-threshold=1. These key shares are written to the output as unseal keys in JSON format -format=json. Here the output is redirected to a file named cluster-keys.json. Display the unseal key found in cluster-keys.json. $ jq -r \".unseal_keys_b64[]\" cluster-keys.json Note ``` Insecure operation Do not run an unsealed Vault in production with a single key share and a single key threshold. This approach is only used here to simplify the unsealing process for this demonstration. ``` Create a variable named VAULT_UNSEAL_KEY to capture the Vault unseal key. $ VAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json) After initialization, Vault is configured to know where and how to access the storage, but does not know how to decrypt any of it. Unsealing is the process of constructing the root key necessary to read the decryption key to decrypt the data, allowing access to the Vault. Unseal Vault running on the vault-0 pod. $ kubectl exec vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY Note ``` Insecure operation Providing the unseal key with the command writes the key to your shell's history. This approach is only used here to simplify the unsealing process for this demonstration. ``` The Vault server is initialized and unsealed. Join the vault-1 pod to the Raft cluster. $ kubectl exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 Key Value --- ----- Joined true Join the vault-2 pod to the Raft cluster. $ kubectl exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 Key Value --- ----- Joined true Use the unseal key from above to unseal vault-1. $ kubectl exec -ti vault-1 -- vault operator unseal $VAULT_UNSEAL_KEY Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 1 Threshold 1 Unseal Progress 0/1 Unseal Nonce n/a Version 1.14.0 Build Date 2023-06-19T11:40:23Z Storage Type raft HA Enabled true Use the unseal key from above to unseal vault-2. $ kubectl exec -ti vault-2 -- vault operator unseal $VAULT_UNSEAL_KEY Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 1 Threshold 1 Unseal Progress 0/1 Unseal Nonce n/a Version 1.14.0 Build Date 2023-06-19T11:40:23Z Storage Type raft HA Enabled true Expose the service $ minikube service list |-------------|--------------------------|--------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|--------------------------|--------------|-----| | default | kubernetes | No node port | | default | vault | No node port | | default | vault-active | No node port | | default | vault-agent-injector-svc | No node port | | default | vault-internal | No node port | | default | vault-standby | No node port | | kube-system | kube-dns | No node port | |-------------|--------------------------|--------------|-----| $ minikube service expose vault |-----------|-------|-------------|--------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|--------------| | default | vault | | No node port | |-----------|-------|-------------|--------------| \ud83d\ude3f service default/vault has no node port \ud83c\udfc3 Starting tunnel for service vault. |-----------|-------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|------------------------| | default | vault | | http://127.0.0.1:62499 | | | | | http://127.0.0.1:62500 | |-----------|-------|-------------|------------------------| [default vault http://127.0.0.1:62499 http://127.0.0.1:62500] \u2757 Because you are using a Docker driver on darwin, the terminal needs to be open to run it.","title":"Hashicorp vault installation kubernetes minikube"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault-instalation.html#set-a-secret-in-vault","text":"Vault generated an initial root token when it was initialized. Display the root token found in cluster-keys.json. $ jq -r \".root_token\" cluster-keys.json Login to vault in CLI $ export VAULT_ADDR=\"http://127.0.0.1:62499\" $ vault login Token (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token xxxxxxxxxxxxxxxxxxxxxxxx token_accessor xxxxxxxxxxxxxxxxxxxxxxxx token_duration \u221e token_renewable false token_policies [\"root\"] identity_policies [] policies [\"root\"] Enable an instance of the kv-v2 secrets engine at the path secret. $ vault secrets enable -path=secret kv-v2 Success! Enabled the kv-v2 secrets engine at: secret/ Create a secret at path secret/webapp/config with a username and password. $ vault kv put secret/webapp/config username=\"static-user\" password=\"static-password\" ====== Secret Path ====== secret/data/webapp/config ======= Metadata ======= Key Value --- ----- created_time 2023-07-16T16:02:49.336297792Z custom_metadata <nil> deletion_time n/a destroyed false version 1 Verify that the secret is defined at the path secret/webapp/config. $ vault kv get secret/webapp/config ====== Secret Path ====== secret/data/webapp/config ======= Metadata ======= Key Value --- ----- created_time 2023-07-16T16:02:49.336297792Z custom_metadata <nil> deletion_time n/a destroyed false version 1 ====== Data ====== Key Value --- ----- password static-password username static-user ```` ## **Configure Kubernetes authentication** The initial root token is a privileged user that can perform any operation at any path. The web application only requires the ability to read secrets defined at a single path. This application should authenticate and be granted a token with limited access. We recommend that root tokens are used only for initial setup of an authentication method and policies. Afterwards they should be revoked. Vault provides a Kubernetes authentication method that enables clients to authenticate with a Kubernetes Service Account Token. 1. **Enable the Kubernetes authentication method.** $ vault auth enable kubernetes Success! Enabled kubernetes auth method at: kubernetes/ Vault accepts this service token from any client within the Kubernetes cluster. During authentication, Vault verifies that the service account token is valid by querying a configured Kubernetes endpoint. 3. **Configure the Kubernetes authentication method to use the location of the Kubernetes API.** For the best compatibility with recent Kubernetes versions, ensure you are using Vault v1.9.3 or greater. $ vault write auth/kubernetes/config kubernetes_host=\"http://127.0.0.1:62499\" Success! Data written to: auth/kubernetes/config The environment variable KUBERNETES_PORT_443_TCP_ADDR is defined and references the internal network address of the Kubernetes host. For a client to access the secret data defined, at secret/webapp/config, requires that the read capability be granted for the path secret/data/webapp/config. This is an example of a policy. A policy defines a set of capabilities. 4. **Write out the policy named webapp that enables the read capability for secrets at path secret/data/webapp/config.** $ vault policy write webapp - <<EOF path \"secret/data/webapp/config\" { capabilities = [\"read\"] } EOF Success! Uploaded policy: webapp Define an auth method role that uses the webapp policy. A role binds policies and environment parameters together to create a login for the web application. 5. **Create a Kubernetes authentication role, named webapp, that connects the Kubernetes service account name and webapp policy.** $ vault write auth/kubernetes/role/webapp \\ bound_service_account_names=vault \\ bound_service_account_namespaces=default \\ policies=webapp \\ ttl=24h Success! Data written to: auth/kubernetes/role/webapp The role connects the Kubernetes service account, vault, and namespace, default, with the Vault policy, webapp. The tokens returned after authentication are valid for 24 hours. ## **Launch a web application** We've created a web application, published it to DockerHub, and created a Kubernetes deployment that will run the application in your existing cluster. The example web application performs the single function of listening for HTTP requests. During a request it reads the Kubernetes service token, logs into Vault, and then requests the secret. 1. **Use your preferred text editor and review the contents of deployment-01-webapp.yml.** $ vi deployment-01-webapp.yml apiVersion: apps/v1 kind: Deployment metadata: name: webapp labels: app: webapp spec: replicas: 1 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: serviceAccountName: vault containers: - name: app image: hashieducation/simple-vault-client:latest imagePullPolicy: Always env: - name: VAULT_ADDR value: 'http://127.0.0.1:62499' - name: JWT_PATH value: '/var/run/secrets/kubernetes.io/serviceaccount/token' - name: SERVICE_PORT value: '8080' The web application deployment defines a list of environment variables. - **JWT_PATH** sets the path of the JSON web token (JWT) issued by Kubernetes. This token is used by the web application to authenticate with Vault. - **VAULT_ADDR** sets the address of the Vault service. The Helm chart defined a Kubernetes service named vault that forwards requests to its endpoints (i.e. The pods named vault-0, vault-1, and vault-2). - **SERVICE_PORT** sets the port that the service listens for incoming HTTP requests. 2. **Deploy the webapp in Kubernetes by applying the file deployment-01-webapp.yml.** $ kubectl apply --filename deployment-01-webapp.yml deployment.apps/webapp created The webapp runs as a pod within the default namespace. 3. **Get all the pods within the default namespace.** $ kubectl get pods NAME READY STATUS RESTARTS AGE vault-0 1/1 Running 0 72m vault-1 1/1 Running 0 72m vault-2 1/1 Running 0 72m vault-agent-injector-86bbf55bd8-ks72t 1/1 Running 0 72m webapp-b4b9578bb-9s6tq 1/1 Running 0 6m32s ``` The webapp pod is displayed here as the pod prefixed with webapp. The deployment of the service requires the retrieval of the web application container from Docker Hub. This displays the STATUS of ContainerCreating. The pod reports that it is not ready (0/1). Wait until the webapp pod is running and ready (1/1). The webapp pod runs an HTTP service that is listening on port 8080.","title":"Set a secret in Vault"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html","text":"What is Vault? # HashiCorp Vault is an identity-based secrets and encryption management system.A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, and certificates. Vault provides encryption services that are gated by authentication and authorization methods. Using Vault\u2019s UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable. A modern system requires access to a multitude of secrets, including database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. It can be difficult to understand who is accessing which secrets, especially since this can be platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where Vault steps in. Vault validates and authorizes clients (users, machines, apps) before providing them access to secrets or stored sensitive data. How does Vault work? # Vault works primarily with tokens and a token is associated to the client's policy. Each policy is path-based and policy rules constrains the actions and accessibility to the paths for each client. With Vault, you can create tokens manually and assign them to your clients, or the clients can log in and obtain a token. The illustration below displays Vault's core workflow. The core Vault workflow consists of four stages: Authenticate: Authentication in Vault is the process by which a client supplies information that Vault uses to determine if they are who they say they are. Once the client is authenticated against an auth method, a token is generated and associated to a policy. Validation: Vault validates the client against third-party trusted sources, such as Github, LDAP, AppRole, and more. Authorize: A client is matched against the Vault security policy. This policy is a set of rules defining which API endpoints a client has access to with its Vault token. Policies provide a declarative way to grant or forbid access to certain paths and operations in Vault. Access: Vault grants access to secrets, keys, and encryption capabilities by issuing a token based on policies associated with the client\u2019s identity. The client can then use their Vault token for future operations. Why Vault? # Most enterprises today have credentials sprawled across their organizations. Passwords, API keys, and credentials are stored in plain text, app source code, config files, and other locations. Because these credentials live everywhere, the sprawl can make it difficult and daunting to really know who has access and authorization to what. Having credentials in plain text also increases the potential for malicious attacks, both by internal and external attackers. Vault was designed with these challenges in mind. Vault takes all of these credentials and centralizes them so that they are defined in one location, which reduces unwanted exposure to credentials. But Vault takes it a few steps further by making sure users, apps, and systems are authenticated and explicitly authorized to access resources, while also providing an audit trail that captures and preserves a history of clients' actions. The key features of Vault are: Secure Secret Storage: Arbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Vault can write to disk, Consul, and more. Dynamic Secrets: Vault can generate secrets on-demand for some systems, such as AWS or SQL databases. For example, when an application needs to access an S3 bucket, it asks Vault for credentials, and Vault will generate an AWS keypair with valid permissions on demand. After creating these dynamic secrets, Vault will also automatically revoke them after the lease is up. Data Encryption: Vault can encrypt and decrypt data without storing it. This allows security teams to define encryption parameters and developers to store encrypted data in a location such as a SQL database without having to design their own encryption methods. Leasing and Renewal: All secrets in Vault have a lease associated with them. At the end of the lease, Vault will automatically revoke that secret. Clients are able to renew leases via built-in renew APIs. Revocation: Vault has built-in support for secret revocation. Vault can revoke not only single secrets, but a tree of secrets, for example all secrets read by a specific user, or all secrets of a particular type. Revocation assists in key rolling as well as locking down systems in the case of an intrusion. What is HCP Vault? # HashiCorp Cloud Platform (HCP) Vault is a hosted version of Vault, which is operated by HashiCorp to allow organizations to get up and running quickly. HCP Vault uses the same binary as self-hosted Vault, which means you will have a consistent user experience. You can use the same Vault clients to communicate with HCP Vault as you use to communicate with a self-hosted Vault. Use Cases # General Secret Storage # As workloads become more and more ephemeral and short-lived, having long-lived static credentials pose a big security threat vector. What if credentials are accidentally leaked, or an employee leaves with their post it notes that contain the AWS access key, or someone checks their S3 access token to a public GH repo? With Vault, you can generate short-lived, just-in-time credentials that are automatically revoked when their time expires. This means users and security teams do not have to worry about manually revoking or changing these credentials. Static Secrets # Credentials can be long-lived and static, where they don't change or are changed infrequently. Vault can store these secrets bedhind its cryptographic barrier, and clients can request them to use in their applications. Dynamic Secrets # The key value with secrets storage is the ability to dynamically generate credentials. These credentials are created when clients need them. Vault can also manage the lifecycle of these credentials, including but not limited to, deleting them after a defined period of time. In addition to database credential management, Vault can manage your Active Directory accounts, SSH keys, PKI certificates and more. Dynamic Secrets: Database Secrets Engine # Vault can generate secrets on-demand for some systems. For example, when an app needs to access an Amazon S3 bucket, it asks Vault for AWS credentials. Vault will generate an AWS credential granting permissions to access the S3 bucket. In addition, Vault will automatically revoke this credential after the time-to-live (TTL) expires. Challenge # Data protection is a top priority, and database credential rotation is a critical part of any data protection initiative. Each role has a different set of permissions granted to access the database. When a system is attacked by hackers, continuous credential rotation becomes necessary and needs to be automated. Solution # Applications ask Vault for database credentials rather than setting them as environment variables. The administrator specifies the TTL of the database credentials to enforce its validity so that they are automatically revoked when they are no longer used. Personas # Involves two personas: admin with privileged permissions to configure secrets engines apps read the secrets from Vault Policy requirements # Each persona requires a different set of capabilities. These are expressed in policies. If you are not familiar with policies. The admin tasks require these capabilities. # Mount secrets engines path \"sys/mounts/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\" ] } # Configure the database secrets engine and create roles path \"database/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\" ] } # Manage the leases path \"sys/leases/+/database/creds/readonly/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\" ] } path \"sys/leases/+/database/creds/readonly\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\" ] } # Write ACL policies path \"sys/policies/acl/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\" ] } # Manage tokens for verification path \"auth/token/create\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\" ] } The apps tasks require these capabilities. # Get credentials from the database secrets engine 'readonly' role. path \"database/creds/readonly\" { capabilities = [ \"read\" ] } Data Encryption # Many organizations seek solutions to encrypt/decrypt application data within a cloud or multi-datacenter environment; deploying cryptography and maintaining a complex key management infrastructure can be expensive and challenging to develop. Vault provides encryption as a service with centralized key management to simplify encrypting data in transit and stored across clouds and datacenters. Vault can encrypt/decrypt data stored elsewhere, essentially allowing applications to encrypt their data while storing it in the primary data store. Vault's security team manages and maintains the responsibility of the data encryption within the Vault environment, allowing developers to focus solely on encrypting/decrypting data as needed. Resources # Try Encryption as a Service: Transit Secrets Engine to learn the essential workings of the Transit secrets engine handles cryptographic functions on data in-transit. https://developer.hashicorp.com/vault/tutorials/encryption-as-a-service For more advanced data protection, refer to the Advanced Data Protection tutorial series. Vault's Transform secrets engine handles secure data transformation and tokenization against provided input value. https://developer.hashicorp.com/vault/tutorials/adp Identity-Based Access # Organizations need a way to manage identity sprawl with the proliferation of different clouds, services, and systems- all with their identity providers. The risk of compromising an organization's security infrastructure increases as organizations are forced to manage multiple identity management systems as they try to implement solutions to unify a single logical identity across numerous cloud platforms. Different platforms support different methods and constructs for identity, making it difficult to recognize a user or identity across multiple forms of credentials. Vault solves this challenge by using a unified ACL system to broker access to systems and secrets and merges identities across providers. With identity-based access, organizations can leverage any trusted resource identity to regulate and manage system and application access, and authentication across various clouds, systems, and endpoints. Resources # Try Identity: Entities and Groups tutorial to learn how Vault's unified identity system works. https://developer.hashicorp.com/vault/tutorials/auth-methods/identity Follow the Policies tutorial series to learn how Vault enforces role-based access control (RBAC) across multiple cloud environments. https://developer.hashicorp.com/vault/tutorials/policies Key Management # Working with cloud providers requires that you use their security features, which involve encryption keys issued and stored by the provider in its own key management system (KMS). You may also have a requirement to maintain root of trust and control of the encryption key lifecycle, both within and outside of the cloud. The Vault Key Management Secrets Engine provides a consistent workflow for distribution and lifecycle management of cloud provider keys, allowing organizations to maintain centralized control of their keys in Vault while leveraging the cryptographic capabilities native to the KMS providers. Resources # Try Key Management Secrets Engine with Azure Key Vault to enable management of the Key Vault key with the Key Management secrets engine. https://developer.hashicorp.com/vault/tutorials/adp/key-management-secrets-engine-azure-key-vault Try our Key Management Secrets Engine with GCP Cloud KMS to enable management of the Key Value key with the Key Management secrets engine. https://developer.hashicorp.com/vault/tutorials/adp/key-management-secrets-engine-azure-key-vault Vault Agent # A valid client token must accompany most requests to Vault. This includes all API requests, as well as via the Vault CLI and other libraries. Therefore, Vault clients must first authenticate with Vault to acquire a token. Vault provides several different authentication methods to assist in delivering this initial token. If the client can securely acquire the token, all subsequent requests (e.g., request database credentials, read key/value secrets) are processed based on the trust established by a successful authentication. This means that client application must invoke the Vault API to authenticate with Vault and manage the acquired token, in addition to invoking the API to request secrets from Vault. This implies code changes to client applications along with additional testing and maintenance of the application. The following code example implements Vault API to authenticate with Vault through AppRole auth method, and then uses the returned client token to read secrets at kv-v2/data/creds. package main import ( ...snip... vault \"github.com/hashicorp/vault/api\" ) // Fetches a key-value secret (kv-v2) after authenticating via AppRole func getSecretWithAppRole() (string, error) { config := vault.DefaultConfig() client := vault.NewClient(config) wrappingToken := ioutil.ReadFile(\"path/to/wrapping-token\") unwrappedToken := client.Logical().Unwrap(strings.TrimSuffix(string(wrappingToken), \"\\n\")) secretID := unwrappedToken.Data[\"secret_id\"] roleID := os.Getenv(\"APPROLE_ROLE_ID\") params := map[string]interface{}{ \"role_id\": roleID, \"secret_id\": secretID, } resp := client.Logical().Write(\"auth/approle/login\", params) client.SetToken(resp.Auth.ClientToken) secret, err := client.Logical().Read(\"kv-v2/data/creds\") if err != nil { return \"\", fmt.Errorf(\"unable to read secret: %w\", err) } data := secret.Data[\"data\"].(map[string]interface{}) ...snip... } For some Vault deployments, making (and maintaining) these changes to applications may not be a problem, and may actually be preferred. This may be applied to scenarios where you have a small number of applications or you want to keep strict, customized control over how each application interacts with Vault. However, in other situations where you have a large number of applications, as in large enterprises, you may not have the resources or expertise to update and maintain the Vault integration code for every application. When third party applications are being deployed by the application, it is prohibited to add the Vault integration code. Vault Agent aims to remove this initial hurdle to adopt Vault by providing a more scalable and simpler way for applications to integrate with Vault. What is Vault Agent? # Vault Agent is a client daemon that provides the following features: Auto-Auth - Automatically authenticate to Vault and manage the token renewal process for locally-retrieved dynamic secrets. API Proxy - Allows Vault Agent to act as a proxy for Vault's API, optionally using (or forcing the use of) the Auto-Auth token. Caching - Allows client-side caching of responses containing newly created tokens and responses containing leased secrets generated off of these newly created tokens. The agent also manages the renewals of the cached tokens and leases. Windows Service - Allows running the Vault Agent as a Windows service. Templating - Allows rendering of user-supplied templates by Vault Agent, using the token generated by the Auto-Auth step. Auto-Auth # Vault Agent allows easy authentication to Vault in a wide variety of environments. Auto-Auth functionality takes place within an auto_auth configuration stanza. API Proxy # Vault Agent can act as an API proxy for Vault, allowing you to talk to Vault's API via a listener defined for Agent. It can be configured to optionally allow or force the automatic use of the Auto-Auth token for these requests. API Proxy functionality takes place within a defined listener, and its behaviour can be configured with an api_proxy stanza. Caching # Vault Agent allows client-side caching of responses containing newly created tokens and responses containing leased secrets generated off of these newly created tokens. Start Vault Agent # To run Vault Agent: Download the Vault binary where the client application runs (virtual machine, Kubernetes pod, etc.) Create a Vault Agent configuration file. (See the Example Configuration section for an example configuration.) Start a Vault Agent with the configuration file. Example: vault agent -config=/etc/vault/agent-config.hcl To get help, run: vault agent -h Example Configuration # An example configuration, with very contrived values, follows: pid_file = \"./pidfile\" vault { address = \"https://vault-fqdn:8200\" retry { num_retries = 5 } } auto_auth { method \"aws\" { mount_path = \"auth/aws-subaccount\" config = { type = \"iam\" role = \"foobar\" } } sink \"file\" { config = { path = \"/tmp/file-foo\" } } sink \"file\" { wrap_ttl = \"5m\" aad_env_var = \"TEST_AAD_ENV\" dh_type = \"curve25519\" dh_path = \"/tmp/file-foo-dhpath2\" config = { path = \"/tmp/file-bar\" } } } cache { // An empty cache stanza still enables caching } api_proxy { use_auto_auth_token = true } listener \"unix\" { address = \"/path/to/socket\" tls_disable = true agent_api { enable_quit = true } } listener \"tcp\" { address = \"127.0.0.1:8100\" tls_disable = true } template { source = \"/etc/vault/server.key.ctmpl\" destination = \"/etc/vault/server.key\" } template { source = \"/etc/vault/server.crt.ctmpl\" destination = \"/etc/vault/server.crt\" } Secrets Engines # Secrets engines are components which store, generate, or encrypt data. Secrets engines are incredibly flexible, so it is easiest to think about them in terms of their function. Secrets engines are provided some set of data, they take some action on that data, and they return a result. Some secrets engines simply store and read data - like encrypted Redis/Memcached. Other secrets engines connect to other services and generate dynamic credentials on demand. Other secrets engines provide encryption as a service, totp generation, certificates, and much more. Secrets engines are enabled at a path in Vault. When a request comes to Vault, the router automatically routes anything with the route prefix to the secrets engine. In this way, each secrets engine defines its own paths and properties. To the user, secrets engines behave similar to a virtual filesystem, supporting operations like read, write, and delete. Secrets Engines Lifecycle # Most secrets engines can be enabled, disabled, tuned, and moved via the CLI or API. Enable - This enables a secrets engine at a given path. With a few exceptions, secrets engines can be enabled at multiple paths. Each secrets engine is isolated to its path. By default, they are enabled at their \"type\" (e.g. \"aws\" enables at aws/). Case-sensitive: The path where you enable secrets engines is case-sensitive. For example, the KV secrets engine enabled at kv/ and KV/ are treated as two distinct instances of KV secrets engine. Disable - This disables an existing secrets engine. When a secrets engine is disabled, all of its secrets are revoked (if they support it), and all the data stored for that engine in the physical storage layer is deleted. Move - This moves the path for an existing secrets engine. This process revokes all secrets, since secret leases are tied to the path where they were created. The configuration data stored for the engine persists through the move. Tune - This tunes global configuration for the secrets engine such as the TTLs. Once a secrets engine is enabled, you can interact with it directly at its path according to its own API. Use vault path-help to determine the paths it responds to. Note that mount points cannot conflict with each other in Vault. There are two broad implications of this fact. The first is that you cannot have a mount which is prefixed with an existing mount. The second is that you cannot create a mount point that is named as a prefix of an existing mount. As an example, the mounts foo/bar and foo/baz can peacefully coexist with each other whereas foo and foo/baz cannot Barrier View # Secrets engines receive a barrier view to the configured Vault physical storage. This is a lot like a chroot. When a secrets engine is enabled, a random UUID is generated. This becomes the data root for that engine. Whenever that engine writes to the physical storage layer, it is prefixed with that UUID folder. Since the Vault storage layer doesn't support relative access (such as ../), this makes it impossible for an enabled secrets engine to access other data. This is an important security feature in Vault - even a malicious engine cannot access the data from any other engine. Active Directory Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/ad) AliCloud Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/alicloud) AWS Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/aws) Azure Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/azure) Consul Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/consul) Cubbyhole Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/cubbyhole) Databases: - Cassandra Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/cassandra) - Couchbase Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/couchbase) - Elasticsearch Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/elasticdb) - HANA Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/hanadb) - IBM Db2 (https://developer.hashicorp.com/vault/docs/secrets/databases/db2) - InfluxDB Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/influxdb) - MongoDB Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mongodb) - MongoDB Atlas Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mongodbatlas) - MSSQL Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mssql) - MySQL/MariaDB Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mysql-maria) - Oracle Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/oracle) - PostgreSQL Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/postgresql) - Redis Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/redis) - Redis ElastiCache Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/rediselasticache) - Redshift Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/redshift) - Snowflake Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/snowflake) - Custom Database Secrets Engines (https://developer.hashicorp.com/vault/docs/secrets/databases/custom) Google Cloud Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/gcp) Google Cloud KMS Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/gcpkms) Identity Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/identity) - Identity Tokens (https://developer.hashicorp.com/vault/docs/secrets/identity/identity-token) - OIDC Identity Provider (https://developer.hashicorp.com/vault/docs/secrets/identity/oidc-provider) Key Management Secrets Engine(ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/key-management) - Azure Key Vault (https://developer.hashicorp.com/vault/docs/secrets/key-management/azurekeyvault) - AWS KMS (https://developer.hashicorp.com/vault/docs/secrets/key-management/awskms) - GCP Cloud KMS (https://developer.hashicorp.com/vault/docs/secrets/key-management/gcpkms) KV Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/kv) - KV Secrets Engine - Version 1 (https://developer.hashicorp.com/vault/docs/secrets/kv/kv-v1) - KV Secrets Engine - Version 2 (https://developer.hashicorp.com/vault/docs/secrets/kv/kv-v2) KMIP Secrets Engine(ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/kmip) Kubernetes Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/kubernetes) MongoDB Atlas Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/mongodbatlas) Nomad Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/nomad) LDAP Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/ldap) PKI Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/pki) - PKI Secrets Engine - Setup and Usage (https://developer.hashicorp.com/vault/docs/secrets/pki/setup) - PKI Secrets Engine - Quick Start - Root CA Setup (https://developer.hashicorp.com/vault/docs/secrets/pki/quick-start-root-ca) - PKI Secrets Engine - Quick Start - Intermediate CA Setup (https://developer.hashicorp.com/vault/docs/secrets/pki/quick-start-intermediate-ca) - PKI Secrets Engine - Considerations (https://developer.hashicorp.com/vault/docs/secrets/pki/considerations) - PKI Secrets Engine - Rotation Primitives (https://developer.hashicorp.com/vault/docs/secrets/pki/rotation-primitives) RabbitMQ Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/rabbitmq) SSH Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/ssh) - Signed SSH Certificates (https://developer.hashicorp.com/vault/docs/secrets/ssh/signed-ssh-certificates) - One-Time SSH Passwords (https://developer.hashicorp.com/vault/docs/secrets/ssh/one-time-ssh-passwords) Terraform Cloud Secret Backend (https://developer.hashicorp.com/vault/docs/secrets/terraform) TOTP Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/totp) Transform Secrets Engine(ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/transform) - FF3-1 Tweak Usage Documentation (https://developer.hashicorp.com/vault/docs/secrets/transform/ff3-tweak-details) - Tokenization Transform (ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/transform/tokenization) Transit Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/transit) - Key Wrapping for Transit Key Import (https://developer.hashicorp.com/vault/docs/secrets/transit/key-wrapping-guide) Venafi Secrets Engine for HashiCorp Vault (https://developer.hashicorp.com/vault/docs/secrets/venafi) Auth Methods # AppRole Auth Method (https://developer.hashicorp.com/vault/docs/auth/approle) AliCloud Auth Method (https://developer.hashicorp.com/vault/docs/auth/alicloud) AWS Auth Method (https://developer.hashicorp.com/vault/docs/auth/aws) Azure Auth Method (https://developer.hashicorp.com/vault/docs/auth/azure) Cloud Foundry (CF) Auth Method (https://developer.hashicorp.com/vault/docs/auth/cf) GitHub Auth Method (https://developer.hashicorp.com/vault/docs/auth/github) Google Cloud Auth Method (https://developer.hashicorp.com/vault/docs/auth/gcp) JWT/OIDC Auth Method (https://developer.hashicorp.com/vault/docs/auth/jwt) - OIDC Provider Configuration (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers) - Auth0 (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/auth0) - Azure Active Directory (AAD) (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/azuread) - ForgeRock (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/forgerock) - Gitlab (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/gitlab) - Google (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/google) - Keycloak (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/keycloak) - Kubernetes (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/kubernetes) - Okta (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/okta) - SecureAuth (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/secureauth) Kerberos Auth Method (https://developer.hashicorp.com/vault/docs/auth/kerberos) Kubernetes Auth Method (https://developer.hashicorp.com/vault/docs/auth/kubernetes) LDAP Auth Method (https://developer.hashicorp.com/vault/docs/auth/ldap) Login MFA (https://developer.hashicorp.com/vault/docs/auth/login-mfa) OCI Auth Method (https://developer.hashicorp.com/vault/docs/auth/oci) Okta Auth Method (https://developer.hashicorp.com/vault/docs/auth/okta) RADIUS Auth Method (https://developer.hashicorp.com/vault/docs/auth/radius) TLS Certificates Auth Method (https://developer.hashicorp.com/vault/docs/auth/cert) Token Auth Method (https://developer.hashicorp.com/vault/docs/auth/token) Userpass Auth Method (https://developer.hashicorp.com/vault/docs/auth/userpass) Audit Devices # Audit devices are the components in Vault that collectively keep a detailed log of all requests and response to Vault. Because every operation with Vault is an API request/response, when using a single audit device, the audit log contains every authenticated interaction with Vault, including errors. https://developer.hashicorp.com/vault/docs/audit File Audit Device (https://developer.hashicorp.com/vault/docs/audit/file) Syslog Audit Device (https://developer.hashicorp.com/vault/docs/audit/syslog) Socket Audit Device (https://developer.hashicorp.com/vault/docs/audit/socket) Plugin System # Vault supports 3 types of plugins; auth methods, secret engines, and database plugins. This concept allows both built-in and external plugins to be treated like building blocks. Any plugin can exist at multiple different mount paths. Different versions of a plugin may be at each location, with each version differing from Vault's version. A plugin is uniquely identified by its type (one of secret, auth, or database), name (e.g. aws), and version (e.g v1.0.0). An empty version implies either the built-in plugin or the single unversioned plugin that can be registered. https://developer.hashicorp.com/vault/docs/upgrading/plugins#plugin-upgrade-procedure External Plugin Architecture (https://developer.hashicorp.com/vault/docs/plugins/plugin-architecture) Plugin Development (https://developer.hashicorp.com/vault/docs/plugins/plugin-development) Plugin Management (https://developer.hashicorp.com/vault/docs/plugins/plugin-management) Vault Interoperability Matrix # Vault integrates with various appliances, platforms and applications for different use cases. Below are two tables indicating the partner\u2019s product that has been verified to work with Vault for Auto Unsealing / HSM Support and External Key Management. Auto Unseal and HSM Support was developed to aid in reducing the operational complexity of keeping the unseal key secure. This feature delegates the responsibility of securing the unseal key from users to a trusted device or service. At startup Vault will connect to the device or service implementing the seal and ask it to decrypt the root key Vault read from storage. Vault centrally manages and automates encryption keys across environments allowing customers to control their own encryption keys used in third party services or products. https://developer.hashicorp.com/vault/docs/interoperability-matrix Monitor & Troubleshoot # Troubleshooting Vault (https://developer.hashicorp.com/vault/tutorials/monitoring/troubleshooting-vault) Troubleshooting Vault on Kubernetes (https://developer.hashicorp.com/vault/tutorials/monitoring/kubernetes-troubleshooting) Diagnose Server Issues (https://developer.hashicorp.com/vault/tutorials/monitoring/diagnose-startup-issues) Use hcdiag with Vault (https://developer.hashicorp.com/vault/tutorials/monitoring/hcdiag-with-vault) Monitoring Vault Replication (https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-replication) Vault Usage Metrics (https://developer.hashicorp.com/vault/tutorials/monitoring/usage-metrics) Monitor Telemetry & Audit Device Log Data (https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-telemetry-audit-splunk) Monitor Telemetry with Prometheus & Grafana (https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-telemetry-grafana-prometheus) Inspect Data in BoltDB (https://developer.hashicorp.com/vault/tutorials/monitoring/inspect-data-boltdb) Inspecting Data in Consul Storage (https://developer.hashicorp.com/vault/tutorials/monitoring/inspecting-data-consul) Inspect Data in Integrated Storage (https://developer.hashicorp.com/vault/tutorials/monitoring/inspect-data-integrated-storage) Blocked Audit Devices (https://developer.hashicorp.com/vault/tutorials/monitoring/blocked-audit-devices) Query audit device logs (https://developer.hashicorp.com/vault/tutorials/monitoring/query-audit-device-logs) Troubleshoot Irrevocable Leases (https://developer.hashicorp.com/vault/tutorials/monitoring/troubleshoot-irrevocable-leases) Vault Cluster Lost Quorum Recovery (https://developer.hashicorp.com/vault/tutorials/monitoring/raft-lost-quorum) Operate Vault in Recovery Mode (https://developer.hashicorp.com/vault/tutorials/monitoring/recovery-mode) Telemetry Metrics Reference (https://developer.hashicorp.com/vault/tutorials/monitoring/telemetry-metrics-reference) Monitoring Vault with Datadog (https://developer.hashicorp.com/vault/tutorials/monitoring/monitoring-vault-with-datadog) Audit device logs and incident response with Elasticsearch (https://developer.hashicorp.com/vault/tutorials/monitoring/audit-elastic-incident-response) App Integration # Secure Introduction of Vault Clients (https://developer.hashicorp.com/vault/tutorials/app-integration/secure-introduction) Use Consul Template and Envconsul with Vault (https://developer.hashicorp.com/vault/tutorials/app-integration/application-integration) AppRole With Terraform & Chef (https://developer.hashicorp.com/vault/tutorials/app-integration/approle-trusted-entities) Java Application Demo (https://developer.hashicorp.com/vault/tutorials/app-integration/eaas-spring-demo) Transit Secrets Re-wrapping (https://developer.hashicorp.com/vault/tutorials/app-integration/eaas-transit-rewrap) Encrypting data with Transform secrets engine (https://developer.hashicorp.com/vault/tutorials/app-integration/transform-code-example) Using HashiCorp Vault C# Client with .NET Core (https://developer.hashicorp.com/vault/tutorials/app-integration/dotnet-httpclient) Using HashiCorp Vault Agent with .NET Core ( https://developer.hashicorp.com/vault/tutorials/app-integration/dotnet-vault-agent) Build Your Own Plugins (https://developer.hashicorp.com/vault/tutorials/app-integration/plugin-backends) Vault GitHub Actions (https://developer.hashicorp.com/vault/tutorials/app-integration/github-actions) Vault AWS Lambda Extension (https://developer.hashicorp.com/vault/tutorials/app-integration/aws-lambda) Introduction to the Vault AWS Lambda Extension (https://developer.hashicorp.com/vault/tutorials/app-integration/intro-vault-aws-lambda-extension)","title":"Hashicorp vault"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#what-is-vault","text":"HashiCorp Vault is an identity-based secrets and encryption management system.A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, and certificates. Vault provides encryption services that are gated by authentication and authorization methods. Using Vault\u2019s UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable. A modern system requires access to a multitude of secrets, including database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. It can be difficult to understand who is accessing which secrets, especially since this can be platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where Vault steps in. Vault validates and authorizes clients (users, machines, apps) before providing them access to secrets or stored sensitive data.","title":"What is Vault?"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#how-does-vault-work","text":"Vault works primarily with tokens and a token is associated to the client's policy. Each policy is path-based and policy rules constrains the actions and accessibility to the paths for each client. With Vault, you can create tokens manually and assign them to your clients, or the clients can log in and obtain a token. The illustration below displays Vault's core workflow. The core Vault workflow consists of four stages: Authenticate: Authentication in Vault is the process by which a client supplies information that Vault uses to determine if they are who they say they are. Once the client is authenticated against an auth method, a token is generated and associated to a policy. Validation: Vault validates the client against third-party trusted sources, such as Github, LDAP, AppRole, and more. Authorize: A client is matched against the Vault security policy. This policy is a set of rules defining which API endpoints a client has access to with its Vault token. Policies provide a declarative way to grant or forbid access to certain paths and operations in Vault. Access: Vault grants access to secrets, keys, and encryption capabilities by issuing a token based on policies associated with the client\u2019s identity. The client can then use their Vault token for future operations.","title":"How does Vault work?"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#why-vault","text":"Most enterprises today have credentials sprawled across their organizations. Passwords, API keys, and credentials are stored in plain text, app source code, config files, and other locations. Because these credentials live everywhere, the sprawl can make it difficult and daunting to really know who has access and authorization to what. Having credentials in plain text also increases the potential for malicious attacks, both by internal and external attackers. Vault was designed with these challenges in mind. Vault takes all of these credentials and centralizes them so that they are defined in one location, which reduces unwanted exposure to credentials. But Vault takes it a few steps further by making sure users, apps, and systems are authenticated and explicitly authorized to access resources, while also providing an audit trail that captures and preserves a history of clients' actions. The key features of Vault are: Secure Secret Storage: Arbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Vault can write to disk, Consul, and more. Dynamic Secrets: Vault can generate secrets on-demand for some systems, such as AWS or SQL databases. For example, when an application needs to access an S3 bucket, it asks Vault for credentials, and Vault will generate an AWS keypair with valid permissions on demand. After creating these dynamic secrets, Vault will also automatically revoke them after the lease is up. Data Encryption: Vault can encrypt and decrypt data without storing it. This allows security teams to define encryption parameters and developers to store encrypted data in a location such as a SQL database without having to design their own encryption methods. Leasing and Renewal: All secrets in Vault have a lease associated with them. At the end of the lease, Vault will automatically revoke that secret. Clients are able to renew leases via built-in renew APIs. Revocation: Vault has built-in support for secret revocation. Vault can revoke not only single secrets, but a tree of secrets, for example all secrets read by a specific user, or all secrets of a particular type. Revocation assists in key rolling as well as locking down systems in the case of an intrusion.","title":"Why Vault?"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#what-is-hcp-vault","text":"HashiCorp Cloud Platform (HCP) Vault is a hosted version of Vault, which is operated by HashiCorp to allow organizations to get up and running quickly. HCP Vault uses the same binary as self-hosted Vault, which means you will have a consistent user experience. You can use the same Vault clients to communicate with HCP Vault as you use to communicate with a self-hosted Vault.","title":"What is HCP Vault?"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#use-cases","text":"","title":"Use Cases"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#general-secret-storage","text":"As workloads become more and more ephemeral and short-lived, having long-lived static credentials pose a big security threat vector. What if credentials are accidentally leaked, or an employee leaves with their post it notes that contain the AWS access key, or someone checks their S3 access token to a public GH repo? With Vault, you can generate short-lived, just-in-time credentials that are automatically revoked when their time expires. This means users and security teams do not have to worry about manually revoking or changing these credentials.","title":"General Secret Storage"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#static-secrets","text":"Credentials can be long-lived and static, where they don't change or are changed infrequently. Vault can store these secrets bedhind its cryptographic barrier, and clients can request them to use in their applications.","title":"Static Secrets"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#dynamic-secrets","text":"The key value with secrets storage is the ability to dynamically generate credentials. These credentials are created when clients need them. Vault can also manage the lifecycle of these credentials, including but not limited to, deleting them after a defined period of time. In addition to database credential management, Vault can manage your Active Directory accounts, SSH keys, PKI certificates and more.","title":"Dynamic Secrets"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#dynamic-secrets-database-secrets-engine","text":"Vault can generate secrets on-demand for some systems. For example, when an app needs to access an Amazon S3 bucket, it asks Vault for AWS credentials. Vault will generate an AWS credential granting permissions to access the S3 bucket. In addition, Vault will automatically revoke this credential after the time-to-live (TTL) expires.","title":"Dynamic Secrets: Database Secrets Engine"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#challenge","text":"Data protection is a top priority, and database credential rotation is a critical part of any data protection initiative. Each role has a different set of permissions granted to access the database. When a system is attacked by hackers, continuous credential rotation becomes necessary and needs to be automated.","title":"Challenge"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#solution","text":"Applications ask Vault for database credentials rather than setting them as environment variables. The administrator specifies the TTL of the database credentials to enforce its validity so that they are automatically revoked when they are no longer used.","title":"Solution"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#personas","text":"Involves two personas: admin with privileged permissions to configure secrets engines apps read the secrets from Vault","title":"Personas"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#policy-requirements","text":"Each persona requires a different set of capabilities. These are expressed in policies. If you are not familiar with policies. The admin tasks require these capabilities. # Mount secrets engines path \"sys/mounts/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\" ] } # Configure the database secrets engine and create roles path \"database/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\" ] } # Manage the leases path \"sys/leases/+/database/creds/readonly/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\" ] } path \"sys/leases/+/database/creds/readonly\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\" ] } # Write ACL policies path \"sys/policies/acl/*\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\" ] } # Manage tokens for verification path \"auth/token/create\" { capabilities = [ \"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\" ] } The apps tasks require these capabilities. # Get credentials from the database secrets engine 'readonly' role. path \"database/creds/readonly\" { capabilities = [ \"read\" ] }","title":"Policy requirements"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#data-encryption","text":"Many organizations seek solutions to encrypt/decrypt application data within a cloud or multi-datacenter environment; deploying cryptography and maintaining a complex key management infrastructure can be expensive and challenging to develop. Vault provides encryption as a service with centralized key management to simplify encrypting data in transit and stored across clouds and datacenters. Vault can encrypt/decrypt data stored elsewhere, essentially allowing applications to encrypt their data while storing it in the primary data store. Vault's security team manages and maintains the responsibility of the data encryption within the Vault environment, allowing developers to focus solely on encrypting/decrypting data as needed.","title":"Data Encryption"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#resources","text":"Try Encryption as a Service: Transit Secrets Engine to learn the essential workings of the Transit secrets engine handles cryptographic functions on data in-transit. https://developer.hashicorp.com/vault/tutorials/encryption-as-a-service For more advanced data protection, refer to the Advanced Data Protection tutorial series. Vault's Transform secrets engine handles secure data transformation and tokenization against provided input value. https://developer.hashicorp.com/vault/tutorials/adp","title":"Resources"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#identity-based-access","text":"Organizations need a way to manage identity sprawl with the proliferation of different clouds, services, and systems- all with their identity providers. The risk of compromising an organization's security infrastructure increases as organizations are forced to manage multiple identity management systems as they try to implement solutions to unify a single logical identity across numerous cloud platforms. Different platforms support different methods and constructs for identity, making it difficult to recognize a user or identity across multiple forms of credentials. Vault solves this challenge by using a unified ACL system to broker access to systems and secrets and merges identities across providers. With identity-based access, organizations can leverage any trusted resource identity to regulate and manage system and application access, and authentication across various clouds, systems, and endpoints.","title":"Identity-Based Access"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#resources_1","text":"Try Identity: Entities and Groups tutorial to learn how Vault's unified identity system works. https://developer.hashicorp.com/vault/tutorials/auth-methods/identity Follow the Policies tutorial series to learn how Vault enforces role-based access control (RBAC) across multiple cloud environments. https://developer.hashicorp.com/vault/tutorials/policies","title":"Resources"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#key-management","text":"Working with cloud providers requires that you use their security features, which involve encryption keys issued and stored by the provider in its own key management system (KMS). You may also have a requirement to maintain root of trust and control of the encryption key lifecycle, both within and outside of the cloud. The Vault Key Management Secrets Engine provides a consistent workflow for distribution and lifecycle management of cloud provider keys, allowing organizations to maintain centralized control of their keys in Vault while leveraging the cryptographic capabilities native to the KMS providers.","title":"Key Management"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#resources_2","text":"Try Key Management Secrets Engine with Azure Key Vault to enable management of the Key Vault key with the Key Management secrets engine. https://developer.hashicorp.com/vault/tutorials/adp/key-management-secrets-engine-azure-key-vault Try our Key Management Secrets Engine with GCP Cloud KMS to enable management of the Key Value key with the Key Management secrets engine. https://developer.hashicorp.com/vault/tutorials/adp/key-management-secrets-engine-azure-key-vault","title":"Resources"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#vault-agent","text":"A valid client token must accompany most requests to Vault. This includes all API requests, as well as via the Vault CLI and other libraries. Therefore, Vault clients must first authenticate with Vault to acquire a token. Vault provides several different authentication methods to assist in delivering this initial token. If the client can securely acquire the token, all subsequent requests (e.g., request database credentials, read key/value secrets) are processed based on the trust established by a successful authentication. This means that client application must invoke the Vault API to authenticate with Vault and manage the acquired token, in addition to invoking the API to request secrets from Vault. This implies code changes to client applications along with additional testing and maintenance of the application. The following code example implements Vault API to authenticate with Vault through AppRole auth method, and then uses the returned client token to read secrets at kv-v2/data/creds. package main import ( ...snip... vault \"github.com/hashicorp/vault/api\" ) // Fetches a key-value secret (kv-v2) after authenticating via AppRole func getSecretWithAppRole() (string, error) { config := vault.DefaultConfig() client := vault.NewClient(config) wrappingToken := ioutil.ReadFile(\"path/to/wrapping-token\") unwrappedToken := client.Logical().Unwrap(strings.TrimSuffix(string(wrappingToken), \"\\n\")) secretID := unwrappedToken.Data[\"secret_id\"] roleID := os.Getenv(\"APPROLE_ROLE_ID\") params := map[string]interface{}{ \"role_id\": roleID, \"secret_id\": secretID, } resp := client.Logical().Write(\"auth/approle/login\", params) client.SetToken(resp.Auth.ClientToken) secret, err := client.Logical().Read(\"kv-v2/data/creds\") if err != nil { return \"\", fmt.Errorf(\"unable to read secret: %w\", err) } data := secret.Data[\"data\"].(map[string]interface{}) ...snip... } For some Vault deployments, making (and maintaining) these changes to applications may not be a problem, and may actually be preferred. This may be applied to scenarios where you have a small number of applications or you want to keep strict, customized control over how each application interacts with Vault. However, in other situations where you have a large number of applications, as in large enterprises, you may not have the resources or expertise to update and maintain the Vault integration code for every application. When third party applications are being deployed by the application, it is prohibited to add the Vault integration code. Vault Agent aims to remove this initial hurdle to adopt Vault by providing a more scalable and simpler way for applications to integrate with Vault.","title":"Vault Agent"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#what-is-vault-agent","text":"Vault Agent is a client daemon that provides the following features: Auto-Auth - Automatically authenticate to Vault and manage the token renewal process for locally-retrieved dynamic secrets. API Proxy - Allows Vault Agent to act as a proxy for Vault's API, optionally using (or forcing the use of) the Auto-Auth token. Caching - Allows client-side caching of responses containing newly created tokens and responses containing leased secrets generated off of these newly created tokens. The agent also manages the renewals of the cached tokens and leases. Windows Service - Allows running the Vault Agent as a Windows service. Templating - Allows rendering of user-supplied templates by Vault Agent, using the token generated by the Auto-Auth step.","title":"What is Vault Agent?"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#auto-auth","text":"Vault Agent allows easy authentication to Vault in a wide variety of environments. Auto-Auth functionality takes place within an auto_auth configuration stanza.","title":"Auto-Auth"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#api-proxy","text":"Vault Agent can act as an API proxy for Vault, allowing you to talk to Vault's API via a listener defined for Agent. It can be configured to optionally allow or force the automatic use of the Auto-Auth token for these requests. API Proxy functionality takes place within a defined listener, and its behaviour can be configured with an api_proxy stanza.","title":"API Proxy"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#caching","text":"Vault Agent allows client-side caching of responses containing newly created tokens and responses containing leased secrets generated off of these newly created tokens.","title":"Caching"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#start-vault-agent","text":"To run Vault Agent: Download the Vault binary where the client application runs (virtual machine, Kubernetes pod, etc.) Create a Vault Agent configuration file. (See the Example Configuration section for an example configuration.) Start a Vault Agent with the configuration file. Example: vault agent -config=/etc/vault/agent-config.hcl To get help, run: vault agent -h","title":"Start Vault Agent"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#example-configuration","text":"An example configuration, with very contrived values, follows: pid_file = \"./pidfile\" vault { address = \"https://vault-fqdn:8200\" retry { num_retries = 5 } } auto_auth { method \"aws\" { mount_path = \"auth/aws-subaccount\" config = { type = \"iam\" role = \"foobar\" } } sink \"file\" { config = { path = \"/tmp/file-foo\" } } sink \"file\" { wrap_ttl = \"5m\" aad_env_var = \"TEST_AAD_ENV\" dh_type = \"curve25519\" dh_path = \"/tmp/file-foo-dhpath2\" config = { path = \"/tmp/file-bar\" } } } cache { // An empty cache stanza still enables caching } api_proxy { use_auto_auth_token = true } listener \"unix\" { address = \"/path/to/socket\" tls_disable = true agent_api { enable_quit = true } } listener \"tcp\" { address = \"127.0.0.1:8100\" tls_disable = true } template { source = \"/etc/vault/server.key.ctmpl\" destination = \"/etc/vault/server.key\" } template { source = \"/etc/vault/server.crt.ctmpl\" destination = \"/etc/vault/server.crt\" }","title":"Example Configuration"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#secrets-engines","text":"Secrets engines are components which store, generate, or encrypt data. Secrets engines are incredibly flexible, so it is easiest to think about them in terms of their function. Secrets engines are provided some set of data, they take some action on that data, and they return a result. Some secrets engines simply store and read data - like encrypted Redis/Memcached. Other secrets engines connect to other services and generate dynamic credentials on demand. Other secrets engines provide encryption as a service, totp generation, certificates, and much more. Secrets engines are enabled at a path in Vault. When a request comes to Vault, the router automatically routes anything with the route prefix to the secrets engine. In this way, each secrets engine defines its own paths and properties. To the user, secrets engines behave similar to a virtual filesystem, supporting operations like read, write, and delete.","title":"Secrets Engines"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#secrets-engines-lifecycle","text":"Most secrets engines can be enabled, disabled, tuned, and moved via the CLI or API. Enable - This enables a secrets engine at a given path. With a few exceptions, secrets engines can be enabled at multiple paths. Each secrets engine is isolated to its path. By default, they are enabled at their \"type\" (e.g. \"aws\" enables at aws/). Case-sensitive: The path where you enable secrets engines is case-sensitive. For example, the KV secrets engine enabled at kv/ and KV/ are treated as two distinct instances of KV secrets engine. Disable - This disables an existing secrets engine. When a secrets engine is disabled, all of its secrets are revoked (if they support it), and all the data stored for that engine in the physical storage layer is deleted. Move - This moves the path for an existing secrets engine. This process revokes all secrets, since secret leases are tied to the path where they were created. The configuration data stored for the engine persists through the move. Tune - This tunes global configuration for the secrets engine such as the TTLs. Once a secrets engine is enabled, you can interact with it directly at its path according to its own API. Use vault path-help to determine the paths it responds to. Note that mount points cannot conflict with each other in Vault. There are two broad implications of this fact. The first is that you cannot have a mount which is prefixed with an existing mount. The second is that you cannot create a mount point that is named as a prefix of an existing mount. As an example, the mounts foo/bar and foo/baz can peacefully coexist with each other whereas foo and foo/baz cannot","title":"Secrets Engines Lifecycle"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#barrier-view","text":"Secrets engines receive a barrier view to the configured Vault physical storage. This is a lot like a chroot. When a secrets engine is enabled, a random UUID is generated. This becomes the data root for that engine. Whenever that engine writes to the physical storage layer, it is prefixed with that UUID folder. Since the Vault storage layer doesn't support relative access (such as ../), this makes it impossible for an enabled secrets engine to access other data. This is an important security feature in Vault - even a malicious engine cannot access the data from any other engine. Active Directory Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/ad) AliCloud Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/alicloud) AWS Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/aws) Azure Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/azure) Consul Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/consul) Cubbyhole Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/cubbyhole) Databases: - Cassandra Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/cassandra) - Couchbase Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/couchbase) - Elasticsearch Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/elasticdb) - HANA Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/hanadb) - IBM Db2 (https://developer.hashicorp.com/vault/docs/secrets/databases/db2) - InfluxDB Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/influxdb) - MongoDB Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mongodb) - MongoDB Atlas Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mongodbatlas) - MSSQL Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mssql) - MySQL/MariaDB Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/mysql-maria) - Oracle Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/oracle) - PostgreSQL Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/postgresql) - Redis Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/redis) - Redis ElastiCache Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/rediselasticache) - Redshift Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/redshift) - Snowflake Database Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/databases/snowflake) - Custom Database Secrets Engines (https://developer.hashicorp.com/vault/docs/secrets/databases/custom) Google Cloud Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/gcp) Google Cloud KMS Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/gcpkms) Identity Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/identity) - Identity Tokens (https://developer.hashicorp.com/vault/docs/secrets/identity/identity-token) - OIDC Identity Provider (https://developer.hashicorp.com/vault/docs/secrets/identity/oidc-provider) Key Management Secrets Engine(ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/key-management) - Azure Key Vault (https://developer.hashicorp.com/vault/docs/secrets/key-management/azurekeyvault) - AWS KMS (https://developer.hashicorp.com/vault/docs/secrets/key-management/awskms) - GCP Cloud KMS (https://developer.hashicorp.com/vault/docs/secrets/key-management/gcpkms) KV Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/kv) - KV Secrets Engine - Version 1 (https://developer.hashicorp.com/vault/docs/secrets/kv/kv-v1) - KV Secrets Engine - Version 2 (https://developer.hashicorp.com/vault/docs/secrets/kv/kv-v2) KMIP Secrets Engine(ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/kmip) Kubernetes Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/kubernetes) MongoDB Atlas Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/mongodbatlas) Nomad Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/nomad) LDAP Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/ldap) PKI Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/pki) - PKI Secrets Engine - Setup and Usage (https://developer.hashicorp.com/vault/docs/secrets/pki/setup) - PKI Secrets Engine - Quick Start - Root CA Setup (https://developer.hashicorp.com/vault/docs/secrets/pki/quick-start-root-ca) - PKI Secrets Engine - Quick Start - Intermediate CA Setup (https://developer.hashicorp.com/vault/docs/secrets/pki/quick-start-intermediate-ca) - PKI Secrets Engine - Considerations (https://developer.hashicorp.com/vault/docs/secrets/pki/considerations) - PKI Secrets Engine - Rotation Primitives (https://developer.hashicorp.com/vault/docs/secrets/pki/rotation-primitives) RabbitMQ Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/rabbitmq) SSH Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/ssh) - Signed SSH Certificates (https://developer.hashicorp.com/vault/docs/secrets/ssh/signed-ssh-certificates) - One-Time SSH Passwords (https://developer.hashicorp.com/vault/docs/secrets/ssh/one-time-ssh-passwords) Terraform Cloud Secret Backend (https://developer.hashicorp.com/vault/docs/secrets/terraform) TOTP Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/totp) Transform Secrets Engine(ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/transform) - FF3-1 Tweak Usage Documentation (https://developer.hashicorp.com/vault/docs/secrets/transform/ff3-tweak-details) - Tokenization Transform (ENTERPRISE) (https://developer.hashicorp.com/vault/docs/secrets/transform/tokenization) Transit Secrets Engine (https://developer.hashicorp.com/vault/docs/secrets/transit) - Key Wrapping for Transit Key Import (https://developer.hashicorp.com/vault/docs/secrets/transit/key-wrapping-guide) Venafi Secrets Engine for HashiCorp Vault (https://developer.hashicorp.com/vault/docs/secrets/venafi)","title":"Barrier View"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#auth-methods","text":"AppRole Auth Method (https://developer.hashicorp.com/vault/docs/auth/approle) AliCloud Auth Method (https://developer.hashicorp.com/vault/docs/auth/alicloud) AWS Auth Method (https://developer.hashicorp.com/vault/docs/auth/aws) Azure Auth Method (https://developer.hashicorp.com/vault/docs/auth/azure) Cloud Foundry (CF) Auth Method (https://developer.hashicorp.com/vault/docs/auth/cf) GitHub Auth Method (https://developer.hashicorp.com/vault/docs/auth/github) Google Cloud Auth Method (https://developer.hashicorp.com/vault/docs/auth/gcp) JWT/OIDC Auth Method (https://developer.hashicorp.com/vault/docs/auth/jwt) - OIDC Provider Configuration (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers) - Auth0 (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/auth0) - Azure Active Directory (AAD) (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/azuread) - ForgeRock (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/forgerock) - Gitlab (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/gitlab) - Google (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/google) - Keycloak (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/keycloak) - Kubernetes (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/kubernetes) - Okta (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/okta) - SecureAuth (https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/secureauth) Kerberos Auth Method (https://developer.hashicorp.com/vault/docs/auth/kerberos) Kubernetes Auth Method (https://developer.hashicorp.com/vault/docs/auth/kubernetes) LDAP Auth Method (https://developer.hashicorp.com/vault/docs/auth/ldap) Login MFA (https://developer.hashicorp.com/vault/docs/auth/login-mfa) OCI Auth Method (https://developer.hashicorp.com/vault/docs/auth/oci) Okta Auth Method (https://developer.hashicorp.com/vault/docs/auth/okta) RADIUS Auth Method (https://developer.hashicorp.com/vault/docs/auth/radius) TLS Certificates Auth Method (https://developer.hashicorp.com/vault/docs/auth/cert) Token Auth Method (https://developer.hashicorp.com/vault/docs/auth/token) Userpass Auth Method (https://developer.hashicorp.com/vault/docs/auth/userpass)","title":"Auth Methods"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#audit-devices","text":"Audit devices are the components in Vault that collectively keep a detailed log of all requests and response to Vault. Because every operation with Vault is an API request/response, when using a single audit device, the audit log contains every authenticated interaction with Vault, including errors. https://developer.hashicorp.com/vault/docs/audit File Audit Device (https://developer.hashicorp.com/vault/docs/audit/file) Syslog Audit Device (https://developer.hashicorp.com/vault/docs/audit/syslog) Socket Audit Device (https://developer.hashicorp.com/vault/docs/audit/socket)","title":"Audit Devices"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#plugin-system","text":"Vault supports 3 types of plugins; auth methods, secret engines, and database plugins. This concept allows both built-in and external plugins to be treated like building blocks. Any plugin can exist at multiple different mount paths. Different versions of a plugin may be at each location, with each version differing from Vault's version. A plugin is uniquely identified by its type (one of secret, auth, or database), name (e.g. aws), and version (e.g v1.0.0). An empty version implies either the built-in plugin or the single unversioned plugin that can be registered. https://developer.hashicorp.com/vault/docs/upgrading/plugins#plugin-upgrade-procedure External Plugin Architecture (https://developer.hashicorp.com/vault/docs/plugins/plugin-architecture) Plugin Development (https://developer.hashicorp.com/vault/docs/plugins/plugin-development) Plugin Management (https://developer.hashicorp.com/vault/docs/plugins/plugin-management)","title":"Plugin System"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#vault-interoperability-matrix","text":"Vault integrates with various appliances, platforms and applications for different use cases. Below are two tables indicating the partner\u2019s product that has been verified to work with Vault for Auto Unsealing / HSM Support and External Key Management. Auto Unseal and HSM Support was developed to aid in reducing the operational complexity of keeping the unseal key secure. This feature delegates the responsibility of securing the unseal key from users to a trusted device or service. At startup Vault will connect to the device or service implementing the seal and ask it to decrypt the root key Vault read from storage. Vault centrally manages and automates encryption keys across environments allowing customers to control their own encryption keys used in third party services or products. https://developer.hashicorp.com/vault/docs/interoperability-matrix","title":"Vault Interoperability Matrix"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#monitor-troubleshoot","text":"Troubleshooting Vault (https://developer.hashicorp.com/vault/tutorials/monitoring/troubleshooting-vault) Troubleshooting Vault on Kubernetes (https://developer.hashicorp.com/vault/tutorials/monitoring/kubernetes-troubleshooting) Diagnose Server Issues (https://developer.hashicorp.com/vault/tutorials/monitoring/diagnose-startup-issues) Use hcdiag with Vault (https://developer.hashicorp.com/vault/tutorials/monitoring/hcdiag-with-vault) Monitoring Vault Replication (https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-replication) Vault Usage Metrics (https://developer.hashicorp.com/vault/tutorials/monitoring/usage-metrics) Monitor Telemetry & Audit Device Log Data (https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-telemetry-audit-splunk) Monitor Telemetry with Prometheus & Grafana (https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-telemetry-grafana-prometheus) Inspect Data in BoltDB (https://developer.hashicorp.com/vault/tutorials/monitoring/inspect-data-boltdb) Inspecting Data in Consul Storage (https://developer.hashicorp.com/vault/tutorials/monitoring/inspecting-data-consul) Inspect Data in Integrated Storage (https://developer.hashicorp.com/vault/tutorials/monitoring/inspect-data-integrated-storage) Blocked Audit Devices (https://developer.hashicorp.com/vault/tutorials/monitoring/blocked-audit-devices) Query audit device logs (https://developer.hashicorp.com/vault/tutorials/monitoring/query-audit-device-logs) Troubleshoot Irrevocable Leases (https://developer.hashicorp.com/vault/tutorials/monitoring/troubleshoot-irrevocable-leases) Vault Cluster Lost Quorum Recovery (https://developer.hashicorp.com/vault/tutorials/monitoring/raft-lost-quorum) Operate Vault in Recovery Mode (https://developer.hashicorp.com/vault/tutorials/monitoring/recovery-mode) Telemetry Metrics Reference (https://developer.hashicorp.com/vault/tutorials/monitoring/telemetry-metrics-reference) Monitoring Vault with Datadog (https://developer.hashicorp.com/vault/tutorials/monitoring/monitoring-vault-with-datadog) Audit device logs and incident response with Elasticsearch (https://developer.hashicorp.com/vault/tutorials/monitoring/audit-elastic-incident-response)","title":"Monitor &amp; Troubleshoot"},{"location":"DevSecOps/Hashicorp-vault/Hashicorp-vault.html#app-integration","text":"Secure Introduction of Vault Clients (https://developer.hashicorp.com/vault/tutorials/app-integration/secure-introduction) Use Consul Template and Envconsul with Vault (https://developer.hashicorp.com/vault/tutorials/app-integration/application-integration) AppRole With Terraform & Chef (https://developer.hashicorp.com/vault/tutorials/app-integration/approle-trusted-entities) Java Application Demo (https://developer.hashicorp.com/vault/tutorials/app-integration/eaas-spring-demo) Transit Secrets Re-wrapping (https://developer.hashicorp.com/vault/tutorials/app-integration/eaas-transit-rewrap) Encrypting data with Transform secrets engine (https://developer.hashicorp.com/vault/tutorials/app-integration/transform-code-example) Using HashiCorp Vault C# Client with .NET Core (https://developer.hashicorp.com/vault/tutorials/app-integration/dotnet-httpclient) Using HashiCorp Vault Agent with .NET Core ( https://developer.hashicorp.com/vault/tutorials/app-integration/dotnet-vault-agent) Build Your Own Plugins (https://developer.hashicorp.com/vault/tutorials/app-integration/plugin-backends) Vault GitHub Actions (https://developer.hashicorp.com/vault/tutorials/app-integration/github-actions) Vault AWS Lambda Extension (https://developer.hashicorp.com/vault/tutorials/app-integration/aws-lambda) Introduction to the Vault AWS Lambda Extension (https://developer.hashicorp.com/vault/tutorials/app-integration/intro-vault-aws-lambda-extension)","title":"App Integration"},{"location":"DevSecOps/Hashicorp-vault/hashicorp-vault-github-auth.html","text":"Hashicorp vault github auth # STEP 1: Install hasicorp vault in kubernetes minikube # $ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" already exists with the same configuration, skipping $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart $ helm install vault hashicorp/vault NAME: vault LAST DEPLOYED: Fri Jul 14 21:17:05 2023 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing HashiCorp Vault! Now that you have deployed Vault, you should look over the docs on using Vault with Kubernetes available here: https://www.vaultproject.io/docs/ Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get manifest vault $ kubectl get pod,svc -n default NAME READY STATUS RESTARTS AGE pod/vault-0 0/1 Running 0 35s pod/vault-agent-injector-7dcd577577-9559g 1/1 Running 0 36s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 108d service/vault ClusterIP 10.106.6.9 <none> 8200/TCP,8201/TCP 36s service/vault-agent-injector-svc ClusterIP 10.103.13.49 <none> 443/TCP 36s service/vault-internal ClusterIP None <none> 8200/TCP,8201/TCP 36s $ minikube service vault |-----------|-------|-------------|--------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|--------------| | default | vault | | No node port | |-----------|-------|-------------|--------------| \ud83d\ude3f service default/vault has no node port \ud83c\udfc3 Starting tunnel for service vault. |-----------|-------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|------------------------| | default | vault | | http://127.0.0.1:56029 | | | | | http://127.0.0.1:56030 | |-----------|-------|-------------|------------------------| [default vault http://127.0.0.1:56029 http://127.0.0.1:56030] \u2757 Because you are using a Docker driver on darwin, the terminal needs to be open to run it. Remove helm deployment $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION vault default 1 2023-07-14 21:23:11.745081 +0530 IST deployed vault-0.23.0 1.12.1 $ helm uninstall vault hashicorp/vault release \"vault\" uninstalled Access vault in Browser # $ vault login -method=github token=\"ghp_xxxxxxxxxxxxxxxxxxxxxx\" Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.xxxxxxxxxxxxxxxxxxxxx token_accessor xxxxxxxxxxxxxxxxxxxxxxxxx token_duration 768h token_renewable true token_policies [\"default\"] identity_policies [] policies [\"default\"] token_meta_org ganesh-document token_meta_username ganeshkinkar $ vault kv get -field=password github/dev1 test123","title":"Hashicorp vault github auth"},{"location":"DevSecOps/Hashicorp-vault/hashicorp-vault-github-auth.html#hashicorp-vault-github-auth","text":"","title":"Hashicorp vault github auth"},{"location":"DevSecOps/Hashicorp-vault/hashicorp-vault-github-auth.html#step-1-install-hasicorp-vault-in-kubernetes-minikube","text":"$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" already exists with the same configuration, skipping $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart $ helm install vault hashicorp/vault NAME: vault LAST DEPLOYED: Fri Jul 14 21:17:05 2023 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing HashiCorp Vault! Now that you have deployed Vault, you should look over the docs on using Vault with Kubernetes available here: https://www.vaultproject.io/docs/ Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get manifest vault $ kubectl get pod,svc -n default NAME READY STATUS RESTARTS AGE pod/vault-0 0/1 Running 0 35s pod/vault-agent-injector-7dcd577577-9559g 1/1 Running 0 36s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 108d service/vault ClusterIP 10.106.6.9 <none> 8200/TCP,8201/TCP 36s service/vault-agent-injector-svc ClusterIP 10.103.13.49 <none> 443/TCP 36s service/vault-internal ClusterIP None <none> 8200/TCP,8201/TCP 36s $ minikube service vault |-----------|-------|-------------|--------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|--------------| | default | vault | | No node port | |-----------|-------|-------------|--------------| \ud83d\ude3f service default/vault has no node port \ud83c\udfc3 Starting tunnel for service vault. |-----------|-------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------|-------------|------------------------| | default | vault | | http://127.0.0.1:56029 | | | | | http://127.0.0.1:56030 | |-----------|-------|-------------|------------------------| [default vault http://127.0.0.1:56029 http://127.0.0.1:56030] \u2757 Because you are using a Docker driver on darwin, the terminal needs to be open to run it. Remove helm deployment $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION vault default 1 2023-07-14 21:23:11.745081 +0530 IST deployed vault-0.23.0 1.12.1 $ helm uninstall vault hashicorp/vault release \"vault\" uninstalled","title":"STEP 1: Install hasicorp vault in kubernetes minikube"},{"location":"DevSecOps/Hashicorp-vault/hashicorp-vault-github-auth.html#access-vault-in-browser","text":"$ vault login -method=github token=\"ghp_xxxxxxxxxxxxxxxxxxxxxx\" Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.xxxxxxxxxxxxxxxxxxxxx token_accessor xxxxxxxxxxxxxxxxxxxxxxxxx token_duration 768h token_renewable true token_policies [\"default\"] identity_policies [] policies [\"default\"] token_meta_org ganesh-document token_meta_username ganeshkinkar $ vault kv get -field=password github/dev1 test123","title":"Access vault in Browser"},{"location":"DevSecOps/IaC/iac.html","text":"","title":"Iac"},{"location":"DevSecOps/OSSC/ossc.html","text":"","title":"Ossc"},{"location":"DevSecOps/SAST/SAST.html","text":"What is SAST? # Static Application Security Testing (SAST) tools are solutions that scan your application source code or binary and find vulnerabilities. It is known as White-box testing, and developers can use it within the IDE or integrate it into CI/CD pipelines. First SAST tools came into the market in 2002* and are part of every modern application development environment. It can help developers in real-time with the potential security issues in the code they are writing. How do SAST tools work? # Most of the SAST tools start the process by creating a common format ( AST ) irrespective of the language of your code. This way it will be easier/faster to query the source code and find security issues. After creating a model from your source code then SAST tools can start looking for known issues with the rule engine. It will include language-specific rules, relevant rules and custom rules that users can add to cover business-logic related issues. In semantic analysis , SAST tools will look for the usage of insecure code and even can detect indirect calls. Structural analysis will check language-specific secure coding violations and detect improper variables/functions/methods access modifier, dead code, insecure multithreading, and memory leaks. Control flow analysis validates the order of operations by checking sequence patterns. It can identify the dangerous sequence of actions, resource leaks, race conditions and Improper variable/object initializing before use. Data flow analysis is the most powerful technique, and It tracks the data flow from the taint source (attacker-controlled inputs) to the vulnerable sink. (exploitable code) It can identify Injections, buffer overflows, and format-string attacks. Configuration analysis checks the application's configuration files (XML, Web.config, properties files) and finds known security misconfigurations. How to integrate SAST tools into DevSecOps? # Integrating SAST tools into automated DevOps workflows, making it much faster to deliver secure software to your end-users. It will save a lot of time during vulnerability management / remediation, and your developers will get an immediate response from the SAST Tool with this proactive scanning approach.","title":"SAST"},{"location":"DevSecOps/SAST/SAST.html#what-is-sast","text":"Static Application Security Testing (SAST) tools are solutions that scan your application source code or binary and find vulnerabilities. It is known as White-box testing, and developers can use it within the IDE or integrate it into CI/CD pipelines. First SAST tools came into the market in 2002* and are part of every modern application development environment. It can help developers in real-time with the potential security issues in the code they are writing.","title":"What is SAST?"},{"location":"DevSecOps/SAST/SAST.html#how-do-sast-tools-work","text":"Most of the SAST tools start the process by creating a common format ( AST ) irrespective of the language of your code. This way it will be easier/faster to query the source code and find security issues. After creating a model from your source code then SAST tools can start looking for known issues with the rule engine. It will include language-specific rules, relevant rules and custom rules that users can add to cover business-logic related issues. In semantic analysis , SAST tools will look for the usage of insecure code and even can detect indirect calls. Structural analysis will check language-specific secure coding violations and detect improper variables/functions/methods access modifier, dead code, insecure multithreading, and memory leaks. Control flow analysis validates the order of operations by checking sequence patterns. It can identify the dangerous sequence of actions, resource leaks, race conditions and Improper variable/object initializing before use. Data flow analysis is the most powerful technique, and It tracks the data flow from the taint source (attacker-controlled inputs) to the vulnerable sink. (exploitable code) It can identify Injections, buffer overflows, and format-string attacks. Configuration analysis checks the application's configuration files (XML, Web.config, properties files) and finds known security misconfigurations.","title":"How do SAST tools work?"},{"location":"DevSecOps/SAST/SAST.html#how-to-integrate-sast-tools-into-devsecops","text":"Integrating SAST tools into automated DevOps workflows, making it much faster to deliver secure software to your end-users. It will save a lot of time during vulnerability management / remediation, and your developers will get an immediate response from the SAST Tool with this proactive scanning approach.","title":"How to integrate SAST tools into DevSecOps?"},{"location":"DevSecOps/SAST/Tools/Appknox.html","text":"","title":"Appknox"},{"location":"DevSecOps/SAST/Tools/Checkmarx.html","text":"","title":"Checkmarx"},{"location":"DevSecOps/SAST/Tools/CodeScan.html","text":"","title":"CodeScan"},{"location":"DevSecOps/SAST/Tools/Coverity.html","text":"","title":"Coverity"},{"location":"DevSecOps/SAST/Tools/DeepSource.html","text":"","title":"DeepSource"},{"location":"DevSecOps/SAST/Tools/Fortify-Static-Code-Analyzer.html","text":"","title":"Fortify Static Code Analyzer"},{"location":"DevSecOps/SAST/Tools/GitGuardian.html","text":"","title":"GitGuardian"},{"location":"DevSecOps/SAST/Tools/GitLab.html","text":"","title":"GitLab"},{"location":"DevSecOps/SAST/Tools/Github.html","text":"","title":"Github"},{"location":"DevSecOps/SAST/Tools/HCLAppScan.html","text":"","title":"HCLAppScan"},{"location":"DevSecOps/SAST/Tools/Kiuwan-Code-Security-Insights.html","text":"","title":"Kiuwan Code Security Insights"},{"location":"DevSecOps/SAST/Tools/Semgrep.html","text":"","title":"Semgrep"},{"location":"DevSecOps/SAST/Tools/SonarQube.html","text":"","title":"SonarQube"},{"location":"DevSecOps/SAST/Tools/snyk.html","text":"","title":"Snyk"},{"location":"DevSecOps/SCA/sca.html","text":"","title":"Sca"},{"location":"DevSecOps/ThreatModeling/ThreatModeling.html","text":"Download File","title":"ThreatModeling"},{"location":"Devops/devops.html","text":"","title":"Devops"},{"location":"Devops/tools/CD/ArgoCD.html","text":"","title":"ArgoCD"},{"location":"Devops/tools/CD/GitLab.html","text":"","title":"GitLab"},{"location":"Devops/tools/CD/GoCD.html","text":"","title":"GoCD"},{"location":"Devops/tools/CD/Jenkins.html","text":"","title":"Jenkins"},{"location":"Devops/tools/CD/Spinnaker.html","text":"","title":"Spinnaker"},{"location":"Devops/tools/CI/AWS-CodePipeline.html","text":"","title":"AWS CodePipeline"},{"location":"Devops/tools/CI/Azure-Pipelines.html","text":"","title":"Azure Pipelines"},{"location":"Devops/tools/CI/Bamboo.html","text":"","title":"Bamboo"},{"location":"Devops/tools/CI/Bitbucket.html","text":"","title":"Bitbucket"},{"location":"Devops/tools/CI/CircleCI.html","text":"","title":"CircleCI"},{"location":"Devops/tools/CI/Github-action.html","text":"","title":"Github action"},{"location":"Devops/tools/CI/Gitlab.html","text":"","title":"Gitlab"},{"location":"Devops/tools/CI/Jenkins.html","text":"","title":"Jenkins"},{"location":"Devops/tools/CI/TeamCity.html","text":"","title":"TeamCity"},{"location":"Devops/tools/CI/TravisCI.html","text":"","title":"TravisCI"},{"location":"Devops/tools/CQT/Sonarqube.html","text":"How to Install SonarQube on Ubuntu 20.04 LTS # Prerequisites: Ubuntu 20.04 LTS with minimum 2GB RAM and 1 CPU. PostgreSQL Version 9.3 or higher SSH access with sudo privileges Firewall Port: 9000 Hardware requirements: A small-scale (individual or small team) instance of the SonarQube server requires at least 2GB of RAM to run efficiently and 1GB of free RAM for the OS. If you are installing an instance for a large team or an enterprise, please consider the additional recommendations below. The amount of disk space you need will depend on how much code you analyze with SonarQube. SonarQube must be installed on hard drives that have excellent read & write performance. Most importantly, the \"data\" folder houses the Elasticsearch indices on which a huge amount of I/O will be done when the server is up and running. Read and write hard drive performance will therefore have a big impact on the overall SonarQube server performance. sudo sysctl -w vm.max_map_count=262144 sudo sysctl -w fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 To Increase the vm.max_map_count kernal ,file discriptor and ulimit permanently . Open the below config file and Insert the below value as shown below, sudo vi /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 4096 Enterprise hardware recommendations: For large teams or enterprise-scale installations of SonarQube, additional hardware is required. At the enterprise level, monitoring your SonarQube instance is essential and should guide further hardware upgrades as your instance grows. A starting configuration should include at least: 8 cores, to allow the main SonarQube platform to run with multiple compute engine workers 16GB of RAM For additional requirements and recommendations relating to database and Elasticsearch. Supported platforms: 1. Java: The SonarQube server requires Java version 17 and the SonarQube scanners require Java version 11 or 17. 2. Database: PostgreSQL version = 11,12,13,14,15 3. xxxxxx: xxxxx To Increase the vm.max_map_count kernal ,file discriptor and ulimit permanently . Open the below config file and Insert the below value as shown below, sudo vi /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 4096 OR If you are using systemd to manage the sonarqube services then add below value in sonarqube unit file under [service] section. [Service] ... LimitNOFILE=65536 LimitNPROC=4096 ... Before installing, Lets update and upgrade System Packages sudo apt-get update sudo apt-get upgrade Install wget and unzip package sudo apt-get install wget unzip -y Step #1: Install OpenJDK # Install OpenJDK and JRE 11 using following command, sudo apt-get install openjdk-11-jdk -y sudo apt-get install openjdk-11-jre -y SET Default JDK # sudo update-alternatives --config java Check JAVA Version: # java -version Step #2: Install and Setup PostgreSQL 10 Database For SonarQube # Add and download the PostgreSQL Repo sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - Install the PostgreSQL database Server by using following command, sudo apt-get -y install postgresql postgresql-contrib Start PostgreSQL Database server sudo systemctl start postgresql Enable it to start automatically at boot time. sudo systemctl enable postgresql Change the password for the default PostgreSQL user. sudo passwd postgres Switch to the postgres user. su - postgres Create a new user by typing: createuser sonar Switch to the PostgreSQL shell. psql Set a password for the newly created user for SonarQube database. ALTER USER sonar WITH ENCRYPTED password 'sonar'; Create a new database for PostgreSQL database by running: CREATE DATABASE sonarqube OWNER sonar; grant all privileges to sonar user on sonarqube Database. grant all privileges on DATABASE sonarqube to sonar; Exit from the psql shell: \\q Switch back to the sudo user by running the exit command. exit Step #3: How to Install SonarQube on Ubuntu 20.04 LTS # Download sonaqube installer files archieve To download latest version of visit SonarQube (https://www.sonarsource.com/products/sonarqube/downloads/) cd /tmp sudo https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.1.44547.zip Unzip the archeve setup to /opt directory sudo unzip sonarqube-8.9.1.zip -d /opt Move extracted setup to /opt/sonarqube directory sudo mv /opt/sonarqube-8.9.1 /opt/sonarqube Step #4: Configure SonarQube # We can\u2019t run Sonarqube as a root user , if you run using root user it stops automatically. We have found solution on this to create saparate group and user to run sonarqube. 1. Create Group and User: # Create a group as sonar sudo groupadd sonar Now add the user with directory access sudo useradd -c \"user to run SonarQube\" -d /opt/sonarqube -g sonar sonar sudo chown sonar:sonar /opt/sonarqube -R Open the SonarQube configuration file using your favorite text editor. sudo nano /opt/sonarqube/conf/sonar.properties Find the following lines. #sonar.jdbc.username= #sonar.jdbc.password= Uncomment and Type the PostgreSQL Database username and password which we have created in above steps and add the postgres connection string. vi /opt/sonarqube/conf/sonar.properties #-------------------------------------------------------------------------------------------------- # DATABASE # # IMPORTANT: # - The embedded H2 database is used by default. It is recommended for tests but not for # production use. Supported databases are Oracle, PostgreSQL and Microsoft SQLServer. # - Changes to database connection URL (sonar.jdbc.url) can affect SonarSource licensed products. # User credentials. # Permissions to create tables, indices and triggers must be granted to JDBC user. # The schema must be created first. sonar.jdbc.username=sonar sonar.jdbc.password=sonar sonar.jdbc.url=jdbc:postgresql://localhost:5432/sonarqube 2. Start SonarQube: # Now to start SonarQube we need to do following: Switch to sonar user sudo su sonar Move to the script directory cd /opt/sonarqube/bin/linux-x86-64/ Run the script to start SonarQube ./sonar.sh start We can also add this in service and can run as a service. 3. Check SonarQube Running Status: # To check if sonaqube is running enter below command, ./sonar.sh status 4. SonarQube Logs: # To check sonarqube logs, navigate to /opt/sonarqube/logs/sonar.log directory tail /opt/sonarqube/logs/sonar.log Step #5: Configure Systemd service # First stop the SonarQube service as we started manually using above steps Navigate to the SonarQube installed path cd /opt/sonarqube/bin/linux-x86-64/ Run the script to start SonarQube ./sonar.sh stop Create a systemd service file for SonarQube to run as System Startup. sudo vi /etc/systemd/system/sonar.service Add the below lines, [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target Save and close the file. Now stop the sonarqube script earlier we started to run using as daemon. Start the Sonarqube daemon by running: sudo systemctl start sonar Enable the SonarQube service to automatically at boot time System Startup. sudo systemctl enable sonar check if the sonarqube service is running, sudo systemctl status sonar Step #6: Access SonarQube # To access the SonarQube using browser type server IP followed by port 9000. http://server_IP:9000 OR http://localhost:9000 Login to SonarQube with default administrator username and password is admin. Ref: # https://www.fosstechnix.com/how-to-install-sonarqube-on-ubuntu-20-04/","title":"Sonarqube"},{"location":"Devops/tools/CQT/Sonarqube.html#how-to-install-sonarqube-on-ubuntu-2004-lts","text":"Prerequisites: Ubuntu 20.04 LTS with minimum 2GB RAM and 1 CPU. PostgreSQL Version 9.3 or higher SSH access with sudo privileges Firewall Port: 9000 Hardware requirements: A small-scale (individual or small team) instance of the SonarQube server requires at least 2GB of RAM to run efficiently and 1GB of free RAM for the OS. If you are installing an instance for a large team or an enterprise, please consider the additional recommendations below. The amount of disk space you need will depend on how much code you analyze with SonarQube. SonarQube must be installed on hard drives that have excellent read & write performance. Most importantly, the \"data\" folder houses the Elasticsearch indices on which a huge amount of I/O will be done when the server is up and running. Read and write hard drive performance will therefore have a big impact on the overall SonarQube server performance. sudo sysctl -w vm.max_map_count=262144 sudo sysctl -w fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 To Increase the vm.max_map_count kernal ,file discriptor and ulimit permanently . Open the below config file and Insert the below value as shown below, sudo vi /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 4096 Enterprise hardware recommendations: For large teams or enterprise-scale installations of SonarQube, additional hardware is required. At the enterprise level, monitoring your SonarQube instance is essential and should guide further hardware upgrades as your instance grows. A starting configuration should include at least: 8 cores, to allow the main SonarQube platform to run with multiple compute engine workers 16GB of RAM For additional requirements and recommendations relating to database and Elasticsearch. Supported platforms: 1. Java: The SonarQube server requires Java version 17 and the SonarQube scanners require Java version 11 or 17. 2. Database: PostgreSQL version = 11,12,13,14,15 3. xxxxxx: xxxxx To Increase the vm.max_map_count kernal ,file discriptor and ulimit permanently . Open the below config file and Insert the below value as shown below, sudo vi /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 4096 OR If you are using systemd to manage the sonarqube services then add below value in sonarqube unit file under [service] section. [Service] ... LimitNOFILE=65536 LimitNPROC=4096 ... Before installing, Lets update and upgrade System Packages sudo apt-get update sudo apt-get upgrade Install wget and unzip package sudo apt-get install wget unzip -y","title":"How to Install SonarQube on Ubuntu 20.04 LTS"},{"location":"Devops/tools/CQT/Sonarqube.html#step-1-install-openjdk","text":"Install OpenJDK and JRE 11 using following command, sudo apt-get install openjdk-11-jdk -y sudo apt-get install openjdk-11-jre -y","title":"Step #1: Install OpenJDK"},{"location":"Devops/tools/CQT/Sonarqube.html#set-default-jdk","text":"sudo update-alternatives --config java","title":"SET Default JDK"},{"location":"Devops/tools/CQT/Sonarqube.html#check-java-version","text":"java -version","title":"Check JAVA Version:"},{"location":"Devops/tools/CQT/Sonarqube.html#step-2-install-and-setup-postgresql-10-database-for-sonarqube","text":"Add and download the PostgreSQL Repo sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - Install the PostgreSQL database Server by using following command, sudo apt-get -y install postgresql postgresql-contrib Start PostgreSQL Database server sudo systemctl start postgresql Enable it to start automatically at boot time. sudo systemctl enable postgresql Change the password for the default PostgreSQL user. sudo passwd postgres Switch to the postgres user. su - postgres Create a new user by typing: createuser sonar Switch to the PostgreSQL shell. psql Set a password for the newly created user for SonarQube database. ALTER USER sonar WITH ENCRYPTED password 'sonar'; Create a new database for PostgreSQL database by running: CREATE DATABASE sonarqube OWNER sonar; grant all privileges to sonar user on sonarqube Database. grant all privileges on DATABASE sonarqube to sonar; Exit from the psql shell: \\q Switch back to the sudo user by running the exit command. exit","title":"Step #2: Install and Setup PostgreSQL 10 Database For SonarQube"},{"location":"Devops/tools/CQT/Sonarqube.html#step-3-how-to-install-sonarqube-on-ubuntu-2004-lts","text":"Download sonaqube installer files archieve To download latest version of visit SonarQube (https://www.sonarsource.com/products/sonarqube/downloads/) cd /tmp sudo https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.1.44547.zip Unzip the archeve setup to /opt directory sudo unzip sonarqube-8.9.1.zip -d /opt Move extracted setup to /opt/sonarqube directory sudo mv /opt/sonarqube-8.9.1 /opt/sonarqube","title":"Step #3: How to Install SonarQube on Ubuntu 20.04 LTS"},{"location":"Devops/tools/CQT/Sonarqube.html#step-4-configure-sonarqube","text":"We can\u2019t run Sonarqube as a root user , if you run using root user it stops automatically. We have found solution on this to create saparate group and user to run sonarqube.","title":"Step #4: Configure SonarQube"},{"location":"Devops/tools/CQT/Sonarqube.html#1-create-group-and-user","text":"Create a group as sonar sudo groupadd sonar Now add the user with directory access sudo useradd -c \"user to run SonarQube\" -d /opt/sonarqube -g sonar sonar sudo chown sonar:sonar /opt/sonarqube -R Open the SonarQube configuration file using your favorite text editor. sudo nano /opt/sonarqube/conf/sonar.properties Find the following lines. #sonar.jdbc.username= #sonar.jdbc.password= Uncomment and Type the PostgreSQL Database username and password which we have created in above steps and add the postgres connection string. vi /opt/sonarqube/conf/sonar.properties #-------------------------------------------------------------------------------------------------- # DATABASE # # IMPORTANT: # - The embedded H2 database is used by default. It is recommended for tests but not for # production use. Supported databases are Oracle, PostgreSQL and Microsoft SQLServer. # - Changes to database connection URL (sonar.jdbc.url) can affect SonarSource licensed products. # User credentials. # Permissions to create tables, indices and triggers must be granted to JDBC user. # The schema must be created first. sonar.jdbc.username=sonar sonar.jdbc.password=sonar sonar.jdbc.url=jdbc:postgresql://localhost:5432/sonarqube","title":"1. Create Group and User:"},{"location":"Devops/tools/CQT/Sonarqube.html#2-start-sonarqube","text":"Now to start SonarQube we need to do following: Switch to sonar user sudo su sonar Move to the script directory cd /opt/sonarqube/bin/linux-x86-64/ Run the script to start SonarQube ./sonar.sh start We can also add this in service and can run as a service.","title":"2. Start SonarQube:"},{"location":"Devops/tools/CQT/Sonarqube.html#3-check-sonarqube-running-status","text":"To check if sonaqube is running enter below command, ./sonar.sh status","title":"3. Check SonarQube Running Status:"},{"location":"Devops/tools/CQT/Sonarqube.html#4-sonarqube-logs","text":"To check sonarqube logs, navigate to /opt/sonarqube/logs/sonar.log directory tail /opt/sonarqube/logs/sonar.log","title":"4. SonarQube Logs:"},{"location":"Devops/tools/CQT/Sonarqube.html#step-5-configure-systemd-service","text":"First stop the SonarQube service as we started manually using above steps Navigate to the SonarQube installed path cd /opt/sonarqube/bin/linux-x86-64/ Run the script to start SonarQube ./sonar.sh stop Create a systemd service file for SonarQube to run as System Startup. sudo vi /etc/systemd/system/sonar.service Add the below lines, [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target Save and close the file. Now stop the sonarqube script earlier we started to run using as daemon. Start the Sonarqube daemon by running: sudo systemctl start sonar Enable the SonarQube service to automatically at boot time System Startup. sudo systemctl enable sonar check if the sonarqube service is running, sudo systemctl status sonar","title":"Step #5: Configure Systemd service"},{"location":"Devops/tools/CQT/Sonarqube.html#step-6-access-sonarqube","text":"To access the SonarQube using browser type server IP followed by port 9000. http://server_IP:9000 OR http://localhost:9000 Login to SonarQube with default administrator username and password is admin.","title":"Step #6: Access SonarQube"},{"location":"Devops/tools/CQT/Sonarqube.html#ref","text":"https://www.fosstechnix.com/how-to-install-sonarqube-on-ubuntu-20-04/","title":"Ref:"},{"location":"Devops/tools/CR/Jfrog-artifactory.html","text":"","title":"Jfrog artifactory"},{"location":"Devops/tools/Kubernetes/Kubernetes-Deployments.html","text":"Kubernetes - Deployments # Deployments are upgraded and higher version of replication controller. They manage the deployment of replica sets which is also an upgraded version of the replication controller. They have the capability to update the replica set and are also capable of rolling back to the previous version. They provide many updated features of matchLabels and selectors. We have got a new controller in the Kubernetes master called the deployment controller which makes it happen. It has the capability to change the deployment midway. Changing the Deployment Updating \u2212 The user can update the ongoing deployment before it is completed. In this, the existing deployment will be settled and new deployment will be created. Deleting \u2212 The user can pause/cancel the deployment by deleting it before it is completed. Recreating the same deployment will resume it. Rollback \u2212 We can roll back the deployment or the deployment in progress. The user can create or update the deployment by using DeploymentSpec.PodTemplateSpec = oldRC.PodTemplateSpec. Deployment Strategies Deployment strategies help in defining how the new RC should replace the existing RC. Recreate \u2212 This feature will kill all the existing RC and then bring up the new ones. This results in quick deployment however it will result in downtime when the old pods are down and the new pods have not come up. Rolling Update \u2212 This feature gradually brings down the old RC and brings up the new one. This results in slow deployment, however there is no deployment. At all times, few old pods and few new pods are available in this process. The configuration file of Deployment looks like this. apiVersion: extensions/v1beta1 --------------------->1 kind: Deployment --------------------------> 2 metadata: name: Tomcat-ReplicaSet spec: replicas: 3 template: metadata: lables: app: Tomcat-ReplicaSet tier: Backend spec: containers: - name: Tomcatimage: tomcat: 8.0 ports: - containerPort: 7474 In the above code, the only thing which is different from the replica set is we have defined the kind as deployment. Create Deployment $ kubectl create \u2013f Deployment.yaml -\u2013record deployment \"Deployment\" created Successfully. Fetch the Deployment $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVILABLE AGE Deployment 3 3 3 3 20s Check the Status of Deployment $ kubectl rollout status deployment/Deployment Updating the Deployment $ kubectl set image deployment/Deployment tomcat=tomcat:6.0 Rolling Back to Previous Deployment $ kubectl rollout undo deployment/Deployment \u2013to-revision=2","title":"Kubernetes Deployments"},{"location":"Devops/tools/Kubernetes/Kubernetes-Deployments.html#kubernetes-deployments","text":"Deployments are upgraded and higher version of replication controller. They manage the deployment of replica sets which is also an upgraded version of the replication controller. They have the capability to update the replica set and are also capable of rolling back to the previous version. They provide many updated features of matchLabels and selectors. We have got a new controller in the Kubernetes master called the deployment controller which makes it happen. It has the capability to change the deployment midway. Changing the Deployment Updating \u2212 The user can update the ongoing deployment before it is completed. In this, the existing deployment will be settled and new deployment will be created. Deleting \u2212 The user can pause/cancel the deployment by deleting it before it is completed. Recreating the same deployment will resume it. Rollback \u2212 We can roll back the deployment or the deployment in progress. The user can create or update the deployment by using DeploymentSpec.PodTemplateSpec = oldRC.PodTemplateSpec. Deployment Strategies Deployment strategies help in defining how the new RC should replace the existing RC. Recreate \u2212 This feature will kill all the existing RC and then bring up the new ones. This results in quick deployment however it will result in downtime when the old pods are down and the new pods have not come up. Rolling Update \u2212 This feature gradually brings down the old RC and brings up the new one. This results in slow deployment, however there is no deployment. At all times, few old pods and few new pods are available in this process. The configuration file of Deployment looks like this. apiVersion: extensions/v1beta1 --------------------->1 kind: Deployment --------------------------> 2 metadata: name: Tomcat-ReplicaSet spec: replicas: 3 template: metadata: lables: app: Tomcat-ReplicaSet tier: Backend spec: containers: - name: Tomcatimage: tomcat: 8.0 ports: - containerPort: 7474 In the above code, the only thing which is different from the replica set is we have defined the kind as deployment. Create Deployment $ kubectl create \u2013f Deployment.yaml -\u2013record deployment \"Deployment\" created Successfully. Fetch the Deployment $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVILABLE AGE Deployment 3 3 3 3 20s Check the Status of Deployment $ kubectl rollout status deployment/Deployment Updating the Deployment $ kubectl set image deployment/Deployment tomcat=tomcat:6.0 Rolling Back to Previous Deployment $ kubectl rollout undo deployment/Deployment \u2013to-revision=2","title":"Kubernetes - Deployments"},{"location":"Devops/tools/Kubernetes/Kubernetes-Images.html","text":"Kubernetes - Images # Kubernetes (Docker) images are the key building blocks of Containerized Infrastructure. As of now, we are only supporting Kubernetes to support Docker images. Each container in a pod has its Docker image running inside it. When we are configuring a pod, the image property in the configuration file has the same syntax as the Docker command does. The configuration file has a field to define the image name, which we are planning to pull from the registry. Following is the common configuration structure which will pull image from Docker registry and deploy in to Kubernetes container. apiVersion: v1 kind: pod metadata: name: Tesing_for_Image_pull -----------> 1 spec: containers: - name: neo4j-server ------------------------> 2 image: <Name of the Docker image>----------> 3 imagePullPolicy: Always ------------->4 command: [\"echo\", \"SUCCESS\"] -------------------> In the above code, we have defined \u2212 name: Tesing_for_Image_pull \u2212 This name is given to identify and check what is the name of the container that would get created after pulling the images from Docker registry. name: neo4j-server \u2212 This is the name given to the container that we are trying to create. Like we have given neo4j-server. image: Name of the Docker image \u2212 This is the name of the image which we are trying to pull from the Docker or internal registry of images. We need to define a complete registry path along with the image name that we are trying to pull. imagePullPolicy \u2212 Always - This image pull policy defines that whenever we run this file to create the container, it will pull the same name again. command: [\u201cecho\u201d, \u201cSUCCESS\u201d] \u2212 With this, when we create the container and if everything goes fine, it will display a message when we will access the container. In order to pull the image and create a container, we will run the following command. $ kubectl create \u2013f Tesing_for_Image_pull Once we fetch the log, we will get the output as successful. $ kubectl log Tesing_for_Image_pull","title":"Kubernetes Images"},{"location":"Devops/tools/Kubernetes/Kubernetes-Images.html#kubernetes-images","text":"Kubernetes (Docker) images are the key building blocks of Containerized Infrastructure. As of now, we are only supporting Kubernetes to support Docker images. Each container in a pod has its Docker image running inside it. When we are configuring a pod, the image property in the configuration file has the same syntax as the Docker command does. The configuration file has a field to define the image name, which we are planning to pull from the registry. Following is the common configuration structure which will pull image from Docker registry and deploy in to Kubernetes container. apiVersion: v1 kind: pod metadata: name: Tesing_for_Image_pull -----------> 1 spec: containers: - name: neo4j-server ------------------------> 2 image: <Name of the Docker image>----------> 3 imagePullPolicy: Always ------------->4 command: [\"echo\", \"SUCCESS\"] -------------------> In the above code, we have defined \u2212 name: Tesing_for_Image_pull \u2212 This name is given to identify and check what is the name of the container that would get created after pulling the images from Docker registry. name: neo4j-server \u2212 This is the name given to the container that we are trying to create. Like we have given neo4j-server. image: Name of the Docker image \u2212 This is the name of the image which we are trying to pull from the Docker or internal registry of images. We need to define a complete registry path along with the image name that we are trying to pull. imagePullPolicy \u2212 Always - This image pull policy defines that whenever we run this file to create the container, it will pull the same name again. command: [\u201cecho\u201d, \u201cSUCCESS\u201d] \u2212 With this, when we create the container and if everything goes fine, it will display a message when we will access the container. In order to pull the image and create a container, we will run the following command. $ kubectl create \u2013f Tesing_for_Image_pull Once we fetch the log, we will get the output as successful. $ kubectl log Tesing_for_Image_pull","title":"Kubernetes - Images"},{"location":"Devops/tools/Kubernetes/Kubernetes-Jobs.html","text":"Kubernetes - Jobs # The main function of a job is to create one or more pod and tracks about the success of pods. They ensure that the specified number of pods are completed successfully. When a specified number of successful run of pods is completed, then the job is considered complete. Creating a Job Use the following command to create a job \u2212 apiVersion: v1 kind: Job ------------------------> 1 metadata: name: py spec: template: metadata name: py -------> 2 spec: containers: - name: py ------------------------> 3 image: python----------> 4 command: [\"python\", \"SUCCESS\"] restartPocliy: Never --------> 5 In the above code, we have defined \u2212 kind: Job - We have defined the kind as Job which will tell kubectl that the yaml file being used is to create a job type pod. Name:py - This is the name of the template that we are using and the spec defines the template. name: py - we have given a name as py under container spec which helps to identify the Pod which is going to be created out of it. Image: python - the image which we are going to pull to create the container which will run inside the pod. restartPolicy: Never - This condition of image restart is given as never which means that if the container is killed or if it is false, then it will not restart itself. We will create the job using the following command with yaml which is saved with the name py.yaml. $ kubectl create \u2013f py.yaml The above command will create a job. If you want to check the status of a job, use the following command. $ kubectl describe jobs/py The above command will create a job. If you want to check the status of a job, use the following command. Scheduled Job Scheduled job in Kubernetes uses Cronetes, which takes Kubernetes job and launches them in Kubernetes cluster. Scheduling a job will run a pod at a specified point of time. A parodic job is created for it which invokes itself automatically. Note \u2212 The feature of a scheduled job is supported by version 1.4 and the betch/v2alpha 1 API is turned on by passing the \u2013runtime-config=batch/v2alpha1 while bringing up the API server. We will use the same yaml which we used to create the job and make it a scheduled job. apiVersion: v1 kind: Job metadata: name: py spec: schedule: h/30 * * * * ? -------------------> 1 template: metadata name: py spec: containers: - name: py image: python args: /bin/sh -------> 2 -c ps \u2013eaf ------------> 3 restartPocliy: OnFailure In the above code, we have defined \u2212 schedule: h/30 * * * * ? - To schedule the job to run in every 30 minutes. /bin/sh: This will enter in the container with /bin/sh ps \u2013eaf - Will run ps -eaf command on the machine and list all the running process inside a container. This scheduled job concept is useful when we are trying to build and run a set of tasks at a specified point of time and then complete the process.","title":"Kubernetes Jobs"},{"location":"Devops/tools/Kubernetes/Kubernetes-Jobs.html#kubernetes-jobs","text":"The main function of a job is to create one or more pod and tracks about the success of pods. They ensure that the specified number of pods are completed successfully. When a specified number of successful run of pods is completed, then the job is considered complete. Creating a Job Use the following command to create a job \u2212 apiVersion: v1 kind: Job ------------------------> 1 metadata: name: py spec: template: metadata name: py -------> 2 spec: containers: - name: py ------------------------> 3 image: python----------> 4 command: [\"python\", \"SUCCESS\"] restartPocliy: Never --------> 5 In the above code, we have defined \u2212 kind: Job - We have defined the kind as Job which will tell kubectl that the yaml file being used is to create a job type pod. Name:py - This is the name of the template that we are using and the spec defines the template. name: py - we have given a name as py under container spec which helps to identify the Pod which is going to be created out of it. Image: python - the image which we are going to pull to create the container which will run inside the pod. restartPolicy: Never - This condition of image restart is given as never which means that if the container is killed or if it is false, then it will not restart itself. We will create the job using the following command with yaml which is saved with the name py.yaml. $ kubectl create \u2013f py.yaml The above command will create a job. If you want to check the status of a job, use the following command. $ kubectl describe jobs/py The above command will create a job. If you want to check the status of a job, use the following command. Scheduled Job Scheduled job in Kubernetes uses Cronetes, which takes Kubernetes job and launches them in Kubernetes cluster. Scheduling a job will run a pod at a specified point of time. A parodic job is created for it which invokes itself automatically. Note \u2212 The feature of a scheduled job is supported by version 1.4 and the betch/v2alpha 1 API is turned on by passing the \u2013runtime-config=batch/v2alpha1 while bringing up the API server. We will use the same yaml which we used to create the job and make it a scheduled job. apiVersion: v1 kind: Job metadata: name: py spec: schedule: h/30 * * * * ? -------------------> 1 template: metadata name: py spec: containers: - name: py image: python args: /bin/sh -------> 2 -c ps \u2013eaf ------------> 3 restartPocliy: OnFailure In the above code, we have defined \u2212 schedule: h/30 * * * * ? - To schedule the job to run in every 30 minutes. /bin/sh: This will enter in the container with /bin/sh ps \u2013eaf - Will run ps -eaf command on the machine and list all the running process inside a container. This scheduled job concept is useful when we are trying to build and run a set of tasks at a specified point of time and then complete the process.","title":"Kubernetes - Jobs"},{"location":"Devops/tools/Kubernetes/Kubernetes-Labels-and-Selectors.html","text":"Kubernetes - Labels & Selectors # Labels Labels are key-value pairs which are attached to pods, replication controller and services. They are used as identifying attributes for objects such as pods and replication controller. They can be added to an object at creation time and can be added or modified at the run time. Selectors Labels do not provide uniqueness. In general, we can say many objects can carry the same labels. Labels selector are core grouping primitive in Kubernetes. They are used by the users to select a set of objects. Kubernetes API currently supports two type of selectors \u2212 Equality-based selectors Set-based selectors Equality-based Selectors They allow filtering by key and value. Matching objects should satisfy all the specified labels. Set-based Selectors Set-based selectors allow filtering of keys according to a set of values. apiVersion: v1 kind: Service metadata: name: sp-neo4j-standalone spec: ports: - port: 7474 name: neo4j type: NodePort selector: app: salesplatform ---------> 1 component: neo4j -----------> 2 In the above code, we are using the label selector as app: salesplatform and component as component: neo4j . Once we run the file using the kubectl command, it will create a service with the name sp-neo4j-standalone which will communicate on port 7474. The ype is NodePort with the new label selector as app: salesplatform and component: neo4j .","title":"Kubernetes Labels and Selectors"},{"location":"Devops/tools/Kubernetes/Kubernetes-Labels-and-Selectors.html#kubernetes-labels-selectors","text":"Labels Labels are key-value pairs which are attached to pods, replication controller and services. They are used as identifying attributes for objects such as pods and replication controller. They can be added to an object at creation time and can be added or modified at the run time. Selectors Labels do not provide uniqueness. In general, we can say many objects can carry the same labels. Labels selector are core grouping primitive in Kubernetes. They are used by the users to select a set of objects. Kubernetes API currently supports two type of selectors \u2212 Equality-based selectors Set-based selectors Equality-based Selectors They allow filtering by key and value. Matching objects should satisfy all the specified labels. Set-based Selectors Set-based selectors allow filtering of keys according to a set of values. apiVersion: v1 kind: Service metadata: name: sp-neo4j-standalone spec: ports: - port: 7474 name: neo4j type: NodePort selector: app: salesplatform ---------> 1 component: neo4j -----------> 2 In the above code, we are using the label selector as app: salesplatform and component as component: neo4j . Once we run the file using the kubectl command, it will create a service with the name sp-neo4j-standalone which will communicate on port 7474. The ype is NodePort with the new label selector as app: salesplatform and component: neo4j .","title":"Kubernetes - Labels &amp; Selectors"},{"location":"Devops/tools/Kubernetes/Kubernetes-Namespace.html","text":"Kubernetes - Namespace # Namespace provides an additional qualification to a resource name. This is helpful when multiple teams are using the same cluster and there is a potential of name collision. It can be as a virtual wall between multiple clusters. Functionality of Namespace Following are some of the important functionalities of a Namespace in Kubernetes \u2212 Namespaces help pod-to-pod communication using the same namespace. Namespaces are virtual clusters that can sit on top of the same physical cluster. They provide logical separation between the teams and their environments. Create a Namespace The following command is used to create a namespace. apiVersion: v1 kind: Namespce metadata name: elk Control the Namespace The following command is used to control the namespace. $ kubectl create \u2013f namespace.yml ---------> 1 $ kubectl get namespace -----------------> 2 $ kubectl get namespace <Namespace name> ------->3 $ kubectl describe namespace <Namespace name> ---->4 $ kubectl delete namespace <Namespace name> In the above code, We are using the command to create a namespace. This will list all the available namespace. This will get a particular namespace whose name is specified in the command. This will describe the complete details about the service. This will delete a particular namespace present in the cluster. Using Namespace in Service - Example Following is an example of a sample file for using namespace in service. apiVersion: v1 kind: Service metadata: name: elasticsearch namespace: elk labels: component: elasticsearch spec: type: LoadBalancer selector: component: elasticsearch ports: - name: http port: 9200 protocol: TCP - name: transport port: 9300 protocol: TCP In the above code, we are using the same namespace under service metadata with the name of elk.","title":"Kubernetes Namespace"},{"location":"Devops/tools/Kubernetes/Kubernetes-Namespace.html#kubernetes-namespace","text":"Namespace provides an additional qualification to a resource name. This is helpful when multiple teams are using the same cluster and there is a potential of name collision. It can be as a virtual wall between multiple clusters. Functionality of Namespace Following are some of the important functionalities of a Namespace in Kubernetes \u2212 Namespaces help pod-to-pod communication using the same namespace. Namespaces are virtual clusters that can sit on top of the same physical cluster. They provide logical separation between the teams and their environments. Create a Namespace The following command is used to create a namespace. apiVersion: v1 kind: Namespce metadata name: elk Control the Namespace The following command is used to control the namespace. $ kubectl create \u2013f namespace.yml ---------> 1 $ kubectl get namespace -----------------> 2 $ kubectl get namespace <Namespace name> ------->3 $ kubectl describe namespace <Namespace name> ---->4 $ kubectl delete namespace <Namespace name> In the above code, We are using the command to create a namespace. This will list all the available namespace. This will get a particular namespace whose name is specified in the command. This will describe the complete details about the service. This will delete a particular namespace present in the cluster. Using Namespace in Service - Example Following is an example of a sample file for using namespace in service. apiVersion: v1 kind: Service metadata: name: elasticsearch namespace: elk labels: component: elasticsearch spec: type: LoadBalancer selector: component: elasticsearch ports: - name: http port: 9200 protocol: TCP - name: transport port: 9300 protocol: TCP In the above code, we are using the same namespace under service metadata with the name of elk.","title":"Kubernetes - Namespace"},{"location":"Devops/tools/Kubernetes/Kubernetes-Network-Policy.html","text":"Kubernetes - Network Policy # Network Policy defines how the pods in the same namespace will communicate with each other and the network endpoint. It requires extensions/v1beta1/networkpolicies to be enabled in the runtime configuration in the API server. Its resources use labels to select the pods and define rules to allow traffic to a specific pod in addition to which is defined in the namespace. First, we need to configure Namespace Isolation Policy. Basically, this kind of networking policies are required on the load balancers. kind: Namespace apiVersion: v1 metadata: annotations: net.beta.kubernetes.io/network-policy: | { \"ingress\": { \"isolation\": \"DefaultDeny\" } } $ kubectl annotate ns <namespace> \"net.beta.kubernetes.io/network-policy = {\\\"ingress\\\": {\\\"isolation\\\": \\\"DefaultDeny\\\"}}\" Once the namespace is created, we need to create the Network Policy. Network Policy Yaml kind: NetworkPolicy apiVersion: extensions/v1beta1 metadata: name: allow-frontend namespace: myns spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379","title":"Kubernetes Network Policy"},{"location":"Devops/tools/Kubernetes/Kubernetes-Network-Policy.html#kubernetes-network-policy","text":"Network Policy defines how the pods in the same namespace will communicate with each other and the network endpoint. It requires extensions/v1beta1/networkpolicies to be enabled in the runtime configuration in the API server. Its resources use labels to select the pods and define rules to allow traffic to a specific pod in addition to which is defined in the namespace. First, we need to configure Namespace Isolation Policy. Basically, this kind of networking policies are required on the load balancers. kind: Namespace apiVersion: v1 metadata: annotations: net.beta.kubernetes.io/network-policy: | { \"ingress\": { \"isolation\": \"DefaultDeny\" } } $ kubectl annotate ns <namespace> \"net.beta.kubernetes.io/network-policy = {\\\"ingress\\\": {\\\"isolation\\\": \\\"DefaultDeny\\\"}}\" Once the namespace is created, we need to create the Network Policy. Network Policy Yaml kind: NetworkPolicy apiVersion: extensions/v1beta1 metadata: name: allow-frontend namespace: myns spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379","title":"Kubernetes - Network Policy"},{"location":"Devops/tools/Kubernetes/Kubernetes-Node.html","text":"Kubernetes - Node # A node is a working machine in Kubernetes cluster which is also known as a minion. They are working units which can be physical, VM, or a cloud instance. Each node has all the required configuration required to run a pod on it such as the proxy service and kubelet service along with the Docker, which is used to run the Docker containers on the pod created on the node. They are not created by Kubernetes but they are created externally either by the cloud service provider or the Kubernetes cluster manager on physical or VM machines. The key component of Kubernetes to handle multiple nodes is the controller manager, which runs multiple kind of controllers to manage nodes. To manage nodes, Kubernetes creates an object of kind node which will validate that the object which is created is a valid node. Service with Selector apiVersion: v1 kind: node metadata: name: < ip address of the node> labels: name: <lable name> Node Controller They are the collection of services which run in the Kubernetes master and continuously monitor the node in the cluster on the basis of metadata.name. If all the required services are running, then the node is validated and a newly created pod will be assigned to that node by the controller. If it is not valid, then the master will not assign any pod to it and will wait until it becomes valid. Kubernetes master registers the node automatically, if \u2013register-node flag is true. \u2013register-node = true However, if the cluster administrator wants to manage it manually then it could be done by turning the flat of \u2212 \u2013register-node = false","title":"Kubernetes Node"},{"location":"Devops/tools/Kubernetes/Kubernetes-Node.html#kubernetes-node","text":"A node is a working machine in Kubernetes cluster which is also known as a minion. They are working units which can be physical, VM, or a cloud instance. Each node has all the required configuration required to run a pod on it such as the proxy service and kubelet service along with the Docker, which is used to run the Docker containers on the pod created on the node. They are not created by Kubernetes but they are created externally either by the cloud service provider or the Kubernetes cluster manager on physical or VM machines. The key component of Kubernetes to handle multiple nodes is the controller manager, which runs multiple kind of controllers to manage nodes. To manage nodes, Kubernetes creates an object of kind node which will validate that the object which is created is a valid node. Service with Selector apiVersion: v1 kind: node metadata: name: < ip address of the node> labels: name: <lable name> Node Controller They are the collection of services which run in the Kubernetes master and continuously monitor the node in the cluster on the basis of metadata.name. If all the required services are running, then the node is validated and a newly created pod will be assigned to that node by the controller. If it is not valid, then the master will not assign any pod to it and will wait until it becomes valid. Kubernetes master registers the node automatically, if \u2013register-node flag is true. \u2013register-node = true However, if the cluster administrator wants to manage it manually then it could be done by turning the flat of \u2212 \u2013register-node = false","title":"Kubernetes - Node"},{"location":"Devops/tools/Kubernetes/Kubernetes-Pod.html","text":"Kubernetes - Pod # A pod is a collection of containers and its storage inside a node of a Kubernetes cluster. It is possible to create a pod with multiple containers inside it. For example, keeping a database container and data container in the same pod. Types of Pod There are two types of Pods \u2212 Single container pod Multi container pod Single Container Pod They can be simply created with the kubctl run command, where you have a defined image on the Docker registry which we will pull while creating a pod. $ kubectl run <name of pod> --image=<name of the image from registry> Example \u2212 We will create a pod with a tomcat image which is available on the Docker hub. $ kubectl run tomcat --image = tomcat:8.0 This can also be done by creating the yaml file and then running the kubectl create command. apiVersion: v1 kind: Pod metadata: name: Tomcat spec: containers: - name: Tomcat image: tomcat: 8.0 ports: containerPort: 7500 imagePullPolicy: Always Once the above yaml file is created, we will save the file with the name of tomcat.yml and run the create command to run the document. $ kubectl create \u2013f tomcat.yml It will create a pod with the name of tomcat. We can use the describe command along with kubectl to describe the pod. Multi Container Pod Multi container pods are created using yaml mail with the definition of the containers. apiVersion: v1 kind: Pod metadata: name: Tomcat spec: containers: - name: Tomcat image: tomcat: 8.0 ports: containerPort: 7500 imagePullPolicy: Always -name: Database Image: mongoDB Ports: containerPort: 7501 imagePullPolicy: Always In the above code, we have created one pod with two containers inside it, one for tomcat and the other for MongoDB.","title":"Kubernetes Pod"},{"location":"Devops/tools/Kubernetes/Kubernetes-Pod.html#kubernetes-pod","text":"A pod is a collection of containers and its storage inside a node of a Kubernetes cluster. It is possible to create a pod with multiple containers inside it. For example, keeping a database container and data container in the same pod. Types of Pod There are two types of Pods \u2212 Single container pod Multi container pod Single Container Pod They can be simply created with the kubctl run command, where you have a defined image on the Docker registry which we will pull while creating a pod. $ kubectl run <name of pod> --image=<name of the image from registry> Example \u2212 We will create a pod with a tomcat image which is available on the Docker hub. $ kubectl run tomcat --image = tomcat:8.0 This can also be done by creating the yaml file and then running the kubectl create command. apiVersion: v1 kind: Pod metadata: name: Tomcat spec: containers: - name: Tomcat image: tomcat: 8.0 ports: containerPort: 7500 imagePullPolicy: Always Once the above yaml file is created, we will save the file with the name of tomcat.yml and run the create command to run the document. $ kubectl create \u2013f tomcat.yml It will create a pod with the name of tomcat. We can use the describe command along with kubectl to describe the pod. Multi Container Pod Multi container pods are created using yaml mail with the definition of the containers. apiVersion: v1 kind: Pod metadata: name: Tomcat spec: containers: - name: Tomcat image: tomcat: 8.0 ports: containerPort: 7500 imagePullPolicy: Always -name: Database Image: mongoDB Ports: containerPort: 7501 imagePullPolicy: Always In the above code, we have created one pod with two containers inside it, one for tomcat and the other for MongoDB.","title":"Kubernetes - Pod"},{"location":"Devops/tools/Kubernetes/Kubernetes-Replica-Sets.html","text":"Kubernetes - Replica Sets # Replica Set ensures how many replica of pod should be running. It can be considered as a replacement of replication controller. The key difference between the replica set and the replication controller is, the replication controller only supports equality-based selector whereas the replica set supports set-based selector. apiVersion: extensions/v1beta1 --------------------->1 kind: ReplicaSet --------------------------> 2 metadata: name: Tomcat-ReplicaSet spec: replicas: 3 selector: matchLables: tier: Backend ------------------> 3 matchExpression: { key: tier, operation: In, values: [Backend]} --------------> 4 template: metadata: lables: app: Tomcat-ReplicaSet tier: Backend labels: app: App component: neo4j spec: containers: - name: Tomcat image: tomcat: 8.0 ports: - containerPort: 7474 Setup Details apiVersion: extensions/v1beta1 - In the above code, the API version is the advanced beta version of Kubernetes which supports the concept of replica set. kind: ReplicaSet - We have defined the kind as the replica set which helps kubectl to understand that the file is used to create a replica set. tier: Backend - We have defined the label tier as backend which creates a matching selector. {key: tier, operation: In, values: [Backend]} - This will help matchExpression to understand the matching condition we have defined and in the operation which is used by matchlabel to find details. Run the above file using kubectl and create the backend replica set with the provided definition in the yaml file.","title":"Kubernetes Replica Sets"},{"location":"Devops/tools/Kubernetes/Kubernetes-Replica-Sets.html#kubernetes-replica-sets","text":"Replica Set ensures how many replica of pod should be running. It can be considered as a replacement of replication controller. The key difference between the replica set and the replication controller is, the replication controller only supports equality-based selector whereas the replica set supports set-based selector. apiVersion: extensions/v1beta1 --------------------->1 kind: ReplicaSet --------------------------> 2 metadata: name: Tomcat-ReplicaSet spec: replicas: 3 selector: matchLables: tier: Backend ------------------> 3 matchExpression: { key: tier, operation: In, values: [Backend]} --------------> 4 template: metadata: lables: app: Tomcat-ReplicaSet tier: Backend labels: app: App component: neo4j spec: containers: - name: Tomcat image: tomcat: 8.0 ports: - containerPort: 7474 Setup Details apiVersion: extensions/v1beta1 - In the above code, the API version is the advanced beta version of Kubernetes which supports the concept of replica set. kind: ReplicaSet - We have defined the kind as the replica set which helps kubectl to understand that the file is used to create a replica set. tier: Backend - We have defined the label tier as backend which creates a matching selector. {key: tier, operation: In, values: [Backend]} - This will help matchExpression to understand the matching condition we have defined and in the operation which is used by matchlabel to find details. Run the above file using kubectl and create the backend replica set with the provided definition in the yaml file.","title":"Kubernetes - Replica Sets"},{"location":"Devops/tools/Kubernetes/Kubernetes-Replication-Controller.html","text":"Kubernetes - Replication Controller # Replication Controller is one of the key features of Kubernetes, which is responsible for managing the pod lifecycle. It is responsible for making sure that the specified number of pod replicas are running at any point of time. It is used in time when one wants to make sure that the specified number of pod or at least one pod is running. It has the capability to bring up or down the specified no of pod. It is a best practice to use the replication controller to manage the pod life cycle rather than creating a pod again and again. apiVersion: v1 kind: ReplicationController --------------------------> 1 metadata: name: Tomcat-ReplicationController --------------------------> 2 spec: replicas: 3 ------------------------> 3 template: metadata: name: Tomcat-ReplicationController labels: app: App component: neo4j spec: containers: - name: Tomcat- -----------------------> 4 image: tomcat: 8.0 ports: - containerPort: 7474 ------------------------> 5 Setup Details Kind: ReplicationController - In the above code, we have defined the kind as replication controller which tells the kubectl that the yaml file is going to be used for creating the replication controller. name: Tomcat-ReplicationController - This helps in identifying the name with which the replication controller will be created. If we run the kubctl, get rc < Tomcat-ReplicationController > it will show the replication controller details. replicas: 3 - This helps the replication controller to understand that it needs to maintain three replicas of a pod at any point of time in the pod lifecycle. name: Tomcat - In the spec section, we have defined the name as tomcat which will tell the replication controller that the container present inside the pods is tomcat. containerPort: 7474 - It helps in making sure that all the nodes in the cluster where the pod is running the container inside the pod will be exposed on the same port 7474. Here, the Kubernetes service is working as a load balancer for three tomcat replicas.","title":"Kubernetes Replication Controller"},{"location":"Devops/tools/Kubernetes/Kubernetes-Replication-Controller.html#kubernetes-replication-controller","text":"Replication Controller is one of the key features of Kubernetes, which is responsible for managing the pod lifecycle. It is responsible for making sure that the specified number of pod replicas are running at any point of time. It is used in time when one wants to make sure that the specified number of pod or at least one pod is running. It has the capability to bring up or down the specified no of pod. It is a best practice to use the replication controller to manage the pod life cycle rather than creating a pod again and again. apiVersion: v1 kind: ReplicationController --------------------------> 1 metadata: name: Tomcat-ReplicationController --------------------------> 2 spec: replicas: 3 ------------------------> 3 template: metadata: name: Tomcat-ReplicationController labels: app: App component: neo4j spec: containers: - name: Tomcat- -----------------------> 4 image: tomcat: 8.0 ports: - containerPort: 7474 ------------------------> 5 Setup Details Kind: ReplicationController - In the above code, we have defined the kind as replication controller which tells the kubectl that the yaml file is going to be used for creating the replication controller. name: Tomcat-ReplicationController - This helps in identifying the name with which the replication controller will be created. If we run the kubctl, get rc < Tomcat-ReplicationController > it will show the replication controller details. replicas: 3 - This helps the replication controller to understand that it needs to maintain three replicas of a pod at any point of time in the pod lifecycle. name: Tomcat - In the spec section, we have defined the name as tomcat which will tell the replication controller that the container present inside the pods is tomcat. containerPort: 7474 - It helps in making sure that all the nodes in the cluster where the pod is running the container inside the pod will be exposed on the same port 7474. Here, the Kubernetes service is working as a load balancer for three tomcat replicas.","title":"Kubernetes - Replication Controller"},{"location":"Devops/tools/Kubernetes/Kubernetes-Secrets.html","text":"Kubernetes - Secrets # Secrets can be defined as Kubernetes objects used to store sensitive data such as user name and passwords with encryption. There are multiple ways of creating secrets in Kubernetes. Creating from txt files. Creating from yaml file. Creating From Text File In order to create secrets from a text file such as user name and password, we first need to store them in a txt file and use the following command. $ kubectl create secret generic tomcat-passwd \u2013-from-file = ./username.txt \u2013fromfile = ./. password.txt Creating From Yaml File apiVersion: v1 kind: Secret metadata: name: tomcat-pass type: Opaque data: password: <User Password> username: <User Name> Creating the Secret $ kubectl create \u2013f Secret.yaml secrets/tomcat-pass Using Secrets Once we have created the secrets, it can be consumed in a pod or the replication controller as \u2212 Environment Variable Volume As Environment Variable In order to use the secret as environment variable, we will use env under the spec section of pod yaml file. env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: tomcat-pass As Volume spec: volumes: - name: \"secretstest\" secret: secretName: tomcat-pass containers: - image: tomcat:7.0 name: awebserver volumeMounts: - mountPath: \"/tmp/mysec\" name: \"secretstest\" Secret Configuration As Environment Variable apiVersion: v1 kind: ReplicationController metadata: name: appname spec: replicas: replica_count template: metadata: name: appname spec: nodeSelector: resource-group: containers: - name: appname image: imagePullPolicy: Always ports: - containerPort: 3000 env: -----------------------------> 1 - name: ENV valueFrom: configMapKeyRef: name: appname key: tomcat-secrets In the above code, under the env definition, we are using secrets as environment variable in the replication controller. Secrets As Volume Mount apiVersion: v1 kind: pod metadata: name: appname spec: metadata: name: appname spec: volumes: - name: \"secretstest\" secret: secretName: tomcat-pass containers: - image: tomcat: 8.0 name: awebserver volumeMounts: - mountPath: \"/tmp/mysec\" name: \"secretstest\"","title":"Kubernetes Secrets"},{"location":"Devops/tools/Kubernetes/Kubernetes-Secrets.html#kubernetes-secrets","text":"Secrets can be defined as Kubernetes objects used to store sensitive data such as user name and passwords with encryption. There are multiple ways of creating secrets in Kubernetes. Creating from txt files. Creating from yaml file. Creating From Text File In order to create secrets from a text file such as user name and password, we first need to store them in a txt file and use the following command. $ kubectl create secret generic tomcat-passwd \u2013-from-file = ./username.txt \u2013fromfile = ./. password.txt Creating From Yaml File apiVersion: v1 kind: Secret metadata: name: tomcat-pass type: Opaque data: password: <User Password> username: <User Name> Creating the Secret $ kubectl create \u2013f Secret.yaml secrets/tomcat-pass Using Secrets Once we have created the secrets, it can be consumed in a pod or the replication controller as \u2212 Environment Variable Volume As Environment Variable In order to use the secret as environment variable, we will use env under the spec section of pod yaml file. env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: tomcat-pass As Volume spec: volumes: - name: \"secretstest\" secret: secretName: tomcat-pass containers: - image: tomcat:7.0 name: awebserver volumeMounts: - mountPath: \"/tmp/mysec\" name: \"secretstest\" Secret Configuration As Environment Variable apiVersion: v1 kind: ReplicationController metadata: name: appname spec: replicas: replica_count template: metadata: name: appname spec: nodeSelector: resource-group: containers: - name: appname image: imagePullPolicy: Always ports: - containerPort: 3000 env: -----------------------------> 1 - name: ENV valueFrom: configMapKeyRef: name: appname key: tomcat-secrets In the above code, under the env definition, we are using secrets as environment variable in the replication controller. Secrets As Volume Mount apiVersion: v1 kind: pod metadata: name: appname spec: metadata: name: appname spec: volumes: - name: \"secretstest\" secret: secretName: tomcat-pass containers: - image: tomcat: 8.0 name: awebserver volumeMounts: - mountPath: \"/tmp/mysec\" name: \"secretstest\"","title":"Kubernetes - Secrets"},{"location":"Devops/tools/Kubernetes/Kubernetes-Service.html","text":"Kubernetes - Service # A service can be defined as a logical set of pods. It can be defined as an abstraction on the top of the pod which provides a single IP address and DNS name by which pods can be accessed. With Service, it is very easy to manage load balancing configuration. It helps pods to scale very easily. A service is a REST object in Kubernetes whose definition can be posted to Kubernetes apiServer on the Kubernetes master to create a new instance. Service without Selector apiVersion: v1 kind: Service metadata: name: Tutorial_point_service spec: ports: - port: 8080 targetPort: 31999 The above configuration will create a service with the name Tutorial_point_service. Service Config File with Selector apiVersion: v1 kind: Service metadata: name: Tutorial_point_service spec: selector: application: \"My Application\" -------------------> (Selector) ports: - port: 8080 targetPort: 31999 In this example, we have a selector; so in order to transfer traffic, we need to create an endpoint manually. apiVersion: v1 kind: Endpoints metadata: name: Tutorial_point_service subnets: address: \"ip\": \"192.168.168.40\" -------------------> (Selector) ports: - port: 8080 In the above code, we have created an endpoint which will route the traffic to the endpoint defined as \u201c192.168.168.40:8080\u201d. Multi-Port Service Creation apiVersion: v1 kind: Service metadata: name: Tutorial_point_service spec: selector: application: \u201cMy Application\u201d -------------------> (Selector) ClusterIP: 10.3.0.12 ports: -name: http protocol: TCP port: 80 targetPort: 31999 -name:https Protocol: TCP Port: 443 targetPort: 31998 Types of Services ClusterIP \u2212 This helps in restricting the service within the cluster. It exposes the service within the defined Kubernetes cluster. spec: type: NodePort ports: - port: 8080 nodePort: 31999 name: NodeportService NodePort \u2212 It will expose the service on a static port on the deployed node. A ClusterIP service, to which NodePort service will route, is automatically created. The service can be accessed from outside the cluster using the NodeIP:nodePort. spec: ports: - port: 8080 nodePort: 31999 name: NodeportService clusterIP: 10.20.30.40 Load Balancer \u2212 It uses cloud providers\u2019 load balancer. NodePort and ClusterIP services are created automatically to which the external load balancer will route. A full service yaml file with service type as Node Port. Try to create one yourself. apiVersion: v1 kind: Service metadata: name: appname labels: k8s-app: appname spec: type: NodePort ports: - port: 8080 nodePort: 31999 name: omninginx selector: k8s-app: appname component: nginx env: env_name","title":"Kubernetes Service"},{"location":"Devops/tools/Kubernetes/Kubernetes-Service.html#kubernetes-service","text":"A service can be defined as a logical set of pods. It can be defined as an abstraction on the top of the pod which provides a single IP address and DNS name by which pods can be accessed. With Service, it is very easy to manage load balancing configuration. It helps pods to scale very easily. A service is a REST object in Kubernetes whose definition can be posted to Kubernetes apiServer on the Kubernetes master to create a new instance. Service without Selector apiVersion: v1 kind: Service metadata: name: Tutorial_point_service spec: ports: - port: 8080 targetPort: 31999 The above configuration will create a service with the name Tutorial_point_service. Service Config File with Selector apiVersion: v1 kind: Service metadata: name: Tutorial_point_service spec: selector: application: \"My Application\" -------------------> (Selector) ports: - port: 8080 targetPort: 31999 In this example, we have a selector; so in order to transfer traffic, we need to create an endpoint manually. apiVersion: v1 kind: Endpoints metadata: name: Tutorial_point_service subnets: address: \"ip\": \"192.168.168.40\" -------------------> (Selector) ports: - port: 8080 In the above code, we have created an endpoint which will route the traffic to the endpoint defined as \u201c192.168.168.40:8080\u201d. Multi-Port Service Creation apiVersion: v1 kind: Service metadata: name: Tutorial_point_service spec: selector: application: \u201cMy Application\u201d -------------------> (Selector) ClusterIP: 10.3.0.12 ports: -name: http protocol: TCP port: 80 targetPort: 31999 -name:https Protocol: TCP Port: 443 targetPort: 31998 Types of Services ClusterIP \u2212 This helps in restricting the service within the cluster. It exposes the service within the defined Kubernetes cluster. spec: type: NodePort ports: - port: 8080 nodePort: 31999 name: NodeportService NodePort \u2212 It will expose the service on a static port on the deployed node. A ClusterIP service, to which NodePort service will route, is automatically created. The service can be accessed from outside the cluster using the NodeIP:nodePort. spec: ports: - port: 8080 nodePort: 31999 name: NodeportService clusterIP: 10.20.30.40 Load Balancer \u2212 It uses cloud providers\u2019 load balancer. NodePort and ClusterIP services are created automatically to which the external load balancer will route. A full service yaml file with service type as Node Port. Try to create one yourself. apiVersion: v1 kind: Service metadata: name: appname labels: k8s-app: appname spec: type: NodePort ports: - port: 8080 nodePort: 31999 name: omninginx selector: k8s-app: appname component: nginx env: env_name","title":"Kubernetes - Service"},{"location":"Devops/tools/Kubernetes/Kubernetes-Volumes.html","text":"Kubernetes - Volumes # In Kubernetes, a volume can be thought of as a directory which is accessible to the containers in a pod. We have different types of volumes in Kubernetes and the type defines how the volume is created and its content. The concept of volume was present with the Docker, however the only issue was that the volume was very much limited to a particular pod. As soon as the life of a pod ended, the volume was also lost. On the other hand, the volumes that are created through Kubernetes is not limited to any container. It supports any or all the containers deployed inside the pod of Kubernetes. A key advantage of Kubernetes volume is, it supports different kind of storage wherein the pod can use multiple of them at the same time. Types of Kubernetes Volume Here is a list of some popular Kubernetes Volumes \u2212 emptyDir \u2212 It is a type of volume that is created when a Pod is first assigned to a Node. It remains active as long as the Pod is running on that node. The volume is initially empty and the containers in the pod can read and write the files in the emptyDir volume. Once the Pod is removed from the node, the data in the emptyDir is erased. hostPath \u2212 This type of volume mounts a file or directory from the host node\u2019s filesystem into your pod. gcePersistentDisk \u2212 This type of volume mounts a Google Compute Engine (GCE) Persistent Disk into your Pod. The data in a gcePersistentDisk remains intact when the Pod is removed from the node. awsElasticBlockStore \u2212 This type of volume mounts an Amazon Web Services (AWS) Elastic Block Store into your Pod. Just like gcePersistentDisk, the data in an awsElasticBlockStore remains intact when the Pod is removed from the node. nfs \u2212 An nfs volume allows an existing NFS (Network File System) to be mounted into your pod. The data in an nfs volume is not erased when the Pod is removed from the node. The volume is only unmounted. iscsi \u2212 An iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your pod. flocker \u2212 It is an open-source clustered container data volume manager. It is used for managing data volumes. A flocker volume allows a Flocker dataset to be mounted into a pod. If the dataset does not exist in Flocker, then you first need to create it by using the Flocker API. glusterfs \u2212 Glusterfs is an open-source networked filesystem. A glusterfs volume allows a glusterfs volume to be mounted into your pod. rbd \u2212 RBD stands for Rados Block Device. An rbd volume allows a Rados Block Device volume to be mounted into your pod. Data remains preserved after the Pod is removed from the node. cephfs \u2212 A cephfs volume allows an existing CephFS volume to be mounted into your pod. Data remains intact after the Pod is removed from the node. gitRepo \u2212 A gitRepo volume mounts an empty directory and clones a git repository into it for your pod to use. secret \u2212 A secret volume is used to pass sensitive information, such as passwords, to pods. persistentVolumeClaim \u2212 A persistentVolumeClaim volume is used to mount a PersistentVolume into a pod. PersistentVolumes are a way for users to \u201cclaim\u201d durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment. downwardAPI \u2212 A downwardAPI volume is used to make downward API data available to applications. It mounts a directory and writes the requested data in plain text files. azureDiskVolume \u2212 An AzureDiskVolume is used to mount a Microsoft Azure Data Disk into a Pod. Persistent Volume and Persistent Volume Claim Persistent Volume (PV) \u2212 It\u2019s a piece of network storage that has been provisioned by the administrator. It\u2019s a resource in the cluster which is independent of any individual pod that uses the PV. Persistent Volume Claim (PVC) \u2212 The storage requested by Kubernetes for its pods is known as PVC. The user does not need to know the underlying provisioning. The claims must be created in the same namespace where the pod is created. Creating Persistent Volume kind: PersistentVolume ---------> 1 apiVersion: v1 metadata: name: pv0001 ------------------> 2 labels: type: local spec: capacity: -----------------------> 3 storage: 10Gi ----------------------> 4 accessModes: - ReadWriteOnce -------------------> 5 hostPath: path: \"/tmp/data01\" --------------------------> 6 In the above code, we have defined: kind: PersistentVolume - We have defined the kind as PersistentVolume which tells kubernetes that the yaml file being used is to create the Persistent Volume. name: pv0001 - Name of PersistentVolume that we are creating. capacity: - This spec will define the capacity of PV that we are trying to create. storage: 10Gi - This tells the underlying infrastructure that we are trying to claim 10Gi space on the defined path. ReadWriteOnce - This tells the access rights of the volume that we are creating. path: \"/tmp/data01\" - This definition tells the machine that we are trying to create volume under this path on the underlying infrastructure. Creating PV $ kubectl create \u2013f local-01.yaml persistentvolume \"pv0001\" created Checking PV $ kubectl get pv NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE pv0001 10Gi RWO Available 14s Describing PV $ kubectl describe pv pv0001 Creating Persistent Volume Claim kind: PersistentVolumeClaim --------------> 1 apiVersion: v1 metadata: name: myclaim-1 --------------------> 2 spec: accessModes: - ReadWriteOnce ------------------------> 3 resources: requests: storage: 3Gi ---------------------> 4 In the above code, we have defined \u2212 kind: PersistentVolumeClaim - It instructs the underlying infrastructure that we are trying to claim a specified amount of space. name: myclaim-1 - Name of the claim that we are trying to create. ReadWriteOnce - This specifies the mode of the claim that we are trying to create. storage: 3Gi - This will tell kubernetes about the amount of space we are trying to claim. Creating PVC $ kubectl create \u2013f myclaim-1 persistentvolumeclaim \"myclaim-1\" created Getting Details About PVC $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES AGE myclaim-1 Bound pv0001 10Gi RWO 7s Describe PVC $ kubectl describe pv pv0001 Using PV and PVC with POD kind: Pod apiVersion: v1 metadata: name: mypod labels: name: frontendhttp spec: containers: - name: myfrontend image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: ----------------------------> 1 - mountPath: \"/usr/share/tomcat/html\" name: mypd volumes: -----------------------> 2 - name: mypd persistentVolumeClaim: ------------------------->3 claimName: myclaim-1 In the above code, we have defined \u2212 volumeMounts: - This is the path in the container on which the mounting will take place. Volume: - This definition defines the volume definition that we are going to claim. persistentVolumeClaim: - Under this, we define the volume name which we are going to use in the defined pod.","title":"Kubernetes Volumes"},{"location":"Devops/tools/Kubernetes/Kubernetes-Volumes.html#kubernetes-volumes","text":"In Kubernetes, a volume can be thought of as a directory which is accessible to the containers in a pod. We have different types of volumes in Kubernetes and the type defines how the volume is created and its content. The concept of volume was present with the Docker, however the only issue was that the volume was very much limited to a particular pod. As soon as the life of a pod ended, the volume was also lost. On the other hand, the volumes that are created through Kubernetes is not limited to any container. It supports any or all the containers deployed inside the pod of Kubernetes. A key advantage of Kubernetes volume is, it supports different kind of storage wherein the pod can use multiple of them at the same time. Types of Kubernetes Volume Here is a list of some popular Kubernetes Volumes \u2212 emptyDir \u2212 It is a type of volume that is created when a Pod is first assigned to a Node. It remains active as long as the Pod is running on that node. The volume is initially empty and the containers in the pod can read and write the files in the emptyDir volume. Once the Pod is removed from the node, the data in the emptyDir is erased. hostPath \u2212 This type of volume mounts a file or directory from the host node\u2019s filesystem into your pod. gcePersistentDisk \u2212 This type of volume mounts a Google Compute Engine (GCE) Persistent Disk into your Pod. The data in a gcePersistentDisk remains intact when the Pod is removed from the node. awsElasticBlockStore \u2212 This type of volume mounts an Amazon Web Services (AWS) Elastic Block Store into your Pod. Just like gcePersistentDisk, the data in an awsElasticBlockStore remains intact when the Pod is removed from the node. nfs \u2212 An nfs volume allows an existing NFS (Network File System) to be mounted into your pod. The data in an nfs volume is not erased when the Pod is removed from the node. The volume is only unmounted. iscsi \u2212 An iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your pod. flocker \u2212 It is an open-source clustered container data volume manager. It is used for managing data volumes. A flocker volume allows a Flocker dataset to be mounted into a pod. If the dataset does not exist in Flocker, then you first need to create it by using the Flocker API. glusterfs \u2212 Glusterfs is an open-source networked filesystem. A glusterfs volume allows a glusterfs volume to be mounted into your pod. rbd \u2212 RBD stands for Rados Block Device. An rbd volume allows a Rados Block Device volume to be mounted into your pod. Data remains preserved after the Pod is removed from the node. cephfs \u2212 A cephfs volume allows an existing CephFS volume to be mounted into your pod. Data remains intact after the Pod is removed from the node. gitRepo \u2212 A gitRepo volume mounts an empty directory and clones a git repository into it for your pod to use. secret \u2212 A secret volume is used to pass sensitive information, such as passwords, to pods. persistentVolumeClaim \u2212 A persistentVolumeClaim volume is used to mount a PersistentVolume into a pod. PersistentVolumes are a way for users to \u201cclaim\u201d durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment. downwardAPI \u2212 A downwardAPI volume is used to make downward API data available to applications. It mounts a directory and writes the requested data in plain text files. azureDiskVolume \u2212 An AzureDiskVolume is used to mount a Microsoft Azure Data Disk into a Pod. Persistent Volume and Persistent Volume Claim Persistent Volume (PV) \u2212 It\u2019s a piece of network storage that has been provisioned by the administrator. It\u2019s a resource in the cluster which is independent of any individual pod that uses the PV. Persistent Volume Claim (PVC) \u2212 The storage requested by Kubernetes for its pods is known as PVC. The user does not need to know the underlying provisioning. The claims must be created in the same namespace where the pod is created. Creating Persistent Volume kind: PersistentVolume ---------> 1 apiVersion: v1 metadata: name: pv0001 ------------------> 2 labels: type: local spec: capacity: -----------------------> 3 storage: 10Gi ----------------------> 4 accessModes: - ReadWriteOnce -------------------> 5 hostPath: path: \"/tmp/data01\" --------------------------> 6 In the above code, we have defined: kind: PersistentVolume - We have defined the kind as PersistentVolume which tells kubernetes that the yaml file being used is to create the Persistent Volume. name: pv0001 - Name of PersistentVolume that we are creating. capacity: - This spec will define the capacity of PV that we are trying to create. storage: 10Gi - This tells the underlying infrastructure that we are trying to claim 10Gi space on the defined path. ReadWriteOnce - This tells the access rights of the volume that we are creating. path: \"/tmp/data01\" - This definition tells the machine that we are trying to create volume under this path on the underlying infrastructure. Creating PV $ kubectl create \u2013f local-01.yaml persistentvolume \"pv0001\" created Checking PV $ kubectl get pv NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE pv0001 10Gi RWO Available 14s Describing PV $ kubectl describe pv pv0001 Creating Persistent Volume Claim kind: PersistentVolumeClaim --------------> 1 apiVersion: v1 metadata: name: myclaim-1 --------------------> 2 spec: accessModes: - ReadWriteOnce ------------------------> 3 resources: requests: storage: 3Gi ---------------------> 4 In the above code, we have defined \u2212 kind: PersistentVolumeClaim - It instructs the underlying infrastructure that we are trying to claim a specified amount of space. name: myclaim-1 - Name of the claim that we are trying to create. ReadWriteOnce - This specifies the mode of the claim that we are trying to create. storage: 3Gi - This will tell kubernetes about the amount of space we are trying to claim. Creating PVC $ kubectl create \u2013f myclaim-1 persistentvolumeclaim \"myclaim-1\" created Getting Details About PVC $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES AGE myclaim-1 Bound pv0001 10Gi RWO 7s Describe PVC $ kubectl describe pv pv0001 Using PV and PVC with POD kind: Pod apiVersion: v1 metadata: name: mypod labels: name: frontendhttp spec: containers: - name: myfrontend image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: ----------------------------> 1 - mountPath: \"/usr/share/tomcat/html\" name: mypd volumes: -----------------------> 2 - name: mypd persistentVolumeClaim: ------------------------->3 claimName: myclaim-1 In the above code, we have defined \u2212 volumeMounts: - This is the path in the container on which the mounting will take place. Volume: - This definition defines the volume definition that we are going to claim. persistentVolumeClaim: - Under this, we define the volume name which we are going to use in the defined pod.","title":"Kubernetes - Volumes"},{"location":"Devops/tools/Kubernetes/Kubernetes-overview.html","text":"What is kubernetes? # Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. Kubernetes in an open source container management tool hosted by Cloud Native Computing Foundation (CNCF). Kubernetes comes with a capability of automating deployment, scaling of application, and operations of application containers across clusters. It is capable of creating container centric infrastructure. Kubernetes is a orchestration tool. ## Features of Kubernetes Continues development, integration and deployment Containerized infrastructure Application-centric management Auto-scalable infrastructure Environment consistency across development testing and production Loosely coupled infrastructure, where each component can act as a separate unit Higher density of resource utilization Predictable infrastructure which is going to be created One of the key components of Kubernetes is, it can run application on clusters of physical and virtual machine infrastructure. It also has the capability to run applications on cloud. It helps in moving from host-centric infrastructure to container-centric infrastructure . Kubernetes Architecture # The first and foremost thing you should understand about Kubernetes is, it is a distributed system. Meaning, it has multiple components spread across different servers over a network. These servers could be Virtual machines or bare metal servers. We call it a Kubernetes cluster. A Kubernetes cluster consists of control plane nodes and worker nodes. Control Plane The control plane is responsible for container orchestration and maintaining the desired state of the cluster. It has the following components. kube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager Worker Node The Worker nodes are responsible for running containerized applications. The worker Node has the following components. kubelet kube-proxy Container runtime kube-apiserver The kube-api server is the central hub of the Kubernetes cluster that exposes the Kubernetes API. End users, and other cluster components, talk to the cluster via the API server. Very rarely monitoring systems and third-party services may talk to API servers to interact with the cluster. So when you use kubectl to manage the cluster, at the backend you are actually communicating with the API server through HTTP REST APIs. However, the internal cluster components like the scheduler, controller, etc talk to the API server using gRPC . The communication between the API server and other components in the cluster happens over TLS to prevent unauthorized access to the cluster. Kubernetes api-server is responsible for the following API management: Exposes the cluster API endpoint and handles all API requests. Authentication (Using client certificates, bearer tokens, and HTTP Basic Authentication) and Authorization (ABAC and RBAC evaluation) Processing API requests and validating data for the API objects like pods, services, etc. (Validation and Mutation Admission controllers) It is the only component that communicates with etcd. api-server coordinates all the processes between the control plane and worker node components. api-server has a built-in bastion apiserver proxy. It is part of the API server process. It is primarily used to enable access to ClusterIP services from outside the cluster, even though these services are typically only reachable within the cluster itself. etcd Kubernetes is a distributed system and it needs an efficient distributed database like etcd that supports its distributed nature. It acts as both a backend service discovery and a database. You can call it the brain of the Kubernetes cluster. etcd is an open-source strongly consistent, distributed key-value store. etcd uses raft consensus algorithm for strong consistency and availability. It works in a leader-member fashion for high availability and to withstand node failures. etcd is designed to run on multiple nodes as a cluster without sacrificing consistency. So how etcd works with Kubernetes? To put it simply, when you use kubectl to get kubernetes object details, you are getting it from etcd. Also, when you deploy an object like a pod, an entry gets created in etcd. etcd stores all configurations, states, and metadata of Kubernetes objects (pods, secrets, daemonsets, deployments, configmaps, statefulsets, etc). etcd allows a client to subscribe to events using Watch() API . Kubernetes api-server uses the etcd\u2019s watch functionality to track the change in the state of an object. etcd exposes key-value API using gRPC. Also, the gRPC gateway is a RESTful proxy that translates all the HTTP API calls into gRPC messages. It makes it an ideal database for Kubernetes. etcd stores all objects under the /registry directory key in key-value format. For example, information on a pod named Nginx in the default namespace can be found under /registry/pods/default/nginx Also, etcd it is the only Statefulset component in the control plane. kube-scheduler The kube-scheduler is responsible for scheduling pods on worker nodes . When you deploy a pod, you specify the pod requirements such as CPU, memory, affinity, taints or tolerations, priority, persistent volumes (PV), etc. The scheduler\u2019s primary task is to identify the create request and choose the best node for a pod that satisfies the requirements. In a Kubernetes cluster, there will be more than one worker node. So how does the scheduler select the node out of all worker nodes? To choose the best node, the Kube-scheduler uses filtering and scoring operations. In filtering, the scheduler finds the best-suited nodes where the pod can be scheduled. For example, if there are five worker nodes with resource availability to run the pod, it selects all five nodes. If there are no nodes, then the pod is unschedulable and moved to the scheduling queue. If It is a large cluster, let\u2019s say 100 worker nodes, and the scheduler doesn\u2019t iterate over all the nodes. There is a scheduler configuration parameter called percentageOfNodesToScore . The default value is typically 50%. So it tries to iterate over 50% of nodes in a round-robin fashion. If the worker nodes are spread across multiple zones, then the scheduler iterates over nodes in different zones. For very large clusters the default percentageOfNodesToScore is 5%. In the scoring phase, the scheduler ranks the nodes by assigning a score to the filtered worker nodes. The scheduler makes the scoring by calling multiple scheduling plugins. Finally, the worker node with the highest rank will be selected for scheduling the pod. If all the nodes have the same rank, a node will be selected at random. Once the node is selected, the scheduler creates a binding event in the API server. Meaning an event to bind a pod and It is a controller that listens to pod creation events in the API server. The scheduler has two phases. Scheduling cycle and the Binding cycle.Together it is called the scheduling context. The scheduling cycle selects a worker node and the binding cycle applies that change to the cluster. The scheduler always places the high-priority pods ahead of the low-priority pods for scheduling. Also, in some cases, after the pod started running in the selected node the pod might get evicted or moved to other nodes. You can create custom schedulers and run multiple schedulers in a cluster along with the native scheduler. Kube Controller Manager Controllers are programs that run infinite control loops. Meaning it runs continuously and watches the actual and desired state of objects. If there is a difference in the actual and desired state, it ensures that the kubernetes resource/object is in the desired state. In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. Let\u2019s say you want to create a deployment, you specify the desired state in the manifest YAML file (declarative approach). For example, 2 replicas, one volume mount, configmap, etc. The in-built deployment controller ensures that the deployment is in the desired state all the time. If a user updates the deployment with 5 replicas, the deployment controller recognizes it and ensures the desired state is 5 replicas. Kube controller manager is a component that manages all the Kubernetes controllers. Kubernetes resources/objects like pods, namespaces, jobs, replicaset are managed by respective controllers. Also, the kube scheduler is also a controller managed by Kube controller manager. Following is the list of important built-in Kubernetes controllers. Deployment controller Replicaset controller DaemonSet controller Job Controller (Kubernetes Jobs) CronJob Controller endpoints controller namespace controller service accounts controller. Node controller what you should know about the Kube controller manager. 1. It manages all the controllers and the controllers try to keep the cluster in the desired state. 2. You can extend kubernetes with custom controllers associated with a custom resource definition. Cloud Controller Manager (CCM) When kubernetes is deployed in cloud environments, the cloud controller manager acts as a bridge between Cloud Platform APIs and the Kubernetes cluster. This way the core kubernetes core components can work independently and allow the cloud providers to integrate with kubernetes using plugins. (For example, an interface between kubernetes cluster and AWS cloud API) Cloud controller integration allows Kubernetes cluster to provision cloud resources like instances (for nodes), Load Balancers (for services), and Storage Volumes (for persistent volumes). Cloud Controller Manager contains a set of cloud platform-specific controllers that ensure the desired state of cloud-specific components (nodes, Loadbalancers, storage, etc). Following are the three main controllers that are part of the cloud controller manager. Node controller: This controller updates node-related information by talking to the cloud provider API. For example, node labeling & annotation, getting hostname, CPU & memory availability, nodes health, etc. Route controller: It is responsible for configuring networking routes on a cloud platform. So that pods in different nodes can talk to each other. Service controller: It takes care of deploying load balancers for kubernetes services, assigning IP addresses, etc. Following are some of the classic examples of cloud controller manager. Deploying Kubernetes Service of type Load balancer. Here Kubernetes provisions a Cloud-specific Loadbalancer and integrates with Kubernetes Service. Provisioning storage volumes (PV) for pods backed by cloud storage solutions. Overall Cloud Controller Manager manages the lifecycle of cloud-specific resources used by kubernetes. Kubernetes Worker Node Component # Kubelet Kubelet is an agent component that runs on every node in the cluster. It does not run as a container instead runs as a daemon, managed by systemd. It is responsible for registering worker nodes with the API server and working with the podSpec (Pod specification \u2013 YAML or JSON) primarily from the API server. podSpec defines the containers that should run inside the pod, their resources (e.g. CPU and memory limits), and other settings such as environment variables, volumes, and labels. It then brings the podSpec to the desired state by creating containers. To put it simply, kubelet is responsible for the following. Creating, modifying, and deleting containers for the pod. Responsible for handling liveliness, readiness, and startup probes. Responsible for Mounting volumes by reading pod configuration and creating respective directories on the host for the volume mount. Collecting and reporting Node and pod status via calls to the API server. Kubelet is also a controller where it watches for pod changes and utilizes the node\u2019s container runtime to pull images, run containers, etc. Other than PodSpecs from the API server, kubelet can accept podSpec from a file, HTTP endpoint, and HTTP server. A good example of \u201cpodSpec from a file\u201d is Kubernetes static pods. Static pods are controlled by kubelet, not the API servers. This means you can create pods by providing a pod YAML location to the Kubelet component. However, static pods created by Kubelet are not managed by the API server. Here is a real-world example use case of the static pod. While bootstrapping the control plane, kubelet starts the api-server, scheduler, and controller manager as static pods from podSpecs located at /etc/kubernetes/manifests Following are some of the key things about kubelet. Kubelet uses the CRI (container runtime interface) gRPC interface to talk to the container runtime. It also exposes an HTTP endpoint to stream logs and provides exec sessions for clients. Uses the CSI (container storage interface) gRPC to configure block volumes. It uses the CNI plugin configured in the cluster to allocate the pod IP address and set up any necessary network routes and firewall rules for the pod. Kube proxy To understand kube proxy, you need to have a basic knowledge of Kubernetes Service & endpoint objects. Service in Kubernetes is a way to expose a set of pods internally or to external traffic. When you create the service object, it gets a virtual IP assigned to it. It is called clusterIP. It is only accessible within the Kubernetes cluster. The Endpoint object contains all the IP addresses and ports of pod groups under a Service object. The endpoints controller is responsible for maintaining a list of pod IP addresses (endpoints). The service controller is responsible for configuring endpoints to a service. You cannot ping the ClusterIP because it is only used for service discovery, unlike pod IPs which are pingable. Now let\u2019s understand Kube Proxy. Kube-proxy is a daemon that runs on every node as a daemonset. It is a proxy component that implements the Kubernetes Services concept for pods. (single DNS for a set of pods with load balancing). It primarily proxies UDP, TCP, and SCTP and does not understand HTTP. When you expose pods using a Service (ClusterIP), Kube-proxy creates network rules to send traffic to the backend pods (endpoints) grouped under the Service object. Meaning, all the load balancing, and service discovery are handled by the Kube proxy. So how does Kube-proxy work? Kube proxy talks to the API server to get the details about the Service (ClusterIP) and respective pod IPs & ports (endpoints). It also monitors for changes in service and endpoints. Kube-proxy then uses any one of the following modes to create/update rules for routing traffic to pods behind a Service IPTables: It is the default mode. In IPTables mode, the traffic is handled by IPtable rules. In this mode, kube-proxy chooses the backend pod random for load balancing. Once the connection is established, the requests go to the same pod until the connection is terminated. IPVS: For clusters with services exceeding 1000, IPVS offers performance improvement. It supports the following load-balancing algorithms for the backend. rr: round-robin : It is the default mode. lc: least connection (smallest number of open connections) dh: destination hashing sh: source hashing sed: shortest expected delay nq: never queue Userspace (legacy & not recommended) Kernelspace: This mode is only for windows systems. Also, you can run a Kubernetes cluster without kube-proxy by replacing it with Cilium. Container Runtime You probably know about Java Runtime (JRE). It is the software required to run Java programs on a host. In the same way, container runtime is a software component that is required to run containers. Container runtime runs on all the nodes in the Kubernetes cluster. It is responsible for pulling images from container registries, running containers, allocating and isolating resources for containers, and managing the entire lifecycle of a container on a host. To understand this better, let\u2019s take a look at two key concepts: Container Runtime Interface (CRI): It is a set of APIs that allows Kubernetes to interact with different container runtimes. It allows different container runtimes to be used interchangeably with Kubernetes. The CRI defines the API for creating, starting, stopping, and deleting containers, as well as for managing images and container networks. Open Container Initiative (OCI): It is a set of standards for container formats and runtimes Kubernetes supports multiple container runtimes (CRI-O, Docker Engine, containerd, etc) that are compliant with Container Runtime Interface (CRI). This means, all these container runtimes implement the CRI interface and expose gRPC CRI APIs (runtime and image service endpoints). So how does Kubernetes make use of the container runtime? As we learned in the Kubelet section, the kubelet agent is responsible for interacting with the container runtime using CRI APIs to manage the lifecycle of a container. It also gets all the container information from the container runtime and provides it to the control plane. Let\u2019s take an example of CRI-O container runtime interface. Here is a high-level overview of how container runtime works with kubernetes. When there is a new request for a pod from the API server, the kubelet talks to CRI-O daemon to launch the required containers via Kubernetes Container Runtime Interface. CRI-O checks and pulls the required container image from the configured container registry using containers/image library. CRI-O then generates OCI runtime specification (JSON) for a container. CRI-O then launches an OCI-compatible runtime (runc) to start the container process as per the runtime specification. Kubernetes Cluster Addon Components Apart from the core components, the kubernetes cluster needs addon components to be fully operational. Choosing an addon depends on the project requirements and use cases. Following are some of the popular addon components that you might need on a cluster. CNI Plugin (Container Network Interface) CoreDNS (For DNS server): CoreDNS acts as a DNS server within the Kubernetes cluster. By enabling this addon, you can enable DNS-based service discovery. Metrics Server (For Resource Metrics): This addon helps you collect performance data and resource usage of Nodes and pods in the cluster. Web UI (Kubernetes Dashboard): This addon enables the Kubernetes dashboard for managing the object via web UI. CNI Plugin First, you need to understand Container Networking Interface (CNI) It is a plugin-based architecture with vendor-neutral specifications and libraries for creating network interfaces for Containers. It is not specific to Kubernetes. With CNI container networking can be standardized across container orchestration tools like Kubernetes, Mesos, CloudFoundry, Podman, Docker, etc. When it comes to container networking, companies might have different requirements such as network isolation, security, encryption, etc. As container technology advanced, many network providers created CNI-based solutions for containers with a wide range of networking capabilities. You can call it as CNI-Plugins This allows users to choose a networking solution that best fits their needs from different providers. How does CNI Plugin work with Kubernetes? The Kube-controller-manager is responsible for assigning pod CIDR to each node. Each pod gets a unique IP address from the pod CIDR. Kubelet interacts with container runtime to launch the scheduled pod. The CRI plugin which is part of the Container runtime interacts with the CNI plugin to configure the pod network. CNI Plugin enables networking between pods spread across the same or different nodes using an overlay network. Following are high-level functionalities provided by CNI plugins. Pod Networking Pod network security & isolation using Network Policies to control the traffic flow between pods and between namespaces. Some popular CNI plugins include: Calico Flannel Weave Net Cilium (Uses eBPF) Amazon VPC CNI (For AWS VPC) Azure CNI (For Azure Virtual network)Kubernetes networking is a big topic and it differs based on the hosting platforms.","title":"Kubernetes overview"},{"location":"Devops/tools/Kubernetes/Kubernetes-overview.html#what-is-kubernetes","text":"Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. Kubernetes in an open source container management tool hosted by Cloud Native Computing Foundation (CNCF). Kubernetes comes with a capability of automating deployment, scaling of application, and operations of application containers across clusters. It is capable of creating container centric infrastructure. Kubernetes is a orchestration tool. ## Features of Kubernetes Continues development, integration and deployment Containerized infrastructure Application-centric management Auto-scalable infrastructure Environment consistency across development testing and production Loosely coupled infrastructure, where each component can act as a separate unit Higher density of resource utilization Predictable infrastructure which is going to be created One of the key components of Kubernetes is, it can run application on clusters of physical and virtual machine infrastructure. It also has the capability to run applications on cloud. It helps in moving from host-centric infrastructure to container-centric infrastructure .","title":"What is kubernetes?"},{"location":"Devops/tools/Kubernetes/Kubernetes-overview.html#kubernetes-architecture","text":"The first and foremost thing you should understand about Kubernetes is, it is a distributed system. Meaning, it has multiple components spread across different servers over a network. These servers could be Virtual machines or bare metal servers. We call it a Kubernetes cluster. A Kubernetes cluster consists of control plane nodes and worker nodes. Control Plane The control plane is responsible for container orchestration and maintaining the desired state of the cluster. It has the following components. kube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager Worker Node The Worker nodes are responsible for running containerized applications. The worker Node has the following components. kubelet kube-proxy Container runtime kube-apiserver The kube-api server is the central hub of the Kubernetes cluster that exposes the Kubernetes API. End users, and other cluster components, talk to the cluster via the API server. Very rarely monitoring systems and third-party services may talk to API servers to interact with the cluster. So when you use kubectl to manage the cluster, at the backend you are actually communicating with the API server through HTTP REST APIs. However, the internal cluster components like the scheduler, controller, etc talk to the API server using gRPC . The communication between the API server and other components in the cluster happens over TLS to prevent unauthorized access to the cluster. Kubernetes api-server is responsible for the following API management: Exposes the cluster API endpoint and handles all API requests. Authentication (Using client certificates, bearer tokens, and HTTP Basic Authentication) and Authorization (ABAC and RBAC evaluation) Processing API requests and validating data for the API objects like pods, services, etc. (Validation and Mutation Admission controllers) It is the only component that communicates with etcd. api-server coordinates all the processes between the control plane and worker node components. api-server has a built-in bastion apiserver proxy. It is part of the API server process. It is primarily used to enable access to ClusterIP services from outside the cluster, even though these services are typically only reachable within the cluster itself. etcd Kubernetes is a distributed system and it needs an efficient distributed database like etcd that supports its distributed nature. It acts as both a backend service discovery and a database. You can call it the brain of the Kubernetes cluster. etcd is an open-source strongly consistent, distributed key-value store. etcd uses raft consensus algorithm for strong consistency and availability. It works in a leader-member fashion for high availability and to withstand node failures. etcd is designed to run on multiple nodes as a cluster without sacrificing consistency. So how etcd works with Kubernetes? To put it simply, when you use kubectl to get kubernetes object details, you are getting it from etcd. Also, when you deploy an object like a pod, an entry gets created in etcd. etcd stores all configurations, states, and metadata of Kubernetes objects (pods, secrets, daemonsets, deployments, configmaps, statefulsets, etc). etcd allows a client to subscribe to events using Watch() API . Kubernetes api-server uses the etcd\u2019s watch functionality to track the change in the state of an object. etcd exposes key-value API using gRPC. Also, the gRPC gateway is a RESTful proxy that translates all the HTTP API calls into gRPC messages. It makes it an ideal database for Kubernetes. etcd stores all objects under the /registry directory key in key-value format. For example, information on a pod named Nginx in the default namespace can be found under /registry/pods/default/nginx Also, etcd it is the only Statefulset component in the control plane. kube-scheduler The kube-scheduler is responsible for scheduling pods on worker nodes . When you deploy a pod, you specify the pod requirements such as CPU, memory, affinity, taints or tolerations, priority, persistent volumes (PV), etc. The scheduler\u2019s primary task is to identify the create request and choose the best node for a pod that satisfies the requirements. In a Kubernetes cluster, there will be more than one worker node. So how does the scheduler select the node out of all worker nodes? To choose the best node, the Kube-scheduler uses filtering and scoring operations. In filtering, the scheduler finds the best-suited nodes where the pod can be scheduled. For example, if there are five worker nodes with resource availability to run the pod, it selects all five nodes. If there are no nodes, then the pod is unschedulable and moved to the scheduling queue. If It is a large cluster, let\u2019s say 100 worker nodes, and the scheduler doesn\u2019t iterate over all the nodes. There is a scheduler configuration parameter called percentageOfNodesToScore . The default value is typically 50%. So it tries to iterate over 50% of nodes in a round-robin fashion. If the worker nodes are spread across multiple zones, then the scheduler iterates over nodes in different zones. For very large clusters the default percentageOfNodesToScore is 5%. In the scoring phase, the scheduler ranks the nodes by assigning a score to the filtered worker nodes. The scheduler makes the scoring by calling multiple scheduling plugins. Finally, the worker node with the highest rank will be selected for scheduling the pod. If all the nodes have the same rank, a node will be selected at random. Once the node is selected, the scheduler creates a binding event in the API server. Meaning an event to bind a pod and It is a controller that listens to pod creation events in the API server. The scheduler has two phases. Scheduling cycle and the Binding cycle.Together it is called the scheduling context. The scheduling cycle selects a worker node and the binding cycle applies that change to the cluster. The scheduler always places the high-priority pods ahead of the low-priority pods for scheduling. Also, in some cases, after the pod started running in the selected node the pod might get evicted or moved to other nodes. You can create custom schedulers and run multiple schedulers in a cluster along with the native scheduler. Kube Controller Manager Controllers are programs that run infinite control loops. Meaning it runs continuously and watches the actual and desired state of objects. If there is a difference in the actual and desired state, it ensures that the kubernetes resource/object is in the desired state. In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. Let\u2019s say you want to create a deployment, you specify the desired state in the manifest YAML file (declarative approach). For example, 2 replicas, one volume mount, configmap, etc. The in-built deployment controller ensures that the deployment is in the desired state all the time. If a user updates the deployment with 5 replicas, the deployment controller recognizes it and ensures the desired state is 5 replicas. Kube controller manager is a component that manages all the Kubernetes controllers. Kubernetes resources/objects like pods, namespaces, jobs, replicaset are managed by respective controllers. Also, the kube scheduler is also a controller managed by Kube controller manager. Following is the list of important built-in Kubernetes controllers. Deployment controller Replicaset controller DaemonSet controller Job Controller (Kubernetes Jobs) CronJob Controller endpoints controller namespace controller service accounts controller. Node controller what you should know about the Kube controller manager. 1. It manages all the controllers and the controllers try to keep the cluster in the desired state. 2. You can extend kubernetes with custom controllers associated with a custom resource definition. Cloud Controller Manager (CCM) When kubernetes is deployed in cloud environments, the cloud controller manager acts as a bridge between Cloud Platform APIs and the Kubernetes cluster. This way the core kubernetes core components can work independently and allow the cloud providers to integrate with kubernetes using plugins. (For example, an interface between kubernetes cluster and AWS cloud API) Cloud controller integration allows Kubernetes cluster to provision cloud resources like instances (for nodes), Load Balancers (for services), and Storage Volumes (for persistent volumes). Cloud Controller Manager contains a set of cloud platform-specific controllers that ensure the desired state of cloud-specific components (nodes, Loadbalancers, storage, etc). Following are the three main controllers that are part of the cloud controller manager. Node controller: This controller updates node-related information by talking to the cloud provider API. For example, node labeling & annotation, getting hostname, CPU & memory availability, nodes health, etc. Route controller: It is responsible for configuring networking routes on a cloud platform. So that pods in different nodes can talk to each other. Service controller: It takes care of deploying load balancers for kubernetes services, assigning IP addresses, etc. Following are some of the classic examples of cloud controller manager. Deploying Kubernetes Service of type Load balancer. Here Kubernetes provisions a Cloud-specific Loadbalancer and integrates with Kubernetes Service. Provisioning storage volumes (PV) for pods backed by cloud storage solutions. Overall Cloud Controller Manager manages the lifecycle of cloud-specific resources used by kubernetes.","title":"Kubernetes Architecture"},{"location":"Devops/tools/Kubernetes/Kubernetes-overview.html#kubernetes-worker-node-component","text":"Kubelet Kubelet is an agent component that runs on every node in the cluster. It does not run as a container instead runs as a daemon, managed by systemd. It is responsible for registering worker nodes with the API server and working with the podSpec (Pod specification \u2013 YAML or JSON) primarily from the API server. podSpec defines the containers that should run inside the pod, their resources (e.g. CPU and memory limits), and other settings such as environment variables, volumes, and labels. It then brings the podSpec to the desired state by creating containers. To put it simply, kubelet is responsible for the following. Creating, modifying, and deleting containers for the pod. Responsible for handling liveliness, readiness, and startup probes. Responsible for Mounting volumes by reading pod configuration and creating respective directories on the host for the volume mount. Collecting and reporting Node and pod status via calls to the API server. Kubelet is also a controller where it watches for pod changes and utilizes the node\u2019s container runtime to pull images, run containers, etc. Other than PodSpecs from the API server, kubelet can accept podSpec from a file, HTTP endpoint, and HTTP server. A good example of \u201cpodSpec from a file\u201d is Kubernetes static pods. Static pods are controlled by kubelet, not the API servers. This means you can create pods by providing a pod YAML location to the Kubelet component. However, static pods created by Kubelet are not managed by the API server. Here is a real-world example use case of the static pod. While bootstrapping the control plane, kubelet starts the api-server, scheduler, and controller manager as static pods from podSpecs located at /etc/kubernetes/manifests Following are some of the key things about kubelet. Kubelet uses the CRI (container runtime interface) gRPC interface to talk to the container runtime. It also exposes an HTTP endpoint to stream logs and provides exec sessions for clients. Uses the CSI (container storage interface) gRPC to configure block volumes. It uses the CNI plugin configured in the cluster to allocate the pod IP address and set up any necessary network routes and firewall rules for the pod. Kube proxy To understand kube proxy, you need to have a basic knowledge of Kubernetes Service & endpoint objects. Service in Kubernetes is a way to expose a set of pods internally or to external traffic. When you create the service object, it gets a virtual IP assigned to it. It is called clusterIP. It is only accessible within the Kubernetes cluster. The Endpoint object contains all the IP addresses and ports of pod groups under a Service object. The endpoints controller is responsible for maintaining a list of pod IP addresses (endpoints). The service controller is responsible for configuring endpoints to a service. You cannot ping the ClusterIP because it is only used for service discovery, unlike pod IPs which are pingable. Now let\u2019s understand Kube Proxy. Kube-proxy is a daemon that runs on every node as a daemonset. It is a proxy component that implements the Kubernetes Services concept for pods. (single DNS for a set of pods with load balancing). It primarily proxies UDP, TCP, and SCTP and does not understand HTTP. When you expose pods using a Service (ClusterIP), Kube-proxy creates network rules to send traffic to the backend pods (endpoints) grouped under the Service object. Meaning, all the load balancing, and service discovery are handled by the Kube proxy. So how does Kube-proxy work? Kube proxy talks to the API server to get the details about the Service (ClusterIP) and respective pod IPs & ports (endpoints). It also monitors for changes in service and endpoints. Kube-proxy then uses any one of the following modes to create/update rules for routing traffic to pods behind a Service IPTables: It is the default mode. In IPTables mode, the traffic is handled by IPtable rules. In this mode, kube-proxy chooses the backend pod random for load balancing. Once the connection is established, the requests go to the same pod until the connection is terminated. IPVS: For clusters with services exceeding 1000, IPVS offers performance improvement. It supports the following load-balancing algorithms for the backend. rr: round-robin : It is the default mode. lc: least connection (smallest number of open connections) dh: destination hashing sh: source hashing sed: shortest expected delay nq: never queue Userspace (legacy & not recommended) Kernelspace: This mode is only for windows systems. Also, you can run a Kubernetes cluster without kube-proxy by replacing it with Cilium. Container Runtime You probably know about Java Runtime (JRE). It is the software required to run Java programs on a host. In the same way, container runtime is a software component that is required to run containers. Container runtime runs on all the nodes in the Kubernetes cluster. It is responsible for pulling images from container registries, running containers, allocating and isolating resources for containers, and managing the entire lifecycle of a container on a host. To understand this better, let\u2019s take a look at two key concepts: Container Runtime Interface (CRI): It is a set of APIs that allows Kubernetes to interact with different container runtimes. It allows different container runtimes to be used interchangeably with Kubernetes. The CRI defines the API for creating, starting, stopping, and deleting containers, as well as for managing images and container networks. Open Container Initiative (OCI): It is a set of standards for container formats and runtimes Kubernetes supports multiple container runtimes (CRI-O, Docker Engine, containerd, etc) that are compliant with Container Runtime Interface (CRI). This means, all these container runtimes implement the CRI interface and expose gRPC CRI APIs (runtime and image service endpoints). So how does Kubernetes make use of the container runtime? As we learned in the Kubelet section, the kubelet agent is responsible for interacting with the container runtime using CRI APIs to manage the lifecycle of a container. It also gets all the container information from the container runtime and provides it to the control plane. Let\u2019s take an example of CRI-O container runtime interface. Here is a high-level overview of how container runtime works with kubernetes. When there is a new request for a pod from the API server, the kubelet talks to CRI-O daemon to launch the required containers via Kubernetes Container Runtime Interface. CRI-O checks and pulls the required container image from the configured container registry using containers/image library. CRI-O then generates OCI runtime specification (JSON) for a container. CRI-O then launches an OCI-compatible runtime (runc) to start the container process as per the runtime specification. Kubernetes Cluster Addon Components Apart from the core components, the kubernetes cluster needs addon components to be fully operational. Choosing an addon depends on the project requirements and use cases. Following are some of the popular addon components that you might need on a cluster. CNI Plugin (Container Network Interface) CoreDNS (For DNS server): CoreDNS acts as a DNS server within the Kubernetes cluster. By enabling this addon, you can enable DNS-based service discovery. Metrics Server (For Resource Metrics): This addon helps you collect performance data and resource usage of Nodes and pods in the cluster. Web UI (Kubernetes Dashboard): This addon enables the Kubernetes dashboard for managing the object via web UI. CNI Plugin First, you need to understand Container Networking Interface (CNI) It is a plugin-based architecture with vendor-neutral specifications and libraries for creating network interfaces for Containers. It is not specific to Kubernetes. With CNI container networking can be standardized across container orchestration tools like Kubernetes, Mesos, CloudFoundry, Podman, Docker, etc. When it comes to container networking, companies might have different requirements such as network isolation, security, encryption, etc. As container technology advanced, many network providers created CNI-based solutions for containers with a wide range of networking capabilities. You can call it as CNI-Plugins This allows users to choose a networking solution that best fits their needs from different providers. How does CNI Plugin work with Kubernetes? The Kube-controller-manager is responsible for assigning pod CIDR to each node. Each pod gets a unique IP address from the pod CIDR. Kubelet interacts with container runtime to launch the scheduled pod. The CRI plugin which is part of the Container runtime interacts with the CNI plugin to configure the pod network. CNI Plugin enables networking between pods spread across the same or different nodes using an overlay network. Following are high-level functionalities provided by CNI plugins. Pod Networking Pod network security & isolation using Network Policies to control the traffic flow between pods and between namespaces. Some popular CNI plugins include: Calico Flannel Weave Net Cilium (Uses eBPF) Amazon VPC CNI (For AWS VPC) Azure CNI (For Azure Virtual network)Kubernetes networking is a big topic and it differs based on the hosting platforms.","title":"Kubernetes Worker Node Component"},{"location":"Devops/tools/SCR/Github.html","text":"GitHub Docs # Get started Collaborative coding CI/CD and DevOps Security Client apps Project management Developers Enterprise and Teams Community https://docs.github.com/en run: | chmod +x ./update_git_doc.sh ./update_git_doc.sh ${{ env.GA_SECRET }} shell: bash curl -L \\ -X POST \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/orgs/ganesh-document/repos \\ -d '{\"name\":\"Hello-World\",\"description\":\"This is your first repository\",\"homepage\":\"https://github.com\",\"private\":false,\"has_issues\":true,\"has_projects\":true,\"auto_init\":true}' curl -L \\ -X DELETE \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/repos/ganesh-document/Hello-World curl -L \\ -X PUT \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/repos/ganesh-document/Hello-World/branches/master/protection \\ -d '{\"required_status_checks\":{\"strict\":true,\"contexts\":[\"continuous-integration/travis-ci\"]},\"enforce_admins\":true,\"required_pull_request_reviews\":{\"dismissal_restrictions\":{\"users\":[\"octocat\"],\"teams\":[\"justice-league\"]},\"dismiss_stale_reviews\":true,\"require_code_owner_reviews\":true,\"required_approving_review_count\":2,\"require_last_push_approval\":true,\"bypass_pull_request_allowances\":{\"users\":[\"octocat\"],\"teams\":[\"justice-league\"]}},\"restrictions\":{\"users\":[\"octocat\"],\"teams\":[\"justice-league\"],\"apps\":[\"super-ci\"]},\"required_linear_history\":true,\"allow_force_pushes\":true,\"allow_deletions\":true,\"block_creations\":true,\"required_conversation_resolution\":true,\"lock_branch\":true,\"allow_fork_syncing\":true}' curl -L \\ -X POST \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/repos/ganesh-document/Hello-World/rulesets \\ -d '{\"name\":\"super cool ruleset\",\"target\":\"branch\",\"enforcement\":\"active\",\"bypass_mode\":\"repository\",\"bypass_actors\":[{\"actor_id\":234,\"actor_type\":\"Team\"}],\"conditions\":{\"ref_name\":{\"include\":[\"refs/heads/main\",\"refs/heads/master\"],\"exclude\":[\"refs/heads/dev*\"]}},\"rules\":[{\"type\":\"commit_author_email_pattern\",\"parameters\":{\"operator\":\"contains\",\"pattern\":\"github\"}}]}, \"rules\":[{\"type\":\"pull_request\",\"parameters\":{\"required_approving_review_count\":\"1\"}}]}, \"rules\":[{\"type\":\"non_fast_forward\",\"parameters\":{\"type\":\"non_fast_forward\"}}]}'","title":"GitHub Docs"},{"location":"Devops/tools/SCR/Github.html#github-docs","text":"Get started Collaborative coding CI/CD and DevOps Security Client apps Project management Developers Enterprise and Teams Community https://docs.github.com/en run: | chmod +x ./update_git_doc.sh ./update_git_doc.sh ${{ env.GA_SECRET }} shell: bash curl -L \\ -X POST \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/orgs/ganesh-document/repos \\ -d '{\"name\":\"Hello-World\",\"description\":\"This is your first repository\",\"homepage\":\"https://github.com\",\"private\":false,\"has_issues\":true,\"has_projects\":true,\"auto_init\":true}' curl -L \\ -X DELETE \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/repos/ganesh-document/Hello-World curl -L \\ -X PUT \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/repos/ganesh-document/Hello-World/branches/master/protection \\ -d '{\"required_status_checks\":{\"strict\":true,\"contexts\":[\"continuous-integration/travis-ci\"]},\"enforce_admins\":true,\"required_pull_request_reviews\":{\"dismissal_restrictions\":{\"users\":[\"octocat\"],\"teams\":[\"justice-league\"]},\"dismiss_stale_reviews\":true,\"require_code_owner_reviews\":true,\"required_approving_review_count\":2,\"require_last_push_approval\":true,\"bypass_pull_request_allowances\":{\"users\":[\"octocat\"],\"teams\":[\"justice-league\"]}},\"restrictions\":{\"users\":[\"octocat\"],\"teams\":[\"justice-league\"],\"apps\":[\"super-ci\"]},\"required_linear_history\":true,\"allow_force_pushes\":true,\"allow_deletions\":true,\"block_creations\":true,\"required_conversation_resolution\":true,\"lock_branch\":true,\"allow_fork_syncing\":true}' curl -L \\ -X POST \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxx\"\\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/repos/ganesh-document/Hello-World/rulesets \\ -d '{\"name\":\"super cool ruleset\",\"target\":\"branch\",\"enforcement\":\"active\",\"bypass_mode\":\"repository\",\"bypass_actors\":[{\"actor_id\":234,\"actor_type\":\"Team\"}],\"conditions\":{\"ref_name\":{\"include\":[\"refs/heads/main\",\"refs/heads/master\"],\"exclude\":[\"refs/heads/dev*\"]}},\"rules\":[{\"type\":\"commit_author_email_pattern\",\"parameters\":{\"operator\":\"contains\",\"pattern\":\"github\"}}]}, \"rules\":[{\"type\":\"pull_request\",\"parameters\":{\"required_approving_review_count\":\"1\"}}]}, \"rules\":[{\"type\":\"non_fast_forward\",\"parameters\":{\"type\":\"non_fast_forward\"}}]}'","title":"GitHub Docs"},{"location":"NVIDIA/NIM/Code-Generation.html","text":"Publisher: NVIDIA # 1. Model: nvidia/llama-3.1-nemotron-70b-instruct # Model Overview # Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model is ready for commercial use. Model Architecture: # Architecture Type: Transformer Network Architecture: Llama 3.1 Input: # Input Type(s): Text Input Format: String Input Parameters: One Dimensional (1D) Other Properties Related to Input: Max of 128k tokens Output: # Output Type(s): Text Output Format: String Output Parameters: One Dimensional (1D) Other Properties Related to Output: Max of 4k tokens Supported Hardware Microarchitecture Compatibility: # NVIDIA Ampere NVIDIA Hopper NVIDIA Turing Supported Operating System(s): Linux Training & Evaluation: # Datasets: Data Collection Method by dataset [Hybrid: Human, Synthetic] Labeling Method by dataset [Human] Properties (Quantity, Dataset Descriptions, Sensor(s)): 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity. 20, 324 prompt-responses used for training and 1, 038 used for validation. Inference: # Engine: Triton Test Hardware: H100, A100 80GB, A100 40GB Ethical Considerations: NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concerns Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nvidia/llama-3.1-nemotron-70b-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.5, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.5, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Publisher: NVIDIA # 2. Model: nvidia/mistral-nemo-minitron-8b-8k-instruct # Project Build a Customizable Hybrid RAG Chatbot Model Overview # Mistral-NeMo-Minitron-8B-Instruct is a model for generating responses for various text-generation tasks including roleplaying, retrieval augmented generation, and function calling. It is a fine-tuned version of nvidia/Mistral-NeMo-Minitron-8B-Base, which was pruned and distilled from Mistral-NeMo 12B using our LLM compression technique. The model was trained using a multi-stage SFT and preference-based alignment technique with NeMo Aligner. For details on the alignment technique, please refer to the Nemotron-4 340B Technical Report. The model supports a context length of 8,192 tokens. License/Terms of Use: NVIDIA Open Model License Model Architecture: # Architecture Type: Transformer Network Architecture: Decoder-only Input: # Input Type(s): Text (Prompt) Input Format(s): String Input Parameters: One Dimensional (1D) Other Properties Related to Input: The model has a maximum of 8192 input tokens. Output: # Output Type(s): Text (Response) Output Format: String Output Parameters: 1D Other Properties Related to Output: The model has a maximum of 8192 input tokens. Maximum output for both versions can be set apart from input. Prompt Format: We recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it. <extra_id_0>System {system prompt} <extra_id_1>User {prompt} <extra_id_1>Assistant\\n Note that a newline character \\n should be added at the end of the prompt. We recommend using as a stop token. Evaluation Results Software Integration: (Cloud) Runtime Engine: NeMo Framework 24.09 Supported Hardware Microarchitecture Compatibility: [NVIDIA Ampere] [NVIDIA Blackwell] [NVIDIA Hopper] [NVIDIA Lovelace] Model Version(s) Mistral-NeMo-Minitron 8B Instruct Training & Evaluation: # Training Dataset: ** Data Collection Method by dataset Hybrid: Automated, Human ** Labeling Method by dataset Hybrid: Automated, Human Evaluation Dataset: # ** Data Collection Method by dataset Hybrid: Automated, Human ** Labeling Method by dataset Human Inference: # Engine: TRT-LLM Test Hardware: A100 A10G H100 L40S Supported Hardware Platform(s): L40S, A10G, A100, H100 Ethical Considerations: # NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nvidia/mistral-nemo-minitron-8b-8k-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.5, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nvidia/mistral-nemo-minitron-8b-8k-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.5, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run nvidia/mistral-nemo-minitron-8b-8k-instruct using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/nv-mistralai/mistral-nemo-minitron-8b-8k-instruct:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"nv-mistralai/mistral-nemo-minitron-8b-8k-instruct\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"max_tokens\": 64 }' Publisher: meta # 3. Model: meta/llama-3.2-3b-instruct # Model Information # The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. Llama 3.2 models are ready for commercial use. Models are accelerated by TensorRT-LLM, a library for optimizing Large Language Model (LLM) inference on NVIDIA GPUs. Models in this Collection: Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Third-Party Community Consideration: This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party\u2019s requirements for this application and use case; see link to Non-NVIDIA Llama 3.2 Model Card. License: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement). Model Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Supported Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly. Intended Use Intended Use Cases: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pre-trained models can be adapted for a variety of additional natural language generation tasks. Out of Scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card. Hardware and Software # Training Factors: We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pre-training. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Energy Use: Training utilized a cumulative of 916k GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 240 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Data # Data Collection Method: Unknown Labeling Method: Unknown Overview: Llama 3.2 was pre-trained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). Data Freshness: The pre-training data has a cutoff of December 2023. Benchmarks - English Text # In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library. Base Pre-trained Models Inference Supported Hardware Microarchitecture Compatibility: NVIDIA Ampere NVIDIA Hopper NVIDIA Lovelace NVIDIA Jetson Supported Operating System(s): Linux Windows Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"meta/llama-3.2-3b-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"meta/llama-3.2-3b-instruct\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell invoke_url='https://integrate.api.nvidia.com/v1/chat/completions' authorization_header='Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC' accept_header='Accept: application/json' content_type_header='Content-Type: application/json' data=$'{ \"messages\": [ { \"role\": \"user\", \"content\": \"Write a limerick about the wonders of GPU computing.\" } ], \"stream\": true, \"model\": \"meta/llama-3.2-3b-instruct\", \"max_tokens\": 1024, \"presence_penalty\": 0, \"frequency_penalty\": 0, \"top_p\": 0.7, \"temperature\": 0.2 }' response=$(curl --silent -i -w \"\\n%{http_code}\" --request POST \\ --url \"$invoke_url\" \\ --header \"$authorization_header\" \\ --header \"$accept_header\" \\ --header \"$content_type_header\" \\ --data \"$data\" ) echo \"$response\" Ref Link: [meta/llama-3.2-3b-instruct] (https://build.nvidia.com/meta/llama-3.2-3b-instruct) Publisher: google # 4. Model: google/gemma-2-2b-it # Model Information # Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Model Information Summary description and brief definition of inputs and outputs. Limitations Training Data The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. The scope of the training dataset determines the subject areas the model can handle effectively. Model Architecture: # Architecture Type: Transformer Network Architecture: Gemma-2 Model Version: 0.1 Input: Input Type(s): Text Input Format(s): String Input Parameters: One-Dimensional (1D) Other Properties Related to Output: Text can be question, a prompt, or a document to be summarized. Output: Output Type(s): Text Output Format(s): String Output Parameters: One-Dimensional (1D) Other Properties Related to Output: Generated English-language text in response to the input (e.g., an answer to the question, a summary of the document). Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"google/gemma-2-2b-it\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Langchain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"google/gemma-2-2b-it\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"google/gemma-2-2b-it\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.2, \"top_p\": 0.7, \"max_tokens\": 1024, \"stream\": true }' Ref Link: [google/gemma-2-2b-it] (https://build.nvidia.com/google/gemma-2-2b-it) Publisher: NVIDIA(NIM microservice) # 5. Model: nvidia/usdcode-llama3-70b-instruct # Model Overview # USD Code (usdcode-llama3-70b-instruct) is an OpenUSD Python code generation and knowledge answering model that helps developers to write OpenUSD code and answer OpenUSD knowledge questions. This model is available for preview, demonstration, and non-production usage on the NVIDIA API Catalog. References: Llama3 - https://ai.meta.com/blog/meta-llama-3/ OpenUSD - https://www.openusd.org/ Model Architecture: Architecture Type: Transformer-Based Architecture Network Architecture: Llama-3 Input Input Type(s): Text Input Format(s): String Other Properties Related to Input: Max context length of 8k tokens Output Output Type(s): Text (Code, Python) Output Format: String Other Properties Related to Output: Max output length of 8k tokens Software Integration: Runtime Engine(s): NIM 1.0.0 Supported Hardware Microarchitecture Compatibility: NVIDIA Hopper Training, Testing, and Evaluation Datasets: Training Dataset: Data Collection Method by dataset Hybrid: Automated, Synthetic Labeling Method by dataset Unknown Properties (Quantity, Dataset Descriptions, Sensor(s)): 59,729 question/answer pairs (text) Evaluation Dataset: Data Collection Method by dataset Hybrid: Automated, Synthetic Labeling Method by dataset Not Applicable Properties (Quantity, Dataset Descriptions, Sensor(s)): 100 question/answer pairs (text) Inference: Engine: TensorRT Test Hardware: H100 Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nvidia/usdcode-llama3-70b-instruct\", messages=[{\"role\":\"user\",\"content\":\"What is LIVRPS?\"}], temperature=0.1, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nvidia/usdcode-llama3-70b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"What is LIVRPS?\"}], \"temperature\": 0.1, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Ref Link: [nvidia/usdcode-llama3-70b-instruct] (https://build.nvidia.com/nvidia/usdcode-llama3-70b-instruct) Publisher: NVIDIA(NIM microservice) # 6. Model: nv-mistralai/mistral-nemo-12b-instruct # Model Overview # Mistral-NeMo is a Large Language Model (LLM) composed of 12B parameters. This model leads accuracy on popular benchmarks across common sense reasoning, coding, math, multilingual and multi-turn chat tasks; it significantly outperforms existing models smaller or similar in size. Model Architecture: Architecture Type: Transformer Network Architecture: Mistral This transformer model has the following characteristics: Layers: 40 Dim: 5,120 Head dim: 128 Hidden dim: 14,436 Activation Function: SwiGLU Number of heads: 32 Number of kv-heads: 8 (GQA) Rotary embeddings (theta = 1M) Vocabulary size: 2**17 ~= 128k Input Input Type: Text Input Format: String Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output Output Type: Text Output Format: String Inference Engine: TensorRT-LLM Test Hardware: H100 Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nv-mistralai/mistral-nemo-12b-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"nv-mistralai/mistral-nemo-12b-instruct\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nv-mistralai/mistral-nemo-12b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.2, \"top_p\": 0.7, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run nv-mistralai/mistral-nemo-12b-instruct using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"mistral-nemo-12b-instruct\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"max_tokens\": 64 }' Ref Link: [nv-mistralai/mistral-nemo-12b-instruct] (https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct) Publisher: NVIDIA(NIM microservice) # 7. Model: microsoft/phi-3-small-8k-instruct # Model Overview # Phi-3-Small is a lightweight, state-of-the-art open model built upon datasets used for Phi-2 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family, and the small version comes in two variants 8K and 128K which is the context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. This model is ready for commercial and research use. Loading the model locally The model requires tiktoken and Triton packages. After obtaining the Phi-3-Small-8K-Instruct model checkpoints, users can use this sample code for inference. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline torch.random.manual_seed(0) model_id = \"microsoft/Phi-3-small-8k-instruct\" model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_id) messages = [ {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, ] pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, ) generation_args = { \"max_new_tokens\": 500, \"return_full_text\": False, \"temperature\": 0.0, \"do_sample\": False, } output = pipe(messages, \\*\\*generation_args) print(output[0]['generated_text']) Cross Platform Support ONNX runtime now supports Phi3 small models across platforms and hardware. Optimized phi-3 models are also published here in ONNX format. The ONNX models provided run with ONNX Runtime on GPU across server platforms. Support for DML (for Windows GPU), CPU, and mobile variants will be added later. Here are some of the optimized configurations the authors have added: ONNX model for fp16 CUDA ONNX model for int4 CUDA: Quantized to int4 via RTN Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"microsoft/phi-3-small-8k-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"microsoft/phi-3-small-8k-instruct\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"microsoft/phi-3-small-8k-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.2, \"top_p\": 0.7, \"max_tokens\": 1024, \"stream\": true }' Ref Link: [microsoft/phi-3-small-8k-instruct] (https://build.nvidia.com/microsoft/phi-3-small-8k-instruct) Publisher: NVIDIA(NIM microservice) # 8. Model: mistralai/mixtral-8x22b-instruct-v0.1 # Model Overview # Mixtral 8x22B is MistralAI's latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Model Architecture: Architecture Type: Transformer Network Architecture: Sparse Mixture of GPT-based experts Inference: Engine: Triton Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"mistralai/mixtral-8x22b-instruct-v0.1\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.5, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Langchain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"mistralai/mixtral-8x22b-instruct-v0.1\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.5, top_p=1, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"mistralai/mixtral-8x22b-instruct-v0.1\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.5, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run mistralai/mixtral-8x22b-instruct using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/mistralai/mixtral-8x22b-instruct-v01:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"mistralai/mixtral-8x22b-instruct-v0.1\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"max_tokens\": 64 }' Ref Link: [mistralai/mixtral-8x22b-instruct-v0.1] (https://build.nvidia.com/mistralai/mixtral-8x22b-instruct) Publisher: NVIDIA(NIM microservice) # 9. Model: meta/codellama-70b # Model Overview # Code Llama is a large language artificial intelligence (AI) model that generates code and natural language about code built from a collection of models based on Llama 2. Inference: Engine: Triton Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"meta/codellama-70b\", messages=[{\"role\":\"user\",\"content\":\"Write a Python function to calculate the factorial of a number.\"}], temperature=0.1, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"meta/codellama-70b\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.1, top_p=1, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a Python function to calculate the factorial of a number.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"meta/codellama-70b\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a Python function to calculate the factorial of a number.\"}], \"temperature\": 0.1, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run meta/codellama-70b using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/meta/codellama-70b-instruct:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"codellama/codellama-70b-instruct\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a Python function to calculate the factorial of a number.\"}], \"max_tokens\": 64 }' Ref Link: [meta/codellama-70b] (https://build.nvidia.com/meta/codellama-70b)","title":"Code Generation"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidia","text":"","title":"Publisher: NVIDIA"},{"location":"NVIDIA/NIM/Code-Generation.html#1-model-nvidiallama-31-nemotron-70b-instruct","text":"","title":"1. Model: nvidia/llama-3.1-nemotron-70b-instruct"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview","text":"Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model is ready for commercial use.","title":"Model Overview"},{"location":"NVIDIA/NIM/Code-Generation.html#model-architecture","text":"Architecture Type: Transformer Network Architecture: Llama 3.1","title":"Model Architecture:"},{"location":"NVIDIA/NIM/Code-Generation.html#input","text":"Input Type(s): Text Input Format: String Input Parameters: One Dimensional (1D) Other Properties Related to Input: Max of 128k tokens","title":"Input:"},{"location":"NVIDIA/NIM/Code-Generation.html#output","text":"Output Type(s): Text Output Format: String Output Parameters: One Dimensional (1D) Other Properties Related to Output: Max of 4k tokens","title":"Output:"},{"location":"NVIDIA/NIM/Code-Generation.html#supported-hardware-microarchitecture-compatibility","text":"NVIDIA Ampere NVIDIA Hopper NVIDIA Turing Supported Operating System(s): Linux","title":"Supported Hardware Microarchitecture Compatibility:"},{"location":"NVIDIA/NIM/Code-Generation.html#training-evaluation","text":"Datasets: Data Collection Method by dataset [Hybrid: Human, Synthetic] Labeling Method by dataset [Human] Properties (Quantity, Dataset Descriptions, Sensor(s)): 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity. 20, 324 prompt-responses used for training and 1, 038 used for validation.","title":"Training &amp; Evaluation:"},{"location":"NVIDIA/NIM/Code-Generation.html#inference","text":"Engine: Triton Test Hardware: H100, A100 80GB, A100 40GB Ethical Considerations: NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concerns Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nvidia/llama-3.1-nemotron-70b-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.5, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.5, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }'","title":"Inference:"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidia_1","text":"","title":"Publisher: NVIDIA"},{"location":"NVIDIA/NIM/Code-Generation.html#2-model-nvidiamistral-nemo-minitron-8b-8k-instruct","text":"Project Build a Customizable Hybrid RAG Chatbot","title":"2. Model: nvidia/mistral-nemo-minitron-8b-8k-instruct"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview_1","text":"Mistral-NeMo-Minitron-8B-Instruct is a model for generating responses for various text-generation tasks including roleplaying, retrieval augmented generation, and function calling. It is a fine-tuned version of nvidia/Mistral-NeMo-Minitron-8B-Base, which was pruned and distilled from Mistral-NeMo 12B using our LLM compression technique. The model was trained using a multi-stage SFT and preference-based alignment technique with NeMo Aligner. For details on the alignment technique, please refer to the Nemotron-4 340B Technical Report. The model supports a context length of 8,192 tokens. License/Terms of Use: NVIDIA Open Model License","title":"Model Overview"},{"location":"NVIDIA/NIM/Code-Generation.html#model-architecture_1","text":"Architecture Type: Transformer Network Architecture: Decoder-only","title":"Model Architecture:"},{"location":"NVIDIA/NIM/Code-Generation.html#input_1","text":"Input Type(s): Text (Prompt) Input Format(s): String Input Parameters: One Dimensional (1D) Other Properties Related to Input: The model has a maximum of 8192 input tokens.","title":"Input:"},{"location":"NVIDIA/NIM/Code-Generation.html#output_1","text":"Output Type(s): Text (Response) Output Format: String Output Parameters: 1D Other Properties Related to Output: The model has a maximum of 8192 input tokens. Maximum output for both versions can be set apart from input. Prompt Format: We recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it. <extra_id_0>System {system prompt} <extra_id_1>User {prompt} <extra_id_1>Assistant\\n Note that a newline character \\n should be added at the end of the prompt. We recommend using as a stop token. Evaluation Results Software Integration: (Cloud) Runtime Engine: NeMo Framework 24.09 Supported Hardware Microarchitecture Compatibility: [NVIDIA Ampere] [NVIDIA Blackwell] [NVIDIA Hopper] [NVIDIA Lovelace] Model Version(s) Mistral-NeMo-Minitron 8B Instruct","title":"Output:"},{"location":"NVIDIA/NIM/Code-Generation.html#training-evaluation_1","text":"Training Dataset: ** Data Collection Method by dataset Hybrid: Automated, Human ** Labeling Method by dataset Hybrid: Automated, Human","title":"Training &amp; Evaluation:"},{"location":"NVIDIA/NIM/Code-Generation.html#evaluation-dataset","text":"** Data Collection Method by dataset Hybrid: Automated, Human ** Labeling Method by dataset Human","title":"Evaluation Dataset:"},{"location":"NVIDIA/NIM/Code-Generation.html#inference_1","text":"Engine: TRT-LLM Test Hardware: A100 A10G H100 L40S Supported Hardware Platform(s): L40S, A10G, A100, H100","title":"Inference:"},{"location":"NVIDIA/NIM/Code-Generation.html#ethical-considerations","text":"NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nvidia/mistral-nemo-minitron-8b-8k-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.5, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nvidia/mistral-nemo-minitron-8b-8k-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.5, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run nvidia/mistral-nemo-minitron-8b-8k-instruct using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/nv-mistralai/mistral-nemo-minitron-8b-8k-instruct:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"nv-mistralai/mistral-nemo-minitron-8b-8k-instruct\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"max_tokens\": 64 }'","title":"Ethical Considerations:"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-meta","text":"","title":"Publisher: meta"},{"location":"NVIDIA/NIM/Code-Generation.html#3-model-metallama-32-3b-instruct","text":"","title":"3. Model: meta/llama-3.2-3b-instruct"},{"location":"NVIDIA/NIM/Code-Generation.html#model-information","text":"The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. Llama 3.2 models are ready for commercial use. Models are accelerated by TensorRT-LLM, a library for optimizing Large Language Model (LLM) inference on NVIDIA GPUs. Models in this Collection: Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Third-Party Community Consideration: This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party\u2019s requirements for this application and use case; see link to Non-NVIDIA Llama 3.2 Model Card. License: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement). Model Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Supported Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly. Intended Use Intended Use Cases: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pre-trained models can be adapted for a variety of additional natural language generation tasks. Out of Scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.","title":"Model Information"},{"location":"NVIDIA/NIM/Code-Generation.html#hardware-and-software","text":"Training Factors: We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pre-training. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Energy Use: Training utilized a cumulative of 916k GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 240 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.","title":"Hardware and Software"},{"location":"NVIDIA/NIM/Code-Generation.html#training-data","text":"Data Collection Method: Unknown Labeling Method: Unknown Overview: Llama 3.2 was pre-trained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). Data Freshness: The pre-training data has a cutoff of December 2023.","title":"Training Data"},{"location":"NVIDIA/NIM/Code-Generation.html#benchmarks-english-text","text":"In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library. Base Pre-trained Models Inference Supported Hardware Microarchitecture Compatibility: NVIDIA Ampere NVIDIA Hopper NVIDIA Lovelace NVIDIA Jetson Supported Operating System(s): Linux Windows Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"meta/llama-3.2-3b-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"meta/llama-3.2-3b-instruct\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell invoke_url='https://integrate.api.nvidia.com/v1/chat/completions' authorization_header='Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC' accept_header='Accept: application/json' content_type_header='Content-Type: application/json' data=$'{ \"messages\": [ { \"role\": \"user\", \"content\": \"Write a limerick about the wonders of GPU computing.\" } ], \"stream\": true, \"model\": \"meta/llama-3.2-3b-instruct\", \"max_tokens\": 1024, \"presence_penalty\": 0, \"frequency_penalty\": 0, \"top_p\": 0.7, \"temperature\": 0.2 }' response=$(curl --silent -i -w \"\\n%{http_code}\" --request POST \\ --url \"$invoke_url\" \\ --header \"$authorization_header\" \\ --header \"$accept_header\" \\ --header \"$content_type_header\" \\ --data \"$data\" ) echo \"$response\" Ref Link: [meta/llama-3.2-3b-instruct] (https://build.nvidia.com/meta/llama-3.2-3b-instruct)","title":"Benchmarks - English Text"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-google","text":"","title":"Publisher: google"},{"location":"NVIDIA/NIM/Code-Generation.html#4-model-googlegemma-2-2b-it","text":"","title":"4. Model: google/gemma-2-2b-it"},{"location":"NVIDIA/NIM/Code-Generation.html#model-information_1","text":"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Model Information Summary description and brief definition of inputs and outputs. Limitations Training Data The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. The scope of the training dataset determines the subject areas the model can handle effectively.","title":"Model Information"},{"location":"NVIDIA/NIM/Code-Generation.html#model-architecture_2","text":"Architecture Type: Transformer Network Architecture: Gemma-2 Model Version: 0.1 Input: Input Type(s): Text Input Format(s): String Input Parameters: One-Dimensional (1D) Other Properties Related to Output: Text can be question, a prompt, or a document to be summarized. Output: Output Type(s): Text Output Format(s): String Output Parameters: One-Dimensional (1D) Other Properties Related to Output: Generated English-language text in response to the input (e.g., an answer to the question, a summary of the document). Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"google/gemma-2-2b-it\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Langchain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"google/gemma-2-2b-it\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"google/gemma-2-2b-it\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.2, \"top_p\": 0.7, \"max_tokens\": 1024, \"stream\": true }' Ref Link: [google/gemma-2-2b-it] (https://build.nvidia.com/google/gemma-2-2b-it)","title":"Model Architecture:"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidianim-microservice","text":"","title":"Publisher: NVIDIA(NIM microservice)"},{"location":"NVIDIA/NIM/Code-Generation.html#5-model-nvidiausdcode-llama3-70b-instruct","text":"","title":"5. Model: nvidia/usdcode-llama3-70b-instruct"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview_2","text":"USD Code (usdcode-llama3-70b-instruct) is an OpenUSD Python code generation and knowledge answering model that helps developers to write OpenUSD code and answer OpenUSD knowledge questions. This model is available for preview, demonstration, and non-production usage on the NVIDIA API Catalog. References: Llama3 - https://ai.meta.com/blog/meta-llama-3/ OpenUSD - https://www.openusd.org/ Model Architecture: Architecture Type: Transformer-Based Architecture Network Architecture: Llama-3 Input Input Type(s): Text Input Format(s): String Other Properties Related to Input: Max context length of 8k tokens Output Output Type(s): Text (Code, Python) Output Format: String Other Properties Related to Output: Max output length of 8k tokens Software Integration: Runtime Engine(s): NIM 1.0.0 Supported Hardware Microarchitecture Compatibility: NVIDIA Hopper Training, Testing, and Evaluation Datasets: Training Dataset: Data Collection Method by dataset Hybrid: Automated, Synthetic Labeling Method by dataset Unknown Properties (Quantity, Dataset Descriptions, Sensor(s)): 59,729 question/answer pairs (text) Evaluation Dataset: Data Collection Method by dataset Hybrid: Automated, Synthetic Labeling Method by dataset Not Applicable Properties (Quantity, Dataset Descriptions, Sensor(s)): 100 question/answer pairs (text) Inference: Engine: TensorRT Test Hardware: H100 Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nvidia/usdcode-llama3-70b-instruct\", messages=[{\"role\":\"user\",\"content\":\"What is LIVRPS?\"}], temperature=0.1, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nvidia/usdcode-llama3-70b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"What is LIVRPS?\"}], \"temperature\": 0.1, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Ref Link: [nvidia/usdcode-llama3-70b-instruct] (https://build.nvidia.com/nvidia/usdcode-llama3-70b-instruct)","title":"Model Overview"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidianim-microservice_1","text":"","title":"Publisher: NVIDIA(NIM microservice)"},{"location":"NVIDIA/NIM/Code-Generation.html#6-model-nv-mistralaimistral-nemo-12b-instruct","text":"","title":"6. Model: nv-mistralai/mistral-nemo-12b-instruct"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview_3","text":"Mistral-NeMo is a Large Language Model (LLM) composed of 12B parameters. This model leads accuracy on popular benchmarks across common sense reasoning, coding, math, multilingual and multi-turn chat tasks; it significantly outperforms existing models smaller or similar in size. Model Architecture: Architecture Type: Transformer Network Architecture: Mistral This transformer model has the following characteristics: Layers: 40 Dim: 5,120 Head dim: 128 Hidden dim: 14,436 Activation Function: SwiGLU Number of heads: 32 Number of kv-heads: 8 (GQA) Rotary embeddings (theta = 1M) Vocabulary size: 2**17 ~= 128k Input Input Type: Text Input Format: String Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output Output Type: Text Output Format: String Inference Engine: TensorRT-LLM Test Hardware: H100 Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"nv-mistralai/mistral-nemo-12b-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"nv-mistralai/mistral-nemo-12b-instruct\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"nv-mistralai/mistral-nemo-12b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.2, \"top_p\": 0.7, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run nv-mistralai/mistral-nemo-12b-instruct using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"mistral-nemo-12b-instruct\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"max_tokens\": 64 }' Ref Link: [nv-mistralai/mistral-nemo-12b-instruct] (https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct)","title":"Model Overview"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidianim-microservice_2","text":"","title":"Publisher: NVIDIA(NIM microservice)"},{"location":"NVIDIA/NIM/Code-Generation.html#7-model-microsoftphi-3-small-8k-instruct","text":"","title":"7. Model: microsoft/phi-3-small-8k-instruct"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview_4","text":"Phi-3-Small is a lightweight, state-of-the-art open model built upon datasets used for Phi-2 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family, and the small version comes in two variants 8K and 128K which is the context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. This model is ready for commercial and research use. Loading the model locally The model requires tiktoken and Triton packages. After obtaining the Phi-3-Small-8K-Instruct model checkpoints, users can use this sample code for inference. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline torch.random.manual_seed(0) model_id = \"microsoft/Phi-3-small-8k-instruct\" model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_id) messages = [ {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, ] pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, ) generation_args = { \"max_new_tokens\": 500, \"return_full_text\": False, \"temperature\": 0.0, \"do_sample\": False, } output = pipe(messages, \\*\\*generation_args) print(output[0]['generated_text']) Cross Platform Support ONNX runtime now supports Phi3 small models across platforms and hardware. Optimized phi-3 models are also published here in ONNX format. The ONNX models provided run with ONNX Runtime on GPU across server platforms. Support for DML (for Windows GPU), CPU, and mobile variants will be added later. Here are some of the optimized configurations the authors have added: ONNX model for fp16 CUDA ONNX model for int4 CUDA: Quantized to int4 via RTN Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"microsoft/phi-3-small-8k-instruct\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.2, top_p=0.7, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"microsoft/phi-3-small-8k-instruct\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.2, top_p=0.7, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"microsoft/phi-3-small-8k-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.2, \"top_p\": 0.7, \"max_tokens\": 1024, \"stream\": true }' Ref Link: [microsoft/phi-3-small-8k-instruct] (https://build.nvidia.com/microsoft/phi-3-small-8k-instruct)","title":"Model Overview"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidianim-microservice_3","text":"","title":"Publisher: NVIDIA(NIM microservice)"},{"location":"NVIDIA/NIM/Code-Generation.html#8-model-mistralaimixtral-8x22b-instruct-v01","text":"","title":"8. Model: mistralai/mixtral-8x22b-instruct-v0.1"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview_5","text":"Mixtral 8x22B is MistralAI's latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Model Architecture: Architecture Type: Transformer Network Architecture: Sparse Mixture of GPT-based experts Inference: Engine: Triton Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"mistralai/mixtral-8x22b-instruct-v0.1\", messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], temperature=0.5, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Langchain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"mistralai/mixtral-8x22b-instruct-v0.1\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.5, top_p=1, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"mistralai/mixtral-8x22b-instruct-v0.1\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"temperature\": 0.5, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run mistralai/mixtral-8x22b-instruct using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/mistralai/mixtral-8x22b-instruct-v01:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"mistralai/mixtral-8x22b-instruct-v0.1\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}], \"max_tokens\": 64 }' Ref Link: [mistralai/mixtral-8x22b-instruct-v0.1] (https://build.nvidia.com/mistralai/mixtral-8x22b-instruct)","title":"Model Overview"},{"location":"NVIDIA/NIM/Code-Generation.html#publisher-nvidianim-microservice_4","text":"","title":"Publisher: NVIDIA(NIM microservice)"},{"location":"NVIDIA/NIM/Code-Generation.html#9-model-metacodellama-70b","text":"","title":"9. Model: meta/codellama-70b"},{"location":"NVIDIA/NIM/Code-Generation.html#model-overview_6","text":"Code Llama is a large language artificial intelligence (AI) model that generates code and natural language about code built from a collection of models based on Llama 2. Inference: Engine: Triton Python from openai import OpenAI client = OpenAI( base_url = \"https://integrate.api.nvidia.com/v1\", api_key = \"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" ) completion = client.chat.completions.create( model=\"meta/codellama-70b\", messages=[{\"role\":\"user\",\"content\":\"Write a Python function to calculate the factorial of a number.\"}], temperature=0.1, top_p=1, max_tokens=1024, stream=True ) for chunk in completion: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") LangChain from langchain_nvidia_ai_endpoints import ChatNVIDIA client = ChatNVIDIA( model=\"meta/codellama-70b\", api_key=\"$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", temperature=0.1, top_p=1, max_tokens=1024, ) for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a Python function to calculate the factorial of a number.\"}]): print(chunk.content, end=\"\") Shell curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -d '{ \"model\": \"meta/codellama-70b\", \"messages\": [{\"role\":\"user\",\"content\":\"Write a Python function to calculate the factorial of a number.\"}], \"temperature\": 0.1, \"top_p\": 1, \"max_tokens\": 1024, \"stream\": true }' Docker Pull and run meta/codellama-70b using Docker (this will download the full model and run it in your local environment) $ docker login nvcr.io Username: $oauthtoken Password: <PASTE_API_KEY_HERE> Pull and run the NVIDIA NIM with the command below. This will download the optimized model for your infrastructure. export NGC_API_KEY=<PASTE_API_KEY_HERE> export LOCAL_NIM_CACHE=~/.cache/nim mkdir -p \"$LOCAL_NIM_CACHE\" docker run -it --rm \\ --gpus all \\ --shm-size=16GB \\ -e NGC_API_KEY \\ -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\ -u $(id -u) \\ -p 8000:8000 \\ nvcr.io/nim/meta/codellama-70b-instruct:latest You can now make a local API call using this curl command: curl -X 'POST' \\ 'http://0.0.0.0:8000/v1/chat/completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"model\": \"codellama/codellama-70b-instruct\", \"messages\": [{\"role\":\"user\", \"content\":\"Write a Python function to calculate the factorial of a number.\"}], \"max_tokens\": 64 }' Ref Link: [meta/codellama-70b] (https://build.nvidia.com/meta/codellama-70b)","title":"Model Overview"},{"location":"NVIDIA/NIM/digital_twin.html","text":"","title":"Digital Twin"},{"location":"NVIDIA/NIM/dna_sequencing.html","text":"","title":"DNA Sequencing"},{"location":"NVIDIA/NIM/drug_discovery.html","text":"","title":"Drug Discovery"},{"location":"NVIDIA/NIM/image_classification.html","text":"","title":"Image Classification"},{"location":"NVIDIA/NIM/image_generation.html","text":"","title":"Image Generation"},{"location":"NVIDIA/NIM/image_to_360.html","text":"","title":"Image To 360"},{"location":"NVIDIA/NIM/image_to_embedding.html","text":"","title":"Image To Embedding"},{"location":"NVIDIA/NIM/image_to_text.html","text":"Publisher: meta # 1. Model: meta/llama-3.2-11b-vision-instruct # Model Information # The Meta Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pre-trained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2 Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks. Llama 3.2 Vision models are ready for commercial use. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\" stream = True with open(\"image.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"model\": 'meta/llama-3.2-11b-vision-instruct', \"messages\": [ { \"role\": \"user\", \"content\": f'What is in this image? <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 1.00, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi image_b64=$( base64 image.png ) echo '{ \"model\": \"meta/llama-3.2-11b-vision-instruct\", \"messages\": [ { \"role\": \"user\", \"content\": \"What is in this image? <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 1.00, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [meta/llama-3.2-11b-vision-instruct] (https://build.nvidia.com/meta/llama-3.2-11b-vision-instruct) Publisher: meta # 2. Model: microsoft/phi-3.5-vision-instruct # Model Information # Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. This model is ready for commercial and research use. Loading the model locally from PIL import Image import requests from transformers import AutoModelForCausalLM from transformers import AutoProcessor model_id = \"microsoft/Phi-3.5-vision-instruct\" model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, num_crops=4) images = [] placeholder = \"\" for i in range(1,20): url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\" images.append(Image.open(requests.get(url, stream=True).raw)) placeholder += f\"<|image_{i}|>\\n\" messages = [ {\"role\": \"user\", \"content\": placeholder+\"Summarize the deck of slides.\"}, ] prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\") generation_args = { \"max_new_tokens\": 1000, \"temperature\": 0.0, \"do_sample\": False, } generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) # remove input tokens generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:] response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] print(response) Python import requests, base64 invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\" stream = True with open(\"dog.jpeg\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"model\": 'microsoft/phi-3.5-vision-instruct', \"messages\": [ { \"role\": \"user\", \"content\": f'Describe the image. <img src=\"data:image/jpeg;base64,{image_b64}\" />' } ], \"max_tokens\": 512, \"temperature\": 0.20, \"top_p\": 0.70, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi image_b64=$( base64 dog.jpeg ) echo '{ \"model\": \"microsoft/phi-3.5-vision-instruct\", \"messages\": [ { \"role\": \"user\", \"content\": \"Describe the image. <img src=\\\"data:image/jpeg;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 512, \"temperature\": 0.20, \"top_p\": 0.70, \"stream\": true }' > payload.json curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [microsoft/phi-3.5-vision-instruct] (https://build.nvidia.com/microsoft/phi-3_5-vision-instruct) Publisher: microsoft # 3. Model: microsoft/florence-2 # Model Information # Florence-2 is an advanced vision foundation model using a prompt-based approach to handle a wide range of vision and vision-language tasks. It can interpret simple text prompts to perform tasks like captioning, object detection and segmentation. Python # The model can perform 14 different vision language model and computer vision tasks. The input ```content``` field should be formatted as ```\"<TASK_PROMPT><text_prompt (only when needed)><img>\"```. # Users need to specify the task type at the beginning. Image supports both base64 and NvCF asset id. Some tasks require a text prompt, and users need to provide that after image. Below are the examples for each task. # For <CAPTION_TO_PHRASE_GROUNDING>, <REFERRING_EXPRESSION_SEGMENTATION>, <OPEN_VOCABULARY_DETECTION>, users can change the text prompt as other descriptions. # For <REGION_TO_SEGMENTATION>, <REGION_TO_CATEGORY>, <REGION_TO_DESCRIPTION>, the text prompt is formatted as <loc_x1><loc_y1><loc_x2><loc_y2>, which is the normalized coordinates from region of interest bbox. x1=int(top_left_x_coor/width*999), y1=int(top_left_y_coor/height*999), x2=int(bottom_right_x_coor/width*999), y2=int(bottom_right_y_coor/height*999). import os import sys import zipfile import requests nvai_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/florence-2\" header_auth = f'Bearer {os.getenv(\"API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"\")}' prompts = [\"<CAPTION>\", \"<DETAILED_CAPTION>\", \"<MORE_DETAILED_CAPTION>\", \"<OD>\", \"<DENSE_REGION_CAPTION>\", \"<REGION_PROPOSAL>\", \"<CAPTION_TO_PHRASE_GROUNDING>A black and brown dog is laying on a grass field.\", \"<REFERRING_EXPRESSION_SEGMENTATION>a black and brown dog\", \"<REGION_TO_SEGMENTATION><loc_312><loc_168><loc_998><loc_846>\", \"<OPEN_VOCABULARY_DETECTION>a black and brown dog\", \"<REGION_TO_CATEGORY><loc_312><loc_168><loc_998><loc_846>\", \"<REGION_TO_DESCRIPTION><loc_312><loc_168><loc_998><loc_846>\", \"<OCR>\", \"<OCR_WITH_REGION>\"] def _upload_asset(input, description): \"\"\" Uploads an asset to the NVCF API. :param input: The binary asset to upload :param description: A description of the asset \"\"\" authorize = requests.post( \"https://api.nvcf.nvidia.com/v2/nvcf/assets\", headers={ \"Authorization\": header_auth, \"Content-Type\": \"application/json\", \"accept\": \"application/json\", }, json={\"contentType\": \"image/jpeg\", \"description\": description}, timeout=30, ) authorize.raise_for_status() response = requests.put( authorize.json()[\"uploadUrl\"], data=input, headers={ \"x-amz-meta-nvcf-asset-description\": description, \"content-type\": \"image/jpeg\", }, timeout=300, ) response.raise_for_status() return str(authorize.json()[\"assetId\"]) def _generate_content(task_id, asset_id): if task_id < 0 or task_id >= len(prompts): print(f\"task_id should within [0, {len(prompts)-1}]\") exit(1) prompt = prompts[task_id] content = f'{prompt}<img src=\"data:image/jpeg;asset_id,{asset_id}\" />' return content if __name__ == \"__main__\": \"\"\"Uploads two images of your choosing to the NVCF API and sends a request to the Visual ChangeNet model to compare them. The response is saved to <output_dir> \"\"\" if len(sys.argv) != 4: print(\"Usage: python test.py <test_image> <result_dir> <task_id>\\n\" \"For example: python test.py car.jpg result_dir 0\") sys.exit(1) if len(os.getenv(\"API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"\")) == 0: print(\"API_KEY not set. Please export API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC=<Your API Key> as environment variable.\") sys.exit(1) # Local images asset_id = _upload_asset(open(sys.argv[1], \"rb\"), \"Test Image\") content = _generate_content(int(sys.argv[3]), asset_id) # Asset IDs returned by the _upload_asset function inputs = { \"messages\": [{ \"role\": \"user\", \"content\": content }] } # asset_list = f\"{asset_id}\" headers = { \"Content-Type\": \"application/json\", \"NVCF-INPUT-ASSET-REFERENCES\": asset_id, \"NVCF-FUNCTION-ASSET-IDS\": asset_id, \"Authorization\": header_auth, \"Accept\": \"application/json\" } print(asset_id, inputs) # Send the request to the NIM API. response = requests.post(nvai_url, headers=headers, json=inputs) with open(f\"{sys.argv[2]}.zip\", \"wb\") as out: out.write(response.content) with zipfile.ZipFile(f\"{sys.argv[2]}.zip\", \"r\") as z: z.extractall(sys.argv[2]) print(f\"Response saved to path: {sys.argv[2]}. File list: {os.listdir(sys.argv[2])}\") Shell #!/bin/bash # The model can perform 14 different vision language model and computer vision tasks. The input ```content``` field should be formatted as ```\"<TASK_PROMPT><text_prompt (only when needed)><img>\"```. # Users need to specify the task type at the beginning. Image supports both base64 and NvCF asset id. Some tasks require a text prompt, and users need to provide that after image. Below are the examples for each task. # For <CAPTION_TO_PHRASE_GROUNDING>, <REFERRING_EXPRESSION_SEGMENTATION>, <OPEN_VOCABULARY_DETECTION>, users can change the text prompt as other descriptions. # For <REGION_TO_SEGMENTATION>, <REGION_TO_CATEGORY>, <REGION_TO_DESCRIPTION>, the text prompt is formatted as <loc_x1><loc_y1><loc_x2><loc_y2>, which is the normalized coordinates from region of interest bbox. x1=int(top_left_x_coor/width*999), y1=int(top_left_y_coor/height*999), x2=int(bottom_right_x_coor/width*999), y2=int(bottom_right_y_coor/height*999). set -e # Check arguments if [ \"$#\" -ne 3 ]; then printf \"Usage: ./test.sh <test_image> <result_dir> <task_id>\\nFor example: ./test.sh car.jpg result_dir 0\\n\" exit 1 fi if [[ -z \"${API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC}\" ]]; then echo \"API_KEY not set. Please export API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC=<Your API Key> as environment variable.\" exit 1 fi # Set variables nvai_url=\"https://ai.api.nvidia.com/v1/vlm/microsoft/florence-2\" api_key=$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC assets_url=\"https://api.nvcf.nvidia.com/v2/nvcf/assets\" prompts=( \"<CAPTION>\" \"<DETAILED_CAPTION>\" \"<MORE_DETAILED_CAPTION>\" \"<OD>\" \"<DENSE_REGION_CAPTION>\" \"<REGION_PROPOSAL>\" \"<CAPTION_TO_PHRASE_GROUNDING>A black and brown dog is laying on a grass field.\" \"<REFERRING_EXPRESSION_SEGMENTATION>a black and brown dog\" \"<REGION_TO_SEGMENTATION><loc_312><loc_168><loc_998><loc_846>\" \"<OPEN_VOCABULARY_DETECTION>a black and brown dog\" \"<REGION_TO_CATEGORY><loc_312><loc_168><loc_998><loc_846>\" \"<REGION_TO_DESCRIPTION><loc_312><loc_168><loc_998><loc_846>\" \"<OCR>\" \"<OCR_WITH_REGION>\" ) content_type=\"image/jpeg\" description=\"Test Image\" # Function to upload an asset upload_asset() { local input=$1 local description=$2 # Authorize upload authorize=$(curl -s -X POST $assets_url \\ -H \"Authorization: Bearer $api_key\" \\ -H \"Content-Type: application/json\" \\ -H \"accept: application/json\" \\ -d \"{\\\"contentType\\\": \\\"$content_type\\\", \\\"description\\\": \\\"$description\\\"}\") # Get upload URL and asset ID upload_url=$(echo $authorize | jq -r '.uploadUrl') asset_id=$(echo $authorize | jq -r '.assetId') # Upload asset curl -s -X PUT $upload_url \\ -H \"x-amz-meta-nvcf-asset-description: $description\" \\ -H \"content-type: $content_type\" \\ --upload-file $input echo $asset_id } # Function to generate content generate_content() { local task_id=$1 local asset_id=$2 prompt=${prompts[$task_id]} content=\"$prompt<img src=\\\\\\\"data:image/jpeg;asset_id,$asset_id\\\\\\\" />\" echo $content } # Upload images asset_id=$(upload_asset $1 $description) content=$(generate_content $3 $asset_id) echo '{ \"messages\":[{ \"role\": \"user\", \"content\": \"'\"$content\"'\" }] }' > payload.json mkdir -p $2 # Compare images via microservice location_command=\"curl -D - -s -X POST $nvai_url \\ -H \\\"Content-Type: application/json\\\" \\ -H \\\"NVCF-INPUT-ASSET-REFERENCES: $asset_id\\\" \\ -H \\\"NVCF-FUNCTION-ASSET-IDS: $asset_id\\\" \\ -H \\\"Authorization: Bearer $api_key\\\" \\ -d @payload.json \\ | grep location | awk '{print \\$2}'\" location=$(eval ${location_command} | tr -d '\\n' | tr -d '\\r' | tr -d ' ' | tr -d '\"' | tr -d ',') # The download command will download the file from the location header download_command=\"curl -s '${location}' > $2.zip\" echo $location_command # Download the .zip file response=$(eval ${download_command}) # Unzip the file unzip -q $2.zip -d $2 echo \"Response saved to $2.zip\" echo $(ls $2) Ref Link: [microsoft/florence-2] (https://build.nvidia.com/microsoft/microsoft-florence-2) Publisher: google # 4. Model: google/paligemma # Model Information # The Google PaLIGemma-3B-mix model is a one-shot visual language understanding solution for image-to-text generation. This model is ready for commercial use. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/google/paligemma\" stream = True with open(\"dog.jpeg\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Describe the image. <img src=\"data:image/jpeg;base64,{image_b64}\" />' } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 0.70, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell image_b64=$( base64 dog.jpeg ) stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Describe the image. <img src=\\\"data:image/jpeg;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 0.70, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/google/paligemma \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [google/paligemma] (https://build.nvidia.com/google/google-paligemma) Publisher: nvidia # 4. Model: nvidia/neva-22b # Model Information # NeVA is NVIDIA's version of the LLaVA model where the open source LLaMA model is replaced with a GPT model trained by NVIDIA. At a high level the image is encoded using a frozen hugging face CLIP model and projected to the text embedding dimensions. This is then concatenated with the embeddings of the prompt and passed in through the language model. Training happens in two stages: Pretraining: Here the language model is frozen and only the projection layer (that maps the image encoding to the embedding space) is trained. Here, image-caption pairs are used to pretrain the model. Finetuning: Here the language model is also trained along with the projection layer. To finetune the model synthetic instruction data generated using GPT4 is used. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b\" stream = True with open(\"dog.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Describe what you see in this image. <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.70, \"seed\": 0, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell image_b64=$( base64 dog.png ) stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Describe what you see in this image. <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.70, \"seed\": 0, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [nvidia/neva-22b] (https://build.nvidia.com/google/google-paligemma) Publisher: microsoft # 5. Model: microsoft/kosmos-2 # Model Information # Kosmos-2 model is a groundbreaking multimodal large language model (MLLM). Kosmos-2 is designed to ground text to the visual world, enabling it to understand and reason about visual elements in images. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2\" with open(\"soccer.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Who is in this photo? <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20 } response = requests.post(invoke_url, headers=headers, json=payload) print(response.json()) Shell image_b64=$( base64 soccer.png ) echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is in this photo? <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20 }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2 \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json\" \\ -d @payload.json Ref Link: [microsoft/kosmos-2] (https://build.nvidia.com/microsoft/microsoft-kosmos-2) Publisher: google # 6. Model: google/deplot # Model Information # The Google DePlot model is a one-shot visual language understanding solution that translates images of plots or charts into linearized tables. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/google/deplot\" stream = True with open(\"economic-assistance-chart.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Generate underlying data table of the figure below: <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell image_b64=$( base64 economic-assistance-chart.png ) stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Generate underlying data table of the figure below: <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/google/deplot \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [google/deplot] (https://build.nvidia.com/google/google-deplot)","title":"Image To Text"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-meta","text":"","title":"Publisher: meta"},{"location":"NVIDIA/NIM/image_to_text.html#1-model-metallama-32-11b-vision-instruct","text":"","title":"1. Model: meta/llama-3.2-11b-vision-instruct"},{"location":"NVIDIA/NIM/image_to_text.html#model-information","text":"The Meta Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pre-trained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2 Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks. Llama 3.2 Vision models are ready for commercial use. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\" stream = True with open(\"image.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"model\": 'meta/llama-3.2-11b-vision-instruct', \"messages\": [ { \"role\": \"user\", \"content\": f'What is in this image? <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 1.00, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi image_b64=$( base64 image.png ) echo '{ \"model\": \"meta/llama-3.2-11b-vision-instruct\", \"messages\": [ { \"role\": \"user\", \"content\": \"What is in this image? <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 1.00, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [meta/llama-3.2-11b-vision-instruct] (https://build.nvidia.com/meta/llama-3.2-11b-vision-instruct)","title":"Model Information"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-meta_1","text":"","title":"Publisher: meta"},{"location":"NVIDIA/NIM/image_to_text.html#2-model-microsoftphi-35-vision-instruct","text":"","title":"2. Model: microsoft/phi-3.5-vision-instruct"},{"location":"NVIDIA/NIM/image_to_text.html#model-information_1","text":"Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. This model is ready for commercial and research use. Loading the model locally from PIL import Image import requests from transformers import AutoModelForCausalLM from transformers import AutoProcessor model_id = \"microsoft/Phi-3.5-vision-instruct\" model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, num_crops=4) images = [] placeholder = \"\" for i in range(1,20): url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\" images.append(Image.open(requests.get(url, stream=True).raw)) placeholder += f\"<|image_{i}|>\\n\" messages = [ {\"role\": \"user\", \"content\": placeholder+\"Summarize the deck of slides.\"}, ] prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\") generation_args = { \"max_new_tokens\": 1000, \"temperature\": 0.0, \"do_sample\": False, } generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) # remove input tokens generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:] response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] print(response) Python import requests, base64 invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\" stream = True with open(\"dog.jpeg\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"model\": 'microsoft/phi-3.5-vision-instruct', \"messages\": [ { \"role\": \"user\", \"content\": f'Describe the image. <img src=\"data:image/jpeg;base64,{image_b64}\" />' } ], \"max_tokens\": 512, \"temperature\": 0.20, \"top_p\": 0.70, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi image_b64=$( base64 dog.jpeg ) echo '{ \"model\": \"microsoft/phi-3.5-vision-instruct\", \"messages\": [ { \"role\": \"user\", \"content\": \"Describe the image. <img src=\\\"data:image/jpeg;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 512, \"temperature\": 0.20, \"top_p\": 0.70, \"stream\": true }' > payload.json curl https://integrate.api.nvidia.com/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [microsoft/phi-3.5-vision-instruct] (https://build.nvidia.com/microsoft/phi-3_5-vision-instruct)","title":"Model Information"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-microsoft","text":"","title":"Publisher: microsoft"},{"location":"NVIDIA/NIM/image_to_text.html#3-model-microsoftflorence-2","text":"","title":"3. Model: microsoft/florence-2"},{"location":"NVIDIA/NIM/image_to_text.html#model-information_2","text":"Florence-2 is an advanced vision foundation model using a prompt-based approach to handle a wide range of vision and vision-language tasks. It can interpret simple text prompts to perform tasks like captioning, object detection and segmentation. Python # The model can perform 14 different vision language model and computer vision tasks. The input ```content``` field should be formatted as ```\"<TASK_PROMPT><text_prompt (only when needed)><img>\"```. # Users need to specify the task type at the beginning. Image supports both base64 and NvCF asset id. Some tasks require a text prompt, and users need to provide that after image. Below are the examples for each task. # For <CAPTION_TO_PHRASE_GROUNDING>, <REFERRING_EXPRESSION_SEGMENTATION>, <OPEN_VOCABULARY_DETECTION>, users can change the text prompt as other descriptions. # For <REGION_TO_SEGMENTATION>, <REGION_TO_CATEGORY>, <REGION_TO_DESCRIPTION>, the text prompt is formatted as <loc_x1><loc_y1><loc_x2><loc_y2>, which is the normalized coordinates from region of interest bbox. x1=int(top_left_x_coor/width*999), y1=int(top_left_y_coor/height*999), x2=int(bottom_right_x_coor/width*999), y2=int(bottom_right_y_coor/height*999). import os import sys import zipfile import requests nvai_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/florence-2\" header_auth = f'Bearer {os.getenv(\"API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"\")}' prompts = [\"<CAPTION>\", \"<DETAILED_CAPTION>\", \"<MORE_DETAILED_CAPTION>\", \"<OD>\", \"<DENSE_REGION_CAPTION>\", \"<REGION_PROPOSAL>\", \"<CAPTION_TO_PHRASE_GROUNDING>A black and brown dog is laying on a grass field.\", \"<REFERRING_EXPRESSION_SEGMENTATION>a black and brown dog\", \"<REGION_TO_SEGMENTATION><loc_312><loc_168><loc_998><loc_846>\", \"<OPEN_VOCABULARY_DETECTION>a black and brown dog\", \"<REGION_TO_CATEGORY><loc_312><loc_168><loc_998><loc_846>\", \"<REGION_TO_DESCRIPTION><loc_312><loc_168><loc_998><loc_846>\", \"<OCR>\", \"<OCR_WITH_REGION>\"] def _upload_asset(input, description): \"\"\" Uploads an asset to the NVCF API. :param input: The binary asset to upload :param description: A description of the asset \"\"\" authorize = requests.post( \"https://api.nvcf.nvidia.com/v2/nvcf/assets\", headers={ \"Authorization\": header_auth, \"Content-Type\": \"application/json\", \"accept\": \"application/json\", }, json={\"contentType\": \"image/jpeg\", \"description\": description}, timeout=30, ) authorize.raise_for_status() response = requests.put( authorize.json()[\"uploadUrl\"], data=input, headers={ \"x-amz-meta-nvcf-asset-description\": description, \"content-type\": \"image/jpeg\", }, timeout=300, ) response.raise_for_status() return str(authorize.json()[\"assetId\"]) def _generate_content(task_id, asset_id): if task_id < 0 or task_id >= len(prompts): print(f\"task_id should within [0, {len(prompts)-1}]\") exit(1) prompt = prompts[task_id] content = f'{prompt}<img src=\"data:image/jpeg;asset_id,{asset_id}\" />' return content if __name__ == \"__main__\": \"\"\"Uploads two images of your choosing to the NVCF API and sends a request to the Visual ChangeNet model to compare them. The response is saved to <output_dir> \"\"\" if len(sys.argv) != 4: print(\"Usage: python test.py <test_image> <result_dir> <task_id>\\n\" \"For example: python test.py car.jpg result_dir 0\") sys.exit(1) if len(os.getenv(\"API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"\")) == 0: print(\"API_KEY not set. Please export API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC=<Your API Key> as environment variable.\") sys.exit(1) # Local images asset_id = _upload_asset(open(sys.argv[1], \"rb\"), \"Test Image\") content = _generate_content(int(sys.argv[3]), asset_id) # Asset IDs returned by the _upload_asset function inputs = { \"messages\": [{ \"role\": \"user\", \"content\": content }] } # asset_list = f\"{asset_id}\" headers = { \"Content-Type\": \"application/json\", \"NVCF-INPUT-ASSET-REFERENCES\": asset_id, \"NVCF-FUNCTION-ASSET-IDS\": asset_id, \"Authorization\": header_auth, \"Accept\": \"application/json\" } print(asset_id, inputs) # Send the request to the NIM API. response = requests.post(nvai_url, headers=headers, json=inputs) with open(f\"{sys.argv[2]}.zip\", \"wb\") as out: out.write(response.content) with zipfile.ZipFile(f\"{sys.argv[2]}.zip\", \"r\") as z: z.extractall(sys.argv[2]) print(f\"Response saved to path: {sys.argv[2]}. File list: {os.listdir(sys.argv[2])}\") Shell #!/bin/bash # The model can perform 14 different vision language model and computer vision tasks. The input ```content``` field should be formatted as ```\"<TASK_PROMPT><text_prompt (only when needed)><img>\"```. # Users need to specify the task type at the beginning. Image supports both base64 and NvCF asset id. Some tasks require a text prompt, and users need to provide that after image. Below are the examples for each task. # For <CAPTION_TO_PHRASE_GROUNDING>, <REFERRING_EXPRESSION_SEGMENTATION>, <OPEN_VOCABULARY_DETECTION>, users can change the text prompt as other descriptions. # For <REGION_TO_SEGMENTATION>, <REGION_TO_CATEGORY>, <REGION_TO_DESCRIPTION>, the text prompt is formatted as <loc_x1><loc_y1><loc_x2><loc_y2>, which is the normalized coordinates from region of interest bbox. x1=int(top_left_x_coor/width*999), y1=int(top_left_y_coor/height*999), x2=int(bottom_right_x_coor/width*999), y2=int(bottom_right_y_coor/height*999). set -e # Check arguments if [ \"$#\" -ne 3 ]; then printf \"Usage: ./test.sh <test_image> <result_dir> <task_id>\\nFor example: ./test.sh car.jpg result_dir 0\\n\" exit 1 fi if [[ -z \"${API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC}\" ]]; then echo \"API_KEY not set. Please export API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC=<Your API Key> as environment variable.\" exit 1 fi # Set variables nvai_url=\"https://ai.api.nvidia.com/v1/vlm/microsoft/florence-2\" api_key=$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC assets_url=\"https://api.nvcf.nvidia.com/v2/nvcf/assets\" prompts=( \"<CAPTION>\" \"<DETAILED_CAPTION>\" \"<MORE_DETAILED_CAPTION>\" \"<OD>\" \"<DENSE_REGION_CAPTION>\" \"<REGION_PROPOSAL>\" \"<CAPTION_TO_PHRASE_GROUNDING>A black and brown dog is laying on a grass field.\" \"<REFERRING_EXPRESSION_SEGMENTATION>a black and brown dog\" \"<REGION_TO_SEGMENTATION><loc_312><loc_168><loc_998><loc_846>\" \"<OPEN_VOCABULARY_DETECTION>a black and brown dog\" \"<REGION_TO_CATEGORY><loc_312><loc_168><loc_998><loc_846>\" \"<REGION_TO_DESCRIPTION><loc_312><loc_168><loc_998><loc_846>\" \"<OCR>\" \"<OCR_WITH_REGION>\" ) content_type=\"image/jpeg\" description=\"Test Image\" # Function to upload an asset upload_asset() { local input=$1 local description=$2 # Authorize upload authorize=$(curl -s -X POST $assets_url \\ -H \"Authorization: Bearer $api_key\" \\ -H \"Content-Type: application/json\" \\ -H \"accept: application/json\" \\ -d \"{\\\"contentType\\\": \\\"$content_type\\\", \\\"description\\\": \\\"$description\\\"}\") # Get upload URL and asset ID upload_url=$(echo $authorize | jq -r '.uploadUrl') asset_id=$(echo $authorize | jq -r '.assetId') # Upload asset curl -s -X PUT $upload_url \\ -H \"x-amz-meta-nvcf-asset-description: $description\" \\ -H \"content-type: $content_type\" \\ --upload-file $input echo $asset_id } # Function to generate content generate_content() { local task_id=$1 local asset_id=$2 prompt=${prompts[$task_id]} content=\"$prompt<img src=\\\\\\\"data:image/jpeg;asset_id,$asset_id\\\\\\\" />\" echo $content } # Upload images asset_id=$(upload_asset $1 $description) content=$(generate_content $3 $asset_id) echo '{ \"messages\":[{ \"role\": \"user\", \"content\": \"'\"$content\"'\" }] }' > payload.json mkdir -p $2 # Compare images via microservice location_command=\"curl -D - -s -X POST $nvai_url \\ -H \\\"Content-Type: application/json\\\" \\ -H \\\"NVCF-INPUT-ASSET-REFERENCES: $asset_id\\\" \\ -H \\\"NVCF-FUNCTION-ASSET-IDS: $asset_id\\\" \\ -H \\\"Authorization: Bearer $api_key\\\" \\ -d @payload.json \\ | grep location | awk '{print \\$2}'\" location=$(eval ${location_command} | tr -d '\\n' | tr -d '\\r' | tr -d ' ' | tr -d '\"' | tr -d ',') # The download command will download the file from the location header download_command=\"curl -s '${location}' > $2.zip\" echo $location_command # Download the .zip file response=$(eval ${download_command}) # Unzip the file unzip -q $2.zip -d $2 echo \"Response saved to $2.zip\" echo $(ls $2) Ref Link: [microsoft/florence-2] (https://build.nvidia.com/microsoft/microsoft-florence-2)","title":"Model Information"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-google","text":"","title":"Publisher: google"},{"location":"NVIDIA/NIM/image_to_text.html#4-model-googlepaligemma","text":"","title":"4. Model: google/paligemma"},{"location":"NVIDIA/NIM/image_to_text.html#model-information_3","text":"The Google PaLIGemma-3B-mix model is a one-shot visual language understanding solution for image-to-text generation. This model is ready for commercial use. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/google/paligemma\" stream = True with open(\"dog.jpeg\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Describe the image. <img src=\"data:image/jpeg;base64,{image_b64}\" />' } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 0.70, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell image_b64=$( base64 dog.jpeg ) stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Describe the image. <img src=\\\"data:image/jpeg;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 512, \"temperature\": 1.00, \"top_p\": 0.70, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/google/paligemma \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [google/paligemma] (https://build.nvidia.com/google/google-paligemma)","title":"Model Information"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-nvidia","text":"","title":"Publisher: nvidia"},{"location":"NVIDIA/NIM/image_to_text.html#4-model-nvidianeva-22b","text":"","title":"4. Model: nvidia/neva-22b"},{"location":"NVIDIA/NIM/image_to_text.html#model-information_4","text":"NeVA is NVIDIA's version of the LLaVA model where the open source LLaMA model is replaced with a GPT model trained by NVIDIA. At a high level the image is encoded using a frozen hugging face CLIP model and projected to the text embedding dimensions. This is then concatenated with the embeddings of the prompt and passed in through the language model. Training happens in two stages: Pretraining: Here the language model is frozen and only the projection layer (that maps the image encoding to the embedding space) is trained. Here, image-caption pairs are used to pretrain the model. Finetuning: Here the language model is also trained along with the projection layer. To finetune the model synthetic instruction data generated using GPT4 is used. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b\" stream = True with open(\"dog.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Describe what you see in this image. <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.70, \"seed\": 0, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell image_b64=$( base64 dog.png ) stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Describe what you see in this image. <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.70, \"seed\": 0, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [nvidia/neva-22b] (https://build.nvidia.com/google/google-paligemma)","title":"Model Information"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-microsoft_1","text":"","title":"Publisher: microsoft"},{"location":"NVIDIA/NIM/image_to_text.html#5-model-microsoftkosmos-2","text":"","title":"5. Model: microsoft/kosmos-2"},{"location":"NVIDIA/NIM/image_to_text.html#model-information_5","text":"Kosmos-2 model is a groundbreaking multimodal large language model (MLLM). Kosmos-2 is designed to ground text to the visual world, enabling it to understand and reason about visual elements in images. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2\" with open(\"soccer.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Who is in this photo? <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20 } response = requests.post(invoke_url, headers=headers, json=payload) print(response.json()) Shell image_b64=$( base64 soccer.png ) echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is in this photo? <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20 }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2 \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json\" \\ -d @payload.json Ref Link: [microsoft/kosmos-2] (https://build.nvidia.com/microsoft/microsoft-kosmos-2)","title":"Model Information"},{"location":"NVIDIA/NIM/image_to_text.html#publisher-google_1","text":"","title":"Publisher: google"},{"location":"NVIDIA/NIM/image_to_text.html#6-model-googledeplot","text":"","title":"6. Model: google/deplot"},{"location":"NVIDIA/NIM/image_to_text.html#model-information_6","text":"The Google DePlot model is a one-shot visual language understanding solution that translates images of plots or charts into linearized tables. Python import requests, base64 invoke_url = \"https://ai.api.nvidia.com/v1/vlm/google/deplot\" stream = True with open(\"economic-assistance-chart.png\", \"rb\") as f: image_b64 = base64.b64encode(f.read()).decode() assert len(image_b64) < 180_000, \\ \"To upload larger images, use the assets API (see docs)\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"text/event-stream\" if stream else \"application/json\" } payload = { \"messages\": [ { \"role\": \"user\", \"content\": f'Generate underlying data table of the figure below: <img src=\"data:image/png;base64,{image_b64}\" />' } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20, \"stream\": stream } response = requests.post(invoke_url, headers=headers, json=payload) if stream: for line in response.iter_lines(): if line: print(line.decode(\"utf-8\")) else: print(response.json()) Shell image_b64=$( base64 economic-assistance-chart.png ) stream=true if [ \"$stream\" = true ]; then accept_header='Accept: text/event-stream' else accept_header='Accept: application/json' fi echo '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Generate underlying data table of the figure below: <img src=\\\"data:image/png;base64,'\"$image_b64\"'\\\" />\" } ], \"max_tokens\": 1024, \"temperature\": 0.20, \"top_p\": 0.20, \"stream\": true }' > payload.json curl https://ai.api.nvidia.com/v1/vlm/google/deplot \\ -H \"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" \\ -H \"Content-Type: application/json\" \\ -H \"$accept_header\" \\ -d @payload.json Ref Link: [google/deplot] (https://build.nvidia.com/google/google-deplot)","title":"Model Information"},{"location":"NVIDIA/NIM/medical_imaging.html","text":"Publisher: nvidia # 1. Model: nvidia/maisi # Model Information # NVIDIA MAISI (Medical AI for Synthetic Imaging) is a state-of-the-art three-dimensional (3D) Latent Diffusion Model designed for generating high-quality synthetic CT images with or without anatomical annotations. This AI model excels in data augmentation and creating realistic medical imaging data to supplement limited datasets due to privacy concerns or rare conditions. It can also significantly enhance the performance of other medical imaging AI models by generating diverse and realistic training data. Python import io import os import time import requests import shutil import tempfile import zipfile invoke_url = \"https://health.api.nvidia.com/v1/medicalimaging/nvidia/maisi\" fetch_url=\"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"application/json\", \"content_type_header\": \"Content-Type: application/json\" } result = \"result-1\" payload = { \"num_output_samples\": 1, \"body_region\": [\"chest\"], \"anatomy_list\": [\"liver\"], \"controllable_anatomy_size\": [[\"hepatic tumor\", 0.3], [\"liver\", 0.5]], \"output_size\": [512, 512, 512], \"image_output_ext\": \".nii.gz\", \"label_output_ext\": \".nii.gz\", \"pre_signed_url\": \"\", \"spacing\": [1, 1, 1], } # re-use connections session = requests.Session() response = session.post(invoke_url, headers=headers, json=payload) response.raise_for_status() while response.status_code == 202: req_id = response.headers.get(\"NVCF-REQID\") req_url = os.path.join(fetch_url, req_id) response = session.get(req_url, headers=headers) print(f\"Please wait util response returned...\") time.sleep(1) if response.status_code != 200: print(f\"Error with code {response.status_code}.\") exit() with tempfile.TemporaryDirectory() as temp_dir: z = zipfile.ZipFile(io.BytesIO(response.content)) z.extractall(temp_dir) os.makedirs(result) shutil.move(temp_dir, f\"{result}\") print(\"Success!\") Shell invoke_url=\"https://health.api.nvidia.com/v1/medicalimaging/nvidia/maisi\" authorization_header=\"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" content_type_header=\"Content-Type: application/json\" accept_header=\"Accept: application/json\" FETCH_URL=\"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\" OUTPUT_DIR=\"result-1\" data='{ \"num_output_samples\": 1, \"body_region\": [\"chest\"], \"anatomy_list\": [\"liver\"], \"output_size\": [512, 512, 512], \"controllable_anatomy_size\": [[\"hepatic tumor\", 0.3], [\"liver\", 0.5]], \"image_output_ext\": \".nii.gz\", \"label_output_ext\": \".nii.gz\", \"pre_signed_url\": \"\", \"spacing\": [1, 1, 1] }' echo \"Invoking the MAISI microservice...\" response=$(curl --silent -i --request POST --url \"$invoke_url\" \\ --header \"$authorization_header\" \\ --header \"$content_type_header\" \\ --header \"$accept_header\" \\ --data \"$data\") status_code=$(echo \"$response\" | head -n 1 | awk '{print $2}') if [ $status_code = \"202\" ]; then echo \"Request accepted. Fetching the output...\" req_id=$(echo \"$response\" | grep -i '^nvcf-reqid:' | awk '{print $2}' | tr -d '\\r') while true; do response=$(curl -s -L -o output.zip -w \"%{http_code}\" -H \"$content_type_header\" -H \"$authorization_header\" -H \"$accept_header\" \"${FETCH_URL}${req_id}\") if [ \"$response\" -eq 200 ]; then echo \"Download of output zip file is complete.\" break else echo \"Please waiting until response returned...\" sleep 1 fi done unzip output.zip -d $OUTPUT_DIR && rm output.zip && rm $OUTPUT_DIR/*.response elif [ $status_code = \"302\" ]; then location=$(echo \"$response\" | grep -i '^location:' | awk '{print $2}' | tr -d '\\r') echo \"Redirecting url...\" curl -s -o output.zip -H \"$accept_header\" $location unzip output.zip -d $OUTPUT_DIR && rm output.zip && rm $OUTPUT_DIR/*.response else echo \"Error Response: $response\" fi Ref Link: [nvidia/maisi] (https://build.nvidia.com/nvidia/maisi)","title":"Medical Imaging"},{"location":"NVIDIA/NIM/medical_imaging.html#publisher-nvidia","text":"","title":"Publisher: nvidia"},{"location":"NVIDIA/NIM/medical_imaging.html#1-model-nvidiamaisi","text":"","title":"1. Model: nvidia/maisi"},{"location":"NVIDIA/NIM/medical_imaging.html#model-information","text":"NVIDIA MAISI (Medical AI for Synthetic Imaging) is a state-of-the-art three-dimensional (3D) Latent Diffusion Model designed for generating high-quality synthetic CT images with or without anatomical annotations. This AI model excels in data augmentation and creating realistic medical imaging data to supplement limited datasets due to privacy concerns or rare conditions. It can also significantly enhance the performance of other medical imaging AI models by generating diverse and realistic training data. Python import io import os import time import requests import shutil import tempfile import zipfile invoke_url = \"https://health.api.nvidia.com/v1/medicalimaging/nvidia/maisi\" fetch_url=\"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\" headers = { \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\", \"Accept\": \"application/json\", \"content_type_header\": \"Content-Type: application/json\" } result = \"result-1\" payload = { \"num_output_samples\": 1, \"body_region\": [\"chest\"], \"anatomy_list\": [\"liver\"], \"controllable_anatomy_size\": [[\"hepatic tumor\", 0.3], [\"liver\", 0.5]], \"output_size\": [512, 512, 512], \"image_output_ext\": \".nii.gz\", \"label_output_ext\": \".nii.gz\", \"pre_signed_url\": \"\", \"spacing\": [1, 1, 1], } # re-use connections session = requests.Session() response = session.post(invoke_url, headers=headers, json=payload) response.raise_for_status() while response.status_code == 202: req_id = response.headers.get(\"NVCF-REQID\") req_url = os.path.join(fetch_url, req_id) response = session.get(req_url, headers=headers) print(f\"Please wait util response returned...\") time.sleep(1) if response.status_code != 200: print(f\"Error with code {response.status_code}.\") exit() with tempfile.TemporaryDirectory() as temp_dir: z = zipfile.ZipFile(io.BytesIO(response.content)) z.extractall(temp_dir) os.makedirs(result) shutil.move(temp_dir, f\"{result}\") print(\"Success!\") Shell invoke_url=\"https://health.api.nvidia.com/v1/medicalimaging/nvidia/maisi\" authorization_header=\"Authorization: Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\" content_type_header=\"Content-Type: application/json\" accept_header=\"Accept: application/json\" FETCH_URL=\"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\" OUTPUT_DIR=\"result-1\" data='{ \"num_output_samples\": 1, \"body_region\": [\"chest\"], \"anatomy_list\": [\"liver\"], \"output_size\": [512, 512, 512], \"controllable_anatomy_size\": [[\"hepatic tumor\", 0.3], [\"liver\", 0.5]], \"image_output_ext\": \".nii.gz\", \"label_output_ext\": \".nii.gz\", \"pre_signed_url\": \"\", \"spacing\": [1, 1, 1] }' echo \"Invoking the MAISI microservice...\" response=$(curl --silent -i --request POST --url \"$invoke_url\" \\ --header \"$authorization_header\" \\ --header \"$content_type_header\" \\ --header \"$accept_header\" \\ --data \"$data\") status_code=$(echo \"$response\" | head -n 1 | awk '{print $2}') if [ $status_code = \"202\" ]; then echo \"Request accepted. Fetching the output...\" req_id=$(echo \"$response\" | grep -i '^nvcf-reqid:' | awk '{print $2}' | tr -d '\\r') while true; do response=$(curl -s -L -o output.zip -w \"%{http_code}\" -H \"$content_type_header\" -H \"$authorization_header\" -H \"$accept_header\" \"${FETCH_URL}${req_id}\") if [ \"$response\" -eq 200 ]; then echo \"Download of output zip file is complete.\" break else echo \"Please waiting until response returned...\" sleep 1 fi done unzip output.zip -d $OUTPUT_DIR && rm output.zip && rm $OUTPUT_DIR/*.response elif [ $status_code = \"302\" ]; then location=$(echo \"$response\" | grep -i '^location:' | awk '{print $2}' | tr -d '\\r') echo \"Redirecting url...\" curl -s -o output.zip -H \"$accept_header\" $location unzip output.zip -d $OUTPUT_DIR && rm output.zip && rm $OUTPUT_DIR/*.response else echo \"Error Response: $response\" fi Ref Link: [nvidia/maisi] (https://build.nvidia.com/nvidia/maisi)","title":"Model Information"},{"location":"NVIDIA/NIM/object_detect.html","text":"","title":"Object Detect"},{"location":"NVIDIA/NIM/optical_character_recog.html","text":"","title":"Optical Character Recognition"},{"location":"NVIDIA/NIM/rag.html","text":"","title":"Retrieval Augmented Generation"},{"location":"NVIDIA/NIM/sdg.html","text":"","title":"Synthetic Data Generation"},{"location":"NVIDIA/NIM/speech_to_animation.html","text":"","title":"Speech To Animation"},{"location":"NVIDIA/NIM/speech_to_text.html","text":"","title":"Speech To Text"},{"location":"NVIDIA/NIM/text_to_360.html","text":"","title":"Text To 360"},{"location":"NVIDIA/NIM/text_to_embedding.html","text":"","title":"Text To Embedding"},{"location":"NVIDIA/NIM/text_to_image.html","text":"","title":"Text To Image"},{"location":"NVIDIA/NIM/text_to_speech.html","text":"","title":"Text To Speech"},{"location":"NVIDIA/NIM/text_translation.html","text":"","title":"Text Translation"},{"location":"NVIDIA/NIM/weather_simulation.html","text":"","title":"Weather Simulation"}]}